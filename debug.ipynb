{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78000c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "whoops\n",
      "Starting process with 1 experiments\n",
      "['test_tboard.json']\n",
      "Running test_tboard\n",
      "Starting experiment from test_tboard\n",
      "{'experiment': 'GraphMerge', 'group': 'test1', 'move': False, 'model': 'Sage', 'log': True, 'run_params': {'n_epochs': 1000, 'n_trials': 2, 'batch_size': 128, 'val_epoch': 2, 'early_stopping': True, 'patience': 10, 'loss_func': 'L2', 'metrics': 'scatter', 'performance_plot': 'SAM_base', 'shuffle': True, 'save': True, 'seed': True}, 'learn_params': {'learning_rate': 0.003, 'schedule': 'warmup_exp', 'g_up': 1.2, 'g_down': 0.96, 'warmup': 3}, 'hyper_params': {'hidden_channels': 32, 'conv_layers': 3, 'conv_activation': 'relu', 'decode_layers': 1, 'layernorm': True}, 'data_params': {'case': 'test_all_smass', 'split': 0.8}, 'experiment_name': 'test_tboard'}\n",
      "RelU conv activation\n",
      "Made folder for saving model\n",
      "<dev.lr_schedule.warmup_exp object at 0x2af965eb2970>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_18059/1961285497.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Starting experiment from {experiment[:-5]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconstruct_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# print(f'Exited training after {epochexit} epochs')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/GraphMerge/dev/train_script_cpu.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(construct_dict)\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mtrainloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;31m#learning rate scheduler step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mval_epoch\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/GraphMerge/dev/lr_schedule.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarmup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/juptorch/lib/python3.9/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_epoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCH_DEPRECATION_WARNING\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUserWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/juptorch/lib/python3.9/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mget_lr\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_epoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_lrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m         return [group['lr'] * self.gamma\n\u001b[0m\u001b[1;32m    454\u001b[0m                 for group in self.optimizer.param_groups]\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/juptorch/lib/python3.9/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_epoch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_lrs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m         return [group['lr'] * self.gamma\n\u001b[0m\u001b[1;32m    454\u001b[0m                 for group in self.optimizer.param_groups]\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'dict'"
     ]
    }
   ],
   "source": [
    "import os, sys, json, shutil, argparse\n",
    "\n",
    "import os.path as osp\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "##folder to take experiments from\n",
    "parser.add_argument(\"-f\", \"--f\", type=str, required=True)\n",
    "parser.add_argument(\"-gpu\", \"--gpu\", type=str, required=False)\n",
    "\n",
    "##should be implemented in the slurm script --------------------------------\n",
    "\n",
    "# parser.add_argument(\"-gpu\", \"--gpu\", type=str, required=False)\n",
    "# parser.add_argument(\"-cpus\", \"--cpus\", type=str, required=False)\n",
    "\n",
    "\n",
    "## slurm  ------------------------------------------------------------------\n",
    "\n",
    "os.chdir(osp.expanduser(\"~/work/GraphMerge\"))\n",
    "\n",
    "\n",
    "exp0_folder = 'exp_test'\n",
    "\n",
    "##########################################################\n",
    "#      Loop over JSON files and train models             # \n",
    "##########################################################\n",
    "\n",
    "# Generate list over experiments to run\n",
    "from dev.utils import list_experiments\n",
    "from dev.train_script_cpu import train_model\n",
    "    # from dev.train_script_cpu_debug import train_model\n",
    "\n",
    "\n",
    "\n",
    "#### --------------------------------- ####\n",
    "# implement for slurm script and slurm .out\n",
    "# clean_done(exp0_folder)\n",
    "#### --------------------------------- ####\n",
    "\n",
    "exp_folder, exp_list = list_experiments(exp0_folder)\n",
    "print('whoops')\n",
    "\n",
    "print(f\"Starting process with {len(exp_list)} experiments\" )\n",
    "print(exp_list)\n",
    "# Loop over the experiments\n",
    "for i, experiment in enumerate(exp_list):\n",
    "    print('Running ' + experiment[:-5])\n",
    "    # Load construction dictionary from json file\n",
    "    with open(osp.join(exp_folder, experiment)) as file:\n",
    "        construct_dict = json.load(file)\n",
    "    construct_dict['experiment_name']=experiment[:-5]\n",
    "\n",
    "\n",
    "    print(f\"Starting experiment from {experiment[:-5]}\")\n",
    "\n",
    "    train_model(construct_dict)\n",
    "\n",
    "    # print(f'Exited training after {epochexit} epochs')\n",
    "    if construct_dict['move']:\n",
    "        shutil.move(osp.join(exp_folder, experiment), osp.join(exp0_folder+\"/done\", experiment))\n",
    "    print(f\"Experiment {experiment[:-5]} done \\t {experiment}: {i + 1} / {len(exp_list)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ea9d5cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, tqdm, json, shutil, glob, argparse\n",
    "\n",
    "import os.path as osp\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "##########################################################\n",
    "#      Loop over JSON files and train models             # \n",
    "##########################################################\n",
    "\n",
    "# Generate list over experiments to run\n",
    "from dev.utils import perms\n",
    "from dev.train_script_cpu import train_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "99b0e255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting process with 7290 experiments\n",
      "{'learning_rate': [0.0003, 0.001, 0.003, 0.01, 0.03, 0.1], 'g_up': [1.1, 1.3, 1.5], 'g_down': [0.9, 0.95, 0.98], 'warmup': [2, 5, 10], 'conv_layers': [2, 3, 5, 7, 10], 'decode_layers': [1, 2, 3], 'hidden_channels': [16, 32, 64]}\n",
      "Selected too many compare runs (7290), running 200 random runs\n"
     ]
    }
   ],
   "source": [
    "N_parallel=20\n",
    "N=200\n",
    "# Load construction dictionary from json file\n",
    "with open(\"exp_compare/diff0.json\") as file:\n",
    "    diff = json.load(file)\n",
    "    \n",
    "keys=list(diff.keys())\n",
    "exp_list=perms(diff)\n",
    "# Load construction dictionary from json file\n",
    "with open(\"exp_compare/base0.json\") as file:\n",
    "    base = json.load(file)\n",
    "\n",
    "print(f\"Starting process with {len(exp_list)} experiments\")\n",
    "print(diff)\n",
    "# Loop over the experiments\n",
    "\n",
    "if len(exp_list)>N:\n",
    "    print(f'Selected too many compare runs ({len(exp_list)}), running {N} random runs')\n",
    "    idxs = np.random.permutation(len(exp_list))\n",
    "    exp_list=exp_list[idxs[:N]]\n",
    "    \n",
    "cds=[]\n",
    "for i in range(len(exp_list)):\n",
    "    construct_dict=base\n",
    "#     print('Exploring', keys)\n",
    "#     print('Currently doing', exp_list[i])\n",
    "    # if i==0:\n",
    "    #     construct_dict['data_params']['restart']=True\n",
    "    # else:\n",
    "    #     construct_dict['data_params']['restart']=False\n",
    "    for j, key in enumerate(keys):\n",
    "        if key in construct_dict['learn_params']:\n",
    "            typ=type(construct_dict['learn_params'][key])\n",
    "        if key in construct_dict['run_params']:\n",
    "            typ=type(construct_dict['run_params'][key])\n",
    "            construct_dict['run_params'][key]=typ(exp_list[i][j])\n",
    "        elif key in construct_dict['hyper_params']:\n",
    "            typ=type(construct_dict['hyper_params'][key])\n",
    "            construct_dict['hyper_params'][key]=typ(exp_list[i][j])\n",
    "        elif key in construct_dict['data_params']:\n",
    "            typ=type(construct_dict['data_params'][key])\n",
    "            construct_dict['data_params'][key]=typ(exp_list[i][j])\n",
    "    #make_title\n",
    "    title=''\n",
    "    for key, val in zip(keys, exp_list[i]):\n",
    "        title+=key[:4]+str(val)\n",
    "    construct_dict['experiment_name']=construct_dict['model']+title\n",
    "    cds.append(construct_dict)\n",
    "di=''\n",
    "for key in keys:\n",
    "    di+=key[:4]+'_'\n",
    "dirs=[]\n",
    "\n",
    "exps=np.split(np.array(cds),N_parallel)\n",
    "for i in range(N_parallel):\n",
    "    fn=di+str(i)\n",
    "    group=construct_dict['group']\n",
    "    fn=f'exp_sweep/{fn}_{group}'\n",
    "    if not osp.exists(fn+'/todo'):\n",
    "        os.makedirs(fn+'/todo')\n",
    "        os.makedirs(fn+'/done')\n",
    "    for j in range(len(exps[i])):\n",
    "        with open(fn+'/todo/'+f'exp{j}.json', 'w') as write:\n",
    "            json.dump(exps[i][j], write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8f6d61e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0a384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in dirs:\n",
    "    fn=f'exp_sweep/{d}'\n",
    "    os.mkdir(fn)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ffc4eb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('exp_compare/test.json', 'w') as write:\n",
    "    json.dump(construct_dict, write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5fbb09a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(exp_list[:N])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
