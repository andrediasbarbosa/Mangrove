Starting process with 6 experiments
{'conv_layers': [1, 2, 3], 'batch_size': [256, 512]}
Exploring ['conv_layers', 'batch_size']
Currently doing [  1 256]
{'experiment': 'GraphMerge', 'group': 'meta_pysr', 'move': False, 'model': 'MetaEdge', 'log': True, 'run_params': {'n_epochs': 500, 'n_trials': 1, 'batch_size': 256, 'val_epoch': 2, 'early_stopping': True, 'patience': 20, 'l1_lambda': 3e-05, 'l2_lambda': 3e-05, 'loss_func': 'GaussNd', 'metrics': 'test_multi_varrho', 'performance_plot': 'multi_base', 'save': True, 'seed': True, 'num_workers': 4}, 'learn_params': {'learning_rate': 0.01, 'schedule': 'onecycle', 'g_up': 1, 'g_down': 0.95, 'warmup': 4, 'period': 5, 'eta_min': 1e-05}, 'hyper_params': {'hidden_channels': 128, 'conv_layers': 1, 'conv_activation': 'relu', 'decode_activation': 'leakyrelu', 'decode_layers': 2, 'layernorm': True, 'agg': 'sum', 'variance': True, 'rho': 0}, 'data_params': {'case': 'vlarge_all_4t_z0.0_quantile_raw', 'targets': [0, 1, 2, 3], 'del_feats': [], 'split': 0.8, 'test': 0, 'scale': 0}, 'experiment_name': 'con1bat256'}
/home/cj1223/../../scratch/gpfs/cj1223/GraphResults/results_meta_pysr_120322/MetaEdge_vlarge_all_4t_z0.0_quantile_raw_egfquh
RelU conv activation
LeakyRelU decode activation
N_params 298056
Made folder for saving model
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:34<4:43:12, 34.05s/it]  0%|          | 2/500 [01:21<5:50:26, 42.22s/it]Epoch: 2 done with learning rate 4.17E-04, Train loss: 1.27E+07, Train scatter: [0.9352 0.1644 0.5441 0.9954]
L1 regularization loss: 4.56E-01, L2 regularization loss: 9.93E-02
Test scatter: [0.9196 0.1642 0.5355 0.9851], Lowest was [0.9196 0.1642 0.5355 0.9851]
Median for last 10 epochs: [0.9196 0.1642 0.5355 0.9851], Epochs since improvement 2
  1%|          | 3/500 [01:54<5:13:22, 37.83s/it]  1%|          | 4/500 [02:43<5:49:47, 42.31s/it]Epoch: 4 done with learning rate 4.67E-04, Train loss: 6.59E+06, Train scatter: [0.9352 0.1468 0.544  0.9954]
L1 regularization loss: 4.62E-01, L2 regularization loss: 1.06E-01
Test scatter: [0.9197 0.1431 0.5354 0.985 ], Lowest was [0.9196 0.1431 0.5354 0.985 ]
Median for last 10 epochs: [0.9196 0.1431 0.5354 0.985 ], Epochs since improvement 0
  1%|          | 5/500 [03:15<5:18:26, 38.60s/it]  1%|          | 6/500 [04:05<5:48:29, 42.33s/it]Epoch: 6 done with learning rate 5.51E-04, Train loss: 5.14E+06, Train scatter: [0.934  0.1201 0.5416 0.6722]
L1 regularization loss: 4.73E-01, L2 regularization loss: 1.17E-01
Test scatter: [0.9182 0.1193 0.5328 0.6614], Lowest was [0.9182 0.1193 0.5328 0.6614]
Median for last 10 epochs: [0.9182 0.1193 0.5328 0.6614], Epochs since improvement 0
  1%|▏         | 7/500 [04:38<5:22:22, 39.23s/it]  2%|▏         | 8/500 [05:28<5:50:01, 42.69s/it]Epoch: 8 done with learning rate 6.67E-04, Train loss: 4.26E+06, Train scatter: [0.9147 0.1036 0.5331 0.6162]
L1 regularization loss: 4.79E-01, L2 regularization loss: 1.21E-01
Test scatter: [0.8991 0.1029 0.5241 0.6052], Lowest was [0.8991 0.1029 0.5241 0.6052]
Median for last 10 epochs: [0.9087 0.1111 0.5285 0.6333], Epochs since improvement 0
  2%|▏         | 9/500 [06:00<5:23:15, 39.50s/it]Epoch: 10 done with learning rate 8.15E-04, Train loss: 3.32E+06, Train scatter: [0.7896 0.0963 0.4428 0.6085]
L1 regularization loss: 4.86E-01, L2 regularization loss: 1.24E-01
Test scatter: [0.7795 0.0976 0.435  0.5979], Lowest was [0.7795 0.0976 0.435  0.5979]
Median for last 10 epochs: [0.8991 0.1029 0.5241 0.6052], Epochs since improvement 0
  2%|▏         | 10/500 [06:55<5:59:48, 44.06s/it]  2%|▏         | 11/500 [07:26<5:27:47, 40.22s/it]  2%|▏         | 12/500 [08:16<5:50:08, 43.05s/it]Epoch: 12 done with learning rate 9.94E-04, Train loss: 2.60E+06, Train scatter: [0.5886 0.0942 0.4571 0.6168]
L1 regularization loss: 4.91E-01, L2 regularization loss: 1.28E-01
Test scatter: [0.5928 0.0955 0.458  0.6144], Lowest was [0.5928 0.0955 0.435  0.5979]
Median for last 10 epochs: [0.8991 0.1029 0.5241 0.6144], Epochs since improvement 0
  3%|▎         | 13/500 [08:47<5:21:28, 39.61s/it]  3%|▎         | 14/500 [09:36<5:42:20, 42.26s/it]Epoch: 14 done with learning rate 1.20E-03, Train loss: 1.73E+06, Train scatter: [0.577  0.0896 0.3731 0.6032]
L1 regularization loss: 4.96E-01, L2 regularization loss: 1.31E-01
Test scatter: [0.577  0.0911 0.3742 0.6111], Lowest was [0.577  0.0911 0.3742 0.5979]
Median for last 10 epochs: [0.7795 0.0976 0.458  0.6111], Epochs since improvement 0
  3%|▎         | 15/500 [10:08<5:16:24, 39.14s/it]  3%|▎         | 16/500 [10:56<5:39:13, 42.05s/it]Epoch: 16 done with learning rate 1.44E-03, Train loss: 1.20E+06, Train scatter: [0.5202 0.0899 0.341  0.5894]
L1 regularization loss: 5.01E-01, L2 regularization loss: 1.33E-01
Test scatter: [0.5181 0.0907 0.3467 0.5937], Lowest was [0.5181 0.0907 0.3467 0.5937]
Median for last 10 epochs: [0.5928 0.0955 0.435  0.6052], Epochs since improvement 0
  3%|▎         | 17/500 [11:28<5:12:24, 38.81s/it]  4%|▎         | 18/500 [12:17<5:36:13, 41.85s/it]Epoch: 18 done with learning rate 1.70E-03, Train loss: 1.05E+06, Train scatter: [0.5064 0.0873 0.3305 0.5644]
L1 regularization loss: 5.09E-01, L2 regularization loss: 1.37E-01
Test scatter: [0.5001 0.0905 0.346  0.5711], Lowest was [0.5001 0.0905 0.346  0.5711]
Median for last 10 epochs: [0.577  0.0911 0.3742 0.5979], Epochs since improvement 0
  4%|▍         | 19/500 [12:49<5:12:04, 38.93s/it]Epoch: 20 done with learning rate 1.99E-03, Train loss: 8.13E+05, Train scatter: [0.521  0.0819 0.3029 0.5448]
L1 regularization loss: 5.16E-01, L2 regularization loss: 1.40E-01
Test scatter: [0.504  0.083  0.3096 0.5463], Lowest was [0.5001 0.083  0.3096 0.5463]
Median for last 10 epochs: [0.5181 0.0907 0.3467 0.5937], Epochs since improvement 0
  4%|▍         | 20/500 [13:44<5:49:41, 43.71s/it]  4%|▍         | 21/500 [14:16<5:21:44, 40.30s/it]  4%|▍         | 22/500 [15:05<5:41:38, 42.88s/it]Epoch: 22 done with learning rate 2.30E-03, Train loss: 7.04E+05, Train scatter: [0.4886 0.0809 0.303  0.5363]
L1 regularization loss: 5.24E-01, L2 regularization loss: 1.44E-01
Test scatter: [0.4732 0.0832 0.3157 0.5391], Lowest was [0.4732 0.083  0.3096 0.5391]
Median for last 10 epochs: [0.504  0.0905 0.346  0.5711], Epochs since improvement 0
  5%|▍         | 23/500 [15:38<5:17:11, 39.90s/it]  5%|▍         | 24/500 [16:28<5:40:44, 42.95s/it]Epoch: 24 done with learning rate 2.63E-03, Train loss: 6.38E+05, Train scatter: [0.4593 0.0789 0.2912 0.5285]
L1 regularization loss: 5.33E-01, L2 regularization loss: 1.48E-01
Test scatter: [0.4562 0.0802 0.2966 0.5268], Lowest was [0.4562 0.0802 0.2966 0.5268]
Median for last 10 epochs: [0.5001 0.0832 0.3157 0.5463], Epochs since improvement 0
  5%|▌         | 25/500 [17:00<5:13:44, 39.63s/it]  5%|▌         | 26/500 [17:49<5:35:44, 42.50s/it]Epoch: 26 done with learning rate 2.98E-03, Train loss: 6.08E+05, Train scatter: [0.5729 0.0787 0.2862 0.5289]
L1 regularization loss: 5.43E-01, L2 regularization loss: 1.53E-01
Test scatter: [0.5748 0.0804 0.2991 0.5326], Lowest was [0.4562 0.0802 0.2966 0.5268]
Median for last 10 epochs: [0.5001 0.083  0.3096 0.5391], Epochs since improvement 2
  5%|▌         | 27/500 [18:20<5:09:01, 39.20s/it]  6%|▌         | 28/500 [19:11<5:34:52, 42.57s/it]Epoch: 28 done with learning rate 3.34E-03, Train loss: 5.26E+05, Train scatter: [0.4571 0.0779 0.282  0.5118]
L1 regularization loss: 5.54E-01, L2 regularization loss: 1.58E-01
Test scatter: [0.4702 0.0802 0.2924 0.5133], Lowest was [0.4562 0.0802 0.2924 0.5133]
Median for last 10 epochs: [0.4732 0.0804 0.2991 0.5326], Epochs since improvement 0
  6%|▌         | 29/500 [19:43<5:08:47, 39.34s/it]Epoch: 30 done with learning rate 3.72E-03, Train loss: 5.08E+05, Train scatter: [0.4857 0.0801 0.2915 0.5378]
L1 regularization loss: 5.67E-01, L2 regularization loss: 1.65E-01
Test scatter: [0.4831 0.0805 0.3    0.5328], Lowest was [0.4562 0.0802 0.2924 0.5133]
Median for last 10 epochs: [0.4732 0.0804 0.2991 0.5326], Epochs since improvement 2
  6%|▌         | 30/500 [20:36<5:41:37, 43.61s/it]  6%|▌         | 31/500 [21:08<5:12:27, 39.97s/it]  6%|▋         | 32/500 [21:57<5:34:16, 42.85s/it]Epoch: 32 done with learning rate 4.10E-03, Train loss: 1.27E+06, Train scatter: [0.9429 0.1592 0.544  0.9775]
L1 regularization loss: 5.82E-01, L2 regularization loss: 1.73E-01
Test scatter: [0.9281 0.1526 0.5353 0.9678], Lowest was [0.4562 0.0802 0.2924 0.5133]
Median for last 10 epochs: [0.4831 0.0804 0.2991 0.5326], Epochs since improvement 4
  7%|▋         | 33/500 [22:30<5:09:48, 39.80s/it]  7%|▋         | 34/500 [23:18<5:28:59, 42.36s/it]Epoch: 34 done with learning rate 4.50E-03, Train loss: 2.74E+06, Train scatter: [0.6151 0.1366 0.5269 0.7755]
L1 regularization loss: 7.13E-01, L2 regularization loss: 2.64E-01
Test scatter: [0.6178 0.1332 0.5203 0.7742], Lowest was [0.4562 0.0802 0.2924 0.5133]
Median for last 10 epochs: [0.5748 0.0805 0.3    0.5328], Epochs since improvement 6
  7%|▋         | 35/500 [23:51<5:06:00, 39.49s/it]  7%|▋         | 36/500 [24:41<5:28:48, 42.52s/it]Epoch: 36 done with learning rate 4.90E-03, Train loss: 6.24E+05, Train scatter: [0.5034 0.1058 0.485  0.6562]
L1 regularization loss: 7.27E-01, L2 regularization loss: 2.82E-01
Test scatter: [0.4981 0.1043 0.4818 0.6491], Lowest was [0.4562 0.0802 0.2924 0.5133]
Median for last 10 epochs: [0.4981 0.1043 0.4818 0.6491], Epochs since improvement 8
  7%|▋         | 37/500 [25:13<5:03:37, 39.35s/it]  8%|▊         | 38/500 [26:02<5:26:09, 42.36s/it]Epoch: 38 done with learning rate 5.30E-03, Train loss: 1.08E+03, Train scatter: [0.4243 0.0902 0.4214 0.6028]
L1 regularization loss: 7.38E-01, L2 regularization loss: 2.91E-01
Test scatter: [0.428  0.09   0.4219 0.5964], Lowest was [0.428  0.0802 0.2924 0.5133]
Median for last 10 epochs: [0.4981 0.1043 0.4818 0.6491], Epochs since improvement 0
  8%|▊         | 39/500 [26:34<5:01:14, 39.21s/it]Epoch: 40 done with learning rate 5.70E-03, Train loss: -1.06E+05, Train scatter: [0.4626 0.0828 0.3981 0.5695]
L1 regularization loss: 7.43E-01, L2 regularization loss: 2.98E-01
Test scatter: [0.4571 0.0829 0.3999 0.5661], Lowest was [0.428  0.0802 0.2924 0.5133]
Median for last 10 epochs: [0.4981 0.1043 0.4818 0.6491], Epochs since improvement 2
  8%|▊         | 40/500 [27:29<5:36:45, 43.93s/it]  8%|▊         | 41/500 [28:02<5:10:50, 40.63s/it]  8%|▊         | 42/500 [28:51<5:30:21, 43.28s/it]Epoch: 42 done with learning rate 6.10E-03, Train loss: -1.50E+05, Train scatter: [0.4339 0.0784 0.3838 0.554 ]
L1 regularization loss: 7.49E-01, L2 regularization loss: 3.04E-01
Test scatter: [0.43   0.0785 0.39   0.5502], Lowest was [0.428  0.0785 0.2924 0.5133]
Median for last 10 epochs: [0.4571 0.09   0.4219 0.5964], Epochs since improvement 0
  9%|▊         | 43/500 [29:23<5:03:02, 39.79s/it]  9%|▉         | 44/500 [30:13<5:25:42, 42.86s/it]Epoch: 44 done with learning rate 6.49E-03, Train loss: -1.72E+05, Train scatter: [0.3504 0.0756 0.3656 0.5475]
L1 regularization loss: 7.54E-01, L2 regularization loss: 3.09E-01
Test scatter: [0.3975 0.0756 0.3694 0.5455], Lowest was [0.3975 0.0756 0.2924 0.5133]
Median for last 10 epochs: [0.43   0.0829 0.3999 0.5661], Epochs since improvement 0
  9%|▉         | 45/500 [30:45<5:01:37, 39.78s/it]  9%|▉         | 46/500 [31:35<5:22:34, 42.63s/it]Epoch: 46 done with learning rate 6.87E-03, Train loss: -1.86E+05, Train scatter: [0.38   0.0749 0.3584 0.5459]
L1 regularization loss: 7.63E-01, L2 regularization loss: 3.16E-01
Test scatter: [0.3786 0.0746 0.36   0.5401], Lowest was [0.3786 0.0746 0.2924 0.5133]
Median for last 10 epochs: [0.428  0.0785 0.39   0.5502], Epochs since improvement 0
  9%|▉         | 47/500 [32:08<4:59:58, 39.73s/it] 10%|▉         | 48/500 [32:56<5:18:36, 42.29s/it]Epoch: 48 done with learning rate 7.24E-03, Train loss: -2.05E+05, Train scatter: [0.3686 0.0724 0.3617 0.5332]
L1 regularization loss: 7.73E-01, L2 regularization loss: 3.27E-01
Test scatter: [0.3672 0.0724 0.362  0.5297], Lowest was [0.3672 0.0724 0.2924 0.5133]
Median for last 10 epochs: [0.3975 0.0756 0.3694 0.5455], Epochs since improvement 0
 10%|▉         | 49/500 [33:28<4:55:28, 39.31s/it]Epoch: 50 done with learning rate 7.60E-03, Train loss: -2.14E+05, Train scatter: [0.4125 0.0709 0.3617 0.5346]
L1 regularization loss: 7.86E-01, L2 regularization loss: 3.47E-01
Test scatter: [0.4058 0.0707 0.3636 0.5293], Lowest was [0.3672 0.0707 0.2924 0.5133]
Median for last 10 epochs: [0.3975 0.0746 0.3636 0.5401], Epochs since improvement 0
 10%|█         | 50/500 [34:22<5:27:22, 43.65s/it] 10%|█         | 51/500 [34:55<5:02:13, 40.39s/it] 10%|█         | 52/500 [35:43<5:19:55, 42.85s/it]Epoch: 52 done with learning rate 7.94E-03, Train loss: 1.99E+05, Train scatter: [0.495  0.1011 0.4847 0.6325]
L1 regularization loss: 8.19E-01, L2 regularization loss: 3.84E-01
Test scatter: [0.498  0.1021 0.4826 0.637 ], Lowest was [0.3672 0.0707 0.2924 0.5133]
Median for last 10 epochs: [0.3975 0.0746 0.3636 0.5401], Epochs since improvement 2
 11%|█         | 53/500 [36:17<4:57:18, 39.91s/it] 11%|█         | 54/500 [37:07<5:19:56, 43.04s/it]Epoch: 54 done with learning rate 8.26E-03, Train loss: -1.50E+05, Train scatter: [0.498  0.0819 0.5036 0.5588]
L1 regularization loss: 8.33E-01, L2 regularization loss: 4.00E-01
Test scatter: [0.4834 0.0841 0.4993 0.5569], Lowest was [0.3672 0.0707 0.2924 0.5133]
Median for last 10 epochs: [0.4058 0.0746 0.3636 0.5401], Epochs since improvement 4
 11%|█         | 55/500 [37:39<4:55:53, 39.90s/it] 11%|█         | 56/500 [38:29<5:15:45, 42.67s/it]Epoch: 56 done with learning rate 8.56E-03, Train loss: -1.98E+05, Train scatter: [0.3276 0.0744 0.3805 0.5609]
L1 regularization loss: 8.36E-01, L2 regularization loss: 4.10E-01
Test scatter: [0.4262 0.0753 0.3802 0.5551], Lowest was [0.3672 0.0707 0.2924 0.5133]
Median for last 10 epochs: [0.4262 0.0753 0.3802 0.5551], Epochs since improvement 6
 11%|█▏        | 57/500 [39:00<4:49:54, 39.26s/it] 12%|█▏        | 58/500 [39:50<5:13:12, 42.52s/it]Epoch: 58 done with learning rate 8.83E-03, Train loss: -2.31E+05, Train scatter: [0.3614 0.0696 0.3352 0.519 ]
L1 regularization loss: 8.45E-01, L2 regularization loss: 4.21E-01
Test scatter: [0.3603 0.071  0.3418 0.5186], Lowest was [0.3603 0.0707 0.2924 0.5133]
Median for last 10 epochs: [0.4262 0.0753 0.3802 0.5551], Epochs since improvement 0
 12%|█▏        | 59/500 [40:21<4:48:00, 39.19s/it]Epoch: 60 done with learning rate 9.08E-03, Train loss: 1.25E+06, Train scatter: [0.9352 0.1728 0.5441 0.9954]
L1 regularization loss: 1.71E+00, L2 regularization loss: 7.42E-01
Test scatter: [0.9196 0.169  0.5355 0.985 ], Lowest was [0.3603 0.0707 0.2924 0.5133]
Median for last 10 epochs: [0.4834 0.0841 0.4826 0.5569], Epochs since improvement 2
 12%|█▏        | 60/500 [41:16<5:20:22, 43.69s/it] 12%|█▏        | 61/500 [41:48<4:54:06, 40.20s/it] 12%|█▏        | 62/500 [42:37<5:13:18, 42.92s/it]Epoch: 62 done with learning rate 9.31E-03, Train loss: 9.80E+05, Train scatter: [0.9353 0.1728 0.5441 0.9954]
L1 regularization loss: 1.71E+00, L2 regularization loss: 7.42E-01
Test scatter: [0.9197 0.169  0.5355 0.985 ], Lowest was [0.3603 0.0707 0.2924 0.5133]
Median for last 10 epochs: [0.4834 0.0841 0.4993 0.5569], Epochs since improvement 4
 13%|█▎        | 63/500 [43:08<4:47:25, 39.46s/it] 13%|█▎        | 64/500 [43:58<5:08:10, 42.41s/it]Epoch: 64 done with learning rate 9.50E-03, Train loss: 8.89E+05, Train scatter: [0.9354 0.1728 0.5441 0.9954]
L1 regularization loss: 1.71E+00, L2 regularization loss: 7.42E-01
Test scatter: [0.9197 0.169  0.5355 0.985 ], Lowest was [0.3603 0.0707 0.2924 0.5133]
Median for last 10 epochs: [0.9196 0.169  0.5355 0.985 ], Epochs since improvement 6
 13%|█▎        | 65/500 [44:29<4:43:36, 39.12s/it] 13%|█▎        | 66/500 [45:18<5:04:27, 42.09s/it]Epoch: 66 done with learning rate 9.66E-03, Train loss: 8.33E+05, Train scatter: [0.9354 0.1728 0.5441 0.9954]
L1 regularization loss: 1.70E+00, L2 regularization loss: 7.41E-01
Test scatter: [0.9197 0.169  0.5355 0.985 ], Lowest was [0.3603 0.0707 0.2924 0.5133]
Median for last 10 epochs: [0.9197 0.169  0.5355 0.985 ], Epochs since improvement 8
 13%|█▎        | 67/500 [45:50<4:41:02, 38.94s/it] 14%|█▎        | 68/500 [46:40<5:05:45, 42.47s/it]Epoch: 68 done with learning rate 9.80E-03, Train loss: 7.89E+05, Train scatter: [0.9353 0.1728 0.5441 0.9954]
L1 regularization loss: 1.70E+00, L2 regularization loss: 7.39E-01
Test scatter: [0.9196 0.169  0.5355 0.985 ], Lowest was [0.3603 0.0707 0.2924 0.5133]
Median for last 10 epochs: [0.9197 0.169  0.5355 0.985 ], Epochs since improvement 10
 14%|█▍        | 69/500 [47:12<4:40:57, 39.11s/it]Epoch: 70 done with learning rate 9.90E-03, Train loss: 7.50E+05, Train scatter: [0.9353 0.1728 0.5441 0.9954]
L1 regularization loss: 1.70E+00, L2 regularization loss: 7.42E-01
Test scatter: [0.9197 0.169  0.5355 0.985 ], Lowest was [0.3603 0.0707 0.2924 0.5133]
Median for last 10 epochs: [0.9197 0.169  0.5355 0.985 ], Epochs since improvement 12
 14%|█▍        | 70/500 [48:08<5:17:13, 44.26s/it] 14%|█▍        | 71/500 [48:40<4:51:06, 40.71s/it] 14%|█▍        | 72/500 [49:29<5:07:09, 43.06s/it]Epoch: 72 done with learning rate 9.96E-03, Train loss: 7.21E+05, Train scatter: [0.9353 0.1728 0.5441 0.9954]
L1 regularization loss: 1.69E+00, L2 regularization loss: 7.43E-01
Test scatter: [0.9196 0.169  0.5355 0.985 ], Lowest was [0.3603 0.0707 0.2924 0.5133]
Median for last 10 epochs: [0.9197 0.169  0.5355 0.985 ], Epochs since improvement 14
 15%|█▍        | 73/500 [50:01<4:43:38, 39.86s/it] 15%|█▍        | 74/500 [50:50<5:01:59, 42.53s/it]Epoch: 74 done with learning rate 1.00E-02, Train loss: 6.83E+05, Train scatter: [0.9352 0.1728 0.5441 0.9954]
L1 regularization loss: 1.69E+00, L2 regularization loss: 7.49E-01
Test scatter: [0.9195 0.169  0.5355 0.985 ], Lowest was [0.3603 0.0707 0.2924 0.5133]
Median for last 10 epochs: [0.9196 0.169  0.5355 0.985 ], Epochs since improvement 16
 15%|█▌        | 75/500 [51:23<4:41:01, 39.67s/it] 15%|█▌        | 76/500 [52:12<5:00:26, 42.52s/it]Epoch: 76 done with learning rate 1.00E-02, Train loss: 6.35E+05, Train scatter: [0.9351 0.1728 0.5441 0.9954]
L1 regularization loss: 1.68E+00, L2 regularization loss: 7.55E-01
Test scatter: [0.9195 0.169  0.5355 0.985 ], Lowest was [0.3603 0.0707 0.2924 0.5133]
Median for last 10 epochs: [0.9196 0.169  0.5355 0.985 ], Epochs since improvement 18
 15%|█▌        | 77/500 [52:44<4:37:02, 39.30s/it] 16%|█▌        | 78/500 [53:34<4:59:56, 42.65s/it]Epoch: 78 done with learning rate 1.00E-02, Train loss: 6.02E+05, Train scatter: [0.9351 0.1728 0.5441 0.9954]
L1 regularization loss: 1.67E+00, L2 regularization loss: 7.62E-01
Test scatter: [0.9194 0.169  0.5355 0.985 ], Lowest was [0.3603 0.0707 0.2924 0.5133]
Median for last 10 epochs: [0.9195 0.169  0.5355 0.985 ], Epochs since improvement 20
 16%|█▌        | 79/500 [54:07<4:38:18, 39.66s/it]Epoch: 80 done with learning rate 1.00E-02, Train loss: 5.54E+05, Train scatter: [0.935  0.1728 0.5441 0.9954]
L1 regularization loss: 1.66E+00, L2 regularization loss: 7.66E-01
Test scatter: [0.9194 0.169  0.5355 0.985 ], Lowest was [0.3603 0.0707 0.2924 0.5133]
Median for last 10 epochs: [0.9195 0.169  0.5355 0.985 ], Epochs since improvement 22
 16%|█▌        | 79/500 [55:02<4:53:20, 41.81s/it]
Exited after 80 epochs due to early stopping
3303.35 seconds spent training, 6.607 seconds per epoch. Processed 10540 trees per second
[0.9193769  0.16896592 0.53545994 0.9849656 ]
{'epoch_exit': 79, 'scatter_m_star': 0.9193769, 'lowest_m_star': 0.36025107, 'last20_m_star': 0.9196322, 'last10_m_star': 0.9194777, 'scatter_v_disk': 0.16896592, 'lowest_v_disk': 0.07070691, 'last20_v_disk': 0.16899583, 'last10_v_disk': 0.16897972, 'scatter_m_cold': 0.53545994, 'lowest_m_cold': 0.292392, 'last20_m_cold': 0.53548944, 'last10_m_cold': 0.5354843, 'scatter_sfr_100': 0.9849656, 'lowest_sfr_100': 0.5132572, 'last20_sfr_100': 0.985028, 'last10_sfr_100': 0.9850134}
Finished 1/1
Exited training after None epochs
Experiment 1 done: 1 / 6
Exploring ['conv_layers', 'batch_size']
Currently doing [  1 512]
{'experiment': 'GraphMerge', 'group': 'meta_pysr', 'move': False, 'model': 'MetaEdge', 'log': True, 'run_params': {'n_epochs': 500, 'n_trials': 1, 'batch_size': 512, 'val_epoch': 2, 'early_stopping': True, 'patience': 20, 'l1_lambda': 3e-05, 'l2_lambda': 3e-05, 'loss_func': 'GaussNd', 'metrics': 'test_multi_varrho', 'performance_plot': 'multi_base', 'save': True, 'seed': True, 'num_workers': 4}, 'learn_params': {'learning_rate': 0.01, 'schedule': 'onecycle', 'g_up': 1, 'g_down': 0.95, 'warmup': 4, 'period': 5, 'eta_min': 1e-05}, 'hyper_params': {'hidden_channels': 128, 'conv_layers': 1, 'conv_activation': 'relu', 'decode_activation': 'leakyrelu', 'decode_layers': 2, 'layernorm': True, 'agg': 'sum', 'variance': True, 'rho': 0, 'in_channels': 43, 'out_channels': 4}, 'data_params': {'case': 'vlarge_all_4t_z0.0_quantile_raw', 'targets': [0, 1, 2, 3], 'del_feats': [], 'split': 0.8, 'test': 0, 'scale': 0}, 'experiment_name': 'con1bat512'}
/home/cj1223/../../scratch/gpfs/cj1223/GraphResults/results_meta_pysr_120322/MetaEdge_vlarge_all_4t_z0.0_quantile_raw_izsjov
RelU conv activation
LeakyRelU decode activation
N_params 298056
Made folder for saving model
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:28<3:54:09, 28.16s/it]  0%|          | 2/500 [01:12<5:13:41, 37.80s/it]Epoch: 2 done with learning rate 4.17E-04, Train loss: 2.36E+07, Train scatter: [0.9353 0.1638 0.5442 0.9954]
L1 regularization loss: 4.54E-01, L2 regularization loss: 9.77E-02
Test scatter: [0.9197 0.166  0.5356 0.9851], Lowest was [0.9197 0.166  0.5356 0.9851]
Median for last 10 epochs: [0.9197 0.166  0.5356 0.9851], Epochs since improvement 2
  1%|          | 3/500 [01:40<4:34:54, 33.19s/it]  1%|          | 4/500 [02:25<5:13:59, 37.98s/it]Epoch: 4 done with learning rate 4.67E-04, Train loss: 1.20E+07, Train scatter: [0.9353 0.177  0.5441 0.9954]
L1 regularization loss: 4.58E-01, L2 regularization loss: 9.96E-02
Test scatter: [0.9197 0.1785 0.5355 0.9851], Lowest was [0.9197 0.166  0.5355 0.9851]
Median for last 10 epochs: [0.9197 0.1722 0.5355 0.9851], Epochs since improvement 0
  1%|          | 5/500 [02:53<4:42:24, 34.23s/it]  1%|          | 6/500 [03:38<5:12:30, 37.96s/it]Epoch: 6 done with learning rate 5.51E-04, Train loss: 9.65E+06, Train scatter: [0.9352 0.1629 0.5441 0.9954]
L1 regularization loss: 4.61E-01, L2 regularization loss: 1.03E-01
Test scatter: [0.9196 0.1578 0.5356 0.9851], Lowest was [0.9196 0.1578 0.5355 0.9851]
Median for last 10 epochs: [0.9196 0.1578 0.5355 0.9851], Epochs since improvement 0
  1%|▏         | 7/500 [04:05<4:42:54, 34.43s/it]  2%|▏         | 8/500 [04:51<5:11:08, 37.94s/it]Epoch: 8 done with learning rate 6.67E-04, Train loss: 8.41E+06, Train scatter: [0.9352 0.1473 0.5441 0.995 ]
L1 regularization loss: 4.67E-01, L2 regularization loss: 1.10E-01
Test scatter: [0.9196 0.1419 0.5355 0.9846], Lowest was [0.9196 0.1419 0.5355 0.9846]
Median for last 10 epochs: [0.9196 0.1498 0.5355 0.9848], Epochs since improvement 0
  2%|▏         | 9/500 [05:18<4:43:08, 34.60s/it]Epoch: 10 done with learning rate 8.15E-04, Train loss: 7.76E+06, Train scatter: [0.935  0.1306 0.5441 0.6881]
L1 regularization loss: 4.76E-01, L2 regularization loss: 1.19E-01
Test scatter: [0.9194 0.1266 0.5355 0.6873], Lowest was [0.9194 0.1266 0.5355 0.6873]
Median for last 10 epochs: [0.9196 0.1419 0.5355 0.9846], Epochs since improvement 0
  2%|▏         | 10/500 [06:09<5:24:19, 39.71s/it]  2%|▏         | 11/500 [06:37<4:53:15, 35.98s/it]  2%|▏         | 12/500 [07:23<5:18:50, 39.20s/it]Epoch: 12 done with learning rate 9.94E-04, Train loss: 6.80E+06, Train scatter: [0.9308 0.1154 0.5439 0.6201]
L1 regularization loss: 4.80E-01, L2 regularization loss: 1.21E-01
Test scatter: [0.9148 0.1117 0.5352 0.6124], Lowest was [0.9148 0.1117 0.5352 0.6124]
Median for last 10 epochs: [0.9196 0.1419 0.5355 0.9846], Epochs since improvement 0
  3%|▎         | 13/500 [07:50<4:48:18, 35.52s/it]  3%|▎         | 14/500 [08:36<5:13:34, 38.71s/it]Epoch: 14 done with learning rate 1.20E-03, Train loss: 6.35E+06, Train scatter: [0.8834 0.1086 0.5422 0.5907]
L1 regularization loss: 4.84E-01, L2 regularization loss: 1.24E-01
Test scatter: [0.8691 0.1062 0.5335 0.5893], Lowest was [0.8691 0.1062 0.5335 0.5893]
Median for last 10 epochs: [0.9194 0.1266 0.5355 0.6873], Epochs since improvement 0
  3%|▎         | 15/500 [09:04<4:46:23, 35.43s/it]  3%|▎         | 16/500 [09:50<5:11:16, 38.59s/it]Epoch: 16 done with learning rate 1.44E-03, Train loss: 5.97E+06, Train scatter: [0.6962 0.1037 0.5393 0.5777]
L1 regularization loss: 4.89E-01, L2 regularization loss: 1.27E-01
Test scatter: [0.687  0.1027 0.5307 0.5717], Lowest was [0.687  0.1027 0.5307 0.5717]
Median for last 10 epochs: [0.9148 0.1117 0.5352 0.6124], Epochs since improvement 0
  3%|▎         | 17/500 [10:17<4:43:04, 35.17s/it]  4%|▎         | 18/500 [11:04<5:09:45, 38.56s/it]Epoch: 18 done with learning rate 1.70E-03, Train loss: 5.58E+06, Train scatter: [0.5001 0.0995 0.5362 0.5658]
L1 regularization loss: 4.94E-01, L2 regularization loss: 1.30E-01
Test scatter: [0.5009 0.0984 0.5275 0.56  ], Lowest was [0.5009 0.0984 0.5275 0.56  ]
Median for last 10 epochs: [0.8691 0.1062 0.5335 0.5893], Epochs since improvement 0
  4%|▍         | 19/500 [11:31<4:42:56, 35.29s/it]Epoch: 20 done with learning rate 1.99E-03, Train loss: 5.28E+06, Train scatter: [0.4735 0.0958 0.5323 0.5804]
L1 regularization loss: 4.98E-01, L2 regularization loss: 1.32E-01
Test scatter: [0.4643 0.0946 0.5238 0.5793], Lowest was [0.4643 0.0946 0.5238 0.56  ]
Median for last 10 epochs: [0.687  0.1027 0.5307 0.5793], Epochs since improvement 0
  4%|▍         | 20/500 [12:22<5:18:30, 39.81s/it]  4%|▍         | 21/500 [12:50<4:49:11, 36.22s/it]  4%|▍         | 22/500 [13:36<5:12:41, 39.25s/it]Epoch: 22 done with learning rate 2.30E-03, Train loss: 4.97E+06, Train scatter: [0.5217 0.093  0.5243 0.5594]
L1 regularization loss: 5.03E-01, L2 regularization loss: 1.35E-01
Test scatter: [0.5119 0.0933 0.5165 0.5544], Lowest was [0.4643 0.0933 0.5165 0.5544]
Median for last 10 epochs: [0.5119 0.0984 0.5275 0.5717], Epochs since improvement 0
  5%|▍         | 23/500 [14:04<4:44:33, 35.79s/it]  5%|▍         | 24/500 [14:50<5:09:52, 39.06s/it]Epoch: 24 done with learning rate 2.63E-03, Train loss: 4.31E+06, Train scatter: [0.5436 0.099  0.4981 0.6337]
L1 regularization loss: 5.07E-01, L2 regularization loss: 1.38E-01
Test scatter: [0.5716 0.1037 0.4927 0.6494], Lowest was [0.4643 0.0933 0.4927 0.5544]
Median for last 10 epochs: [0.5119 0.0984 0.5238 0.5717], Epochs since improvement 0
  5%|▌         | 25/500 [15:18<4:41:08, 35.51s/it]  5%|▌         | 26/500 [16:03<5:04:48, 38.58s/it]Epoch: 26 done with learning rate 2.98E-03, Train loss: 3.04E+06, Train scatter: [0.5898 0.1017 0.4053 0.634 ]
L1 regularization loss: 5.15E-01, L2 regularization loss: 1.42E-01
Test scatter: [0.5887 0.102  0.3955 0.6282], Lowest was [0.4643 0.0933 0.3955 0.5544]
Median for last 10 epochs: [0.5119 0.0984 0.5165 0.5793], Epochs since improvement 0
  5%|▌         | 27/500 [16:32<4:39:40, 35.48s/it]  6%|▌         | 28/500 [17:17<5:03:31, 38.58s/it]Epoch: 28 done with learning rate 3.34E-03, Train loss: 2.91E+06, Train scatter: [0.4912 0.0903 0.4356 0.5846]
L1 regularization loss: 5.21E-01, L2 regularization loss: 1.46E-01
Test scatter: [0.4912 0.092  0.4241 0.5794], Lowest was [0.4643 0.092  0.3955 0.5544]
Median for last 10 epochs: [0.5119 0.0946 0.4927 0.5794], Epochs since improvement 0
  6%|▌         | 29/500 [17:45<4:37:51, 35.40s/it]Epoch: 30 done with learning rate 3.72E-03, Train loss: 1.89E+06, Train scatter: [0.4504 0.0886 0.3442 0.5726]
L1 regularization loss: 5.26E-01, L2 regularization loss: 1.49E-01
Test scatter: [0.465  0.0905 0.3414 0.5715], Lowest was [0.4643 0.0905 0.3414 0.5544]
Median for last 10 epochs: [0.5119 0.0933 0.4241 0.5794], Epochs since improvement 0
  6%|▌         | 30/500 [18:37<5:14:40, 40.17s/it]  6%|▌         | 31/500 [19:04<4:44:10, 36.36s/it]  6%|▋         | 32/500 [19:50<5:06:08, 39.25s/it]Epoch: 32 done with learning rate 4.10E-03, Train loss: 1.72E+06, Train scatter: [0.4216 0.0856 0.3396 0.5564]
L1 regularization loss: 5.34E-01, L2 regularization loss: 1.53E-01
Test scatter: [0.4509 0.0876 0.3433 0.5565], Lowest was [0.4509 0.0876 0.3414 0.5544]
Median for last 10 epochs: [0.4912 0.092  0.3955 0.5794], Epochs since improvement 0
  7%|▋         | 33/500 [20:17<4:37:23, 35.64s/it]  7%|▋         | 34/500 [21:03<5:00:54, 38.74s/it]Epoch: 34 done with learning rate 4.50E-03, Train loss: 1.66E+06, Train scatter: [0.3595 0.0859 0.3353 0.5434]
L1 regularization loss: 5.43E-01, L2 regularization loss: 1.58E-01
Test scatter: [0.3719 0.0871 0.3317 0.5405], Lowest was [0.3719 0.0871 0.3317 0.5405]
Median for last 10 epochs: [0.465  0.0905 0.3433 0.5715], Epochs since improvement 0
  7%|▋         | 35/500 [21:31<4:34:21, 35.40s/it]  7%|▋         | 36/500 [22:17<4:59:35, 38.74s/it]Epoch: 36 done with learning rate 4.90E-03, Train loss: 1.77E+06, Train scatter: [0.4815 0.0859 0.3666 0.5323]
L1 regularization loss: 5.52E-01, L2 regularization loss: 1.63E-01
Test scatter: [0.4956 0.0894 0.3865 0.5371], Lowest was [0.3719 0.0871 0.3317 0.5371]
Median for last 10 epochs: [0.465  0.0894 0.3433 0.5565], Epochs since improvement 0
  7%|▋         | 37/500 [22:45<4:32:28, 35.31s/it]  8%|▊         | 38/500 [23:31<4:57:14, 38.60s/it]Epoch: 38 done with learning rate 5.30E-03, Train loss: 1.51E+06, Train scatter: [0.435  0.0821 0.3292 0.5249]
L1 regularization loss: 5.59E-01, L2 regularization loss: 1.66E-01
Test scatter: [0.4316 0.0855 0.3311 0.523 ], Lowest was [0.3719 0.0855 0.3311 0.523 ]
Median for last 10 epochs: [0.4509 0.0876 0.3414 0.5405], Epochs since improvement 0
  8%|▊         | 39/500 [23:58<4:30:40, 35.23s/it]Epoch: 40 done with learning rate 5.70E-03, Train loss: 1.35E+06, Train scatter: [0.3698 0.0831 0.337  0.5519]
L1 regularization loss: 5.68E-01, L2 regularization loss: 1.71E-01
Test scatter: [0.3861 0.0854 0.3466 0.5539], Lowest was [0.3719 0.0854 0.3311 0.523 ]
Median for last 10 epochs: [0.4316 0.0871 0.3433 0.5405], Epochs since improvement 0
  8%|▊         | 40/500 [24:50<5:07:56, 40.17s/it]  8%|▊         | 41/500 [25:18<4:38:12, 36.37s/it]  8%|▊         | 42/500 [26:04<5:00:51, 39.41s/it]Epoch: 42 done with learning rate 6.10E-03, Train loss: 1.22E+06, Train scatter: [0.4222 0.0816 0.311  0.5094]
L1 regularization loss: 5.77E-01, L2 regularization loss: 1.76E-01
Test scatter: [0.4204 0.0831 0.3129 0.5041], Lowest was [0.3719 0.0831 0.3129 0.5041]
Median for last 10 epochs: [0.4204 0.0855 0.3317 0.5371], Epochs since improvement 0
  9%|▊         | 43/500 [26:32<4:34:06, 35.99s/it]  9%|▉         | 44/500 [27:19<4:57:57, 39.21s/it]Epoch: 44 done with learning rate 6.49E-03, Train loss: 1.27E+06, Train scatter: [0.3994 0.0803 0.3117 0.5107]
L1 regularization loss: 5.87E-01, L2 regularization loss: 1.81E-01
Test scatter: [0.4026 0.0817 0.316  0.5062], Lowest was [0.3719 0.0817 0.3129 0.5041]
Median for last 10 epochs: [0.4204 0.0854 0.3311 0.523 ], Epochs since improvement 0
  9%|▉         | 45/500 [27:47<4:32:39, 35.95s/it]  9%|▉         | 46/500 [28:34<4:56:43, 39.21s/it]Epoch: 46 done with learning rate 6.87E-03, Train loss: 1.10E+06, Train scatter: [0.2863 0.0804 0.3022 0.5104]
L1 regularization loss: 5.95E-01, L2 regularization loss: 1.87E-01
Test scatter: [0.3978 0.0811 0.3059 0.5085], Lowest was [0.3719 0.0811 0.3059 0.5041]
Median for last 10 epochs: [0.4026 0.0831 0.316  0.5085], Epochs since improvement 0
  9%|▉         | 47/500 [29:02<4:31:22, 35.94s/it] 10%|▉         | 48/500 [29:48<4:53:45, 38.99s/it]Epoch: 48 done with learning rate 7.24E-03, Train loss: 1.05E+06, Train scatter: [0.3288 0.0792 0.3062 0.5129]
L1 regularization loss: 6.03E-01, L2 regularization loss: 1.92E-01
Test scatter: [0.3447 0.0821 0.3155 0.5177], Lowest was [0.3447 0.0811 0.3059 0.5041]
Median for last 10 epochs: [0.3978 0.0821 0.3155 0.5085], Epochs since improvement 0
 10%|▉         | 49/500 [30:17<4:28:43, 35.75s/it]Epoch: 50 done with learning rate 7.60E-03, Train loss: 1.10E+06, Train scatter: [0.3009 0.0771 0.3056 0.5425]
L1 regularization loss: 6.12E-01, L2 regularization loss: 1.98E-01
Test scatter: [0.3086 0.0772 0.3063 0.5376], Lowest was [0.3086 0.0772 0.3059 0.5041]
Median for last 10 epochs: [0.3978 0.0817 0.3129 0.5085], Epochs since improvement 0
 10%|█         | 50/500 [31:08<5:03:12, 40.43s/it] 10%|█         | 51/500 [31:36<4:34:10, 36.64s/it] 10%|█         | 52/500 [32:22<4:54:56, 39.50s/it]Epoch: 52 done with learning rate 7.94E-03, Train loss: 1.19E+06, Train scatter: [0.297  0.076  0.2939 0.503 ]
L1 regularization loss: 6.23E-01, L2 regularization loss: 2.06E-01
Test scatter: [0.3065 0.0765 0.2994 0.5006], Lowest was [0.3065 0.0765 0.2994 0.5006]
Median for last 10 epochs: [0.3447 0.0811 0.3063 0.5085], Epochs since improvement 0
 11%|█         | 53/500 [32:50<4:28:30, 36.04s/it] 11%|█         | 54/500 [33:37<4:52:43, 39.38s/it]Epoch: 54 done with learning rate 8.26E-03, Train loss: 1.03E+06, Train scatter: [0.2864 0.0745 0.2922 0.5005]
L1 regularization loss: 6.32E-01, L2 regularization loss: 2.13E-01
Test scatter: [0.2926 0.0763 0.2994 0.5004], Lowest was [0.2926 0.0763 0.2994 0.5004]
Median for last 10 epochs: [0.3086 0.0772 0.3059 0.5085], Epochs since improvement 0
 11%|█         | 55/500 [34:05<4:26:40, 35.96s/it] 11%|█         | 56/500 [34:52<4:50:52, 39.31s/it]Epoch: 56 done with learning rate 8.56E-03, Train loss: 9.86E+05, Train scatter: [0.2785 0.0731 0.2885 0.509 ]
L1 regularization loss: 6.39E-01, L2 regularization loss: 2.19E-01
Test scatter: [0.2903 0.0752 0.2961 0.5079], Lowest was [0.2903 0.0752 0.2961 0.5004]
Median for last 10 epochs: [0.3065 0.0765 0.2994 0.5079], Epochs since improvement 0
 11%|█▏        | 57/500 [35:20<4:25:24, 35.95s/it] 12%|█▏        | 58/500 [36:07<4:49:48, 39.34s/it]Epoch: 58 done with learning rate 8.83E-03, Train loss: 8.30E+05, Train scatter: [0.2874 0.0749 0.3    0.5221]
L1 regularization loss: 6.46E-01, L2 regularization loss: 2.25E-01
Test scatter: [0.2922 0.0782 0.3086 0.5255], Lowest was [0.2903 0.0752 0.2961 0.5004]
Median for last 10 epochs: [0.2926 0.0765 0.2994 0.5079], Epochs since improvement 2
 12%|█▏        | 59/500 [36:36<4:24:57, 36.05s/it]Epoch: 60 done with learning rate 9.08E-03, Train loss: 7.47E+05, Train scatter: [0.2641 0.0737 0.2967 0.5055]
L1 regularization loss: 6.54E-01, L2 regularization loss: 2.32E-01
Test scatter: [0.2684 0.0747 0.2995 0.4965], Lowest was [0.2684 0.0747 0.2961 0.4965]
Median for last 10 epochs: [0.2922 0.0763 0.2994 0.5006], Epochs since improvement 0
 12%|█▏        | 60/500 [37:28<4:59:00, 40.77s/it] 12%|█▏        | 61/500 [37:56<4:31:28, 37.10s/it] 12%|█▏        | 62/500 [38:42<4:50:49, 39.84s/it]Epoch: 62 done with learning rate 9.31E-03, Train loss: 8.06E+05, Train scatter: [0.283  0.071  0.2815 0.4954]
L1 regularization loss: 6.60E-01, L2 regularization loss: 2.39E-01
Test scatter: [0.2902 0.0735 0.2896 0.4936], Lowest was [0.2684 0.0735 0.2896 0.4936]
Median for last 10 epochs: [0.2903 0.0752 0.2994 0.5004], Epochs since improvement 0
 13%|█▎        | 63/500 [39:10<4:24:06, 36.26s/it] 13%|█▎        | 64/500 [39:57<4:45:29, 39.29s/it]Epoch: 64 done with learning rate 9.50E-03, Train loss: 7.79E+05, Train scatter: [0.2856 0.0723 0.2961 0.5127]
L1 regularization loss: 6.69E-01, L2 regularization loss: 2.47E-01
Test scatter: [0.2939 0.0732 0.3002 0.5087], Lowest was [0.2684 0.0732 0.2896 0.4936]
Median for last 10 epochs: [0.2903 0.0747 0.2995 0.5079], Epochs since improvement 0
 13%|█▎        | 65/500 [40:24<4:19:42, 35.82s/it] 13%|█▎        | 66/500 [41:11<4:42:31, 39.06s/it]Epoch: 66 done with learning rate 9.66E-03, Train loss: 6.78E+05, Train scatter: [0.2861 0.0694 0.2754 0.4928]
L1 regularization loss: 6.79E-01, L2 regularization loss: 2.56E-01
Test scatter: [0.2931 0.0711 0.2825 0.4898], Lowest was [0.2684 0.0711 0.2825 0.4898]
Median for last 10 epochs: [0.2922 0.0735 0.2995 0.4965], Epochs since improvement 0
 13%|█▎        | 67/500 [41:39<4:18:16, 35.79s/it] 14%|█▎        | 68/500 [42:27<4:43:24, 39.36s/it]Epoch: 68 done with learning rate 9.80E-03, Train loss: 6.17E+05, Train scatter: [0.3104 0.0713 0.273  0.4931]
L1 regularization loss: 6.87E-01, L2 regularization loss: 2.65E-01
Test scatter: [0.3175 0.0721 0.2785 0.4883], Lowest was [0.2684 0.0711 0.2785 0.4883]
Median for last 10 epochs: [0.2931 0.0732 0.2896 0.4936], Epochs since improvement 0
 14%|█▍        | 69/500 [42:55<4:18:28, 35.98s/it]Epoch: 70 done with learning rate 9.90E-03, Train loss: 5.39E+05, Train scatter: [0.2767 0.0682 0.294  0.4995]
L1 regularization loss: 6.94E-01, L2 regularization loss: 2.72E-01
Test scatter: [0.2836 0.0692 0.3033 0.4956], Lowest was [0.2684 0.0692 0.2785 0.4883]
Median for last 10 epochs: [0.2931 0.0721 0.2896 0.4936], Epochs since improvement 0
 14%|█▍        | 70/500 [43:46<4:49:54, 40.45s/it] 14%|█▍        | 71/500 [44:14<4:22:00, 36.64s/it] 14%|█▍        | 72/500 [45:00<4:41:21, 39.44s/it]Epoch: 72 done with learning rate 9.96E-03, Train loss: 6.16E+05, Train scatter: [0.2858 0.0681 0.2765 0.5199]
L1 regularization loss: 7.06E-01, L2 regularization loss: 2.83E-01
Test scatter: [0.2932 0.0691 0.282  0.5181], Lowest was [0.2684 0.0691 0.2785 0.4883]
Median for last 10 epochs: [0.2932 0.0711 0.2825 0.4956], Epochs since improvement 0
 15%|█▍        | 73/500 [45:27<4:15:10, 35.86s/it] 15%|█▍        | 74/500 [46:14<4:37:26, 39.08s/it]Epoch: 74 done with learning rate 1.00E-02, Train loss: 4.23E+05, Train scatter: [0.2727 0.0697 0.279  0.4899]
L1 regularization loss: 7.17E-01, L2 regularization loss: 2.92E-01
Test scatter: [0.2731 0.0718 0.2872 0.4881], Lowest was [0.2684 0.0691 0.2785 0.4881]
Median for last 10 epochs: [0.2931 0.0711 0.2825 0.4898], Epochs since improvement 0
 15%|█▌        | 75/500 [46:41<4:12:53, 35.70s/it] 15%|█▌        | 76/500 [47:28<4:35:52, 39.04s/it]Epoch: 76 done with learning rate 1.00E-02, Train loss: 4.38E+05, Train scatter: [0.2531 0.0669 0.2692 0.4857]
L1 regularization loss: 7.28E-01, L2 regularization loss: 3.03E-01
Test scatter: [0.2564 0.0687 0.279  0.4828], Lowest was [0.2564 0.0687 0.2785 0.4828]
Median for last 10 epochs: [0.2836 0.0692 0.282  0.4883], Epochs since improvement 0
 15%|█▌        | 77/500 [47:56<4:11:41, 35.70s/it] 16%|█▌        | 78/500 [48:43<4:34:06, 38.97s/it]Epoch: 78 done with learning rate 1.00E-02, Train loss: 8.10E+08, Train scatter: [0.9349 0.1716 0.5441 0.9939]
L1 regularization loss: 1.16E+00, L2 regularization loss: 5.02E-01
Test scatter: [0.9193 0.1677 0.5355 0.9836], Lowest was [0.2564 0.0687 0.2785 0.4828]
Median for last 10 epochs: [0.2836 0.0692 0.2872 0.4956], Epochs since improvement 2
 16%|█▌        | 79/500 [49:10<4:09:07, 35.51s/it]Epoch: 80 done with learning rate 1.00E-02, Train loss: 9.39E+06, Train scatter: [0.9328 0.1405 0.5439 0.994 ]
L1 regularization loss: 1.18E+00, L2 regularization loss: 5.51E-01
Test scatter: [0.9172 0.1373 0.5353 0.9837], Lowest was [0.2564 0.0687 0.2785 0.4828]
Median for last 10 epochs: [0.2932 0.0718 0.2872 0.5181], Epochs since improvement 4
 16%|█▌        | 80/500 [50:03<4:45:10, 40.74s/it] 16%|█▌        | 81/500 [50:31<4:16:24, 36.72s/it] 16%|█▋        | 82/500 [51:17<4:36:47, 39.73s/it]Epoch: 82 done with learning rate 9.99E-03, Train loss: 6.13E+06, Train scatter: [0.5549 0.1202 0.5439 0.9916]
L1 regularization loss: 1.19E+00, L2 regularization loss: 5.80E-01
Test scatter: [0.5541 0.1181 0.5353 0.9813], Lowest was [0.2564 0.0687 0.2785 0.4828]
Median for last 10 epochs: [0.5541 0.1181 0.5353 0.9813], Epochs since improvement 6
 17%|█▋        | 83/500 [51:45<4:11:23, 36.17s/it] 17%|█▋        | 84/500 [52:32<4:32:27, 39.30s/it]Epoch: 84 done with learning rate 9.99E-03, Train loss: 1.67E+07, Train scatter: [0.9331 0.1724 0.5441 0.997 ]
L1 regularization loss: 1.50E+00, L2 regularization loss: 7.73E-01
Test scatter: [0.9175 0.1686 0.5355 0.9865], Lowest was [0.2564 0.0687 0.2785 0.4828]
Median for last 10 epochs: [0.9172 0.1373 0.5353 0.9836], Epochs since improvement 8
 17%|█▋        | 85/500 [52:59<4:07:26, 35.78s/it] 17%|█▋        | 86/500 [53:45<4:27:37, 38.79s/it]Epoch: 86 done with learning rate 9.98E-03, Train loss: 1.16E+07, Train scatter: [0.933  0.1719 0.5441 0.9965]
L1 regularization loss: 1.50E+00, L2 regularization loss: 7.74E-01
Test scatter: [0.9174 0.1681 0.5355 0.986 ], Lowest was [0.2564 0.0687 0.2785 0.4828]
Median for last 10 epochs: [0.9174 0.1677 0.5355 0.9837], Epochs since improvement 10
 17%|█▋        | 87/500 [54:13<4:04:58, 35.59s/it] 18%|█▊        | 88/500 [54:59<4:26:12, 38.77s/it]Epoch: 88 done with learning rate 9.98E-03, Train loss: 9.38E+06, Train scatter: [0.9328 0.1713 0.5441 0.996 ]
L1 regularization loss: 1.50E+00, L2 regularization loss: 7.74E-01
Test scatter: [0.9172 0.1675 0.5355 0.9856], Lowest was [0.2564 0.0687 0.2785 0.4828]
Median for last 10 epochs: [0.9172 0.1675 0.5355 0.9856], Epochs since improvement 12
 18%|█▊        | 89/500 [55:27<4:03:24, 35.53s/it]Epoch: 90 done with learning rate 9.97E-03, Train loss: 8.03E+06, Train scatter: [0.9326 0.1705 0.5441 0.9956]
L1 regularization loss: 1.50E+00, L2 regularization loss: 7.74E-01
Test scatter: [0.917  0.1667 0.5355 0.9852], Lowest was [0.2564 0.0687 0.2785 0.4828]
Median for last 10 epochs: [0.9172 0.1675 0.5355 0.9856], Epochs since improvement 14
 18%|█▊        | 90/500 [56:19<4:34:52, 40.23s/it] 18%|█▊        | 91/500 [56:46<4:08:52, 36.51s/it] 18%|█▊        | 92/500 [57:33<4:29:00, 39.56s/it]Epoch: 92 done with learning rate 9.96E-03, Train loss: 7.07E+06, Train scatter: [0.9323 0.1696 0.5441 0.9952]
L1 regularization loss: 1.50E+00, L2 regularization loss: 7.75E-01
Test scatter: [0.9167 0.1658 0.5355 0.9847], Lowest was [0.2564 0.0687 0.2785 0.4828]
Median for last 10 epochs: [0.9172 0.1675 0.5355 0.9856], Epochs since improvement 16
 19%|█▊        | 93/500 [58:01<4:04:19, 36.02s/it] 19%|█▉        | 94/500 [58:48<4:26:28, 39.38s/it]Epoch: 94 done with learning rate 9.95E-03, Train loss: 6.33E+06, Train scatter: [0.9319 0.1684 0.5441 0.9947]
L1 regularization loss: 1.50E+00, L2 regularization loss: 7.75E-01
Test scatter: [0.9163 0.1646 0.5355 0.9843], Lowest was [0.2564 0.0687 0.2785 0.4828]
Median for last 10 epochs: [0.917  0.1667 0.5355 0.9852], Epochs since improvement 18
 19%|█▉        | 95/500 [59:16<4:03:19, 36.05s/it] 19%|█▉        | 96/500 [1:00:02<4:22:40, 39.01s/it]Epoch: 96 done with learning rate 9.94E-03, Train loss: 5.71E+06, Train scatter: [0.9314 0.1667 0.544  0.9943]
L1 regularization loss: 1.50E+00, L2 regularization loss: 7.77E-01
Test scatter: [0.9158 0.163  0.5355 0.9839], Lowest was [0.2564 0.0687 0.2785 0.4828]
Median for last 10 epochs: [0.9167 0.1658 0.5355 0.9847], Epochs since improvement 20
 19%|█▉        | 97/500 [1:00:30<3:59:38, 35.68s/it] 19%|█▉        | 97/500 [1:01:17<4:14:36, 37.91s/it]
Epoch: 98 done with learning rate 9.93E-03, Train loss: 5.19E+06, Train scatter: [0.9307 0.1645 0.544  0.9938]
L1 regularization loss: 1.50E+00, L2 regularization loss: 7.79E-01
Test scatter: [0.9151 0.1607 0.5355 0.9834], Lowest was [0.2564 0.0687 0.2785 0.4828]
Median for last 10 epochs: [0.9163 0.1646 0.5355 0.9843], Epochs since improvement 22
Exited after 98 epochs due to early stopping
3677.10 seconds spent training, 7.354 seconds per epoch. Processed 9469 trees per second
[0.91507924 0.16073717 0.5354345  0.9833586 ]
{'epoch_exit': 97, 'scatter_m_star': 0.91507924, 'lowest_m_star': 0.25644436, 'last20_m_star': 0.9168375, 'last10_m_star': 0.91631436, 'scatter_v_disk': 0.16073717, 'lowest_v_disk': 0.06869285, 'last20_v_disk': 0.16516173, 'last10_v_disk': 0.16456074, 'scatter_m_cold': 0.5354345, 'lowest_m_cold': 0.2784622, 'last20_m_cold': 0.53549427, 'last10_m_cold': 0.53548163, 'scatter_sfr_100': 0.9833586, 'lowest_sfr_100': 0.48281586, 'last20_sfr_100': 0.9845153, 'last10_sfr_100': 0.9843007}
Finished 1/1
Exited training after None epochs
Experiment 2 done: 2 / 6
Exploring ['conv_layers', 'batch_size']
Currently doing [  2 256]
{'experiment': 'GraphMerge', 'group': 'meta_pysr', 'move': False, 'model': 'MetaEdge', 'log': True, 'run_params': {'n_epochs': 500, 'n_trials': 1, 'batch_size': 256, 'val_epoch': 2, 'early_stopping': True, 'patience': 20, 'l1_lambda': 3e-05, 'l2_lambda': 3e-05, 'loss_func': 'GaussNd', 'metrics': 'test_multi_varrho', 'performance_plot': 'multi_base', 'save': True, 'seed': True, 'num_workers': 4}, 'learn_params': {'learning_rate': 0.01, 'schedule': 'onecycle', 'g_up': 1, 'g_down': 0.95, 'warmup': 4, 'period': 5, 'eta_min': 1e-05}, 'hyper_params': {'hidden_channels': 128, 'conv_layers': 2, 'conv_activation': 'relu', 'decode_activation': 'leakyrelu', 'decode_layers': 2, 'layernorm': True, 'agg': 'sum', 'variance': True, 'rho': 0, 'in_channels': 43, 'out_channels': 4}, 'data_params': {'case': 'vlarge_all_4t_z0.0_quantile_raw', 'targets': [0, 1, 2, 3], 'del_feats': [], 'split': 0.8, 'test': 0, 'scale': 0}, 'experiment_name': 'con2bat256'}
/home/cj1223/../../scratch/gpfs/cj1223/GraphResults/results_meta_pysr_120322/MetaEdge_vlarge_all_4t_z0.0_quantile_raw_azystx
RelU conv activation
LeakyRelU decode activation
N_params 422280
Made folder for saving model
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:48<6:41:37, 48.29s/it]  0%|          | 2/500 [01:59<8:32:21, 61.73s/it]Epoch: 2 done with learning rate 4.17E-04, Train loss: 2.29E+07, Train scatter: [0.9351 0.1384 0.544  0.9954]
L1 regularization loss: 5.97E-01, L2 regularization loss: 1.15E-01
Test scatter: [0.9195 0.1358 0.5355 0.9851], Lowest was [0.9195 0.1358 0.5355 0.9851]
Median for last 10 epochs: [0.9195 0.1358 0.5355 0.9851], Epochs since improvement 2
  1%|          | 3/500 [02:48<7:42:01, 55.78s/it]  1%|          | 4/500 [04:00<8:36:11, 62.44s/it]Epoch: 4 done with learning rate 4.67E-04, Train loss: 1.50E+07, Train scatter: [0.9288 0.0972 0.5439 0.9946]
L1 regularization loss: 6.05E-01, L2 regularization loss: 1.24E-01
Test scatter: [0.9132 0.096  0.5354 0.9842], Lowest was [0.9132 0.096  0.5354 0.9842]
Median for last 10 epochs: [0.9132 0.096  0.5354 0.9842], Epochs since improvement 0
  1%|          | 5/500 [04:49<7:53:32, 57.40s/it]  1%|          | 6/500 [06:00<8:32:23, 62.23s/it]Epoch: 6 done with learning rate 5.51E-04, Train loss: 9.06E+06, Train scatter: [0.6702 0.0858 0.5438 0.6322]
L1 regularization loss: 6.19E-01, L2 regularization loss: 1.37E-01
Test scatter: [0.6552 0.0852 0.5353 0.6269], Lowest was [0.6552 0.0852 0.5353 0.6269]
Median for last 10 epochs: [0.6552 0.0852 0.5353 0.6269], Epochs since improvement 0
  1%|▏         | 7/500 [06:49<7:53:52, 57.67s/it]  2%|▏         | 8/500 [07:59<8:25:01, 61.59s/it]Epoch: 8 done with learning rate 6.67E-04, Train loss: 5.50E+06, Train scatter: [0.4688 0.0778 0.5438 0.5548]
L1 regularization loss: 6.23E-01, L2 regularization loss: 1.41E-01
Test scatter: [0.47   0.078  0.5352 0.5512], Lowest was [0.47   0.078  0.5352 0.5512]
Median for last 10 epochs: [0.5626 0.0816 0.5352 0.589 ], Epochs since improvement 0
  2%|▏         | 9/500 [08:48<7:51:58, 57.68s/it]Epoch: 10 done with learning rate 8.15E-04, Train loss: 4.17E+06, Train scatter: [0.3991 0.0731 0.5437 0.5308]
L1 regularization loss: 6.25E-01, L2 regularization loss: 1.44E-01
Test scatter: [0.4056 0.0729 0.5352 0.5283], Lowest was [0.4056 0.0729 0.5352 0.5283]
Median for last 10 epochs: [0.47   0.078  0.5352 0.5512], Epochs since improvement 0
  2%|▏         | 10/500 [10:07<8:44:27, 64.22s/it]  2%|▏         | 11/500 [10:54<8:00:35, 58.97s/it]  2%|▏         | 12/500 [12:05<8:29:13, 62.61s/it]Epoch: 12 done with learning rate 9.94E-04, Train loss: 3.71E+06, Train scatter: [0.2456 0.072  0.5437 0.5254]
L1 regularization loss: 6.28E-01, L2 regularization loss: 1.45E-01
Test scatter: [0.2516 0.071  0.5352 0.5195], Lowest was [0.2516 0.071  0.5352 0.5195]
Median for last 10 epochs: [0.47   0.078  0.5352 0.5512], Epochs since improvement 0
  3%|▎         | 13/500 [12:53<7:53:21, 58.32s/it]  3%|▎         | 14/500 [14:06<8:27:27, 62.65s/it]Epoch: 14 done with learning rate 1.20E-03, Train loss: 3.44E+06, Train scatter: [0.2571 0.0694 0.5437 0.5121]
L1 regularization loss: 6.30E-01, L2 regularization loss: 1.47E-01
Test scatter: [0.2656 0.0696 0.5352 0.511 ], Lowest was [0.2516 0.0696 0.5352 0.511 ]
Median for last 10 epochs: [0.4056 0.0729 0.5352 0.5283], Epochs since improvement 0
  3%|▎         | 15/500 [14:53<7:50:03, 58.15s/it]  3%|▎         | 16/500 [16:04<8:20:29, 62.04s/it]Epoch: 16 done with learning rate 1.44E-03, Train loss: 3.32E+06, Train scatter: [0.2278 0.0691 0.5437 0.5087]
L1 regularization loss: 6.33E-01, L2 regularization loss: 1.48E-01
Test scatter: [0.2311 0.0689 0.5352 0.5026], Lowest was [0.2311 0.0689 0.5352 0.5026]
Median for last 10 epochs: [0.2656 0.071  0.5352 0.5195], Epochs since improvement 0
  3%|▎         | 17/500 [16:53<7:46:16, 57.92s/it]  4%|▎         | 18/500 [18:06<8:22:40, 62.57s/it]Epoch: 18 done with learning rate 1.70E-03, Train loss: 3.30E+06, Train scatter: [0.2364 0.0692 0.5437 0.5036]
L1 regularization loss: 6.36E-01, L2 regularization loss: 1.49E-01
Test scatter: [0.2443 0.0701 0.5352 0.5009], Lowest was [0.2311 0.0689 0.5352 0.5009]
Median for last 10 epochs: [0.2516 0.0701 0.5352 0.511 ], Epochs since improvement 0
  4%|▍         | 19/500 [18:53<7:43:52, 57.86s/it]Epoch: 20 done with learning rate 1.99E-03, Train loss: 3.24E+06, Train scatter: [0.3381 0.0736 0.5437 0.5677]
L1 regularization loss: 6.38E-01, L2 regularization loss: 1.50E-01
Test scatter: [0.346  0.0754 0.5352 0.5754], Lowest was [0.2311 0.0689 0.5352 0.5009]
Median for last 10 epochs: [0.2516 0.0701 0.5352 0.511 ], Epochs since improvement 0
  4%|▍         | 20/500 [20:13<8:36:05, 64.51s/it]  4%|▍         | 21/500 [21:02<7:57:35, 59.82s/it]  4%|▍         | 22/500 [22:14<8:26:47, 63.61s/it]Epoch: 22 done with learning rate 2.30E-03, Train loss: 3.19E+06, Train scatter: [0.2007 0.0654 0.5436 0.4993]
L1 regularization loss: 6.43E-01, L2 regularization loss: 1.52E-01
Test scatter: [0.2088 0.0659 0.5351 0.4957], Lowest was [0.2088 0.0659 0.5351 0.4957]
Median for last 10 epochs: [0.2443 0.0696 0.5352 0.5026], Epochs since improvement 0
  5%|▍         | 23/500 [23:01<7:45:11, 58.52s/it]  5%|▍         | 24/500 [24:15<8:20:44, 63.12s/it]Epoch: 24 done with learning rate 2.63E-03, Train loss: 3.15E+06, Train scatter: [0.2009 0.065  0.5436 0.5015]
L1 regularization loss: 6.47E-01, L2 regularization loss: 1.53E-01
Test scatter: [0.2082 0.0658 0.5351 0.4991], Lowest was [0.2082 0.0658 0.5351 0.4957]
Median for last 10 epochs: [0.2311 0.0689 0.5352 0.5009], Epochs since improvement 0
  5%|▌         | 25/500 [25:02<7:42:36, 58.43s/it]  5%|▌         | 26/500 [26:15<8:16:13, 62.81s/it]Epoch: 26 done with learning rate 2.98E-03, Train loss: 3.08E+06, Train scatter: [0.1869 0.0612 0.5434 0.4919]
L1 regularization loss: 6.52E-01, L2 regularization loss: 1.55E-01
Test scatter: [0.1938 0.0615 0.5349 0.4877], Lowest was [0.1938 0.0615 0.5349 0.4877]
Median for last 10 epochs: [0.2088 0.0659 0.5351 0.4991], Epochs since improvement 0
  5%|▌         | 27/500 [27:04<7:40:41, 58.44s/it]  6%|▌         | 28/500 [28:16<8:12:03, 62.55s/it]Epoch: 28 done with learning rate 3.34E-03, Train loss: 3.06E+06, Train scatter: [0.2826 0.0652 0.5434 0.5024]
L1 regularization loss: 6.58E-01, L2 regularization loss: 1.57E-01
Test scatter: [0.287  0.0652 0.5348 0.4986], Lowest was [0.1938 0.0615 0.5348 0.4877]
Median for last 10 epochs: [0.2088 0.0658 0.5351 0.4986], Epochs since improvement 0
  6%|▌         | 29/500 [29:04<7:37:50, 58.32s/it]Epoch: 30 done with learning rate 3.72E-03, Train loss: 3.03E+06, Train scatter: [0.2012 0.0629 0.5433 0.492 ]
L1 regularization loss: 6.64E-01, L2 regularization loss: 1.59E-01
Test scatter: [0.207  0.0626 0.5348 0.4858], Lowest was [0.1938 0.0615 0.5348 0.4858]
Median for last 10 epochs: [0.2082 0.0652 0.5349 0.4957], Epochs since improvement 0
  6%|▌         | 30/500 [30:23<8:25:42, 64.56s/it]  6%|▌         | 31/500 [31:10<7:43:37, 59.31s/it]  6%|▋         | 32/500 [32:24<8:16:01, 63.59s/it]Epoch: 32 done with learning rate 4.10E-03, Train loss: 3.00E+06, Train scatter: [0.2503 0.0636 0.5433 0.5251]
L1 regularization loss: 6.71E-01, L2 regularization loss: 1.61E-01
Test scatter: [0.252  0.0642 0.5348 0.5234], Lowest was [0.1938 0.0615 0.5348 0.4858]
Median for last 10 epochs: [0.2082 0.0642 0.5348 0.4986], Epochs since improvement 0
  7%|▋         | 33/500 [33:13<7:41:16, 59.26s/it]  7%|▋         | 34/500 [34:25<8:09:16, 63.00s/it]Epoch: 34 done with learning rate 4.50E-03, Train loss: 3.01E+06, Train scatter: [0.2228 0.0648 0.5431 0.5142]
L1 regularization loss: 6.81E-01, L2 regularization loss: 1.65E-01
Test scatter: [0.2301 0.0647 0.5345 0.5084], Lowest was [0.1938 0.0615 0.5345 0.4858]
Median for last 10 epochs: [0.2301 0.0642 0.5348 0.4986], Epochs since improvement 0
  7%|▋         | 35/500 [35:13<7:33:14, 58.48s/it]  7%|▋         | 36/500 [36:24<8:01:41, 62.29s/it]Epoch: 36 done with learning rate 4.90E-03, Train loss: 3.02E+06, Train scatter: [0.3483 0.0692 0.5432 0.5255]
L1 regularization loss: 6.90E-01, L2 regularization loss: 1.69E-01
Test scatter: [0.3413 0.0696 0.5347 0.5221], Lowest was [0.1938 0.0615 0.5345 0.4858]
Median for last 10 epochs: [0.252  0.0647 0.5348 0.5084], Epochs since improvement 2
  7%|▋         | 37/500 [37:13<7:29:06, 58.20s/it]  8%|▊         | 38/500 [38:24<7:57:44, 62.04s/it]Epoch: 38 done with learning rate 5.30E-03, Train loss: 3.02E+06, Train scatter: [0.2357 0.0602 0.5432 0.4958]
L1 regularization loss: 7.03E-01, L2 regularization loss: 1.74E-01
Test scatter: [0.2394 0.0603 0.5346 0.4891], Lowest was [0.1938 0.0603 0.5345 0.4858]
Median for last 10 epochs: [0.2394 0.0642 0.5347 0.5084], Epochs since improvement 0
  8%|▊         | 39/500 [39:11<7:23:26, 57.71s/it]Epoch: 40 done with learning rate 5.70E-03, Train loss: 3.00E+06, Train scatter: [0.2394 0.0671 0.543  0.4873]
L1 regularization loss: 7.13E-01, L2 regularization loss: 1.79E-01
Test scatter: [0.2429 0.0667 0.5345 0.4826], Lowest was [0.1938 0.0603 0.5345 0.4826]
Median for last 10 epochs: [0.2429 0.0647 0.5346 0.5084], Epochs since improvement 0
  8%|▊         | 40/500 [40:30<8:09:42, 63.87s/it]  8%|▊         | 41/500 [41:16<7:29:19, 58.74s/it]  8%|▊         | 42/500 [42:30<8:02:54, 63.26s/it]Epoch: 42 done with learning rate 6.10E-03, Train loss: 2.98E+06, Train scatter: [0.2582 0.066  0.5429 0.5036]
L1 regularization loss: 7.21E-01, L2 regularization loss: 1.83E-01
Test scatter: [0.2605 0.0661 0.5343 0.4973], Lowest was [0.1938 0.0603 0.5343 0.4826]
Median for last 10 epochs: [0.2429 0.0661 0.5345 0.4973], Epochs since improvement 0
  9%|▊         | 43/500 [43:17<7:24:22, 58.34s/it]  9%|▉         | 44/500 [44:30<7:57:54, 62.88s/it]Epoch: 44 done with learning rate 6.49E-03, Train loss: 2.96E+06, Train scatter: [0.2558 0.0689 0.5426 0.5085]
L1 regularization loss: 7.26E-01, L2 regularization loss: 1.88E-01
Test scatter: [0.2643 0.0673 0.5341 0.5055], Lowest was [0.1938 0.0603 0.5341 0.4826]
Median for last 10 epochs: [0.2605 0.0667 0.5345 0.4973], Epochs since improvement 0
  9%|▉         | 45/500 [45:17<7:19:32, 57.96s/it]  9%|▉         | 46/500 [46:30<7:52:37, 62.46s/it]Epoch: 46 done with learning rate 6.87E-03, Train loss: 2.96E+06, Train scatter: [0.3016 0.0649 0.5421 0.4977]
L1 regularization loss: 7.56E-01, L2 regularization loss: 2.01E-01
Test scatter: [0.2991 0.0645 0.5336 0.4931], Lowest was [0.1938 0.0603 0.5336 0.4826]
Median for last 10 epochs: [0.2605 0.0661 0.5343 0.4931], Epochs since improvement 0
  9%|▉         | 47/500 [47:18<7:18:56, 58.14s/it] 10%|▉         | 48/500 [48:30<7:48:29, 62.19s/it]Epoch: 48 done with learning rate 7.24E-03, Train loss: 2.93E+06, Train scatter: [0.2329 0.0583 0.5386 0.4787]
L1 regularization loss: 7.68E-01, L2 regularization loss: 2.10E-01
Test scatter: [0.2443 0.0587 0.5302 0.4734], Lowest was [0.1938 0.0587 0.5302 0.4734]
Median for last 10 epochs: [0.2605 0.0661 0.5341 0.4931], Epochs since improvement 0
 10%|▉         | 49/500 [49:19<7:17:58, 58.27s/it]Epoch: 50 done with learning rate 7.60E-03, Train loss: 4.26E+06, Train scatter: [0.9337 0.1706 0.5441 0.9944]
L1 regularization loss: 9.17E-01, L2 regularization loss: 2.70E-01
Test scatter: [0.9182 0.1668 0.5355 0.9843], Lowest was [0.1938 0.0587 0.5302 0.4734]
Median for last 10 epochs: [0.2643 0.0661 0.5341 0.4973], Epochs since improvement 2
 10%|█         | 50/500 [50:38<8:04:04, 64.54s/it] 10%|█         | 51/500 [51:25<7:23:43, 59.29s/it] 10%|█         | 52/500 [52:36<7:49:47, 62.92s/it]Epoch: 52 done with learning rate 7.94E-03, Train loss: 3.91E+06, Train scatter: [0.9331 0.1413 0.5441 0.9891]
L1 regularization loss: 9.28E-01, L2 regularization loss: 2.80E-01
Test scatter: [0.9176 0.1398 0.5355 0.9795], Lowest was [0.1938 0.0587 0.5302 0.4734]
Median for last 10 epochs: [0.2991 0.0673 0.5341 0.5055], Epochs since improvement 4
 11%|█         | 53/500 [53:24<7:15:21, 58.44s/it] 11%|█         | 54/500 [54:36<7:44:13, 62.45s/it]Epoch: 54 done with learning rate 8.26E-03, Train loss: 3.71E+06, Train scatter: [0.9318 0.122  0.544  0.9816]
L1 regularization loss: 9.46E-01, L2 regularization loss: 3.02E-01
Test scatter: [0.9164 0.1216 0.5355 0.9722], Lowest was [0.1938 0.0587 0.5302 0.4734]
Median for last 10 epochs: [0.9164 0.1216 0.5355 0.9722], Epochs since improvement 6
 11%|█         | 55/500 [55:23<7:07:51, 57.69s/it] 11%|█         | 56/500 [56:34<7:37:09, 61.78s/it]Epoch: 56 done with learning rate 8.56E-03, Train loss: 3.63E+06, Train scatter: [0.9306 0.1185 0.5441 0.767 ]
L1 regularization loss: 9.60E-01, L2 regularization loss: 3.14E-01
Test scatter: [0.9153 0.1183 0.5355 0.7665], Lowest was [0.1938 0.0587 0.5302 0.4734]
Median for last 10 epochs: [0.9164 0.1216 0.5355 0.9722], Epochs since improvement 8
 11%|█▏        | 57/500 [57:21<7:03:26, 57.35s/it] 12%|█▏        | 58/500 [58:33<7:35:09, 61.79s/it]Epoch: 58 done with learning rate 8.83E-03, Train loss: 3.58E+06, Train scatter: [0.9279 0.1112 0.5441 0.6989]
L1 regularization loss: 9.57E-01, L2 regularization loss: 3.22E-01
Test scatter: [0.9128 0.1119 0.5355 0.6985], Lowest was [0.1938 0.0587 0.5302 0.4734]
Median for last 10 epochs: [0.9164 0.1216 0.5355 0.9722], Epochs since improvement 10
 12%|█▏        | 59/500 [59:20<7:02:01, 57.42s/it]Epoch: 60 done with learning rate 9.08E-03, Train loss: 3.55E+06, Train scatter: [0.921  0.1129 0.544  0.6839]
L1 regularization loss: 9.55E-01, L2 regularization loss: 3.40E-01
Test scatter: [0.9063 0.1142 0.5355 0.6883], Lowest was [0.1938 0.0587 0.5302 0.4734]
Median for last 10 epochs: [0.9153 0.1183 0.5355 0.7665], Epochs since improvement 12
 12%|█▏        | 60/500 [1:00:40<7:49:15, 63.99s/it] 12%|█▏        | 61/500 [1:01:28<7:14:08, 59.34s/it] 12%|█▏        | 62/500 [1:02:39<7:37:48, 62.71s/it]Epoch: 62 done with learning rate 9.31E-03, Train loss: 3.53E+06, Train scatter: [0.8893 0.1305 0.544  0.6573]
L1 regularization loss: 9.65E-01, L2 regularization loss: 3.72E-01
Test scatter: [0.8761 0.1291 0.5354 0.6584], Lowest was [0.1938 0.0587 0.5302 0.4734]
Median for last 10 epochs: [0.9128 0.1183 0.5355 0.6985], Epochs since improvement 14
 13%|█▎        | 63/500 [1:03:26<7:01:53, 57.93s/it] 13%|█▎        | 64/500 [1:04:38<7:32:54, 62.33s/it]Epoch: 64 done with learning rate 9.50E-03, Train loss: 3.48E+06, Train scatter: [0.5249 0.1049 0.544  0.6426]
L1 regularization loss: 9.68E-01, L2 regularization loss: 3.91E-01
Test scatter: [0.5205 0.1051 0.5354 0.6422], Lowest was [0.1938 0.0587 0.5302 0.4734]
Median for last 10 epochs: [0.9063 0.1142 0.5355 0.6883], Epochs since improvement 16
 13%|█▎        | 65/500 [1:05:27<7:03:05, 58.36s/it] 13%|█▎        | 66/500 [1:06:41<7:35:17, 62.94s/it]Epoch: 66 done with learning rate 9.66E-03, Train loss: 3.43E+06, Train scatter: [0.6309 0.097  0.5439 0.6421]
L1 regularization loss: 9.68E-01, L2 regularization loss: 3.99E-01
Test scatter: [0.6171 0.0964 0.5353 0.6354], Lowest was [0.1938 0.0587 0.5302 0.4734]
Median for last 10 epochs: [0.8761 0.1119 0.5354 0.6584], Epochs since improvement 18
 13%|█▎        | 67/500 [1:07:28<7:00:56, 58.33s/it] 14%|█▎        | 68/500 [1:08:42<7:32:42, 62.88s/it]Epoch: 68 done with learning rate 9.80E-03, Train loss: 3.39E+06, Train scatter: [0.4686 0.1007 0.5437 0.6061]
L1 regularization loss: 9.71E-01, L2 regularization loss: 4.08E-01
Test scatter: [0.4598 0.1029 0.5352 0.6147], Lowest was [0.1938 0.0587 0.5302 0.4734]
Median for last 10 epochs: [0.6171 0.1051 0.5354 0.6422], Epochs since improvement 20
 14%|█▍        | 69/500 [1:09:29<6:57:26, 58.11s/it]Epoch: 70 done with learning rate 9.90E-03, Train loss: 3.54E+06, Train scatter: [0.9266 0.1689 0.5441 0.966 ]
L1 regularization loss: 1.18E+00, L2 regularization loss: 5.53E-01
Test scatter: [0.9112 0.1648 0.5355 0.9557], Lowest was [0.1938 0.0587 0.5302 0.4734]
Median for last 10 epochs: [0.6171 0.1051 0.5354 0.6422], Epochs since improvement 22
 14%|█▍        | 69/500 [1:10:48<7:22:20, 61.58s/it]
Exited after 70 epochs due to early stopping
4248.94 seconds spent training, 8.498 seconds per epoch. Processed 8195 trees per second
[0.9111702  0.16482322 0.5354647  0.9556904 ]
{'epoch_exit': 69, 'scatter_m_star': 0.9111702, 'lowest_m_star': 0.19375901, 'last20_m_star': 0.908756, 'last10_m_star': 0.61707133, 'scatter_v_disk': 0.16482322, 'lowest_v_disk': 0.058744535, 'last20_v_disk': 0.11623161, 'last10_v_disk': 0.10509812, 'scatter_m_cold': 0.5354647, 'lowest_m_cold': 0.5302496, 'last20_m_cold': 0.5354577, 'last10_m_cold': 0.53540397, 'scatter_sfr_100': 0.9556904, 'lowest_sfr_100': 0.47337002, 'last20_sfr_100': 0.6934059, 'last10_sfr_100': 0.64219224}
Finished 1/1
Exited training after None epochs
Experiment 3 done: 3 / 6
Exploring ['conv_layers', 'batch_size']
Currently doing [  2 512]
{'experiment': 'GraphMerge', 'group': 'meta_pysr', 'move': False, 'model': 'MetaEdge', 'log': True, 'run_params': {'n_epochs': 500, 'n_trials': 1, 'batch_size': 512, 'val_epoch': 2, 'early_stopping': True, 'patience': 20, 'l1_lambda': 3e-05, 'l2_lambda': 3e-05, 'loss_func': 'GaussNd', 'metrics': 'test_multi_varrho', 'performance_plot': 'multi_base', 'save': True, 'seed': True, 'num_workers': 4}, 'learn_params': {'learning_rate': 0.01, 'schedule': 'onecycle', 'g_up': 1, 'g_down': 0.95, 'warmup': 4, 'period': 5, 'eta_min': 1e-05}, 'hyper_params': {'hidden_channels': 128, 'conv_layers': 2, 'conv_activation': 'relu', 'decode_activation': 'leakyrelu', 'decode_layers': 2, 'layernorm': True, 'agg': 'sum', 'variance': True, 'rho': 0, 'in_channels': 43, 'out_channels': 4}, 'data_params': {'case': 'vlarge_all_4t_z0.0_quantile_raw', 'targets': [0, 1, 2, 3], 'del_feats': [], 'split': 0.8, 'test': 0, 'scale': 0}, 'experiment_name': 'con2bat512'}
/home/cj1223/../../scratch/gpfs/cj1223/GraphResults/results_meta_pysr_120322/MetaEdge_vlarge_all_4t_z0.0_quantile_raw_cvlalk
RelU conv activation
LeakyRelU decode activation
N_params 422280
Made folder for saving model
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:40<5:40:39, 40.96s/it]  0%|          | 2/500 [01:43<7:26:24, 53.78s/it]Epoch: 2 done with learning rate 4.17E-04, Train loss: 3.03E+07, Train scatter: [0.9352 0.1709 0.5441 0.9954]
L1 regularization loss: 5.94E-01, L2 regularization loss: 1.13E-01
Test scatter: [0.9196 0.1673 0.5356 0.9851], Lowest was [0.9196 0.1673 0.5356 0.9851]
Median for last 10 epochs: [0.9196 0.1673 0.5356 0.9851], Epochs since improvement 2
  1%|          | 3/500 [02:23<6:34:15, 47.60s/it]  1%|          | 4/500 [03:27<7:24:33, 53.78s/it]Epoch: 4 done with learning rate 4.67E-04, Train loss: 2.23E+07, Train scatter: [0.9352 0.157  0.5441 0.9954]
L1 regularization loss: 5.98E-01, L2 regularization loss: 1.16E-01
Test scatter: [0.9196 0.1531 0.5356 0.9851], Lowest was [0.9196 0.1531 0.5356 0.9851]
Median for last 10 epochs: [0.9196 0.1531 0.5356 0.9851], Epochs since improvement 0
  1%|          | 5/500 [04:08<6:46:38, 49.29s/it]  1%|          | 6/500 [05:12<7:27:23, 54.34s/it]Epoch: 6 done with learning rate 5.51E-04, Train loss: 1.77E+07, Train scatter: [0.9349 0.1095 0.5441 0.9953]
L1 regularization loss: 6.04E-01, L2 regularization loss: 1.21E-01
Test scatter: [0.9193 0.1084 0.5355 0.985 ], Lowest was [0.9193 0.1084 0.5355 0.985 ]
Median for last 10 epochs: [0.9193 0.1084 0.5355 0.985 ], Epochs since improvement 0
  1%|▏         | 7/500 [05:54<6:52:27, 50.20s/it]  2%|▏         | 8/500 [06:58<7:28:18, 54.67s/it]Epoch: 8 done with learning rate 6.67E-04, Train loss: 1.31E+07, Train scatter: [0.9279 0.0934 0.544  0.8668]
L1 regularization loss: 6.12E-01, L2 regularization loss: 1.28E-01
Test scatter: [0.9127 0.0926 0.5355 0.868 ], Lowest was [0.9127 0.0926 0.5355 0.868 ]
Median for last 10 epochs: [0.916  0.1005 0.5355 0.9265], Epochs since improvement 0
  2%|▏         | 9/500 [07:38<6:50:45, 50.19s/it]Epoch: 10 done with learning rate 8.15E-04, Train loss: 9.50E+06, Train scatter: [0.6456 0.0913 0.5439 0.6349]
L1 regularization loss: 6.20E-01, L2 regularization loss: 1.37E-01
Test scatter: [0.6687 0.0905 0.5354 0.6278], Lowest was [0.6687 0.0905 0.5354 0.6278]
Median for last 10 epochs: [0.9127 0.0926 0.5355 0.868 ], Epochs since improvement 0
  2%|▏         | 10/500 [08:48<7:39:34, 56.27s/it]  2%|▏         | 11/500 [09:29<7:00:14, 51.56s/it]  2%|▏         | 12/500 [10:32<7:27:47, 55.06s/it]Epoch: 12 done with learning rate 9.94E-04, Train loss: 7.90E+06, Train scatter: [0.7269 0.0983 0.544  0.6433]
L1 regularization loss: 6.25E-01, L2 regularization loss: 1.41E-01
Test scatter: [0.7155 0.0978 0.5354 0.6369], Lowest was [0.6687 0.0905 0.5354 0.6278]
Median for last 10 epochs: [0.9127 0.0978 0.5355 0.868 ], Epochs since improvement 2
  3%|▎         | 13/500 [11:13<6:51:31, 50.70s/it]  3%|▎         | 14/500 [12:17<7:23:26, 54.75s/it]Epoch: 14 done with learning rate 1.20E-03, Train loss: 5.65E+06, Train scatter: [0.6196 0.0887 0.5439 0.6065]
L1 regularization loss: 6.28E-01, L2 regularization loss: 1.45E-01
Test scatter: [0.617  0.0885 0.5354 0.6098], Lowest was [0.617  0.0885 0.5354 0.6098]
Median for last 10 epochs: [0.7155 0.0926 0.5354 0.6369], Epochs since improvement 0
  3%|▎         | 15/500 [12:58<6:49:11, 50.62s/it]  3%|▎         | 16/500 [14:03<7:22:09, 54.81s/it]Epoch: 16 done with learning rate 1.44E-03, Train loss: 4.92E+06, Train scatter: [0.496  0.0848 0.5439 0.5557]
L1 regularization loss: 6.31E-01, L2 regularization loss: 1.48E-01
Test scatter: [0.4899 0.0841 0.5354 0.5533], Lowest was [0.4899 0.0841 0.5354 0.5533]
Median for last 10 epochs: [0.6687 0.0905 0.5354 0.6278], Epochs since improvement 0
  3%|▎         | 17/500 [14:44<6:49:26, 50.86s/it]  4%|▎         | 18/500 [15:49<7:21:02, 54.90s/it]Epoch: 18 done with learning rate 1.70E-03, Train loss: 4.59E+06, Train scatter: [0.4632 0.086  0.5439 0.5537]
L1 regularization loss: 6.35E-01, L2 regularization loss: 1.51E-01
Test scatter: [0.4562 0.0857 0.5354 0.5468], Lowest was [0.4562 0.0841 0.5354 0.5468]
Median for last 10 epochs: [0.617  0.0885 0.5354 0.6098], Epochs since improvement 0
  4%|▍         | 19/500 [16:29<6:45:41, 50.61s/it]Epoch: 20 done with learning rate 1.99E-03, Train loss: 4.19E+06, Train scatter: [0.3064 0.082  0.5439 0.5496]
L1 regularization loss: 6.41E-01, L2 regularization loss: 1.55E-01
Test scatter: [0.3087 0.0818 0.5353 0.5432], Lowest was [0.3087 0.0818 0.5353 0.5432]
Median for last 10 epochs: [0.4899 0.0857 0.5354 0.5533], Epochs since improvement 0
  4%|▍         | 20/500 [17:39<7:30:56, 56.37s/it]  4%|▍         | 21/500 [18:20<6:53:24, 51.78s/it]  4%|▍         | 22/500 [19:24<7:21:00, 55.36s/it]Epoch: 22 done with learning rate 2.30E-03, Train loss: 3.81E+06, Train scatter: [0.2622 0.0773 0.5439 0.5281]
L1 regularization loss: 6.44E-01, L2 regularization loss: 1.56E-01
Test scatter: [0.2622 0.0774 0.5354 0.5238], Lowest was [0.2622 0.0774 0.5353 0.5238]
Median for last 10 epochs: [0.4562 0.0841 0.5354 0.5468], Epochs since improvement 0
  5%|▍         | 23/500 [20:04<6:44:16, 50.85s/it]  5%|▍         | 24/500 [21:10<7:19:05, 55.35s/it]Epoch: 24 done with learning rate 2.63E-03, Train loss: 3.71E+06, Train scatter: [0.2465 0.0746 0.5438 0.5212]
L1 regularization loss: 6.46E-01, L2 regularization loss: 1.57E-01
Test scatter: [0.2482 0.0752 0.5353 0.5172], Lowest was [0.2482 0.0752 0.5353 0.5172]
Median for last 10 epochs: [0.3087 0.0818 0.5354 0.5432], Epochs since improvement 0
  5%|▌         | 25/500 [21:51<6:44:25, 51.08s/it]  5%|▌         | 26/500 [22:55<7:13:13, 54.84s/it]Epoch: 26 done with learning rate 2.98E-03, Train loss: 3.64E+06, Train scatter: [0.3138 0.0724 0.5438 0.5521]
L1 regularization loss: 6.48E-01, L2 regularization loss: 1.58E-01
Test scatter: [0.3126 0.073  0.5353 0.5495], Lowest was [0.2482 0.073  0.5353 0.5172]
Median for last 10 epochs: [0.3087 0.0774 0.5353 0.5432], Epochs since improvement 0
  5%|▌         | 27/500 [23:37<6:41:25, 50.92s/it]  6%|▌         | 28/500 [24:41<7:12:32, 54.98s/it]Epoch: 28 done with learning rate 3.34E-03, Train loss: 3.58E+06, Train scatter: [0.2969 0.072  0.5438 0.5268]
L1 regularization loss: 6.52E-01, L2 regularization loss: 1.60E-01
Test scatter: [0.2984 0.0727 0.5352 0.527 ], Lowest was [0.2482 0.0727 0.5352 0.5172]
Median for last 10 epochs: [0.2984 0.0752 0.5353 0.527 ], Epochs since improvement 0
  6%|▌         | 29/500 [25:23<6:40:30, 51.02s/it]Epoch: 30 done with learning rate 3.72E-03, Train loss: 3.55E+06, Train scatter: [0.2632 0.0732 0.5437 0.5189]
L1 regularization loss: 6.54E-01, L2 regularization loss: 1.61E-01
Test scatter: [0.2636 0.0739 0.5352 0.5169], Lowest was [0.2482 0.0727 0.5352 0.5169]
Median for last 10 epochs: [0.2636 0.0739 0.5353 0.5238], Epochs since improvement 0
  6%|▌         | 30/500 [26:32<7:22:58, 56.55s/it]  6%|▌         | 31/500 [27:13<6:44:30, 51.75s/it]  6%|▋         | 32/500 [28:17<7:12:41, 55.47s/it]Epoch: 32 done with learning rate 4.10E-03, Train loss: 3.54E+06, Train scatter: [0.2324 0.0727 0.5436 0.5105]
L1 regularization loss: 6.58E-01, L2 regularization loss: 1.63E-01
Test scatter: [0.2349 0.0725 0.5351 0.5089], Lowest was [0.2349 0.0725 0.5351 0.5089]
Median for last 10 epochs: [0.2636 0.073  0.5352 0.5172], Epochs since improvement 0
  7%|▋         | 33/500 [28:58<6:38:46, 51.24s/it]  7%|▋         | 34/500 [30:03<7:08:29, 55.17s/it]Epoch: 34 done with learning rate 4.50E-03, Train loss: 3.52E+06, Train scatter: [0.2093 0.0712 0.5436 0.5087]
L1 regularization loss: 6.62E-01, L2 regularization loss: 1.65E-01
Test scatter: [0.217  0.0712 0.5351 0.5011], Lowest was [0.217  0.0712 0.5351 0.5011]
Median for last 10 epochs: [0.2636 0.0727 0.5352 0.5169], Epochs since improvement 0
  7%|▋         | 35/500 [30:43<6:34:14, 50.87s/it]  7%|▋         | 36/500 [31:48<7:04:13, 54.86s/it]Epoch: 36 done with learning rate 4.90E-03, Train loss: 3.48E+06, Train scatter: [0.2183 0.0703 0.5436 0.5037]
L1 regularization loss: 6.65E-01, L2 regularization loss: 1.67E-01
Test scatter: [0.2228 0.0702 0.535  0.4994], Lowest was [0.217  0.0702 0.535  0.4994]
Median for last 10 epochs: [0.2349 0.0725 0.5351 0.5089], Epochs since improvement 0
  7%|▋         | 37/500 [32:29<6:31:25, 50.72s/it]  8%|▊         | 38/500 [33:33<7:02:15, 54.84s/it]Epoch: 38 done with learning rate 5.30E-03, Train loss: 3.45E+06, Train scatter: [0.2104 0.0697 0.5435 0.5087]
L1 regularization loss: 6.69E-01, L2 regularization loss: 1.68E-01
Test scatter: [0.218  0.0697 0.535  0.509 ], Lowest was [0.217  0.0697 0.535  0.4994]
Median for last 10 epochs: [0.2228 0.0712 0.5351 0.5089], Epochs since improvement 0
  8%|▊         | 39/500 [34:14<6:28:41, 50.59s/it]Epoch: 40 done with learning rate 5.70E-03, Train loss: 3.54E+06, Train scatter: [0.3821 0.0684 0.5435 0.5007]
L1 regularization loss: 6.76E-01, L2 regularization loss: 1.71E-01
Test scatter: [0.3774 0.0682 0.535  0.4947], Lowest was [0.217  0.0682 0.535  0.4947]
Median for last 10 epochs: [0.2228 0.0702 0.535  0.5011], Epochs since improvement 0
  8%|▊         | 40/500 [35:25<7:15:44, 56.83s/it]  8%|▊         | 41/500 [36:07<6:39:31, 52.22s/it]  8%|▊         | 42/500 [37:11<7:06:06, 55.82s/it]Epoch: 42 done with learning rate 6.10E-03, Train loss: 3.42E+06, Train scatter: [0.2047 0.0682 0.5435 0.5137]
L1 regularization loss: 6.88E-01, L2 regularization loss: 1.74E-01
Test scatter: [0.215  0.0686 0.535  0.514 ], Lowest was [0.215  0.0682 0.535  0.4947]
Median for last 10 epochs: [0.218  0.0697 0.535  0.5011], Epochs since improvement 0
  9%|▊         | 43/500 [37:52<6:30:32, 51.27s/it]  9%|▉         | 44/500 [38:56<7:00:38, 55.35s/it]Epoch: 44 done with learning rate 6.49E-03, Train loss: 3.40E+06, Train scatter: [0.2251 0.0696 0.5434 0.5266]
L1 regularization loss: 6.93E-01, L2 regularization loss: 1.77E-01
Test scatter: [0.2362 0.0697 0.5349 0.531 ], Lowest was [0.215  0.0682 0.5349 0.4947]
Median for last 10 epochs: [0.2228 0.0697 0.535  0.509 ], Epochs since improvement 0
  9%|▉         | 45/500 [39:38<6:28:12, 51.19s/it]  9%|▉         | 46/500 [40:43<6:59:44, 55.47s/it]Epoch: 46 done with learning rate 6.87E-03, Train loss: 3.40E+06, Train scatter: [0.2183 0.0688 0.5433 0.5003]
L1 regularization loss: 6.96E-01, L2 regularization loss: 1.79E-01
Test scatter: [0.2254 0.0684 0.5348 0.4978], Lowest was [0.215  0.0682 0.5348 0.4947]
Median for last 10 epochs: [0.2254 0.0686 0.535  0.509 ], Epochs since improvement 0
  9%|▉         | 47/500 [41:25<6:27:26, 51.32s/it] 10%|▉         | 48/500 [42:29<6:55:16, 55.12s/it]Epoch: 48 done with learning rate 7.24E-03, Train loss: 3.44E+06, Train scatter: [0.3605 0.0678 0.5433 0.4981]
L1 regularization loss: 7.01E-01, L2 regularization loss: 1.82E-01
Test scatter: [0.3577 0.0676 0.5348 0.494 ], Lowest was [0.215  0.0676 0.5348 0.494 ]
Median for last 10 epochs: [0.2362 0.0684 0.5349 0.4978], Epochs since improvement 0
 10%|▉         | 49/500 [43:10<6:22:33, 50.89s/it]Epoch: 50 done with learning rate 7.60E-03, Train loss: 3.43E+06, Train scatter: [0.4263 0.0703 0.5432 0.5202]
L1 regularization loss: 7.07E-01, L2 regularization loss: 1.85E-01
Test scatter: [0.4219 0.0705 0.5347 0.5166], Lowest was [0.215  0.0676 0.5347 0.494 ]
Median for last 10 epochs: [0.2362 0.0686 0.5348 0.514 ], Epochs since improvement 0
 10%|█         | 50/500 [44:21<7:06:55, 56.92s/it] 10%|█         | 51/500 [45:02<6:30:01, 52.12s/it] 10%|█         | 52/500 [46:05<6:54:08, 55.46s/it]Epoch: 52 done with learning rate 7.94E-03, Train loss: 3.39E+06, Train scatter: [0.3978 0.0752 0.5432 0.5051]
L1 regularization loss: 7.21E-01, L2 regularization loss: 1.91E-01
Test scatter: [0.3908 0.0743 0.5346 0.4992], Lowest was [0.215  0.0676 0.5346 0.494 ]
Median for last 10 epochs: [0.3577 0.0697 0.5348 0.4992], Epochs since improvement 0
 11%|█         | 53/500 [46:46<6:21:30, 51.21s/it] 11%|█         | 54/500 [47:50<6:49:01, 55.03s/it]Epoch: 54 done with learning rate 8.26E-03, Train loss: 3.35E+06, Train scatter: [0.2399 0.0713 0.543  0.5216]
L1 regularization loss: 7.21E-01, L2 regularization loss: 1.91E-01
Test scatter: [0.2451 0.0717 0.5345 0.5181], Lowest was [0.215  0.0676 0.5345 0.494 ]
Median for last 10 epochs: [0.3577 0.0705 0.5347 0.4992], Epochs since improvement 0
 11%|█         | 55/500 [48:32<6:18:10, 50.99s/it] 11%|█         | 56/500 [49:36<6:47:02, 55.01s/it]Epoch: 56 done with learning rate 8.56E-03, Train loss: 3.34E+06, Train scatter: [0.2603 0.0718 0.5431 0.5445]
L1 regularization loss: 7.24E-01, L2 regularization loss: 1.93E-01
Test scatter: [0.2563 0.0708 0.5346 0.5454], Lowest was [0.215  0.0676 0.5345 0.494 ]
Median for last 10 epochs: [0.3577 0.0708 0.5346 0.5166], Epochs since improvement 2
 11%|█▏        | 57/500 [50:17<6:15:08, 50.81s/it] 12%|█▏        | 58/500 [51:21<6:43:00, 54.71s/it]Epoch: 58 done with learning rate 8.83E-03, Train loss: 3.34E+06, Train scatter: [0.2548 0.0723 0.5429 0.5236]
L1 regularization loss: 7.30E-01, L2 regularization loss: 1.97E-01
Test scatter: [0.2649 0.0718 0.5343 0.522 ], Lowest was [0.215  0.0676 0.5343 0.494 ]
Median for last 10 epochs: [0.2649 0.0717 0.5346 0.5181], Epochs since improvement 0
 12%|█▏        | 59/500 [52:02<6:11:56, 50.60s/it]Epoch: 60 done with learning rate 9.08E-03, Train loss: 3.35E+06, Train scatter: [0.2222 0.0702 0.5429 0.5022]
L1 regularization loss: 7.35E-01, L2 regularization loss: 2.01E-01
Test scatter: [0.2302 0.0703 0.5344 0.4986], Lowest was [0.215  0.0676 0.5343 0.494 ]
Median for last 10 epochs: [0.2563 0.0717 0.5345 0.5181], Epochs since improvement 2
 12%|█▏        | 60/500 [53:12<6:54:17, 56.49s/it] 12%|█▏        | 61/500 [53:53<6:18:51, 51.78s/it] 12%|█▏        | 62/500 [54:58<6:45:46, 55.58s/it]Epoch: 62 done with learning rate 9.31E-03, Train loss: 3.36E+06, Train scatter: [0.3827 0.0889 0.5428 0.5076]
L1 regularization loss: 7.42E-01, L2 regularization loss: 2.05E-01
Test scatter: [0.369  0.0865 0.5342 0.5025], Lowest was [0.215  0.0676 0.5342 0.494 ]
Median for last 10 epochs: [0.2563 0.0717 0.5344 0.5181], Epochs since improvement 0
 13%|█▎        | 63/500 [55:38<6:12:01, 51.08s/it] 13%|█▎        | 64/500 [56:43<6:39:56, 55.04s/it]Epoch: 64 done with learning rate 9.50E-03, Train loss: 3.36E+06, Train scatter: [0.5178 0.0702 0.5426 0.5472]
L1 regularization loss: 7.48E-01, L2 regularization loss: 2.08E-01
Test scatter: [0.5057 0.0702 0.5341 0.5447], Lowest was [0.215  0.0676 0.5341 0.494 ]
Median for last 10 epochs: [0.2649 0.0708 0.5343 0.522 ], Epochs since improvement 0
 13%|█▎        | 65/500 [57:23<6:07:11, 50.65s/it] 13%|█▎        | 66/500 [58:28<6:37:13, 54.92s/it]Epoch: 66 done with learning rate 9.66E-03, Train loss: 3.33E+06, Train scatter: [0.2794 0.0705 0.5425 0.5342]
L1 regularization loss: 7.54E-01, L2 regularization loss: 2.12E-01
Test scatter: [0.2821 0.0705 0.534  0.5305], Lowest was [0.215  0.0676 0.534  0.494 ]
Median for last 10 epochs: [0.2821 0.0705 0.5342 0.522 ], Epochs since improvement 0
 13%|█▎        | 67/500 [59:09<6:06:00, 50.72s/it] 14%|█▎        | 68/500 [1:00:12<6:31:29, 54.37s/it]Epoch: 68 done with learning rate 9.80E-03, Train loss: 3.32E+06, Train scatter: [0.2132 0.067  0.5425 0.5005]
L1 regularization loss: 7.60E-01, L2 regularization loss: 2.15E-01
Test scatter: [0.2211 0.0675 0.534  0.5025], Lowest was [0.215  0.0675 0.534  0.494 ]
Median for last 10 epochs: [0.2821 0.0703 0.5341 0.5025], Epochs since improvement 0
 14%|█▍        | 69/500 [1:00:53<6:02:39, 50.49s/it]Epoch: 70 done with learning rate 9.90E-03, Train loss: 3.32E+06, Train scatter: [0.7555 0.0688 0.5423 0.4919]
L1 regularization loss: 7.67E-01, L2 regularization loss: 2.21E-01
Test scatter: [0.728  0.0684 0.5338 0.4873], Lowest was [0.215  0.0675 0.5338 0.4873]
Median for last 10 epochs: [0.369  0.0702 0.534  0.5025], Epochs since improvement 0
 14%|█▍        | 70/500 [1:02:03<6:43:39, 56.32s/it] 14%|█▍        | 71/500 [1:02:45<6:11:40, 51.98s/it] 14%|█▍        | 72/500 [1:03:48<6:34:29, 55.30s/it]Epoch: 72 done with learning rate 9.96E-03, Train loss: 3.32E+06, Train scatter: [0.256  0.0956 0.5421 0.5173]
L1 regularization loss: 7.82E-01, L2 regularization loss: 2.30E-01
Test scatter: [0.2578 0.1002 0.5335 0.5117], Lowest was [0.215  0.0675 0.5335 0.4873]
Median for last 10 epochs: [0.2821 0.0702 0.534  0.5117], Epochs since improvement 0
 15%|█▍        | 73/500 [1:04:29<6:02:15, 50.90s/it] 15%|█▍        | 74/500 [1:05:32<6:28:32, 54.72s/it]Epoch: 74 done with learning rate 1.00E-02, Train loss: 3.25E+06, Train scatter: [0.241  0.0652 0.542  0.5071]
L1 regularization loss: 7.98E-01, L2 regularization loss: 2.39E-01
Test scatter: [0.2443 0.0653 0.5336 0.5079], Lowest was [0.215  0.0653 0.5335 0.4873]
Median for last 10 epochs: [0.2578 0.0684 0.5338 0.5079], Epochs since improvement 0
 15%|█▌        | 75/500 [1:06:14<6:00:21, 50.87s/it] 15%|█▌        | 76/500 [1:07:18<6:26:33, 54.70s/it]Epoch: 76 done with learning rate 1.00E-02, Train loss: 3.20E+06, Train scatter: [0.4137 0.0665 0.5417 0.4918]
L1 regularization loss: 8.05E-01, L2 regularization loss: 2.45E-01
Test scatter: [0.413  0.0677 0.5333 0.489 ], Lowest was [0.215  0.0653 0.5333 0.4873]
Median for last 10 epochs: [0.2578 0.0677 0.5336 0.5025], Epochs since improvement 0
 15%|█▌        | 77/500 [1:08:00<5:58:34, 50.86s/it] 16%|█▌        | 78/500 [1:09:03<6:23:18, 54.50s/it]Epoch: 78 done with learning rate 1.00E-02, Train loss: 3.16E+06, Train scatter: [0.2201 0.0592 0.5415 0.4871]
L1 regularization loss: 8.08E-01, L2 regularization loss: 2.48E-01
Test scatter: [0.228  0.0596 0.533  0.4835], Lowest was [0.215  0.0596 0.533  0.4835]
Median for last 10 epochs: [0.2578 0.0677 0.5335 0.489 ], Epochs since improvement 0
 16%|█▌        | 79/500 [1:09:43<5:52:31, 50.24s/it]Epoch: 80 done with learning rate 1.00E-02, Train loss: 3.20E+06, Train scatter: [0.2738 0.0753 0.5413 0.5029]
L1 regularization loss: 8.16E-01, L2 regularization loss: 2.54E-01
Test scatter: [0.2847 0.0753 0.5328 0.5031], Lowest was [0.215  0.0596 0.5328 0.4835]
Median for last 10 epochs: [0.2578 0.0677 0.5333 0.5031], Epochs since improvement 0
 16%|█▌        | 80/500 [1:10:53<6:33:20, 56.19s/it] 16%|█▌        | 81/500 [1:11:34<6:01:36, 51.78s/it] 16%|█▋        | 82/500 [1:12:39<6:26:41, 55.51s/it]Epoch: 82 done with learning rate 9.99E-03, Train loss: 3.13E+06, Train scatter: [0.4484 0.096  0.5399 0.7041]
L1 regularization loss: 8.22E-01, L2 regularization loss: 2.59E-01
Test scatter: [0.4379 0.0936 0.5316 0.7056], Lowest was [0.215  0.0596 0.5316 0.4835]
Median for last 10 epochs: [0.2847 0.0677 0.533  0.5031], Epochs since improvement 0
 17%|█▋        | 83/500 [1:13:19<5:54:18, 50.98s/it] 17%|█▋        | 84/500 [1:14:23<6:19:40, 54.76s/it]Epoch: 84 done with learning rate 9.99E-03, Train loss: 3.32E+06, Train scatter: [0.921  0.1932 0.5436 0.97  ]
L1 regularization loss: 8.66E-01, L2 regularization loss: 2.83E-01
Test scatter: [0.9059 0.189  0.535  0.9594], Lowest was [0.215  0.0596 0.5316 0.4835]
Median for last 10 epochs: [0.413  0.0753 0.533  0.5031], Epochs since improvement 2
 17%|█▋        | 85/500 [1:15:03<5:48:46, 50.43s/it] 17%|█▋        | 86/500 [1:16:07<6:15:43, 54.45s/it]Epoch: 86 done with learning rate 9.98E-03, Train loss: 3.21E+06, Train scatter: [0.9071 0.1058 0.5437 0.9119]
L1 regularization loss: 8.97E-01, L2 regularization loss: 3.09E-01
Test scatter: [0.8935 0.1054 0.5351 0.9077], Lowest was [0.215  0.0596 0.5316 0.4835]
Median for last 10 epochs: [0.4379 0.0936 0.533  0.7056], Epochs since improvement 4
 17%|█▋        | 87/500 [1:16:47<5:45:47, 50.24s/it] 18%|█▊        | 88/500 [1:17:51<6:13:11, 54.35s/it]Epoch: 88 done with learning rate 9.98E-03, Train loss: 3.13E+06, Train scatter: [0.8695 0.113  0.5436 0.6394]
L1 regularization loss: 9.06E-01, L2 regularization loss: 3.30E-01
Test scatter: [0.8586 0.1126 0.5351 0.6362], Lowest was [0.215  0.0596 0.5316 0.4835]
Median for last 10 epochs: [0.8586 0.1054 0.535  0.7056], Epochs since improvement 6
 18%|█▊        | 89/500 [1:18:33<5:46:21, 50.56s/it]Epoch: 90 done with learning rate 9.97E-03, Train loss: 3.08E+06, Train scatter: [0.5139 0.0852 0.5433 0.5495]
L1 regularization loss: 9.09E-01, L2 regularization loss: 3.39E-01
Test scatter: [0.5025 0.0846 0.5347 0.5453], Lowest was [0.215  0.0596 0.5316 0.4835]
Median for last 10 epochs: [0.8586 0.1054 0.535  0.7056], Epochs since improvement 8
 18%|█▊        | 90/500 [1:19:42<6:23:21, 56.10s/it] 18%|█▊        | 91/500 [1:20:23<5:50:52, 51.47s/it] 18%|█▊        | 92/500 [1:21:26<6:15:15, 55.18s/it]Epoch: 92 done with learning rate 9.96E-03, Train loss: 3.03E+06, Train scatter: [0.436  0.0877 0.5424 0.544 ]
L1 regularization loss: 9.09E-01, L2 regularization loss: 3.46E-01
Test scatter: [0.4267 0.0871 0.5339 0.5378], Lowest was [0.215  0.0596 0.5316 0.4835]
Median for last 10 epochs: [0.8586 0.1054 0.535  0.6362], Epochs since improvement 10
 19%|█▊        | 93/500 [1:22:08<5:46:29, 51.08s/it] 19%|█▉        | 94/500 [1:23:12<6:11:43, 54.94s/it]Epoch: 94 done with learning rate 9.95E-03, Train loss: 3.01E+06, Train scatter: [0.6317 0.0856 0.5409 0.5766]
L1 regularization loss: 9.11E-01, L2 regularization loss: 3.52E-01
Test scatter: [0.6153 0.0864 0.5324 0.5793], Lowest was [0.215  0.0596 0.5316 0.4835]
Median for last 10 epochs: [0.6153 0.0871 0.5347 0.5793], Epochs since improvement 12
 19%|█▉        | 95/500 [1:23:52<5:41:29, 50.59s/it] 19%|█▉        | 96/500 [1:24:56<6:06:53, 54.49s/it]Epoch: 96 done with learning rate 9.94E-03, Train loss: 2.99E+06, Train scatter: [0.4616 0.104  0.5403 0.5411]
L1 regularization loss: 9.11E-01, L2 regularization loss: 3.57E-01
Test scatter: [0.4527 0.1026 0.5319 0.536 ], Lowest was [0.215  0.0596 0.5316 0.4835]
Median for last 10 epochs: [0.5025 0.0871 0.5339 0.5453], Epochs since improvement 14
 19%|█▉        | 97/500 [1:25:38<5:40:04, 50.63s/it] 20%|█▉        | 98/500 [1:26:42<6:07:42, 54.88s/it]Epoch: 98 done with learning rate 9.93E-03, Train loss: 2.97E+06, Train scatter: [0.4548 0.077  0.5376 0.5168]
L1 regularization loss: 9.14E-01, L2 regularization loss: 3.64E-01
Test scatter: [0.4442 0.0768 0.5293 0.5117], Lowest was [0.215  0.0596 0.5293 0.4835]
Median for last 10 epochs: [0.4527 0.0864 0.5324 0.5378], Epochs since improvement 0
 20%|█▉        | 99/500 [1:27:23<5:37:49, 50.55s/it]Epoch: 100 done with learning rate 9.91E-03, Train loss: 2.95E+06, Train scatter: [0.45   0.0769 0.5357 0.5212]
L1 regularization loss: 9.15E-01, L2 regularization loss: 3.70E-01
Test scatter: [0.4355 0.0778 0.5274 0.5192], Lowest was [0.215  0.0596 0.5274 0.4835]
Median for last 10 epochs: [0.4442 0.0864 0.5319 0.536 ], Epochs since improvement 0
 20%|██        | 100/500 [1:28:34<6:18:19, 56.75s/it] 20%|██        | 101/500 [1:29:15<5:45:50, 52.01s/it] 20%|██        | 102/500 [1:30:18<6:07:37, 55.42s/it]Epoch: 102 done with learning rate 9.90E-03, Train loss: 2.95E+06, Train scatter: [0.4034 0.1081 0.4716 0.5849]
L1 regularization loss: 9.50E-01, L2 regularization loss: 3.96E-01
Test scatter: [0.398  0.1045 0.4665 0.5802], Lowest was [0.215  0.0596 0.4665 0.4835]
Median for last 10 epochs: [0.4442 0.0864 0.5293 0.536 ], Epochs since improvement 0
 21%|██        | 103/500 [1:30:59<5:37:00, 50.93s/it] 21%|██        | 104/500 [1:32:03<6:01:45, 54.81s/it]Epoch: 104 done with learning rate 9.89E-03, Train loss: 3.35E+06, Train scatter: [0.9246 0.1705 0.5436 0.995 ]
L1 regularization loss: 1.11E+00, L2 regularization loss: 4.91E-01
Test scatter: [0.9093 0.1667 0.535  0.9847], Lowest was [0.215  0.0596 0.4665 0.4835]
Median for last 10 epochs: [0.4442 0.1026 0.5293 0.536 ], Epochs since improvement 2
 21%|██        | 105/500 [1:32:43<5:33:00, 50.58s/it] 21%|██        | 106/500 [1:33:48<6:00:24, 54.88s/it]Epoch: 106 done with learning rate 9.87E-03, Train loss: 3.28E+06, Train scatter: [0.9017 0.1292 0.5434 0.993 ]
L1 regularization loss: 1.11E+00, L2 regularization loss: 4.93E-01
Test scatter: [0.8871 0.1265 0.5348 0.9827], Lowest was [0.215  0.0596 0.4665 0.4835]
Median for last 10 epochs: [0.4442 0.1045 0.5293 0.5802], Epochs since improvement 4
 21%|██▏       | 107/500 [1:34:29<5:32:34, 50.78s/it] 22%|██▏       | 108/500 [1:35:33<5:57:35, 54.73s/it]Epoch: 108 done with learning rate 9.85E-03, Train loss: 3.24E+06, Train scatter: [0.8948 0.1166 0.5431 0.982 ]
L1 regularization loss: 1.11E+00, L2 regularization loss: 4.96E-01
Test scatter: [0.8805 0.1142 0.5345 0.972 ], Lowest was [0.215  0.0596 0.4665 0.4835]
Median for last 10 epochs: [0.8805 0.1142 0.5345 0.972 ], Epochs since improvement 6
 22%|██▏       | 109/500 [1:36:15<5:31:06, 50.81s/it]Epoch: 110 done with learning rate 9.83E-03, Train loss: 3.18E+06, Train scatter: [0.6124 0.1008 0.5422 0.7298]
L1 regularization loss: 1.12E+00, L2 regularization loss: 5.13E-01
Test scatter: [0.598  0.0989 0.5337 0.718 ], Lowest was [0.215  0.0596 0.4665 0.4835]
Median for last 10 epochs: [0.8805 0.1142 0.5345 0.972 ], Epochs since improvement 8
 22%|██▏       | 110/500 [1:37:26<6:09:27, 56.84s/it] 22%|██▏       | 111/500 [1:38:08<5:38:43, 52.25s/it] 22%|██▏       | 112/500 [1:39:15<6:08:19, 56.96s/it]Epoch: 112 done with learning rate 9.81E-03, Train loss: 3.44E+06, Train scatter: [0.9179 0.1555 0.5433 0.9533]
L1 regularization loss: 1.18E+00, L2 regularization loss: 5.61E-01
Test scatter: [0.9029 0.1524 0.5347 0.945 ], Lowest was [0.215  0.0596 0.4665 0.4835]
Median for last 10 epochs: [0.8871 0.1265 0.5347 0.972 ], Epochs since improvement 10
 23%|██▎       | 113/500 [1:39:58<5:39:19, 52.61s/it] 23%|██▎       | 114/500 [1:41:05<6:05:54, 56.88s/it]Epoch: 114 done with learning rate 9.79E-03, Train loss: 3.19E+06, Train scatter: [0.9142 0.1137 0.5425 0.7455]
L1 regularization loss: 1.18E+00, L2 regularization loss: 5.77E-01
Test scatter: [0.8993 0.1126 0.534  0.7404], Lowest was [0.215  0.0596 0.4665 0.4835]
Median for last 10 epochs: [0.8871 0.1142 0.5345 0.945 ], Epochs since improvement 12
 23%|██▎       | 115/500 [1:41:47<5:36:27, 52.43s/it] 23%|██▎       | 116/500 [1:42:52<6:00:18, 56.30s/it]Epoch: 116 done with learning rate 9.77E-03, Train loss: 3.16E+06, Train scatter: [0.8947 0.1123 0.5408 0.7364]
L1 regularization loss: 1.18E+00, L2 regularization loss: 5.89E-01
Test scatter: [0.8808 0.1118 0.5323 0.735 ], Lowest was [0.215  0.0596 0.4665 0.4835]
Median for last 10 epochs: [0.8808 0.1126 0.534  0.7404], Epochs since improvement 14
 23%|██▎       | 117/500 [1:43:34<5:32:31, 52.09s/it] 24%|██▎       | 118/500 [1:44:43<6:03:13, 57.05s/it]Epoch: 118 done with learning rate 9.75E-03, Train loss: 3.19E+06, Train scatter: [0.88   0.1228 0.54   0.7183]
L1 regularization loss: 1.18E+00, L2 regularization loss: 6.14E-01
Test scatter: [0.8668 0.1218 0.5316 0.7183], Lowest was [0.215  0.0596 0.4665 0.4835]
Median for last 10 epochs: [0.8808 0.1126 0.5337 0.735 ], Epochs since improvement 16
 24%|██▍       | 119/500 [1:45:26<5:35:09, 52.78s/it]Epoch: 120 done with learning rate 9.73E-03, Train loss: 3.10E+06, Train scatter: [0.8121 0.1098 0.5376 0.6804]
L1 regularization loss: 1.19E+00, L2 regularization loss: 6.65E-01
Test scatter: [0.8    0.1106 0.5292 0.6843], Lowest was [0.215  0.0596 0.4665 0.4835]
Median for last 10 epochs: [0.8808 0.1126 0.5323 0.735 ], Epochs since improvement 18
 24%|██▍       | 120/500 [1:46:38<6:10:59, 58.58s/it] 24%|██▍       | 121/500 [1:47:19<5:36:22, 53.25s/it] 24%|██▍       | 122/500 [1:48:23<5:55:55, 56.49s/it]Epoch: 122 done with learning rate 9.70E-03, Train loss: 3.13E+06, Train scatter: [0.6716 0.1287 0.4859 0.7242]
L1 regularization loss: 1.20E+00, L2 regularization loss: 7.01E-01
Test scatter: [0.6653 0.1272 0.4784 0.7106], Lowest was [0.215  0.0596 0.4665 0.4835]
Median for last 10 epochs: [0.8668 0.1126 0.5316 0.7183], Epochs since improvement 20
 25%|██▍       | 123/500 [1:49:04<5:26:36, 51.98s/it] 25%|██▍       | 123/500 [1:50:09<5:37:38, 53.74s/it]
Epoch: 124 done with learning rate 9.68E-03, Train loss: 3.08E+06, Train scatter: [0.8797 0.0951 0.5372 0.6295]
L1 regularization loss: 1.21E+00, L2 regularization loss: 7.20E-01
Test scatter: [0.8664 0.0956 0.5287 0.6305], Lowest was [0.215  0.0596 0.4665 0.4835]
Median for last 10 epochs: [0.8664 0.1118 0.5292 0.7106], Epochs since improvement 22
Exited after 124 epochs due to early stopping
6609.69 seconds spent training, 13.219 seconds per epoch. Processed 5268 trees per second
[0.8663791  0.09557848 0.5287297  0.6304481 ]
{'epoch_exit': 123, 'scatter_m_star': 0.8663791, 'lowest_m_star': 0.21495467, 'last20_m_star': 0.87365246, 'last10_m_star': 0.8664042, 'scatter_v_disk': 0.095578484, 'lowest_v_disk': 0.05962715, 'last20_v_disk': 0.11337687, 'last10_v_disk': 0.111764014, 'scatter_m_cold': 0.5287297, 'lowest_m_cold': 0.46647954, 'last20_m_cold': 0.5330188, 'last10_m_cold': 0.5291818, 'scatter_sfr_100': 0.6304481, 'lowest_sfr_100': 0.48354593, 'last20_sfr_100': 0.7266145, 'last10_sfr_100': 0.71058196}
Finished 1/1
Exited training after None epochs
Experiment 4 done: 4 / 6
Exploring ['conv_layers', 'batch_size']
Currently doing [  3 256]
{'experiment': 'GraphMerge', 'group': 'meta_pysr', 'move': False, 'model': 'MetaEdge', 'log': True, 'run_params': {'n_epochs': 500, 'n_trials': 1, 'batch_size': 256, 'val_epoch': 2, 'early_stopping': True, 'patience': 20, 'l1_lambda': 3e-05, 'l2_lambda': 3e-05, 'loss_func': 'GaussNd', 'metrics': 'test_multi_varrho', 'performance_plot': 'multi_base', 'save': True, 'seed': True, 'num_workers': 4}, 'learn_params': {'learning_rate': 0.01, 'schedule': 'onecycle', 'g_up': 1, 'g_down': 0.95, 'warmup': 4, 'period': 5, 'eta_min': 1e-05}, 'hyper_params': {'hidden_channels': 128, 'conv_layers': 3, 'conv_activation': 'relu', 'decode_activation': 'leakyrelu', 'decode_layers': 2, 'layernorm': True, 'agg': 'sum', 'variance': True, 'rho': 0, 'in_channels': 43, 'out_channels': 4}, 'data_params': {'case': 'vlarge_all_4t_z0.0_quantile_raw', 'targets': [0, 1, 2, 3], 'del_feats': [], 'split': 0.8, 'test': 0, 'scale': 0}, 'experiment_name': 'con3bat256'}
/home/cj1223/../../scratch/gpfs/cj1223/GraphResults/results_meta_pysr_120322/MetaEdge_vlarge_all_4t_z0.0_quantile_raw_oyjdwq
RelU conv activation
LeakyRelU decode activation
N_params 546504
Made folder for saving model
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [01:01<8:35:28, 61.98s/it]  0%|          | 2/500 [02:33<11:00:19, 79.56s/it]Epoch: 2 done with learning rate 4.17E-04, Train loss: 9.18E+07, Train scatter: [0.9352 0.1396 0.5441 0.9954]
L1 regularization loss: 7.37E-01, L2 regularization loss: 1.27E-01
Test scatter: [0.9196 0.1357 0.5355 0.9851], Lowest was [0.9196 0.1357 0.5355 0.9851]
Median for last 10 epochs: [0.9196 0.1357 0.5355 0.9851], Epochs since improvement 2
  1%|          | 3/500 [03:36<9:56:46, 72.05s/it]   1%|          | 4/500 [05:09<11:03:20, 80.24s/it]Epoch: 4 done with learning rate 4.67E-04, Train loss: 8.09E+07, Train scatter: [0.9334 0.1017 0.5438 0.9955]
L1 regularization loss: 7.41E-01, L2 regularization loss: 1.29E-01
Test scatter: [0.9177 0.1001 0.5352 0.9851], Lowest was [0.9177 0.1001 0.5352 0.9851]
Median for last 10 epochs: [0.9177 0.1001 0.5352 0.9851], Epochs since improvement 0
  1%|          | 5/500 [06:10<10:04:24, 73.26s/it]  1%|          | 6/500 [07:44<11:01:02, 80.29s/it]Epoch: 6 done with learning rate 5.51E-04, Train loss: 1.02E+08, Train scatter: [0.9349 0.174  0.5435 0.9954]
L1 regularization loss: 7.50E-01, L2 regularization loss: 1.32E-01
Test scatter: [0.9193 0.169  0.5349 0.9851], Lowest was [0.9177 0.1001 0.5349 0.9851]
Median for last 10 epochs: [0.9177 0.1001 0.5349 0.9851], Epochs since improvement 0
  1%|▏         | 7/500 [08:46<10:10:32, 74.31s/it]  2%|▏         | 8/500 [10:17<10:51:54, 79.50s/it]Epoch: 8 done with learning rate 6.67E-04, Train loss: 8.04E+07, Train scatter: [0.9341 0.137  0.5424 0.9954]
L1 regularization loss: 7.57E-01, L2 regularization loss: 1.37E-01
Test scatter: [0.9186 0.133  0.5339 0.9851], Lowest was [0.9177 0.1001 0.5339 0.9851]
Median for last 10 epochs: [0.9182 0.1165 0.5344 0.9851], Epochs since improvement 0
  2%|▏         | 9/500 [11:18<10:03:11, 73.71s/it]Epoch: 10 done with learning rate 8.15E-04, Train loss: 7.47E+07, Train scatter: [0.6992 0.1061 0.4807 0.9954]
L1 regularization loss: 7.67E-01, L2 regularization loss: 1.44E-01
Test scatter: [0.6963 0.105  0.4685 0.9851], Lowest was [0.6963 0.1001 0.4685 0.9851]
Median for last 10 epochs: [0.9177 0.105  0.5339 0.9851], Epochs since improvement 0
  2%|▏         | 10/500 [12:56<11:03:27, 81.24s/it]  2%|▏         | 11/500 [14:00<10:20:39, 76.16s/it]  2%|▏         | 12/500 [15:34<11:01:41, 81.36s/it]Epoch: 12 done with learning rate 9.94E-04, Train loss: 7.17E+07, Train scatter: [0.5347 0.0952 0.4147 0.9954]
L1 regularization loss: 7.70E-01, L2 regularization loss: 1.49E-01
Test scatter: [0.5347 0.0993 0.4188 0.985 ], Lowest was [0.5347 0.0993 0.4188 0.985 ]
Median for last 10 epochs: [0.9177 0.105  0.5339 0.9851], Epochs since improvement 0
  3%|▎         | 13/500 [16:35<10:10:23, 75.20s/it]  3%|▎         | 14/500 [18:08<10:52:44, 80.59s/it]Epoch: 14 done with learning rate 1.20E-03, Train loss: 7.04E+07, Train scatter: [0.5131 0.0879 0.3922 0.9954]
L1 regularization loss: 7.73E-01, L2 regularization loss: 1.51E-01
Test scatter: [0.5167 0.0902 0.3954 0.985 ], Lowest was [0.5167 0.0902 0.3954 0.985 ]
Median for last 10 epochs: [0.6963 0.105  0.4685 0.9851], Epochs since improvement 0
  3%|▎         | 15/500 [19:09<10:05:06, 74.86s/it]  3%|▎         | 16/500 [20:41<10:44:02, 79.84s/it]Epoch: 16 done with learning rate 1.44E-03, Train loss: 6.89E+07, Train scatter: [0.5089 0.0823 0.359  0.9954]
L1 regularization loss: 7.77E-01, L2 regularization loss: 1.54E-01
Test scatter: [0.5092 0.083  0.3573 0.9851], Lowest was [0.5092 0.083  0.3573 0.985 ]
Median for last 10 epochs: [0.5347 0.0993 0.4188 0.9851], Epochs since improvement 0
  3%|▎         | 17/500 [21:44<10:02:59, 74.91s/it]  4%|▎         | 18/500 [23:16<10:42:53, 80.03s/it]Epoch: 18 done with learning rate 1.70E-03, Train loss: 6.04E+07, Train scatter: [0.3873 0.0814 0.3263 0.9952]
L1 regularization loss: 7.80E-01, L2 regularization loss: 1.57E-01
Test scatter: [0.4091 0.0829 0.3329 0.9864], Lowest was [0.4091 0.0829 0.3329 0.985 ]
Median for last 10 epochs: [0.5167 0.0902 0.3954 0.9851], Epochs since improvement 0
  4%|▍         | 19/500 [24:20<10:02:51, 75.20s/it]Epoch: 20 done with learning rate 1.99E-03, Train loss: 6.62E+06, Train scatter: [0.3524 0.078  0.337  0.6305]
L1 regularization loss: 7.86E-01, L2 regularization loss: 1.61E-01
Test scatter: [0.3617 0.0782 0.3406 0.64  ], Lowest was [0.3617 0.0782 0.3329 0.64  ]
Median for last 10 epochs: [0.5092 0.083  0.3573 0.985 ], Epochs since improvement 0
  4%|▍         | 20/500 [26:01<11:02:41, 82.84s/it]  4%|▍         | 21/500 [27:02<10:10:57, 76.53s/it]  4%|▍         | 22/500 [28:33<10:43:48, 80.81s/it]Epoch: 22 done with learning rate 2.30E-03, Train loss: 3.95E+06, Train scatter: [0.3251 0.0744 0.3245 0.5686]
L1 regularization loss: 7.91E-01, L2 regularization loss: 1.64E-01
Test scatter: [0.3314 0.0739 0.3297 0.5727], Lowest was [0.3314 0.0739 0.3297 0.5727]
Median for last 10 epochs: [0.4091 0.0829 0.3406 0.985 ], Epochs since improvement 0
  5%|▍         | 23/500 [29:36<9:59:10, 75.37s/it]   5%|▍         | 24/500 [31:07<10:35:45, 80.14s/it]Epoch: 24 done with learning rate 2.63E-03, Train loss: 3.56E+06, Train scatter: [0.2999 0.0714 0.3206 0.518 ]
L1 regularization loss: 7.97E-01, L2 regularization loss: 1.67E-01
Test scatter: [0.3028 0.072  0.3314 0.52  ], Lowest was [0.3028 0.072  0.3297 0.52  ]
Median for last 10 epochs: [0.3617 0.0782 0.3329 0.64  ], Epochs since improvement 0
  5%|▌         | 25/500 [32:10<9:53:07, 74.92s/it]   5%|▌         | 26/500 [33:42<10:32:29, 80.06s/it]Epoch: 26 done with learning rate 2.98E-03, Train loss: 3.28E+06, Train scatter: [0.3109 0.0731 0.317  0.5841]
L1 regularization loss: 8.03E-01, L2 regularization loss: 1.69E-01
Test scatter: [0.3199 0.0744 0.3225 0.5928], Lowest was [0.3028 0.072  0.3225 0.52  ]
Median for last 10 epochs: [0.3314 0.0744 0.3314 0.5928], Epochs since improvement 0
  5%|▌         | 27/500 [34:43<9:47:04, 74.47s/it]   6%|▌         | 28/500 [36:16<10:28:23, 79.88s/it]Epoch: 28 done with learning rate 3.34E-03, Train loss: 3.18E+06, Train scatter: [0.2925 0.0683 0.3013 0.5041]
L1 regularization loss: 8.10E-01, L2 regularization loss: 1.73E-01
Test scatter: [0.2961 0.0693 0.305  0.5062], Lowest was [0.2961 0.0693 0.305  0.5062]
Median for last 10 epochs: [0.3199 0.0739 0.3297 0.5727], Epochs since improvement 0
  6%|▌         | 29/500 [37:17<9:43:15, 74.30s/it] Epoch: 30 done with learning rate 3.72E-03, Train loss: 3.11E+06, Train scatter: [0.2579 0.0654 0.3017 0.5121]
L1 regularization loss: 8.20E-01, L2 regularization loss: 1.77E-01
Test scatter: [0.2651 0.0651 0.3064 0.5118], Lowest was [0.2651 0.0651 0.305  0.5062]
Median for last 10 epochs: [0.3028 0.072  0.3225 0.52  ], Epochs since improvement 0
  6%|▌         | 30/500 [38:58<10:44:24, 82.26s/it]  6%|▌         | 31/500 [40:01<9:56:54, 76.36s/it]   6%|▋         | 32/500 [41:33<10:32:28, 81.09s/it]Epoch: 32 done with learning rate 4.10E-03, Train loss: 3.05E+06, Train scatter: [0.4882 0.0744 0.3326 0.587 ]
L1 regularization loss: 8.28E-01, L2 regularization loss: 1.81E-01
Test scatter: [0.4916 0.075  0.3337 0.5883], Lowest was [0.2651 0.0651 0.305  0.5062]
Median for last 10 epochs: [0.3028 0.072  0.3225 0.52  ], Epochs since improvement 2
  7%|▋         | 33/500 [42:37<9:52:08, 76.08s/it]   7%|▋         | 34/500 [44:11<10:32:38, 81.46s/it]Epoch: 34 done with learning rate 4.50E-03, Train loss: 2.71E+06, Train scatter: [0.3305 0.065  0.3574 0.5065]
L1 regularization loss: 8.40E-01, L2 regularization loss: 1.87E-01
Test scatter: [0.3237 0.0654 0.3657 0.5075], Lowest was [0.2651 0.0651 0.305  0.5062]
Median for last 10 epochs: [0.3199 0.0693 0.3225 0.5118], Epochs since improvement 4
  7%|▋         | 35/500 [45:13<9:44:44, 75.45s/it]   7%|▋         | 36/500 [46:43<10:19:07, 80.06s/it]Epoch: 36 done with learning rate 4.90E-03, Train loss: 2.80E+06, Train scatter: [0.2489 0.0644 0.3071 0.504 ]
L1 regularization loss: 8.57E-01, L2 regularization loss: 1.96E-01
Test scatter: [0.2621 0.0661 0.3182 0.5105], Lowest was [0.2621 0.0651 0.305  0.5062]
Median for last 10 epochs: [0.2961 0.0661 0.3182 0.5105], Epochs since improvement 0
  7%|▋         | 37/500 [47:46<9:37:57, 74.90s/it]   8%|▊         | 38/500 [49:19<10:17:50, 80.24s/it]Epoch: 38 done with learning rate 5.30E-03, Train loss: 2.59E+06, Train scatter: [0.2404 0.0632 0.3308 0.5119]
L1 regularization loss: 8.70E-01, L2 regularization loss: 2.03E-01
Test scatter: [0.2498 0.0646 0.3367 0.517 ], Lowest was [0.2498 0.0646 0.305  0.5062]
Median for last 10 epochs: [0.2651 0.0654 0.3337 0.5118], Epochs since improvement 0
  8%|▊         | 39/500 [50:22<9:35:43, 74.93s/it] Epoch: 40 done with learning rate 5.70E-03, Train loss: 2.52E+06, Train scatter: [0.2605 0.0642 0.3103 0.5191]
L1 regularization loss: 8.84E-01, L2 regularization loss: 2.11E-01
Test scatter: [0.2625 0.0648 0.3127 0.5255], Lowest was [0.2498 0.0646 0.305  0.5062]
Median for last 10 epochs: [0.2625 0.0654 0.3337 0.517 ], Epochs since improvement 2
  8%|▊         | 40/500 [52:02<10:32:05, 82.45s/it]  8%|▊         | 41/500 [53:04<9:45:47, 76.57s/it]   8%|▊         | 42/500 [54:35<10:17:18, 80.87s/it]Epoch: 42 done with learning rate 6.10E-03, Train loss: 2.46E+06, Train scatter: [0.3612 0.0816 0.3409 0.5233]
L1 regularization loss: 8.97E-01, L2 regularization loss: 2.20E-01
Test scatter: [0.3613 0.0795 0.3414 0.5242], Lowest was [0.2498 0.0646 0.305  0.5062]
Median for last 10 epochs: [0.2625 0.0654 0.3367 0.517 ], Epochs since improvement 4
  9%|▊         | 43/500 [55:39<9:36:30, 75.69s/it]   9%|▉         | 44/500 [57:10<10:10:17, 80.30s/it]Epoch: 44 done with learning rate 6.49E-03, Train loss: 2.32E+06, Train scatter: [0.3011 0.0881 0.3107 0.5143]
L1 regularization loss: 9.10E-01, L2 regularization loss: 2.29E-01
Test scatter: [0.2968 0.0857 0.31   0.5123], Lowest was [0.2498 0.0646 0.305  0.5062]
Median for last 10 epochs: [0.2625 0.0661 0.3182 0.517 ], Epochs since improvement 6
  9%|▉         | 45/500 [58:13<9:29:43, 75.13s/it]   9%|▉         | 46/500 [59:43<10:03:18, 79.73s/it]Epoch: 46 done with learning rate 6.87E-03, Train loss: 2.19E+06, Train scatter: [0.2214 0.0673 0.2742 0.4701]
L1 regularization loss: 9.28E-01, L2 regularization loss: 2.41E-01
Test scatter: [0.2312 0.0669 0.2797 0.4735], Lowest was [0.2312 0.0646 0.2797 0.4735]
Median for last 10 epochs: [0.2625 0.0669 0.3127 0.517 ], Epochs since improvement 0
  9%|▉         | 47/500 [1:00:48<9:26:33, 75.04s/it] 10%|▉         | 48/500 [1:02:19<10:02:02, 79.92s/it]Epoch: 48 done with learning rate 7.24E-03, Train loss: 2.25E+06, Train scatter: [0.2808 0.0589 0.2722 0.4542]
L1 regularization loss: 9.44E-01, L2 regularization loss: 2.53E-01
Test scatter: [0.285  0.0597 0.2744 0.4549], Lowest was [0.2312 0.0597 0.2744 0.4549]
Median for last 10 epochs: [0.285  0.0669 0.31   0.5123], Epochs since improvement 0
 10%|▉         | 49/500 [1:03:22<9:23:46, 75.00s/it] Epoch: 50 done with learning rate 7.60E-03, Train loss: 1.80E+06, Train scatter: [0.227  0.0604 0.2741 0.4678]
L1 regularization loss: 9.59E-01, L2 regularization loss: 2.64E-01
Test scatter: [0.2303 0.0598 0.2752 0.4677], Lowest was [0.2303 0.0597 0.2744 0.4549]
Median for last 10 epochs: [0.285  0.0669 0.2797 0.4735], Epochs since improvement 0
 10%|█         | 50/500 [1:05:02<10:18:50, 82.51s/it] 10%|█         | 51/500 [1:06:06<9:35:29, 76.90s/it]  10%|█         | 52/500 [1:07:37<10:04:53, 81.01s/it]Epoch: 52 done with learning rate 7.94E-03, Train loss: 1.67E+06, Train scatter: [0.2452 0.0608 0.266  0.4438]
L1 regularization loss: 9.74E-01, L2 regularization loss: 2.75E-01
Test scatter: [0.2554 0.0624 0.272  0.4401], Lowest was [0.2303 0.0597 0.272  0.4401]
Median for last 10 epochs: [0.2554 0.0624 0.2752 0.4677], Epochs since improvement 0
 11%|█         | 53/500 [1:08:41<9:24:46, 75.81s/it]  11%|█         | 54/500 [1:10:14<10:02:17, 81.03s/it]Epoch: 54 done with learning rate 8.26E-03, Train loss: 1.84E+06, Train scatter: [0.2702 0.0707 0.2896 0.4782]
L1 regularization loss: 1.00E+00, L2 regularization loss: 2.93E-01
Test scatter: [0.2705 0.0698 0.2891 0.4697], Lowest was [0.2303 0.0597 0.272  0.4401]
Median for last 10 epochs: [0.2554 0.0624 0.2752 0.4677], Epochs since improvement 2
 11%|█         | 55/500 [1:11:16<9:20:15, 75.54s/it]  11%|█         | 56/500 [1:12:47<9:52:44, 80.10s/it]Epoch: 56 done with learning rate 8.56E-03, Train loss: 2.41E+06, Train scatter: [0.9397 0.1716 0.5451 0.9475]
L1 regularization loss: 1.04E+00, L2 regularization loss: 3.16E-01
Test scatter: [0.9244 0.1679 0.5364 0.9411], Lowest was [0.2303 0.0597 0.272  0.4401]
Median for last 10 epochs: [0.2705 0.0624 0.2752 0.4677], Epochs since improvement 4
 11%|█▏        | 57/500 [1:13:48<9:08:37, 74.31s/it] 12%|█▏        | 58/500 [1:15:21<9:47:50, 79.80s/it]Epoch: 58 done with learning rate 8.83E-03, Train loss: 3.50E+06, Train scatter: [0.5008 0.0853 0.493  0.5843]
L1 regularization loss: 1.08E+00, L2 regularization loss: 3.77E-01
Test scatter: [0.5193 0.0857 0.4908 0.5789], Lowest was [0.2303 0.0597 0.272  0.4401]
Median for last 10 epochs: [0.2705 0.0698 0.2891 0.4697], Epochs since improvement 6
 12%|█▏        | 59/500 [1:16:24<9:09:56, 74.82s/it]Epoch: 60 done with learning rate 9.08E-03, Train loss: 2.21E+06, Train scatter: [0.3988 0.0741 0.4541 0.5088]
L1 regularization loss: 1.10E+00, L2 regularization loss: 4.07E-01
Test scatter: [0.4071 0.0741 0.46   0.5034], Lowest was [0.2303 0.0597 0.272  0.4401]
Median for last 10 epochs: [0.4071 0.0741 0.46   0.5034], Epochs since improvement 8
 12%|█▏        | 60/500 [1:18:04<10:05:18, 82.54s/it] 12%|█▏        | 61/500 [1:19:07<9:20:26, 76.60s/it]  12%|█▏        | 62/500 [1:20:38<9:50:53, 80.94s/it]Epoch: 62 done with learning rate 9.31E-03, Train loss: 2.03E+06, Train scatter: [0.3735 0.0686 0.4492 0.5022]
L1 regularization loss: 1.10E+00, L2 regularization loss: 4.26E-01
Test scatter: [0.367  0.0685 0.4512 0.4985], Lowest was [0.2303 0.0597 0.272  0.4401]
Median for last 10 epochs: [0.4071 0.0741 0.46   0.5034], Epochs since improvement 10
 13%|█▎        | 63/500 [1:21:42<9:12:57, 75.92s/it] 13%|█▎        | 64/500 [1:23:16<9:50:07, 81.21s/it]Epoch: 64 done with learning rate 9.50E-03, Train loss: 1.80E+06, Train scatter: [0.3661 0.0682 0.3673 0.4785]
L1 regularization loss: 1.11E+00, L2 regularization loss: 4.42E-01
Test scatter: [0.3592 0.0691 0.374  0.4792], Lowest was [0.2303 0.0597 0.272  0.4401]
Median for last 10 epochs: [0.4071 0.0741 0.46   0.5034], Epochs since improvement 12
 13%|█▎        | 65/500 [1:24:19<9:09:11, 75.75s/it] 13%|█▎        | 66/500 [1:25:50<9:40:19, 80.23s/it]Epoch: 66 done with learning rate 9.66E-03, Train loss: 1.62E+06, Train scatter: [0.2785 0.0643 0.3426 0.4627]
L1 regularization loss: 1.12E+00, L2 regularization loss: 4.55E-01
Test scatter: [0.2787 0.066  0.3504 0.4633], Lowest was [0.2303 0.0597 0.272  0.4401]
Median for last 10 epochs: [0.367  0.0691 0.4512 0.4985], Epochs since improvement 14
 13%|█▎        | 67/500 [1:26:51<8:57:46, 74.52s/it] 14%|█▎        | 68/500 [1:28:23<9:33:54, 79.71s/it]Epoch: 68 done with learning rate 9.80E-03, Train loss: 1.49E+06, Train scatter: [0.2786 0.0631 0.3348 0.4661]
L1 regularization loss: 1.13E+00, L2 regularization loss: 4.70E-01
Test scatter: [0.2911 0.0622 0.3374 0.4627], Lowest was [0.2303 0.0597 0.272  0.4401]
Median for last 10 epochs: [0.3592 0.0685 0.374  0.4792], Epochs since improvement 16
 14%|█▍        | 69/500 [1:29:27<8:59:45, 75.14s/it]Epoch: 70 done with learning rate 9.90E-03, Train loss: 1.42E+06, Train scatter: [0.4485 0.0589 0.3251 0.4532]
L1 regularization loss: 1.14E+00, L2 regularization loss: 4.86E-01
Test scatter: [0.4437 0.0601 0.3337 0.4514], Lowest was [0.2303 0.0597 0.272  0.4401]
Median for last 10 epochs: [0.3592 0.066  0.3504 0.4633], Epochs since improvement 18
 14%|█▍        | 70/500 [1:31:05<9:48:08, 82.07s/it] 14%|█▍        | 71/500 [1:32:09<9:07:54, 76.63s/it] 14%|█▍        | 72/500 [1:33:43<9:42:19, 81.64s/it]Epoch: 72 done with learning rate 9.96E-03, Train loss: 1.19E+06, Train scatter: [0.3759 0.0647 0.3187 0.4488]
L1 regularization loss: 1.15E+00, L2 regularization loss: 5.00E-01
Test scatter: [0.3741 0.0646 0.322  0.447 ], Lowest was [0.2303 0.0597 0.272  0.4401]
Median for last 10 epochs: [0.3592 0.0646 0.3374 0.4627], Epochs since improvement 20
 15%|█▍        | 73/500 [1:34:45<8:59:40, 75.83s/it] 15%|█▍        | 73/500 [1:36:19<9:23:24, 79.17s/it]
Epoch: 74 done with learning rate 1.00E-02, Train loss: 1.21E+06, Train scatter: [0.3934 0.0735 0.3515 0.464 ]
L1 regularization loss: 1.18E+00, L2 regularization loss: 5.29E-01
Test scatter: [0.389  0.073  0.3515 0.4615], Lowest was [0.2303 0.0597 0.272  0.4401]
Median for last 10 epochs: [0.3741 0.0646 0.3374 0.4615], Epochs since improvement 22
Exited after 74 epochs due to early stopping
5779.20 seconds spent training, 11.558 seconds per epoch. Processed 6025 trees per second
[0.3890167  0.07297686 0.35144058 0.46147358]
{'epoch_exit': 73, 'scatter_m_star': 0.3890167, 'lowest_m_star': 0.2303207, 'last20_m_star': 0.38156384, 'last10_m_star': 0.37409842, 'scatter_v_disk': 0.072976865, 'lowest_v_disk': 0.05967907, 'last20_v_disk': 0.06881551, 'last10_v_disk': 0.06455055, 'scatter_m_cold': 0.35144058, 'lowest_m_cold': 0.27201974, 'last20_m_cold': 0.36274362, 'last10_m_cold': 0.3373984, 'scatter_sfr_100': 0.46147358, 'lowest_sfr_100': 0.44008428, 'last20_sfr_100': 0.47128537, 'last10_sfr_100': 0.46148703}
Finished 1/1
Exited training after None epochs
Experiment 5 done: 5 / 6
Exploring ['conv_layers', 'batch_size']
Currently doing [  3 512]
{'experiment': 'GraphMerge', 'group': 'meta_pysr', 'move': False, 'model': 'MetaEdge', 'log': True, 'run_params': {'n_epochs': 500, 'n_trials': 1, 'batch_size': 512, 'val_epoch': 2, 'early_stopping': True, 'patience': 20, 'l1_lambda': 3e-05, 'l2_lambda': 3e-05, 'loss_func': 'GaussNd', 'metrics': 'test_multi_varrho', 'performance_plot': 'multi_base', 'save': True, 'seed': True, 'num_workers': 4}, 'learn_params': {'learning_rate': 0.01, 'schedule': 'onecycle', 'g_up': 1, 'g_down': 0.95, 'warmup': 4, 'period': 5, 'eta_min': 1e-05}, 'hyper_params': {'hidden_channels': 128, 'conv_layers': 3, 'conv_activation': 'relu', 'decode_activation': 'leakyrelu', 'decode_layers': 2, 'layernorm': True, 'agg': 'sum', 'variance': True, 'rho': 0, 'in_channels': 43, 'out_channels': 4}, 'data_params': {'case': 'vlarge_all_4t_z0.0_quantile_raw', 'targets': [0, 1, 2, 3], 'del_feats': [], 'split': 0.8, 'test': 0, 'scale': 0}, 'experiment_name': 'con3bat512'}
/home/cj1223/../../scratch/gpfs/cj1223/GraphResults/results_meta_pysr_120322/MetaEdge_vlarge_all_4t_z0.0_quantile_raw_qgxxov
RelU conv activation
LeakyRelU decode activation
N_params 546504
Made folder for saving model
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:54<7:36:00, 54.83s/it]  0%|          | 2/500 [02:18<9:55:39, 71.77s/it]Epoch: 2 done with learning rate 4.17E-04, Train loss: 1.19E+08, Train scatter: [0.9352 0.174  0.5441 0.9953]
L1 regularization loss: 7.33E-01, L2 regularization loss: 1.27E-01
Test scatter: [0.9196 0.1679 0.5355 0.985 ], Lowest was [0.9196 0.1679 0.5355 0.985 ]
Median for last 10 epochs: [0.9196 0.1679 0.5355 0.985 ], Epochs since improvement 2
  1%|          | 3/500 [03:14<8:53:49, 64.45s/it]  1%|          | 4/500 [04:37<9:52:55, 71.72s/it]Epoch: 4 done with learning rate 4.67E-04, Train loss: 8.75E+07, Train scatter: [0.9352 0.1363 0.5441 0.9954]
L1 regularization loss: 7.37E-01, L2 regularization loss: 1.28E-01
Test scatter: [0.9196 0.1318 0.5355 0.9851], Lowest was [0.9196 0.1318 0.5355 0.985 ]
Median for last 10 epochs: [0.9196 0.1318 0.5355 0.9851], Epochs since improvement 0
  1%|          | 5/500 [05:32<9:02:39, 65.78s/it]  1%|          | 6/500 [06:53<9:45:29, 71.11s/it]Epoch: 6 done with learning rate 5.51E-04, Train loss: 8.28E+07, Train scatter: [0.9348 0.1137 0.5441 0.9954]
L1 regularization loss: 7.39E-01, L2 regularization loss: 1.28E-01
Test scatter: [0.9192 0.1127 0.5355 0.9851], Lowest was [0.9192 0.1127 0.5355 0.985 ]
Median for last 10 epochs: [0.9192 0.1127 0.5355 0.9851], Epochs since improvement 0
  1%|▏         | 7/500 [07:48<9:01:02, 65.85s/it]  2%|▏         | 8/500 [09:09<9:39:27, 70.67s/it]Epoch: 8 done with learning rate 6.67E-04, Train loss: 8.04E+07, Train scatter: [0.9307 0.1029 0.5439 0.9954]
L1 regularization loss: 7.42E-01, L2 regularization loss: 1.30E-01
Test scatter: [0.915  0.1025 0.5353 0.9851], Lowest was [0.915  0.1025 0.5353 0.985 ]
Median for last 10 epochs: [0.9171 0.1076 0.5354 0.9851], Epochs since improvement 0
  2%|▏         | 9/500 [10:04<8:56:38, 65.58s/it]Epoch: 10 done with learning rate 8.15E-04, Train loss: 7.93E+07, Train scatter: [0.7782 0.0946 0.5424 0.9954]
L1 regularization loss: 7.47E-01, L2 regularization loss: 1.32E-01
Test scatter: [0.7701 0.096  0.5339 0.9851], Lowest was [0.7701 0.096  0.5339 0.985 ]
Median for last 10 epochs: [0.915  0.1025 0.5353 0.9851], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
  2%|▏         | 10/500 [11:31<9:51:09, 72.39s/it]  2%|▏         | 11/500 [12:25<9:03:00, 66.63s/it]  2%|▏         | 12/500 [13:48<9:42:34, 71.63s/it]Epoch: 12 done with learning rate 9.94E-04, Train loss: 1.47E+08, Train scatter: [0.9312 0.1743 0.5441 0.9953]
L1 regularization loss: 7.60E-01, L2 regularization loss: 1.36E-01
Test scatter: [0.9152 0.169  0.5355 0.985 ], Lowest was [0.7701 0.096  0.5339 0.985 ]
Median for last 10 epochs: [0.9152 0.1127 0.5355 0.9851], Epochs since improvement 0
  3%|▎         | 13/500 [14:42<8:58:04, 66.29s/it]  3%|▎         | 14/500 [16:04<9:36:47, 71.21s/it]Epoch: 14 done with learning rate 1.20E-03, Train loss: 9.62E+07, Train scatter: [0.9243 0.1633 0.5441 0.9954]
L1 regularization loss: 7.65E-01, L2 regularization loss: 1.39E-01
Test scatter: [0.9104 0.1586 0.5355 0.9851], Lowest was [0.7701 0.096  0.5339 0.985 ]
Median for last 10 epochs: [0.915  0.1127 0.5355 0.9851], Epochs since improvement 2
  3%|▎         | 15/500 [16:59<8:56:05, 66.32s/it]  3%|▎         | 16/500 [18:20<9:29:00, 70.54s/it]Epoch: 16 done with learning rate 1.44E-03, Train loss: 8.92E+07, Train scatter: [0.821  0.1313 0.5438 0.9955]
L1 regularization loss: 7.76E-01, L2 regularization loss: 1.46E-01
Test scatter: [0.8118 0.129  0.5353 0.9851], Lowest was [0.7701 0.096  0.5339 0.985 ]
Median for last 10 epochs: [0.9104 0.129  0.5353 0.9851], Epochs since improvement 4
  3%|▎         | 17/500 [19:14<8:48:14, 65.62s/it]  4%|▎         | 18/500 [20:35<9:24:31, 70.27s/it]Epoch: 18 done with learning rate 1.70E-03, Train loss: 8.31E+07, Train scatter: [0.6265 0.108  0.5326 0.9955]
L1 regularization loss: 7.91E-01, L2 regularization loss: 1.61E-01
Test scatter: [0.6235 0.1074 0.5252 0.9851], Lowest was [0.6235 0.096  0.5252 0.985 ]
Median for last 10 epochs: [0.8118 0.129  0.5353 0.9851], Epochs since improvement 0
  4%|▍         | 19/500 [21:29<8:43:01, 65.24s/it]Epoch: 20 done with learning rate 1.99E-03, Train loss: 7.97E+07, Train scatter: [0.488  0.1005 0.522  0.9954]
L1 regularization loss: 8.00E-01, L2 regularization loss: 1.70E-01
Test scatter: [0.4845 0.0996 0.5147 0.9851], Lowest was [0.4845 0.096  0.5147 0.985 ]
Median for last 10 epochs: [0.8118 0.129  0.5353 0.9851], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
  4%|▍         | 20/500 [22:57<9:38:33, 72.32s/it]  4%|▍         | 21/500 [23:51<8:53:15, 66.80s/it]  4%|▍         | 22/500 [25:15<9:31:50, 71.78s/it]Epoch: 22 done with learning rate 2.30E-03, Train loss: 7.73E+07, Train scatter: [0.4602 0.092  0.4812 0.9954]
L1 regularization loss: 8.04E-01, L2 regularization loss: 1.74E-01
Test scatter: [0.4484 0.0915 0.4758 0.9851], Lowest was [0.4484 0.0915 0.4758 0.985 ]
Median for last 10 epochs: [0.6235 0.1074 0.5252 0.9851], Epochs since improvement 0
  5%|▍         | 23/500 [26:09<8:48:32, 66.48s/it]  5%|▍         | 24/500 [27:31<9:25:07, 71.24s/it]Epoch: 24 done with learning rate 2.63E-03, Train loss: 7.59E+07, Train scatter: [0.4305 0.0926 0.4212 0.9954]
L1 regularization loss: 8.06E-01, L2 regularization loss: 1.76E-01
Test scatter: [0.434  0.0929 0.4183 0.9851], Lowest was [0.434  0.0915 0.4183 0.985 ]
Median for last 10 epochs: [0.4845 0.0996 0.5147 0.9851], Epochs since improvement 0
  5%|▌         | 25/500 [28:26<8:44:21, 66.23s/it]  5%|▌         | 26/500 [29:48<9:20:23, 70.93s/it]Epoch: 26 done with learning rate 2.98E-03, Train loss: 7.50E+07, Train scatter: [0.4263 0.0858 0.4175 0.9954]
L1 regularization loss: 8.10E-01, L2 regularization loss: 1.79E-01
Test scatter: [0.4295 0.0863 0.4143 0.9851], Lowest was [0.4295 0.0863 0.4143 0.985 ]
Median for last 10 epochs: [0.4484 0.0929 0.4758 0.9851], Epochs since improvement 0
  5%|▌         | 27/500 [30:42<8:40:22, 66.01s/it]  6%|▌         | 28/500 [32:03<9:14:32, 70.49s/it]Epoch: 28 done with learning rate 3.34E-03, Train loss: 7.40E+07, Train scatter: [0.4035 0.0815 0.3647 0.9953]
L1 regularization loss: 8.13E-01, L2 regularization loss: 1.82E-01
Test scatter: [0.4161 0.0827 0.3643 0.985 ], Lowest was [0.4161 0.0827 0.3643 0.985 ]
Median for last 10 epochs: [0.434  0.0915 0.4183 0.9851], Epochs since improvement 0
  6%|▌         | 29/500 [32:58<8:36:56, 65.85s/it]Epoch: 30 done with learning rate 3.72E-03, Train loss: 7.29E+07, Train scatter: [0.3748 0.079  0.3709 0.9953]
L1 regularization loss: 8.17E-01, L2 regularization loss: 1.87E-01
Test scatter: [0.3859 0.0806 0.3694 0.985 ], Lowest was [0.3859 0.0806 0.3643 0.985 ]
Median for last 10 epochs: [0.4295 0.0863 0.4143 0.9851], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
  6%|▌         | 30/500 [34:27<9:30:25, 72.82s/it]  6%|▌         | 31/500 [35:22<8:46:49, 67.40s/it]  6%|▋         | 32/500 [36:43<9:17:04, 71.42s/it]Epoch: 32 done with learning rate 4.10E-03, Train loss: 7.29E+07, Train scatter: [0.3088 0.0773 0.3423 0.9953]
L1 regularization loss: 8.20E-01, L2 regularization loss: 1.91E-01
Test scatter: [0.3177 0.0778 0.3446 0.985 ], Lowest was [0.3177 0.0778 0.3446 0.985 ]
Median for last 10 epochs: [0.4161 0.0827 0.3694 0.985 ], Epochs since improvement 0
  7%|▋         | 33/500 [37:36<8:33:38, 65.99s/it]  7%|▋         | 34/500 [38:57<9:07:15, 70.46s/it]Epoch: 34 done with learning rate 4.50E-03, Train loss: 7.23E+07, Train scatter: [0.2777 0.0774 0.3297 0.9953]
L1 regularization loss: 8.23E-01, L2 regularization loss: 1.94E-01
Test scatter: [0.2859 0.0783 0.3337 0.985 ], Lowest was [0.2859 0.0778 0.3337 0.985 ]
Median for last 10 epochs: [0.3859 0.0806 0.3643 0.985 ], Epochs since improvement 0
  7%|▋         | 35/500 [39:51<8:26:58, 65.42s/it]  7%|▋         | 36/500 [41:12<9:02:03, 70.09s/it]Epoch: 36 done with learning rate 4.90E-03, Train loss: 7.14E+07, Train scatter: [0.3877 0.0726 0.339  0.9953]
L1 regularization loss: 8.25E-01, L2 regularization loss: 1.97E-01
Test scatter: [0.3892 0.0728 0.3423 0.985 ], Lowest was [0.2859 0.0728 0.3337 0.985 ]
Median for last 10 epochs: [0.3859 0.0783 0.3446 0.985 ], Epochs since improvement 0
  7%|▋         | 37/500 [42:06<8:25:26, 65.50s/it]  8%|▊         | 38/500 [43:28<9:02:10, 70.41s/it]Epoch: 38 done with learning rate 5.30E-03, Train loss: 7.06E+07, Train scatter: [0.2682 0.0745 0.3332 0.9953]
L1 regularization loss: 8.29E-01, L2 regularization loss: 2.00E-01
Test scatter: [0.2692 0.0726 0.3346 0.9849], Lowest was [0.2692 0.0726 0.3337 0.9849]
Median for last 10 epochs: [0.3177 0.0778 0.3423 0.985 ], Epochs since improvement 0
  8%|▊         | 39/500 [44:22<8:21:32, 65.28s/it]Epoch: 40 done with learning rate 5.70E-03, Train loss: 6.78E+07, Train scatter: [0.2602 0.0714 0.3101 0.9953]
L1 regularization loss: 8.32E-01, L2 regularization loss: 2.04E-01
Test scatter: [0.2696 0.0714 0.3144 0.985 ], Lowest was [0.2692 0.0714 0.3144 0.9849]
Median for last 10 epochs: [0.2859 0.0728 0.3346 0.985 ], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
  8%|▊         | 40/500 [45:50<9:14:11, 72.29s/it]  8%|▊         | 41/500 [46:45<8:32:26, 66.99s/it]  8%|▊         | 42/500 [48:06<9:04:48, 71.37s/it]Epoch: 42 done with learning rate 6.10E-03, Train loss: 6.24E+07, Train scatter: [0.5138 0.0769 0.3313 0.9954]
L1 regularization loss: 8.39E-01, L2 regularization loss: 2.09E-01
Test scatter: [0.5115 0.0758 0.3306 0.9851], Lowest was [0.2692 0.0714 0.3144 0.9849]
Median for last 10 epochs: [0.2859 0.0728 0.3337 0.985 ], Epochs since improvement 2
  9%|▊         | 43/500 [49:01<8:25:13, 66.33s/it]  9%|▉         | 44/500 [50:22<8:56:35, 70.60s/it]Epoch: 44 done with learning rate 6.49E-03, Train loss: 3.49E+07, Train scatter: [0.3711 0.0798 0.3194 0.9949]
L1 regularization loss: 8.42E-01, L2 regularization loss: 2.14E-01
Test scatter: [0.3763 0.0816 0.3258 0.9846], Lowest was [0.2692 0.0714 0.3144 0.9846]
Median for last 10 epochs: [0.3763 0.0728 0.3306 0.985 ], Epochs since improvement 0
  9%|▉         | 45/500 [51:16<8:18:16, 65.71s/it]  9%|▉         | 46/500 [52:37<8:51:12, 70.20s/it]Epoch: 46 done with learning rate 6.87E-03, Train loss: 9.35E+06, Train scatter: [0.2842 0.0726 0.3064 0.6308]
L1 regularization loss: 8.50E-01, L2 regularization loss: 2.22E-01
Test scatter: [0.2945 0.0741 0.3155 0.6283], Lowest was [0.2692 0.0714 0.3144 0.6283]
Median for last 10 epochs: [0.2945 0.0741 0.3258 0.9849], Epochs since improvement 0
  9%|▉         | 47/500 [53:31<8:13:31, 65.37s/it] 10%|▉         | 48/500 [54:53<8:51:16, 70.52s/it]Epoch: 48 done with learning rate 7.24E-03, Train loss: 4.64E+06, Train scatter: [0.2706 0.0712 0.3098 0.5721]
L1 regularization loss: 8.59E-01, L2 regularization loss: 2.28E-01
Test scatter: [0.2767 0.0722 0.3149 0.5687], Lowest was [0.2692 0.0714 0.3144 0.5687]
Median for last 10 epochs: [0.2945 0.0741 0.3155 0.9846], Epochs since improvement 0
 10%|▉         | 49/500 [55:47<8:12:30, 65.52s/it]Epoch: 50 done with learning rate 7.60E-03, Train loss: 4.00E+06, Train scatter: [0.254  0.0698 0.3174 0.5737]
L1 regularization loss: 8.67E-01, L2 regularization loss: 2.35E-01
Test scatter: [0.262  0.0716 0.3248 0.5732], Lowest was [0.262  0.0714 0.3144 0.5687]
Median for last 10 epochs: [0.2945 0.0741 0.3248 0.6283], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 10%|█         | 50/500 [57:16<9:04:03, 72.54s/it] 10%|█         | 51/500 [58:10<8:21:09, 66.97s/it] 10%|█         | 52/500 [59:32<8:53:09, 71.40s/it]Epoch: 52 done with learning rate 7.94E-03, Train loss: 3.77E+06, Train scatter: [0.2606 0.0686 0.3116 0.5124]
L1 regularization loss: 8.71E-01, L2 regularization loss: 2.40E-01
Test scatter: [0.2616 0.0688 0.318  0.5088], Lowest was [0.2616 0.0688 0.3144 0.5088]
Median for last 10 epochs: [0.2767 0.0722 0.318  0.5732], Epochs since improvement 0
 11%|█         | 53/500 [1:00:26<8:14:15, 66.34s/it] 11%|█         | 54/500 [1:01:48<8:48:33, 71.11s/it]Epoch: 54 done with learning rate 8.26E-03, Train loss: 3.44E+06, Train scatter: [0.2636 0.0677 0.3072 0.4978]
L1 regularization loss: 8.74E-01, L2 regularization loss: 2.45E-01
Test scatter: [0.2635 0.0674 0.3136 0.4936], Lowest was [0.2616 0.0674 0.3136 0.4936]
Median for last 10 epochs: [0.2635 0.0716 0.3155 0.5687], Epochs since improvement 0
 11%|█         | 55/500 [1:02:42<8:08:26, 65.86s/it] 11%|█         | 56/500 [1:04:05<8:45:48, 71.05s/it]Epoch: 56 done with learning rate 8.56E-03, Train loss: 3.23E+06, Train scatter: [0.2788 0.0658 0.3008 0.4878]
L1 regularization loss: 8.80E-01, L2 regularization loss: 2.51E-01
Test scatter: [0.2823 0.0686 0.3122 0.4879], Lowest was [0.2616 0.0674 0.3122 0.4879]
Median for last 10 epochs: [0.2635 0.0688 0.3149 0.5088], Epochs since improvement 0
 11%|█▏        | 57/500 [1:05:00<8:08:57, 66.22s/it] 12%|█▏        | 58/500 [1:06:21<8:40:35, 70.67s/it]Epoch: 58 done with learning rate 8.83E-03, Train loss: 3.34E+06, Train scatter: [0.2497 0.0671 0.3396 0.5099]
L1 regularization loss: 8.86E-01, L2 regularization loss: 2.58E-01
Test scatter: [0.2557 0.068  0.352  0.5094], Lowest was [0.2557 0.0674 0.3122 0.4879]
Median for last 10 epochs: [0.262  0.0686 0.318  0.5088], Epochs since improvement 0
 12%|█▏        | 59/500 [1:07:14<8:00:36, 65.39s/it]Epoch: 60 done with learning rate 9.08E-03, Train loss: 3.13E+06, Train scatter: [0.2945 0.0653 0.3133 0.4942]
L1 regularization loss: 8.93E-01, L2 regularization loss: 2.67E-01
Test scatter: [0.2938 0.0656 0.3151 0.4934], Lowest was [0.2557 0.0656 0.3122 0.4879]
Median for last 10 epochs: [0.2635 0.068  0.3151 0.4936], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 12%|█▏        | 60/500 [1:08:43<8:51:20, 72.45s/it] 12%|█▏        | 61/500 [1:09:37<8:08:12, 66.73s/it] 12%|█▏        | 62/500 [1:10:57<8:36:28, 70.75s/it]Epoch: 62 done with learning rate 9.31E-03, Train loss: 3.04E+06, Train scatter: [0.2328 0.0652 0.3041 0.4806]
L1 regularization loss: 8.98E-01, L2 regularization loss: 2.74E-01
Test scatter: [0.2395 0.0663 0.3087 0.4797], Lowest was [0.2395 0.0656 0.3087 0.4797]
Median for last 10 epochs: [0.2635 0.0674 0.3136 0.4934], Epochs since improvement 0
 13%|█▎        | 63/500 [1:11:51<7:59:54, 65.89s/it] 13%|█▎        | 64/500 [1:13:12<8:31:51, 70.44s/it]Epoch: 64 done with learning rate 9.50E-03, Train loss: 4.03E+06, Train scatter: [0.2509 0.0689 0.443  0.5045]
L1 regularization loss: 9.14E-01, L2 regularization loss: 2.87E-01
Test scatter: [0.2641 0.0695 0.4412 0.504 ], Lowest was [0.2395 0.0656 0.3087 0.4797]
Median for last 10 epochs: [0.2641 0.068  0.3151 0.4934], Epochs since improvement 2
 13%|█▎        | 65/500 [1:14:07<7:55:17, 65.56s/it] 13%|█▎        | 66/500 [1:15:29<8:30:52, 70.63s/it]Epoch: 66 done with learning rate 9.66E-03, Train loss: 2.83E+06, Train scatter: [0.2227 0.0632 0.2961 0.4777]
L1 regularization loss: 9.18E-01, L2 regularization loss: 2.92E-01
Test scatter: [0.2326 0.0642 0.3036 0.479 ], Lowest was [0.2326 0.0642 0.3036 0.479 ]
Median for last 10 epochs: [0.2557 0.0663 0.3151 0.4934], Epochs since improvement 0
 13%|█▎        | 67/500 [1:16:22<7:51:57, 65.40s/it] 14%|█▎        | 68/500 [1:17:44<8:26:41, 70.37s/it]Epoch: 68 done with learning rate 9.80E-03, Train loss: 4.80E+06, Train scatter: [0.2628 0.0667 0.3132 0.5128]
L1 regularization loss: 9.26E-01, L2 regularization loss: 3.00E-01
Test scatter: [0.2676 0.0668 0.3186 0.51  ], Lowest was [0.2326 0.0642 0.3036 0.479 ]
Median for last 10 epochs: [0.2641 0.0663 0.3151 0.4934], Epochs since improvement 2
 14%|█▍        | 69/500 [1:18:39<7:51:02, 65.57s/it]Epoch: 70 done with learning rate 9.90E-03, Train loss: 2.93E+06, Train scatter: [0.2382 0.064  0.3169 0.499 ]
L1 regularization loss: 9.29E-01, L2 regularization loss: 3.06E-01
Test scatter: [0.2477 0.0645 0.3247 0.4996], Lowest was [0.2326 0.0642 0.3036 0.479 ]
Median for last 10 epochs: [0.2477 0.0663 0.3186 0.4996], Epochs since improvement 4
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 14%|█▍        | 70/500 [1:20:07<8:39:39, 72.51s/it] 14%|█▍        | 71/500 [1:21:01<7:58:37, 66.94s/it] 14%|█▍        | 72/500 [1:22:23<8:28:57, 71.35s/it]Epoch: 72 done with learning rate 9.96E-03, Train loss: 2.64E+06, Train scatter: [0.236  0.0626 0.3036 0.489 ]
L1 regularization loss: 9.35E-01, L2 regularization loss: 3.13E-01
Test scatter: [0.2405 0.064  0.312  0.4905], Lowest was [0.2326 0.064  0.3036 0.479 ]
Median for last 10 epochs: [0.2477 0.0645 0.3186 0.4996], Epochs since improvement 0
 15%|█▍        | 73/500 [1:23:16<7:49:18, 65.95s/it] 15%|█▍        | 74/500 [1:24:38<8:21:58, 70.70s/it]Epoch: 74 done with learning rate 1.00E-02, Train loss: 2.56E+06, Train scatter: [0.2508 0.0592 0.2921 0.4792]
L1 regularization loss: 9.39E-01, L2 regularization loss: 3.20E-01
Test scatter: [0.2529 0.0599 0.2996 0.4813], Lowest was [0.2326 0.0599 0.2996 0.479 ]
Median for last 10 epochs: [0.2477 0.0642 0.312  0.4905], Epochs since improvement 0
 15%|█▌        | 75/500 [1:25:31<7:44:02, 65.51s/it] 15%|█▌        | 76/500 [1:26:52<8:14:47, 70.02s/it]Epoch: 76 done with learning rate 1.00E-02, Train loss: 3.50E+06, Train scatter: [0.3852 0.0608 0.3155 0.4908]
L1 regularization loss: 9.52E-01, L2 regularization loss: 3.32E-01
Test scatter: [0.3773 0.0616 0.3185 0.4936], Lowest was [0.2326 0.0599 0.2996 0.479 ]
Median for last 10 epochs: [0.2529 0.064  0.3185 0.4936], Epochs since improvement 2
 15%|█▌        | 77/500 [1:27:45<7:38:30, 65.04s/it] 16%|█▌        | 78/500 [1:29:08<8:13:36, 70.18s/it]Epoch: 78 done with learning rate 1.00E-02, Train loss: 2.46E+06, Train scatter: [0.2723 0.0666 0.3165 0.5568]
L1 regularization loss: 9.55E-01, L2 regularization loss: 3.39E-01
Test scatter: [0.275  0.0668 0.3223 0.5583], Lowest was [0.2326 0.0599 0.2996 0.479 ]
Median for last 10 epochs: [0.2529 0.064  0.3185 0.4936], Epochs since improvement 4
 16%|█▌        | 79/500 [1:30:01<7:37:54, 65.26s/it]Epoch: 80 done with learning rate 1.00E-02, Train loss: 2.40E+06, Train scatter: [0.2194 0.057  0.2879 0.4665]
L1 regularization loss: 9.59E-01, L2 regularization loss: 3.46E-01
Test scatter: [0.2242 0.0582 0.2969 0.4678], Lowest was [0.2242 0.0582 0.2969 0.4678]
Median for last 10 epochs: [0.2529 0.0616 0.312  0.4905], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 16%|█▌        | 80/500 [1:31:31<8:28:35, 72.66s/it] 16%|█▌        | 81/500 [1:32:26<7:50:21, 67.35s/it] 16%|█▋        | 82/500 [1:33:48<8:18:42, 71.59s/it]Epoch: 82 done with learning rate 9.99E-03, Train loss: 2.41E+06, Train scatter: [0.2287 0.0616 0.2818 0.4634]
L1 regularization loss: 9.65E-01, L2 regularization loss: 3.56E-01
Test scatter: [0.2368 0.0627 0.2855 0.4643], Lowest was [0.2242 0.0582 0.2855 0.4643]
Median for last 10 epochs: [0.2529 0.0616 0.2996 0.4813], Epochs since improvement 0
 17%|█▋        | 83/500 [1:34:41<7:39:46, 66.15s/it] 17%|█▋        | 84/500 [1:36:03<8:11:24, 70.88s/it]Epoch: 84 done with learning rate 9.99E-03, Train loss: 2.31E+06, Train scatter: [0.2828 0.062  0.2896 0.4936]
L1 regularization loss: 9.70E-01, L2 regularization loss: 3.65E-01
Test scatter: [0.2813 0.0607 0.2908 0.4955], Lowest was [0.2242 0.0582 0.2855 0.4643]
Median for last 10 epochs: [0.275  0.0616 0.2969 0.4936], Epochs since improvement 2
 17%|█▋        | 85/500 [1:36:57<7:34:32, 65.72s/it] 17%|█▋        | 86/500 [1:38:18<8:05:58, 70.43s/it]Epoch: 86 done with learning rate 9.98E-03, Train loss: 2.36E+06, Train scatter: [0.2351 0.0705 0.2944 0.5109]
L1 regularization loss: 9.77E-01, L2 regularization loss: 3.77E-01
Test scatter: [0.2371 0.0707 0.3013 0.5127], Lowest was [0.2242 0.0582 0.2855 0.4643]
Median for last 10 epochs: [0.2371 0.0627 0.2969 0.4955], Epochs since improvement 4
 17%|█▋        | 87/500 [1:39:13<7:32:47, 65.78s/it] 18%|█▊        | 88/500 [1:40:36<8:06:02, 70.78s/it]Epoch: 88 done with learning rate 9.98E-03, Train loss: 2.17E+06, Train scatter: [0.2347 0.0577 0.2829 0.4637]
L1 regularization loss: 9.83E-01, L2 regularization loss: 3.88E-01
Test scatter: [0.2387 0.0579 0.2887 0.4684], Lowest was [0.2242 0.0579 0.2855 0.4643]
Median for last 10 epochs: [0.2371 0.0607 0.2908 0.4684], Epochs since improvement 0
 18%|█▊        | 89/500 [1:41:31<7:33:14, 66.17s/it]Epoch: 90 done with learning rate 9.97E-03, Train loss: 2.39E+06, Train scatter: [0.2233 0.06   0.2841 0.4801]
L1 regularization loss: 9.90E-01, L2 regularization loss: 4.01E-01
Test scatter: [0.2296 0.0613 0.2915 0.4867], Lowest was [0.2242 0.0579 0.2855 0.4643]
Median for last 10 epochs: [0.2371 0.0613 0.2908 0.4867], Epochs since improvement 2
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 18%|█▊        | 90/500 [1:43:00<8:18:52, 73.01s/it] 18%|█▊        | 91/500 [1:43:55<7:41:21, 67.68s/it] 18%|█▊        | 92/500 [1:45:16<8:07:45, 71.73s/it]Epoch: 92 done with learning rate 9.96E-03, Train loss: 2.19E+06, Train scatter: [0.2247 0.0579 0.2804 0.4607]
L1 regularization loss: 1.00E+00, L2 regularization loss: 4.14E-01
Test scatter: [0.2345 0.0588 0.2876 0.4634], Lowest was [0.2242 0.0579 0.2855 0.4634]
Median for last 10 epochs: [0.2371 0.0607 0.2908 0.4867], Epochs since improvement 0
 19%|█▊        | 93/500 [1:46:11<7:31:01, 66.49s/it] 19%|█▉        | 94/500 [1:47:32<8:01:10, 71.11s/it]Epoch: 94 done with learning rate 9.95E-03, Train loss: 2.13E+06, Train scatter: [0.2864 0.057  0.2839 0.4513]
L1 regularization loss: 1.01E+00, L2 regularization loss: 4.25E-01
Test scatter: [0.2886 0.0572 0.2853 0.4502], Lowest was [0.2242 0.0572 0.2853 0.4502]
Median for last 10 epochs: [0.2371 0.0588 0.2887 0.4684], Epochs since improvement 0
 19%|█▉        | 95/500 [1:48:26<7:24:38, 65.87s/it] 19%|█▉        | 96/500 [1:49:48<7:55:32, 70.62s/it]Epoch: 96 done with learning rate 9.94E-03, Train loss: 1.99E+06, Train scatter: [0.231  0.0535 0.2637 0.4447]
L1 regularization loss: 1.01E+00, L2 regularization loss: 4.36E-01
Test scatter: [0.2369 0.0547 0.2715 0.4497], Lowest was [0.2242 0.0547 0.2715 0.4497]
Median for last 10 epochs: [0.2369 0.0579 0.2876 0.4634], Epochs since improvement 0
 19%|█▉        | 97/500 [1:50:42<7:20:19, 65.56s/it] 20%|█▉        | 98/500 [1:52:04<7:52:56, 70.59s/it]Epoch: 98 done with learning rate 9.93E-03, Train loss: 2.88E+06, Train scatter: [0.3482 0.0603 0.2754 0.4613]
L1 regularization loss: 1.02E+00, L2 regularization loss: 4.55E-01
Test scatter: [0.3661 0.0598 0.2813 0.4584], Lowest was [0.2242 0.0547 0.2715 0.4497]
Median for last 10 epochs: [0.2369 0.0588 0.2853 0.4584], Epochs since improvement 2
 20%|█▉        | 99/500 [1:52:58<7:19:17, 65.73s/it]Epoch: 100 done with learning rate 9.91E-03, Train loss: 2.01E+06, Train scatter: [0.2462 0.0543 0.2728 0.4511]
L1 regularization loss: 1.03E+00, L2 regularization loss: 4.65E-01
Test scatter: [0.244  0.0551 0.2795 0.4528], Lowest was [0.2242 0.0547 0.2715 0.4497]
Median for last 10 epochs: [0.244  0.0572 0.2813 0.4528], Epochs since improvement 4
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 20%|██        | 100/500 [1:54:28<8:05:15, 72.79s/it] 20%|██        | 101/500 [1:55:21<7:25:49, 67.04s/it] 20%|██        | 102/500 [1:56:43<7:55:04, 71.62s/it]Epoch: 102 done with learning rate 9.90E-03, Train loss: 1.86E+06, Train scatter: [0.2345 0.0542 0.2686 0.439 ]
L1 regularization loss: 1.03E+00, L2 regularization loss: 4.77E-01
Test scatter: [0.2405 0.055  0.2718 0.4408], Lowest was [0.2242 0.0547 0.2715 0.4408]
Median for last 10 epochs: [0.244  0.0551 0.2795 0.4502], Epochs since improvement 0
 21%|██        | 103/500 [1:57:37<7:18:17, 66.24s/it] 21%|██        | 104/500 [1:58:58<7:45:13, 70.49s/it]Epoch: 104 done with learning rate 9.89E-03, Train loss: 1.79E+06, Train scatter: [0.222  0.0534 0.2608 0.4347]
L1 regularization loss: 1.04E+00, L2 regularization loss: 4.89E-01
Test scatter: [0.2288 0.0544 0.2685 0.4399], Lowest was [0.2242 0.0544 0.2685 0.4399]
Median for last 10 epochs: [0.2405 0.055  0.2718 0.4497], Epochs since improvement 0
 21%|██        | 105/500 [1:59:51<7:11:09, 65.49s/it] 21%|██        | 106/500 [2:01:13<7:41:50, 70.33s/it]Epoch: 106 done with learning rate 9.87E-03, Train loss: 1.73E+06, Train scatter: [0.2283 0.0526 0.2624 0.4409]
L1 regularization loss: 1.04E+00, L2 regularization loss: 5.01E-01
Test scatter: [0.2345 0.0532 0.2679 0.4454], Lowest was [0.2242 0.0532 0.2679 0.4399]
Median for last 10 epochs: [0.2405 0.055  0.2718 0.4454], Epochs since improvement 0
 21%|██▏       | 107/500 [2:02:08<7:10:50, 65.78s/it] 22%|██▏       | 108/500 [2:03:29<7:38:58, 70.25s/it]Epoch: 108 done with learning rate 9.85E-03, Train loss: 1.70E+06, Train scatter: [0.2639 0.0539 0.2661 0.4357]
L1 regularization loss: 1.05E+00, L2 regularization loss: 5.14E-01
Test scatter: [0.2604 0.0548 0.2698 0.4416], Lowest was [0.2242 0.0532 0.2679 0.4399]
Median for last 10 epochs: [0.2405 0.0548 0.2698 0.4416], Epochs since improvement 2
 22%|██▏       | 109/500 [2:04:24<7:07:23, 65.58s/it]Epoch: 110 done with learning rate 9.83E-03, Train loss: 1.61E+06, Train scatter: [0.2245 0.0591 0.2697 0.4519]
L1 regularization loss: 1.06E+00, L2 regularization loss: 5.26E-01
Test scatter: [0.2263 0.0612 0.2859 0.4591], Lowest was [0.2242 0.0532 0.2679 0.4399]
Median for last 10 epochs: [0.2345 0.0548 0.2698 0.4416], Epochs since improvement 4
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 22%|██▏       | 110/500 [2:05:53<7:52:24, 72.68s/it] 22%|██▏       | 111/500 [2:06:47<7:14:23, 67.00s/it] 22%|██▏       | 112/500 [2:08:08<7:40:52, 71.27s/it]Epoch: 112 done with learning rate 9.81E-03, Train loss: 1.53E+06, Train scatter: [0.209  0.0536 0.2695 0.4286]
L1 regularization loss: 1.06E+00, L2 regularization loss: 5.38E-01
Test scatter: [0.2159 0.054  0.2759 0.4321], Lowest was [0.2159 0.0532 0.2679 0.4321]
Median for last 10 epochs: [0.2288 0.0544 0.2698 0.4416], Epochs since improvement 0
 23%|██▎       | 113/500 [2:09:02<7:05:45, 66.01s/it] 23%|██▎       | 114/500 [2:10:23<7:34:50, 70.70s/it]Epoch: 114 done with learning rate 9.79E-03, Train loss: 1.34E+06, Train scatter: [0.2421 0.0506 0.2526 0.4289]
L1 regularization loss: 1.07E+00, L2 regularization loss: 5.48E-01
Test scatter: [0.2481 0.0515 0.2592 0.4298], Lowest was [0.2159 0.0515 0.2592 0.4298]
Median for last 10 epochs: [0.2345 0.054  0.2698 0.4416], Epochs since improvement 0
 23%|██▎       | 115/500 [2:11:17<7:00:47, 65.58s/it] 23%|██▎       | 116/500 [2:12:37<7:27:28, 69.92s/it]Epoch: 116 done with learning rate 9.77E-03, Train loss: 1.36E+06, Train scatter: [0.2219 0.0532 0.2855 0.4242]
L1 regularization loss: 1.08E+00, L2 regularization loss: 5.61E-01
Test scatter: [0.2482 0.0541 0.2883 0.4273], Lowest was [0.2159 0.0515 0.2592 0.4273]
Median for last 10 epochs: [0.2481 0.0541 0.2759 0.4321], Epochs since improvement 0
 23%|██▎       | 117/500 [2:13:30<6:55:01, 65.02s/it] 24%|██▎       | 118/500 [2:14:52<7:25:32, 69.98s/it]Epoch: 118 done with learning rate 9.75E-03, Train loss: 1.31E+06, Train scatter: [0.1982 0.0525 0.2526 0.4215]
L1 regularization loss: 1.08E+00, L2 regularization loss: 5.71E-01
Test scatter: [0.2456 0.0534 0.2598 0.422 ], Lowest was [0.2159 0.0515 0.2592 0.422 ]
Median for last 10 epochs: [0.2456 0.054  0.2759 0.4298], Epochs since improvement 0
 24%|██▍       | 119/500 [2:15:46<6:53:32, 65.12s/it]Epoch: 120 done with learning rate 9.73E-03, Train loss: 3.41E+06, Train scatter: [0.4177 0.0602 0.4669 0.4668]
L1 regularization loss: 1.11E+00, L2 regularization loss: 6.01E-01
Test scatter: [0.4122 0.0599 0.4678 0.4625], Lowest was [0.2159 0.0515 0.2592 0.422 ]
Median for last 10 epochs: [0.2481 0.054  0.2759 0.4298], Epochs since improvement 2
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 24%|██▍       | 120/500 [2:17:13<7:34:44, 71.80s/it] 24%|██▍       | 121/500 [2:18:08<7:01:36, 66.75s/it] 24%|██▍       | 122/500 [2:19:29<7:26:58, 70.95s/it]Epoch: 122 done with learning rate 9.70E-03, Train loss: 1.44E+06, Train scatter: [0.2216 0.0581 0.2885 0.4409]
L1 regularization loss: 1.12E+00, L2 regularization loss: 6.08E-01
Test scatter: [0.2313 0.0585 0.2956 0.4433], Lowest was [0.2159 0.0515 0.2592 0.422 ]
Median for last 10 epochs: [0.2481 0.0541 0.2883 0.4298], Epochs since improvement 4
 25%|██▍       | 123/500 [2:20:24<6:56:23, 66.27s/it] 25%|██▍       | 124/500 [2:21:45<7:22:10, 70.56s/it]Epoch: 124 done with learning rate 9.68E-03, Train loss: 1.38E+06, Train scatter: [0.2199 0.0521 0.2689 0.4289]
L1 regularization loss: 1.13E+00, L2 regularization loss: 6.14E-01
Test scatter: [0.2274 0.0529 0.2749 0.4286], Lowest was [0.2159 0.0515 0.2592 0.422 ]
Median for last 10 epochs: [0.2456 0.0541 0.2883 0.4286], Epochs since improvement 6
 25%|██▌       | 125/500 [2:22:39<6:50:55, 65.75s/it] 25%|██▌       | 126/500 [2:24:01<7:18:46, 70.39s/it]Epoch: 126 done with learning rate 9.65E-03, Train loss: 1.22E+06, Train scatter: [0.2041 0.0534 0.2656 0.4241]
L1 regularization loss: 1.13E+00, L2 regularization loss: 6.19E-01
Test scatter: [0.2126 0.0538 0.2732 0.4284], Lowest was [0.2126 0.0515 0.2592 0.422 ]
Median for last 10 epochs: [0.2313 0.0538 0.2749 0.4286], Epochs since improvement 0
 25%|██▌       | 127/500 [2:24:54<6:46:18, 65.36s/it] 26%|██▌       | 128/500 [2:26:15<7:14:01, 70.00s/it]Epoch: 128 done with learning rate 9.62E-03, Train loss: 1.07E+06, Train scatter: [0.2076 0.0511 0.2589 0.4217]
L1 regularization loss: 1.14E+00, L2 regularization loss: 6.28E-01
Test scatter: [0.2138 0.0514 0.2617 0.4191], Lowest was [0.2126 0.0514 0.2592 0.4191]
Median for last 10 epochs: [0.2274 0.0538 0.2749 0.4286], Epochs since improvement 0
 26%|██▌       | 129/500 [2:27:09<6:44:01, 65.34s/it]Epoch: 130 done with learning rate 9.59E-03, Train loss: 1.03E+06, Train scatter: [0.2034 0.0528 0.2654 0.4247]
L1 regularization loss: 1.14E+00, L2 regularization loss: 6.36E-01
Test scatter: [0.2091 0.0535 0.27   0.4239], Lowest was [0.2091 0.0514 0.2592 0.4191]
Median for last 10 epochs: [0.2138 0.0535 0.2732 0.4284], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 26%|██▌       | 130/500 [2:28:40<7:28:48, 72.78s/it] 26%|██▌       | 131/500 [2:29:33<6:52:38, 67.10s/it] 26%|██▋       | 132/500 [2:30:56<7:20:14, 71.78s/it]Epoch: 132 done with learning rate 9.56E-03, Train loss: 9.09E+05, Train scatter: [0.1879 0.0516 0.2613 0.4172]
L1 regularization loss: 1.15E+00, L2 regularization loss: 6.43E-01
Test scatter: [0.194  0.0519 0.2641 0.4145], Lowest was [0.194  0.0514 0.2592 0.4145]
Median for last 10 epochs: [0.2126 0.0529 0.27   0.4239], Epochs since improvement 0
 27%|██▋       | 133/500 [2:31:50<6:46:51, 66.52s/it] 27%|██▋       | 134/500 [2:33:13<7:15:10, 71.34s/it]Epoch: 134 done with learning rate 9.53E-03, Train loss: 9.13E+05, Train scatter: [0.1965 0.0501 0.2546 0.4157]
L1 regularization loss: 1.15E+00, L2 regularization loss: 6.51E-01
Test scatter: [0.2042 0.0504 0.258  0.4142], Lowest was [0.194  0.0504 0.258  0.4142]
Median for last 10 epochs: [0.2091 0.0519 0.2641 0.4191], Epochs since improvement 0
 27%|██▋       | 135/500 [2:34:07<6:42:09, 66.11s/it] 27%|██▋       | 136/500 [2:35:29<7:09:56, 70.87s/it]Epoch: 136 done with learning rate 9.50E-03, Train loss: 8.66E+05, Train scatter: [0.1888 0.0542 0.27   0.4223]
L1 regularization loss: 1.16E+00, L2 regularization loss: 6.59E-01
Test scatter: [0.1938 0.0545 0.2741 0.4203], Lowest was [0.1938 0.0504 0.258  0.4142]
Median for last 10 epochs: [0.2042 0.0519 0.2641 0.4191], Epochs since improvement 0
 27%|██▋       | 137/500 [2:36:24<6:40:26, 66.19s/it] 28%|██▊       | 138/500 [2:37:45<7:06:00, 70.61s/it]Epoch: 138 done with learning rate 9.47E-03, Train loss: 8.24E+05, Train scatter: [0.2148 0.0482 0.2628 0.4137]
L1 regularization loss: 1.16E+00, L2 regularization loss: 6.65E-01
Test scatter: [0.2132 0.0481 0.2628 0.4131], Lowest was [0.1938 0.0481 0.258  0.4131]
Median for last 10 epochs: [0.2042 0.0519 0.2641 0.4145], Epochs since improvement 0
 28%|██▊       | 139/500 [2:38:40<6:35:49, 65.79s/it]Epoch: 140 done with learning rate 9.43E-03, Train loss: 8.14E+05, Train scatter: [0.22   0.0504 0.2662 0.4202]
L1 regularization loss: 1.17E+00, L2 regularization loss: 6.76E-01
Test scatter: [0.223  0.0514 0.2718 0.4224], Lowest was [0.1938 0.0481 0.258  0.4131]
Median for last 10 epochs: [0.2042 0.0514 0.2641 0.4145], Epochs since improvement 2
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 28%|██▊       | 140/500 [2:40:10<7:18:32, 73.09s/it] 28%|██▊       | 141/500 [2:41:04<6:44:25, 67.59s/it] 28%|██▊       | 142/500 [2:42:27<7:09:17, 71.95s/it]Epoch: 142 done with learning rate 9.40E-03, Train loss: 7.38E+05, Train scatter: [0.1801 0.0497 0.2554 0.4133]
L1 regularization loss: 1.18E+00, L2 regularization loss: 6.86E-01
Test scatter: [0.1873 0.0492 0.2556 0.4063], Lowest was [0.1873 0.0481 0.2556 0.4063]
Median for last 10 epochs: [0.2042 0.0504 0.2628 0.4142], Epochs since improvement 0
 29%|██▊       | 143/500 [2:43:21<6:36:58, 66.72s/it] 29%|██▉       | 144/500 [2:44:42<7:01:57, 71.12s/it]Epoch: 144 done with learning rate 9.36E-03, Train loss: 7.28E+05, Train scatter: [0.1763 0.0494 0.2526 0.4111]
L1 regularization loss: 1.19E+00, L2 regularization loss: 6.96E-01
Test scatter: [0.1796 0.0497 0.2566 0.4142], Lowest was [0.1796 0.0481 0.2556 0.4063]
Median for last 10 epochs: [0.1938 0.0497 0.2628 0.4142], Epochs since improvement 0
 29%|██▉       | 145/500 [2:45:37<6:31:11, 66.12s/it] 29%|██▉       | 146/500 [2:47:00<6:59:43, 71.14s/it]Epoch: 146 done with learning rate 9.33E-03, Train loss: 6.80E+05, Train scatter: [0.1666 0.0466 0.2585 0.4108]
L1 regularization loss: 1.20E+00, L2 regularization loss: 7.05E-01
Test scatter: [0.173  0.0463 0.2593 0.4038], Lowest was [0.173  0.0463 0.2556 0.4038]
Median for last 10 epochs: [0.1873 0.0492 0.2593 0.4131], Epochs since improvement 0
 29%|██▉       | 147/500 [2:47:53<6:27:41, 65.90s/it] 30%|██▉       | 148/500 [2:49:15<6:53:33, 70.49s/it]Epoch: 148 done with learning rate 9.29E-03, Train loss: 6.94E+05, Train scatter: [0.1758 0.0488 0.2523 0.4012]
L1 regularization loss: 1.21E+00, L2 regularization loss: 7.15E-01
Test scatter: [0.1796 0.0483 0.2547 0.4024], Lowest was [0.173  0.0463 0.2547 0.4024]
Median for last 10 epochs: [0.1796 0.0492 0.2566 0.4063], Epochs since improvement 0
 30%|██▉       | 149/500 [2:50:10<6:26:10, 66.01s/it]Epoch: 150 done with learning rate 9.25E-03, Train loss: 5.92E+05, Train scatter: [0.1784 0.0506 0.2585 0.4182]
L1 regularization loss: 1.22E+00, L2 regularization loss: 7.23E-01
Test scatter: [0.184  0.0498 0.2587 0.4095], Lowest was [0.173  0.0463 0.2547 0.4024]
Median for last 10 epochs: [0.1796 0.0492 0.2566 0.4063], Epochs since improvement 2
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 30%|███       | 150/500 [2:51:39<7:04:47, 72.82s/it] 30%|███       | 151/500 [2:52:33<6:30:48, 67.19s/it] 30%|███       | 152/500 [2:53:55<6:55:00, 71.55s/it]Epoch: 152 done with learning rate 9.21E-03, Train loss: 5.68E+05, Train scatter: [0.162  0.046  0.2414 0.4087]
L1 regularization loss: 1.22E+00, L2 regularization loss: 7.32E-01
Test scatter: [0.167  0.0464 0.2438 0.4093], Lowest was [0.167  0.0463 0.2438 0.4024]
Median for last 10 epochs: [0.1796 0.0483 0.2566 0.4093], Epochs since improvement 0
 31%|███       | 153/500 [2:54:49<6:23:55, 66.38s/it] 31%|███       | 154/500 [2:56:12<6:51:15, 71.32s/it]Epoch: 154 done with learning rate 9.17E-03, Train loss: 5.21E+05, Train scatter: [0.1852 0.0463 0.2366 0.4031]
L1 regularization loss: 1.23E+00, L2 regularization loss: 7.41E-01
Test scatter: [0.1851 0.0465 0.2377 0.402 ], Lowest was [0.167  0.0463 0.2377 0.402 ]
Median for last 10 epochs: [0.1796 0.0465 0.2547 0.4038], Epochs since improvement 0
 31%|███       | 155/500 [2:57:06<6:21:07, 66.28s/it] 31%|███       | 156/500 [2:58:29<6:47:43, 71.12s/it]Epoch: 156 done with learning rate 9.13E-03, Train loss: 4.58E+05, Train scatter: [0.1809 0.0441 0.2391 0.3965]
L1 regularization loss: 1.24E+00, L2 regularization loss: 7.51E-01
Test scatter: [0.1821 0.0447 0.2427 0.3947], Lowest was [0.167  0.0447 0.2377 0.3947]
Median for last 10 epochs: [0.1821 0.0465 0.2438 0.4024], Epochs since improvement 0
 31%|███▏      | 157/500 [2:59:24<6:18:46, 66.26s/it] 32%|███▏      | 158/500 [3:00:45<6:44:12, 70.92s/it]Epoch: 158 done with learning rate 9.09E-03, Train loss: 4.18E+05, Train scatter: [0.16   0.046  0.2576 0.4036]
L1 regularization loss: 1.25E+00, L2 regularization loss: 7.60E-01
Test scatter: [0.1638 0.0464 0.2605 0.4038], Lowest was [0.1638 0.0447 0.2377 0.3947]
Median for last 10 epochs: [0.1821 0.0464 0.2438 0.4038], Epochs since improvement 0
 32%|███▏      | 159/500 [3:01:39<6:13:54, 65.79s/it]Epoch: 160 done with learning rate 9.04E-03, Train loss: 3.61E+05, Train scatter: [0.1528 0.0445 0.236  0.3927]
L1 regularization loss: 1.26E+00, L2 regularization loss: 7.66E-01
Test scatter: [0.1594 0.0448 0.239  0.3898], Lowest was [0.1594 0.0447 0.2377 0.3898]
Median for last 10 epochs: [0.167  0.0464 0.2427 0.402 ], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 32%|███▏      | 160/500 [3:03:10<6:54:50, 73.21s/it] 32%|███▏      | 161/500 [3:04:03<6:20:03, 67.27s/it] 32%|███▏      | 162/500 [3:05:26<6:44:48, 71.86s/it]Epoch: 162 done with learning rate 9.00E-03, Train loss: 2.72E+05, Train scatter: [0.1848 0.0446 0.241  0.3932]
L1 regularization loss: 1.26E+00, L2 regularization loss: 7.74E-01
Test scatter: [0.186  0.0443 0.2422 0.3917], Lowest was [0.1594 0.0443 0.2377 0.3898]
Median for last 10 epochs: [0.1821 0.0448 0.2422 0.3947], Epochs since improvement 0
 33%|███▎      | 163/500 [3:06:19<6:12:19, 66.29s/it] 33%|███▎      | 164/500 [3:07:41<6:37:53, 71.05s/it]Epoch: 164 done with learning rate 8.96E-03, Train loss: 2.24E+05, Train scatter: [0.1452 0.0479 0.2392 0.4003]
L1 regularization loss: 1.27E+00, L2 regularization loss: 7.82E-01
Test scatter: [0.155  0.0471 0.2439 0.3977], Lowest was [0.155  0.0443 0.2377 0.3898]
Median for last 10 epochs: [0.1638 0.0448 0.2427 0.3947], Epochs since improvement 0
 33%|███▎      | 165/500 [3:08:35<6:08:31, 66.00s/it] 33%|███▎      | 166/500 [3:09:58<6:35:44, 71.09s/it]Epoch: 166 done with learning rate 8.91E-03, Train loss: 1.70E+05, Train scatter: [0.1422 0.043  0.2316 0.3887]
L1 regularization loss: 1.28E+00, L2 regularization loss: 7.91E-01
Test scatter: [0.1478 0.0425 0.2358 0.3868], Lowest was [0.1478 0.0425 0.2358 0.3868]
Median for last 10 epochs: [0.1594 0.0448 0.2422 0.3917], Epochs since improvement 0
 33%|███▎      | 167/500 [3:10:53<6:06:40, 66.07s/it] 34%|███▎      | 168/500 [3:12:15<6:32:31, 70.94s/it]Epoch: 168 done with learning rate 8.86E-03, Train loss: 7.66E+04, Train scatter: [0.2054 0.0456 0.2351 0.3961]
L1 regularization loss: 1.29E+00, L2 regularization loss: 7.98E-01
Test scatter: [0.2055 0.0458 0.2366 0.3967], Lowest was [0.1478 0.0425 0.2358 0.3868]
Median for last 10 epochs: [0.1594 0.0448 0.239  0.3917], Epochs since improvement 2
 34%|███▍      | 169/500 [3:13:09<6:03:02, 65.81s/it]Epoch: 170 done with learning rate 8.82E-03, Train loss: -6.32E+03, Train scatter: [0.1692 0.047  0.2438 0.3977]
L1 regularization loss: 1.29E+00, L2 regularization loss: 8.06E-01
Test scatter: [0.1789 0.0477 0.2452 0.3987], Lowest was [0.1478 0.0425 0.2358 0.3868]
Median for last 10 epochs: [0.1789 0.0458 0.2422 0.3967], Epochs since improvement 4
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 34%|███▍      | 170/500 [3:14:37<6:39:03, 72.56s/it] 34%|███▍      | 171/500 [3:15:32<6:08:45, 67.25s/it] 34%|███▍      | 172/500 [3:16:55<6:32:50, 71.86s/it]Epoch: 172 done with learning rate 8.77E-03, Train loss: -5.97E+04, Train scatter: [0.1556 0.045  0.2266 0.3922]
L1 regularization loss: 1.31E+00, L2 regularization loss: 8.26E-01
Test scatter: [0.1612 0.0444 0.2301 0.3889], Lowest was [0.1478 0.0425 0.2301 0.3868]
Median for last 10 epochs: [0.1612 0.0458 0.2366 0.3967], Epochs since improvement 0
 35%|███▍      | 173/500 [3:17:48<6:01:43, 66.37s/it] 35%|███▍      | 174/500 [3:19:10<6:25:33, 70.96s/it]Epoch: 174 done with learning rate 8.72E-03, Train loss: -1.06E+05, Train scatter: [0.1385 0.0435 0.2301 0.3832]
L1 regularization loss: 1.32E+00, L2 regularization loss: 8.35E-01
Test scatter: [0.1539 0.0428 0.2342 0.3823], Lowest was [0.1478 0.0425 0.2301 0.3823]
Median for last 10 epochs: [0.1612 0.0444 0.2358 0.3889], Epochs since improvement 0
 35%|███▌      | 175/500 [3:20:03<5:56:01, 65.73s/it] 35%|███▌      | 176/500 [3:21:25<6:20:00, 70.37s/it]Epoch: 176 done with learning rate 8.67E-03, Train loss: -1.71E+05, Train scatter: [0.1546 0.0452 0.229  0.3973]
L1 regularization loss: 1.33E+00, L2 regularization loss: 8.44E-01
Test scatter: [0.2575 0.0445 0.2312 0.3934], Lowest was [0.1478 0.0425 0.2301 0.3823]
Median for last 10 epochs: [0.1789 0.0445 0.2342 0.3934], Epochs since improvement 2
 35%|███▌      | 177/500 [3:22:19<5:52:15, 65.43s/it] 36%|███▌      | 178/500 [3:23:40<6:16:56, 70.24s/it]Epoch: 178 done with learning rate 8.62E-03, Train loss: -2.14E+05, Train scatter: [0.1534 0.0453 0.2283 0.3811]
L1 regularization loss: 1.33E+00, L2 regularization loss: 8.52E-01
Test scatter: [0.1631 0.0452 0.2304 0.3798], Lowest was [0.1478 0.0425 0.2301 0.3798]
Median for last 10 epochs: [0.1631 0.0445 0.2312 0.3889], Epochs since improvement 0
 36%|███▌      | 179/500 [3:24:34<5:48:59, 65.23s/it]Epoch: 180 done with learning rate 8.57E-03, Train loss: -2.63E+05, Train scatter: [0.1431 0.0424 0.2167 0.3801]
L1 regularization loss: 1.34E+00, L2 regularization loss: 8.60E-01
Test scatter: [0.1521 0.0423 0.2202 0.3796], Lowest was [0.1478 0.0423 0.2202 0.3796]
Median for last 10 epochs: [0.1612 0.0444 0.2304 0.3823], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 36%|███▌      | 180/500 [3:26:03<6:26:47, 72.52s/it] 36%|███▌      | 181/500 [3:26:58<5:56:47, 67.11s/it] 36%|███▋      | 182/500 [3:28:20<6:19:11, 71.55s/it]Epoch: 182 done with learning rate 8.52E-03, Train loss: -2.82E+05, Train scatter: [0.1343 0.0457 0.2417 0.3996]
L1 regularization loss: 1.34E+00, L2 regularization loss: 8.67E-01
Test scatter: [0.1413 0.0452 0.2432 0.3942], Lowest was [0.1413 0.0423 0.2202 0.3796]
Median for last 10 epochs: [0.1539 0.0445 0.2312 0.3823], Epochs since improvement 0
 37%|███▋      | 183/500 [3:29:13<5:49:54, 66.23s/it] 37%|███▋      | 184/500 [3:30:36<6:14:05, 71.03s/it]Epoch: 184 done with learning rate 8.46E-03, Train loss: -3.16E+05, Train scatter: [0.1366 0.0421 0.2169 0.3827]
L1 regularization loss: 1.35E+00, L2 regularization loss: 8.79E-01
Test scatter: [0.331  0.0419 0.2204 0.3801], Lowest was [0.1413 0.0419 0.2202 0.3796]
Median for last 10 epochs: [0.1631 0.0445 0.2304 0.3801], Epochs since improvement 0
 37%|███▋      | 185/500 [3:31:31<5:47:44, 66.24s/it] 37%|███▋      | 186/500 [3:32:53<6:11:38, 71.01s/it]Epoch: 186 done with learning rate 8.41E-03, Train loss: -2.93E+05, Train scatter: [0.1368 0.0472 0.2251 0.386 ]
L1 regularization loss: 1.36E+00, L2 regularization loss: 8.97E-01
Test scatter: [0.4058 0.0466 0.2278 0.3855], Lowest was [0.1413 0.0419 0.2202 0.3796]
Median for last 10 epochs: [0.1631 0.0452 0.2278 0.3801], Epochs since improvement 2
 37%|███▋      | 187/500 [3:33:47<5:44:27, 66.03s/it] 38%|███▊      | 188/500 [3:35:09<6:07:53, 70.75s/it]Epoch: 188 done with learning rate 8.35E-03, Train loss: -3.27E+05, Train scatter: [0.1427 0.0433 0.2202 0.3908]
L1 regularization loss: 1.39E+00, L2 regularization loss: 9.19E-01
Test scatter: [0.144  0.043  0.2235 0.3872], Lowest was [0.1413 0.0419 0.2202 0.3796]
Median for last 10 epochs: [0.1521 0.043  0.2235 0.3855], Epochs since improvement 4
 38%|███▊      | 189/500 [3:36:03<5:40:11, 65.63s/it]Epoch: 190 done with learning rate 8.30E-03, Train loss: -3.54E+05, Train scatter: [0.149  0.0496 0.2296 0.3951]
L1 regularization loss: 1.40E+00, L2 regularization loss: 9.29E-01
Test scatter: [0.1528 0.0482 0.2312 0.3919], Lowest was [0.1413 0.0419 0.2202 0.3796]
Median for last 10 epochs: [0.1528 0.0452 0.2278 0.3872], Epochs since improvement 6
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 38%|███▊      | 190/500 [3:37:31<6:14:46, 72.54s/it] 38%|███▊      | 191/500 [3:38:25<5:44:59, 66.99s/it] 38%|███▊      | 192/500 [3:39:48<6:07:25, 71.58s/it]Epoch: 192 done with learning rate 8.24E-03, Train loss: -3.66E+05, Train scatter: [0.1474 0.0516 0.2694 0.4173]
L1 regularization loss: 1.41E+00, L2 regularization loss: 9.37E-01
Test scatter: [0.1503 0.0506 0.2692 0.4099], Lowest was [0.1413 0.0419 0.2202 0.3796]
Median for last 10 epochs: [0.1528 0.0466 0.2278 0.3872], Epochs since improvement 8
 39%|███▊      | 193/500 [3:40:42<5:40:13, 66.49s/it] 39%|███▉      | 194/500 [3:42:03<6:00:12, 70.63s/it]Epoch: 194 done with learning rate 8.19E-03, Train loss: -3.82E+05, Train scatter: [0.1664 0.0417 0.2181 0.3856]
L1 regularization loss: 1.41E+00, L2 regularization loss: 9.43E-01
Test scatter: [0.1676 0.0414 0.2223 0.3843], Lowest was [0.1413 0.0414 0.2202 0.3796]
Median for last 10 epochs: [0.1528 0.0466 0.2278 0.3872], Epochs since improvement 0
 39%|███▉      | 195/500 [3:42:56<5:32:49, 65.47s/it] 39%|███▉      | 196/500 [3:44:16<5:54:33, 69.98s/it]Epoch: 196 done with learning rate 8.13E-03, Train loss: -3.86E+05, Train scatter: [0.1885 0.0475 0.2499 0.4035]
L1 regularization loss: 1.43E+00, L2 regularization loss: 9.58E-01
Test scatter: [0.1891 0.0471 0.2493 0.4036], Lowest was [0.1413 0.0414 0.2202 0.3796]
Median for last 10 epochs: [0.1528 0.0471 0.2312 0.3919], Epochs since improvement 2
 39%|███▉      | 197/500 [3:45:10<5:28:01, 64.96s/it] 40%|███▉      | 198/500 [3:46:30<5:50:23, 69.61s/it]Epoch: 198 done with learning rate 8.07E-03, Train loss: -4.07E+05, Train scatter: [0.1631 0.0481 0.2313 0.4032]
L1 regularization loss: 1.43E+00, L2 regularization loss: 9.62E-01
Test scatter: [0.1677 0.0475 0.2343 0.3977], Lowest was [0.1413 0.0414 0.2202 0.3796]
Median for last 10 epochs: [0.1676 0.0475 0.2343 0.3977], Epochs since improvement 4
 40%|███▉      | 199/500 [3:47:24<5:25:08, 64.81s/it]Epoch: 200 done with learning rate 8.01E-03, Train loss: -3.84E+05, Train scatter: [0.1324 0.042  0.2125 0.3837]
L1 regularization loss: 1.46E+00, L2 regularization loss: 9.81E-01
Test scatter: [0.1366 0.0414 0.216  0.3823], Lowest was [0.1366 0.0414 0.216  0.3796]
Median for last 10 epochs: [0.1676 0.0471 0.2343 0.3977], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 40%|████      | 200/500 [3:48:52<5:58:50, 71.77s/it] 40%|████      | 201/500 [3:49:46<5:31:51, 66.59s/it] 40%|████      | 202/500 [3:51:06<5:50:17, 70.53s/it]Epoch: 202 done with learning rate 7.95E-03, Train loss: -3.89E+05, Train scatter: [0.1217 0.0439 0.2175 0.3932]
L1 regularization loss: 1.47E+00, L2 regularization loss: 9.89E-01
Test scatter: [0.1295 0.043  0.221  0.3868], Lowest was [0.1295 0.0414 0.216  0.3796]
Median for last 10 epochs: [0.1676 0.043  0.2223 0.3868], Epochs since improvement 0
 41%|████      | 203/500 [3:52:01<5:25:25, 65.74s/it] 41%|████      | 204/500 [3:53:21<5:45:28, 70.03s/it]Epoch: 204 done with learning rate 7.89E-03, Train loss: -4.08E+05, Train scatter: [0.1196 0.0504 0.2246 0.4057]
L1 regularization loss: 1.47E+00, L2 regularization loss: 9.95E-01
Test scatter: [0.1269 0.0489 0.2257 0.4014], Lowest was [0.1269 0.0414 0.216  0.3796]
Median for last 10 epochs: [0.1366 0.0471 0.2257 0.3977], Epochs since improvement 0
 41%|████      | 205/500 [3:54:15<5:21:09, 65.32s/it] 41%|████      | 206/500 [3:55:36<5:42:31, 69.90s/it]Epoch: 206 done with learning rate 7.83E-03, Train loss: -3.92E+05, Train scatter: [0.3417 0.0483 0.2288 0.4034]
L1 regularization loss: 1.48E+00, L2 regularization loss: 1.01E+00
Test scatter: [0.3562 0.0483 0.23   0.398 ], Lowest was [0.1269 0.0414 0.216  0.3796]
Median for last 10 epochs: [0.1366 0.0475 0.2257 0.3977], Epochs since improvement 2
 41%|████▏     | 207/500 [3:56:29<5:17:19, 64.98s/it] 42%|████▏     | 208/500 [3:57:50<5:39:51, 69.83s/it]Epoch: 208 done with learning rate 7.77E-03, Train loss: -4.03E+05, Train scatter: [0.1413 0.0413 0.2161 0.3975]
L1 regularization loss: 1.48E+00, L2 regularization loss: 1.01E+00
Test scatter: [0.1483 0.0411 0.2179 0.3907], Lowest was [0.1269 0.0411 0.216  0.3796]
Median for last 10 epochs: [0.1366 0.043  0.221  0.3907], Epochs since improvement 0
 42%|████▏     | 209/500 [3:58:43<5:14:36, 64.87s/it]Epoch: 210 done with learning rate 7.71E-03, Train loss: -4.03E+05, Train scatter: [0.1393 0.0547 0.2497 0.4237]
L1 regularization loss: 1.50E+00, L2 regularization loss: 1.02E+00
Test scatter: [0.1451 0.054  0.2512 0.4175], Lowest was [0.1269 0.0411 0.216  0.3796]
Median for last 10 epochs: [0.1451 0.0483 0.2257 0.398 ], Epochs since improvement 2
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 42%|████▏     | 210/500 [4:00:11<5:46:25, 71.68s/it] 42%|████▏     | 211/500 [4:01:05<5:19:41, 66.37s/it] 42%|████▏     | 212/500 [4:02:26<5:39:11, 70.66s/it]Epoch: 212 done with learning rate 7.65E-03, Train loss: -4.16E+05, Train scatter: [0.1279 0.0457 0.2257 0.3936]
L1 regularization loss: 1.51E+00, L2 regularization loss: 1.04E+00
Test scatter: [0.2392 0.0452 0.231  0.3901], Lowest was [0.1269 0.0411 0.216  0.3796]
Median for last 10 epochs: [0.1483 0.0483 0.23   0.398 ], Epochs since improvement 4
 43%|████▎     | 213/500 [4:03:20<5:14:05, 65.66s/it] 43%|████▎     | 214/500 [4:04:41<5:35:41, 70.42s/it]Epoch: 214 done with learning rate 7.58E-03, Train loss: -3.72E+05, Train scatter: [0.1369 0.0437 0.2385 0.3985]
L1 regularization loss: 1.54E+00, L2 regularization loss: 1.07E+00
Test scatter: [0.1427 0.0431 0.2373 0.3901], Lowest was [0.1269 0.0411 0.216  0.3796]
Median for last 10 epochs: [0.1483 0.0452 0.231  0.3907], Epochs since improvement 6
 43%|████▎     | 215/500 [4:05:35<5:10:18, 65.33s/it] 43%|████▎     | 216/500 [4:06:56<5:32:17, 70.20s/it]Epoch: 216 done with learning rate 7.52E-03, Train loss: -3.89E+05, Train scatter: [0.15   0.0468 0.2346 0.4126]
L1 regularization loss: 1.58E+00, L2 regularization loss: 1.11E+00
Test scatter: [0.1591 0.0462 0.2354 0.4023], Lowest was [0.1269 0.0411 0.216  0.3796]
Median for last 10 epochs: [0.1483 0.0452 0.2354 0.3907], Epochs since improvement 8
 43%|████▎     | 217/500 [4:07:50<5:08:11, 65.34s/it] 44%|████▎     | 218/500 [4:09:11<5:28:53, 69.98s/it]Epoch: 218 done with learning rate 7.46E-03, Train loss: -4.02E+05, Train scatter: [0.1377 0.043  0.2181 0.3964]
L1 regularization loss: 1.60E+00, L2 regularization loss: 1.14E+00
Test scatter: [0.1486 0.0425 0.2239 0.3901], Lowest was [0.1269 0.0411 0.216  0.3796]
Median for last 10 epochs: [0.1486 0.0452 0.2354 0.3901], Epochs since improvement 10
 44%|████▍     | 219/500 [4:10:05<5:04:51, 65.09s/it]Epoch: 220 done with learning rate 7.39E-03, Train loss: -4.09E+05, Train scatter: [0.1814 0.0487 0.2407 0.4267]
L1 regularization loss: 1.60E+00, L2 regularization loss: 1.15E+00
Test scatter: [0.2023 0.0483 0.2411 0.4195], Lowest was [0.1269 0.0411 0.216  0.3796]
Median for last 10 epochs: [0.1591 0.0452 0.2354 0.3901], Epochs since improvement 12
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 44%|████▍     | 220/500 [4:11:34<5:37:21, 72.29s/it] 44%|████▍     | 221/500 [4:12:27<5:09:13, 66.50s/it] 44%|████▍     | 222/500 [4:13:46<5:26:24, 70.45s/it]Epoch: 222 done with learning rate 7.33E-03, Train loss: -3.38E+05, Train scatter: [0.3219 0.0912 0.331  0.5456]
L1 regularization loss: 1.63E+00, L2 regularization loss: 1.18E+00
Test scatter: [0.3134 0.0893 0.331  0.5423], Lowest was [0.1269 0.0411 0.216  0.3796]
Median for last 10 epochs: [0.1591 0.0462 0.2373 0.4023], Epochs since improvement 14
 45%|████▍     | 223/500 [4:14:39<5:00:55, 65.18s/it] 45%|████▍     | 224/500 [4:16:01<5:22:28, 70.10s/it]Epoch: 224 done with learning rate 7.26E-03, Train loss: -3.68E+05, Train scatter: [0.1539 0.0482 0.2514 0.4419]
L1 regularization loss: 1.68E+00, L2 regularization loss: 1.24E+00
Test scatter: [0.158  0.0475 0.2506 0.4318], Lowest was [0.1269 0.0411 0.216  0.3796]
Median for last 10 epochs: [0.1591 0.0475 0.2411 0.4195], Epochs since improvement 16
 45%|████▌     | 225/500 [4:16:54<4:57:48, 64.98s/it] 45%|████▌     | 226/500 [4:18:15<5:18:47, 69.81s/it]Epoch: 226 done with learning rate 7.20E-03, Train loss: -3.98E+05, Train scatter: [0.1493 0.0452 0.2162 0.4037]
L1 regularization loss: 1.68E+00, L2 regularization loss: 1.25E+00
Test scatter: [0.1996 0.0444 0.2192 0.3937], Lowest was [0.1269 0.0411 0.216  0.3796]
Median for last 10 epochs: [0.1996 0.0475 0.2411 0.4195], Epochs since improvement 18
 45%|████▌     | 227/500 [4:19:10<4:56:48, 65.23s/it] 46%|████▌     | 228/500 [4:20:31<5:17:27, 70.03s/it]Epoch: 228 done with learning rate 7.13E-03, Train loss: -4.09E+05, Train scatter: [0.151  0.0523 0.2725 0.4421]
L1 regularization loss: 1.70E+00, L2 regularization loss: 1.26E+00
Test scatter: [0.1514 0.0518 0.2781 0.4391], Lowest was [0.1269 0.0411 0.216  0.3796]
Median for last 10 epochs: [0.1996 0.0483 0.2506 0.4318], Epochs since improvement 20
 46%|████▌     | 229/500 [4:21:24<4:54:07, 65.12s/it]Epoch: 230 done with learning rate 7.06E-03, Train loss: -2.95E+05, Train scatter: [0.1639 0.0455 0.2351 0.4145]
L1 regularization loss: 1.72E+00, L2 regularization loss: 1.29E+00
Test scatter: [0.1662 0.0449 0.236  0.4053], Lowest was [0.1269 0.0411 0.216  0.3796]
Median for last 10 epochs: [0.1662 0.0475 0.2506 0.4318], Epochs since improvement 22
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 46%|████▌     | 229/500 [4:22:53<5:11:06, 68.88s/it]
Exited after 230 epochs due to early stopping
15773.95 seconds spent training, 31.548 seconds per epoch. Processed 2207 trees per second
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
[0.16622722 0.04489148 0.23596548 0.4052705 ]
{'epoch_exit': 229, 'scatter_m_star': 0.16622722, 'lowest_m_star': 0.1269316, 'last20_m_star': 0.16267237, 'last10_m_star': 0.16623214, 'scatter_v_disk': 0.044891477, 'lowest_v_disk': 0.041141085, 'last20_v_disk': 0.04570379, 'last10_v_disk': 0.047532827, 'scatter_m_cold': 0.23596548, 'lowest_m_cold': 0.21604761, 'last20_m_cold': 0.23662877, 'last10_m_cold': 0.2505751, 'scatter_sfr_100': 0.4052705, 'lowest_sfr_100': 0.37957945, 'last20_sfr_100': 0.40381342, 'last10_sfr_100': 0.4317818}
Finished 1/1
Exited training after None epochs
Experiment 6 done: 6 / 6
