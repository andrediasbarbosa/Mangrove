Starting process with 6 experiments
{'conv_layers': [1, 2, 3], 'batch_size': [256, 512]}
Exploring ['conv_layers', 'batch_size']
Currently doing [  1 256]
Folder already exists
{'experiment': 'GraphMerge', 'group': 'meta_pysr', 'move': False, 'model': 'MetaEdge', 'log': True, 'run_params': {'n_epochs': 500, 'n_trials': 1, 'batch_size': 256, 'val_epoch': 2, 'early_stopping': True, 'patience': 20, 'l1_lambda': 3e-05, 'l2_lambda': 3e-05, 'loss_func': 'GaussNd', 'metrics': 'test_multi_varrho', 'performance_plot': 'multi_base', 'save': True, 'seed': True, 'num_workers': 4}, 'learn_params': {'learning_rate': 0.01, 'schedule': 'onecycle', 'g_up': 1, 'g_down': 0.95, 'warmup': 4, 'period': 5, 'eta_min': 1e-05}, 'hyper_params': {'hidden_channels': 128, 'conv_layers': 1, 'conv_activation': 'relu', 'decode_activation': 'leakyrelu', 'decode_layers': 2, 'layernorm': True, 'agg': 'sum', 'variance': True, 'rho': 0}, 'data_params': {'case': 'vlarge_all_4t_z0.0_quantile_raw', 'targets': [0, 1, 2, 3], 'del_feats': [], 'split': 0.8, 'test': 0, 'scale': 0}, 'experiment_name': 'con1bat256'}
/home/cj1223/../../scratch/gpfs/cj1223/GraphResults/results_meta_pysr_120322/MetaEdge_vlarge_all_4t_z0.0_quantile_raw_kgsvdh
RelU conv activation
LeakyRelU decode activation
N_params 298056
Made folder for saving model
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:35<4:58:21, 35.88s/it]  0%|          | 2/500 [01:23<5:56:53, 43.00s/it]Epoch: 2 done with learning rate 4.17E-04, Train loss: 1.27E+07, Train scatter: [0.9352 0.1652 0.5441 0.9954]
L1 regularization loss: 4.56E-01, L2 regularization loss: 9.93E-02
Test scatter: [0.9196 0.1652 0.5355 0.9851], Lowest was [0.9196 0.1652 0.5355 0.9851]
Median for last 10 epochs: [0.9196 0.1652 0.5355 0.9851], Epochs since improvement 2
  1%|          | 3/500 [01:54<5:11:09, 37.56s/it]  1%|          | 4/500 [02:43<5:45:48, 41.83s/it]Epoch: 4 done with learning rate 4.67E-04, Train loss: 6.59E+06, Train scatter: [0.9352 0.1471 0.544  0.9954]
L1 regularization loss: 4.62E-01, L2 regularization loss: 1.06E-01
Test scatter: [0.9197 0.1433 0.5354 0.985 ], Lowest was [0.9196 0.1433 0.5354 0.985 ]
Median for last 10 epochs: [0.9197 0.1433 0.5354 0.985 ], Epochs since improvement 0
  1%|          | 5/500 [03:14<5:13:12, 37.96s/it]  1%|          | 6/500 [04:02<5:41:54, 41.53s/it]Epoch: 6 done with learning rate 5.51E-04, Train loss: 5.18E+06, Train scatter: [0.9342 0.1231 0.5418 0.6914]
L1 regularization loss: 4.74E-01, L2 regularization loss: 1.17E-01
Test scatter: [0.9185 0.123  0.533  0.678 ], Lowest was [0.9185 0.123  0.533  0.678 ]
Median for last 10 epochs: [0.9185 0.123  0.533  0.678 ], Epochs since improvement 0
  1%|▏         | 7/500 [04:33<5:13:11, 38.12s/it]  2%|▏         | 8/500 [05:22<5:39:30, 41.40s/it]Epoch: 8 done with learning rate 6.67E-04, Train loss: 4.23E+06, Train scatter: [0.9173 0.1017 0.5335 0.6276]
L1 regularization loss: 4.79E-01, L2 regularization loss: 1.21E-01
Test scatter: [0.9019 0.1017 0.5243 0.6162], Lowest was [0.9019 0.1017 0.5243 0.6162]
Median for last 10 epochs: [0.9102 0.1123 0.5286 0.6471], Epochs since improvement 0
  2%|▏         | 9/500 [05:53<5:12:56, 38.24s/it]Epoch: 10 done with learning rate 8.15E-04, Train loss: 3.20E+06, Train scatter: [0.8698 0.098  0.4364 0.627 ]
L1 regularization loss: 4.85E-01, L2 regularization loss: 1.25E-01
Test scatter: [0.8593 0.0997 0.4284 0.6101], Lowest was [0.8593 0.0997 0.4284 0.6101]
Median for last 10 epochs: [0.9019 0.1017 0.5243 0.6162], Epochs since improvement 0
  2%|▏         | 10/500 [06:47<5:51:24, 43.03s/it]  2%|▏         | 11/500 [07:18<5:21:12, 39.41s/it]  2%|▏         | 12/500 [08:06<5:41:53, 42.04s/it]Epoch: 12 done with learning rate 9.94E-04, Train loss: 2.50E+06, Train scatter: [0.5665 0.0921 0.4091 0.5982]
L1 regularization loss: 4.90E-01, L2 regularization loss: 1.28E-01
Test scatter: [0.5613 0.0944 0.4127 0.5948], Lowest was [0.5613 0.0944 0.4127 0.5948]
Median for last 10 epochs: [0.9019 0.1017 0.5243 0.6162], Epochs since improvement 0
  3%|▎         | 13/500 [08:37<5:14:15, 38.72s/it]  3%|▎         | 14/500 [09:25<5:35:46, 41.45s/it]Epoch: 14 done with learning rate 1.20E-03, Train loss: 1.44E+06, Train scatter: [0.5955 0.0863 0.3513 0.5829]
L1 regularization loss: 4.96E-01, L2 regularization loss: 1.30E-01
Test scatter: [0.5775 0.0885 0.3543 0.5828], Lowest was [0.5613 0.0885 0.3543 0.5828]
Median for last 10 epochs: [0.8593 0.0997 0.4284 0.6101], Epochs since improvement 0
  3%|▎         | 15/500 [09:56<5:09:51, 38.33s/it]  3%|▎         | 16/500 [10:44<5:33:08, 41.30s/it]Epoch: 16 done with learning rate 1.44E-03, Train loss: 8.72E+07, Train scatter: [0.9363 0.1737 0.5438 0.9901]
L1 regularization loss: 5.16E-01, L2 regularization loss: 1.36E-01
Test scatter: [0.9215 0.1692 0.5352 0.98  ], Lowest was [0.5613 0.0885 0.3543 0.5828]
Median for last 10 epochs: [0.8593 0.0997 0.4284 0.6101], Epochs since improvement 2
  3%|▎         | 17/500 [11:16<5:08:01, 38.26s/it]  4%|▎         | 18/500 [12:04<5:31:50, 41.31s/it]Epoch: 18 done with learning rate 1.70E-03, Train loss: 1.56E+07, Train scatter: [0.756  0.1683 0.5429 0.9873]
L1 regularization loss: 5.27E-01, L2 regularization loss: 1.40E-01
Test scatter: [0.7462 0.164  0.5344 0.9779], Lowest was [0.5613 0.0885 0.3543 0.5828]
Median for last 10 epochs: [0.7462 0.0997 0.4284 0.6101], Epochs since improvement 4
  4%|▍         | 19/500 [12:35<5:06:44, 38.26s/it]Epoch: 20 done with learning rate 1.99E-03, Train loss: 1.16E+07, Train scatter: [0.8842 0.1657 0.5055 0.8436]
L1 regularization loss: 5.56E-01, L2 regularization loss: 1.48E-01
Test scatter: [0.89   0.1616 0.5034 0.8503], Lowest was [0.5613 0.0885 0.3543 0.5828]
Median for last 10 epochs: [0.7462 0.1616 0.5034 0.8503], Epochs since improvement 6
  4%|▍         | 20/500 [13:29<5:42:30, 42.81s/it]  4%|▍         | 21/500 [14:00<5:14:04, 39.34s/it]  4%|▍         | 22/500 [14:48<5:34:49, 42.03s/it]Epoch: 22 done with learning rate 2.30E-03, Train loss: 8.42E+06, Train scatter: [0.9281 0.1528 0.4889 0.7938]
L1 regularization loss: 5.64E-01, L2 regularization loss: 1.51E-01
Test scatter: [0.9081 0.1499 0.4868 0.8041], Lowest was [0.5613 0.0885 0.3543 0.5828]
Median for last 10 epochs: [0.89   0.1616 0.5034 0.8503], Epochs since improvement 8
  5%|▍         | 23/500 [15:19<5:08:16, 38.78s/it]  5%|▍         | 24/500 [16:08<5:31:01, 41.73s/it]Epoch: 24 done with learning rate 2.63E-03, Train loss: 7.80E+06, Train scatter: [0.9315 0.1438 0.492  0.7835]
L1 regularization loss: 5.65E-01, L2 regularization loss: 1.52E-01
Test scatter: [0.9137 0.1425 0.4893 0.7888], Lowest was [0.5613 0.0885 0.3543 0.5828]
Median for last 10 epochs: [0.9081 0.1616 0.5034 0.8503], Epochs since improvement 10
  5%|▌         | 25/500 [16:39<5:05:19, 38.57s/it]  5%|▌         | 26/500 [17:27<5:27:03, 41.40s/it]Epoch: 26 done with learning rate 2.98E-03, Train loss: 7.31E+06, Train scatter: [0.9318 0.138  0.4853 0.7852]
L1 regularization loss: 5.66E-01, L2 regularization loss: 1.53E-01
Test scatter: [0.9157 0.1381 0.4828 0.7881], Lowest was [0.5613 0.0885 0.3543 0.5828]
Median for last 10 epochs: [0.9081 0.1499 0.4893 0.8041], Epochs since improvement 12
  5%|▌         | 27/500 [17:58<5:01:53, 38.29s/it]  6%|▌         | 28/500 [18:46<5:24:51, 41.30s/it]Epoch: 28 done with learning rate 3.34E-03, Train loss: 7.03E+06, Train scatter: [0.9319 0.1342 0.5361 0.7733]
L1 regularization loss: 5.65E-01, L2 regularization loss: 1.53E-01
Test scatter: [0.9165 0.1353 0.5283 0.78  ], Lowest was [0.5613 0.0885 0.3543 0.5828]
Median for last 10 epochs: [0.9137 0.1425 0.4893 0.7888], Epochs since improvement 14
  6%|▌         | 29/500 [19:18<5:00:25, 38.27s/it]Epoch: 30 done with learning rate 3.72E-03, Train loss: 6.05E+06, Train scatter: [0.9323 0.1315 0.4939 0.7612]
L1 regularization loss: 5.66E-01, L2 regularization loss: 1.54E-01
Test scatter: [0.9168 0.1332 0.4952 0.7664], Lowest was [0.5613 0.0885 0.3543 0.5828]
Median for last 10 epochs: [0.9157 0.1381 0.4893 0.7881], Epochs since improvement 16
  6%|▌         | 30/500 [20:11<5:34:15, 42.67s/it]  6%|▌         | 31/500 [20:42<5:06:31, 39.21s/it]  6%|▋         | 32/500 [21:30<5:27:11, 41.95s/it]Epoch: 32 done with learning rate 4.10E-03, Train loss: 5.18E+06, Train scatter: [0.933  0.1288 0.4931 0.7627]
L1 regularization loss: 5.71E-01, L2 regularization loss: 1.58E-01
Test scatter: [0.9173 0.1307 0.4901 0.7772], Lowest was [0.5613 0.0885 0.3543 0.5828]
Median for last 10 epochs: [0.9165 0.1353 0.4901 0.78  ], Epochs since improvement 18
  7%|▋         | 33/500 [22:01<5:01:20, 38.72s/it]  7%|▋         | 34/500 [22:50<5:22:57, 41.58s/it]Epoch: 34 done with learning rate 4.50E-03, Train loss: 3.29E+06, Train scatter: [0.9338 0.1252 0.4921 0.7512]
L1 regularization loss: 5.77E-01, L2 regularization loss: 1.64E-01
Test scatter: [0.9179 0.1271 0.4896 0.7563], Lowest was [0.5613 0.0885 0.3543 0.5828]
Median for last 10 epochs: [0.9168 0.1332 0.4901 0.7772], Epochs since improvement 20
  7%|▋         | 35/500 [23:21<4:58:14, 38.48s/it]  7%|▋         | 35/500 [24:09<5:21:00, 41.42s/it]
Epoch: 36 done with learning rate 4.90E-03, Train loss: 1.68E+06, Train scatter: [0.9342 0.1214 0.4829 0.7419]
L1 regularization loss: 5.79E-01, L2 regularization loss: 1.67E-01
Test scatter: [0.9183 0.1225 0.4827 0.7506], Lowest was [0.5613 0.0885 0.3543 0.5828]
Median for last 10 epochs: [0.9173 0.1307 0.4901 0.7664], Epochs since improvement 22
Exited after 36 epochs due to early stopping
1449.74 seconds spent training, 2.899 seconds per epoch. Processed 24017 trees per second
[0.9182458  0.12251191 0.48272553 0.7505953 ]
{'epoch_exit': 35, 'scatter_m_star': 0.9182458, 'lowest_m_star': 0.5612832, 'last20_m_star': 0.916111, 'last10_m_star': 0.9172745, 'scatter_v_disk': 0.12251191, 'lowest_v_disk': 0.08849968, 'last20_v_disk': 0.13672301, 'last10_v_disk': 0.13065025, 'scatter_m_cold': 0.48272553, 'lowest_m_cold': 0.3542546, 'last20_m_cold': 0.489881, 'last10_m_cold': 0.49013776, 'scatter_sfr_100': 0.7505953, 'lowest_sfr_100': 0.58277255, 'last20_sfr_100': 0.7840762, 'last10_sfr_100': 0.7664408}
Finished 1/1
Exited training after None epochs
Experiment 1 done: 1 / 6
Exploring ['conv_layers', 'batch_size']
Currently doing [  1 512]
{'experiment': 'GraphMerge', 'group': 'meta_pysr', 'move': False, 'model': 'MetaEdge', 'log': True, 'run_params': {'n_epochs': 500, 'n_trials': 1, 'batch_size': 512, 'val_epoch': 2, 'early_stopping': True, 'patience': 20, 'l1_lambda': 3e-05, 'l2_lambda': 3e-05, 'loss_func': 'GaussNd', 'metrics': 'test_multi_varrho', 'performance_plot': 'multi_base', 'save': True, 'seed': True, 'num_workers': 4}, 'learn_params': {'learning_rate': 0.01, 'schedule': 'onecycle', 'g_up': 1, 'g_down': 0.95, 'warmup': 4, 'period': 5, 'eta_min': 1e-05}, 'hyper_params': {'hidden_channels': 128, 'conv_layers': 1, 'conv_activation': 'relu', 'decode_activation': 'leakyrelu', 'decode_layers': 2, 'layernorm': True, 'agg': 'sum', 'variance': True, 'rho': 0, 'in_channels': 43, 'out_channels': 4}, 'data_params': {'case': 'vlarge_all_4t_z0.0_quantile_raw', 'targets': [0, 1, 2, 3], 'del_feats': [], 'split': 0.8, 'test': 0, 'scale': 0}, 'experiment_name': 'con1bat512'}
/home/cj1223/../../scratch/gpfs/cj1223/GraphResults/results_meta_pysr_120322/MetaEdge_vlarge_all_4t_z0.0_quantile_raw_invtgi
RelU conv activation
LeakyRelU decode activation
N_params 298056
Made folder for saving model
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:28<3:54:18, 28.17s/it]  0%|          | 2/500 [01:11<5:08:18, 37.14s/it]Epoch: 2 done with learning rate 4.17E-04, Train loss: 2.37E+07, Train scatter: [0.9353 0.1634 0.5442 0.9954]
L1 regularization loss: 4.54E-01, L2 regularization loss: 9.77E-02
Test scatter: [0.9197 0.1657 0.5356 0.9851], Lowest was [0.9197 0.1657 0.5356 0.9851]
Median for last 10 epochs: [0.9197 0.1657 0.5356 0.9851], Epochs since improvement 2
  1%|          | 3/500 [01:38<4:30:19, 32.63s/it]  1%|          | 4/500 [02:23<5:09:27, 37.43s/it]Epoch: 4 done with learning rate 4.67E-04, Train loss: 1.23E+07, Train scatter: [0.9353 0.1736 0.5441 0.9954]
L1 regularization loss: 4.57E-01, L2 regularization loss: 9.95E-02
Test scatter: [0.9197 0.1742 0.5355 0.9851], Lowest was [0.9197 0.1657 0.5355 0.9851]
Median for last 10 epochs: [0.9197 0.1699 0.5355 0.9851], Epochs since improvement 0
  1%|          | 5/500 [02:50<4:38:24, 33.75s/it]  1%|          | 6/500 [03:35<5:09:12, 37.56s/it]Epoch: 6 done with learning rate 5.51E-04, Train loss: 9.85E+06, Train scatter: [0.9352 0.1731 0.5441 0.9954]
L1 regularization loss: 4.61E-01, L2 regularization loss: 1.03E-01
Test scatter: [0.9196 0.1694 0.5356 0.9851], Lowest was [0.9196 0.1657 0.5355 0.9851]
Median for last 10 epochs: [0.9196 0.1694 0.5355 0.9851], Epochs since improvement 0
  1%|▏         | 7/500 [04:02<4:40:31, 34.14s/it]  2%|▏         | 8/500 [04:47<5:07:10, 37.46s/it]Epoch: 8 done with learning rate 6.67E-04, Train loss: 8.71E+06, Train scatter: [0.9352 0.1544 0.5441 0.9952]
L1 regularization loss: 4.65E-01, L2 regularization loss: 1.09E-01
Test scatter: [0.9196 0.1488 0.5355 0.9848], Lowest was [0.9196 0.1488 0.5355 0.9848]
Median for last 10 epochs: [0.9196 0.1591 0.5355 0.985 ], Epochs since improvement 0
  2%|▏         | 9/500 [05:14<4:40:03, 34.22s/it]Epoch: 10 done with learning rate 8.15E-04, Train loss: 8.09E+06, Train scatter: [0.9351 0.1392 0.5441 0.7106]
L1 regularization loss: 4.75E-01, L2 regularization loss: 1.18E-01
Test scatter: [0.9195 0.1351 0.5355 0.72  ], Lowest was [0.9195 0.1351 0.5355 0.72  ]
Median for last 10 epochs: [0.9196 0.1488 0.5355 0.9848], Epochs since improvement 0
  2%|▏         | 10/500 [06:04<5:18:18, 38.98s/it]  2%|▏         | 11/500 [06:31<4:48:25, 35.39s/it]  2%|▏         | 12/500 [07:16<5:12:31, 38.42s/it]Epoch: 12 done with learning rate 9.94E-04, Train loss: 7.35E+06, Train scatter: [0.9329 0.1323 0.544  0.6478]
L1 regularization loss: 4.80E-01, L2 regularization loss: 1.22E-01
Test scatter: [0.9171 0.1268 0.5354 0.6442], Lowest was [0.9171 0.1268 0.5354 0.6442]
Median for last 10 epochs: [0.9196 0.1488 0.5355 0.9848], Epochs since improvement 0
  3%|▎         | 13/500 [07:44<4:44:23, 35.04s/it]  3%|▎         | 14/500 [08:29<5:08:24, 38.07s/it]Epoch: 14 done with learning rate 1.20E-03, Train loss: 6.51E+06, Train scatter: [0.9035 0.1169 0.5432 0.6153]
L1 regularization loss: 4.83E-01, L2 regularization loss: 1.24E-01
Test scatter: [0.8878 0.1143 0.5346 0.6153], Lowest was [0.8878 0.1143 0.5346 0.6153]
Median for last 10 epochs: [0.9195 0.1351 0.5355 0.72  ], Epochs since improvement 0
  3%|▎         | 15/500 [08:56<4:41:31, 34.83s/it]  3%|▎         | 16/500 [09:41<5:06:14, 37.96s/it]Epoch: 16 done with learning rate 1.44E-03, Train loss: 6.11E+06, Train scatter: [0.7035 0.1111 0.5404 0.6   ]
L1 regularization loss: 4.88E-01, L2 regularization loss: 1.27E-01
Test scatter: [0.6969 0.1103 0.5318 0.6008], Lowest was [0.6969 0.1103 0.5318 0.6008]
Median for last 10 epochs: [0.9171 0.1268 0.5354 0.6442], Epochs since improvement 0
  3%|▎         | 17/500 [10:08<4:39:32, 34.72s/it]  4%|▎         | 18/500 [10:52<5:01:25, 37.52s/it]Epoch: 18 done with learning rate 1.70E-03, Train loss: 5.72E+06, Train scatter: [0.5477 0.1043 0.5366 0.5954]
L1 regularization loss: 4.93E-01, L2 regularization loss: 1.30E-01
Test scatter: [0.5628 0.1045 0.5282 0.5942], Lowest was [0.5628 0.1045 0.5282 0.5942]
Median for last 10 epochs: [0.8878 0.1143 0.5346 0.6153], Epochs since improvement 0
  4%|▍         | 19/500 [11:20<4:36:07, 34.44s/it]Epoch: 20 done with learning rate 1.99E-03, Train loss: 5.20E+06, Train scatter: [0.5368 0.0935 0.5327 0.5607]
L1 regularization loss: 4.97E-01, L2 regularization loss: 1.33E-01
Test scatter: [0.5292 0.0939 0.5245 0.5551], Lowest was [0.5292 0.0939 0.5245 0.5551]
Median for last 10 epochs: [0.6969 0.1103 0.5318 0.6008], Epochs since improvement 0
  4%|▍         | 20/500 [12:09<5:11:59, 39.00s/it]  4%|▍         | 21/500 [12:37<4:43:05, 35.46s/it]  4%|▍         | 22/500 [13:22<5:05:21, 38.33s/it]Epoch: 22 done with learning rate 2.30E-03, Train loss: 4.72E+06, Train scatter: [0.4757 0.0888 0.5301 0.5978]
L1 regularization loss: 5.02E-01, L2 regularization loss: 1.36E-01
Test scatter: [0.4952 0.0891 0.5216 0.6151], Lowest was [0.4952 0.0891 0.5216 0.5551]
Median for last 10 epochs: [0.5628 0.1045 0.5282 0.6008], Epochs since improvement 0
  5%|▍         | 23/500 [13:49<4:38:33, 35.04s/it]  5%|▍         | 24/500 [14:34<5:01:08, 37.96s/it]Epoch: 24 done with learning rate 2.63E-03, Train loss: 4.13E+06, Train scatter: [0.5768 0.0888 0.5041 0.5519]
L1 regularization loss: 5.06E-01, L2 regularization loss: 1.39E-01
Test scatter: [0.556  0.0889 0.499  0.5508], Lowest was [0.4952 0.0889 0.499  0.5508]
Median for last 10 epochs: [0.556  0.0939 0.5245 0.5942], Epochs since improvement 0
  5%|▌         | 25/500 [15:01<4:35:09, 34.76s/it]  5%|▌         | 26/500 [15:46<4:58:33, 37.79s/it]Epoch: 26 done with learning rate 2.98E-03, Train loss: 2.95E+06, Train scatter: [0.5537 0.0968 0.3949 0.6244]
L1 regularization loss: 5.15E-01, L2 regularization loss: 1.43E-01
Test scatter: [0.5423 0.0951 0.3939 0.6192], Lowest was [0.4952 0.0889 0.3939 0.5508]
Median for last 10 epochs: [0.5423 0.0939 0.5216 0.5942], Epochs since improvement 0
  5%|▌         | 27/500 [16:13<4:33:09, 34.65s/it]  6%|▌         | 28/500 [16:58<4:57:04, 37.76s/it]Epoch: 28 done with learning rate 3.34E-03, Train loss: 4.27E+07, Train scatter: [0.9081 0.1756 0.5293 0.9889]
L1 regularization loss: 5.85E-01, L2 regularization loss: 1.60E-01
Test scatter: [0.894  0.1755 0.5226 0.9792], Lowest was [0.4952 0.0889 0.3939 0.5508]
Median for last 10 epochs: [0.5423 0.0939 0.5216 0.6151], Epochs since improvement 2
  6%|▌         | 29/500 [17:25<4:31:41, 34.61s/it]Epoch: 30 done with learning rate 3.72E-03, Train loss: 2.48E+07, Train scatter: [0.8735 0.1696 0.5213 0.9909]
L1 regularization loss: 6.47E-01, L2 regularization loss: 1.80E-01
Test scatter: [0.8595 0.1697 0.5166 0.9807], Lowest was [0.4952 0.0889 0.3939 0.5508]
Median for last 10 epochs: [0.556  0.0951 0.5166 0.6192], Epochs since improvement 4
  6%|▌         | 30/500 [18:15<5:07:13, 39.22s/it]  6%|▌         | 31/500 [18:43<4:38:29, 35.63s/it]  6%|▋         | 32/500 [19:28<5:00:10, 38.48s/it]Epoch: 32 done with learning rate 4.10E-03, Train loss: 1.20E+07, Train scatter: [0.6958 0.1613 0.5051 0.9909]
L1 regularization loss: 6.53E-01, L2 regularization loss: 1.89E-01
Test scatter: [0.7008 0.1605 0.5    0.9809], Lowest was [0.4952 0.0889 0.3939 0.5508]
Median for last 10 epochs: [0.7008 0.1605 0.5    0.9792], Epochs since improvement 6
  7%|▋         | 33/500 [19:55<4:33:26, 35.13s/it]  7%|▋         | 34/500 [20:40<4:55:09, 38.00s/it]Epoch: 34 done with learning rate 4.50E-03, Train loss: 9.51E+06, Train scatter: [0.6693 0.1614 0.5062 0.9889]
L1 regularization loss: 6.56E-01, L2 regularization loss: 1.96E-01
Test scatter: [0.6829 0.1594 0.5004 0.979 ], Lowest was [0.4952 0.0889 0.3939 0.5508]
Median for last 10 epochs: [0.7008 0.1605 0.5004 0.9792], Epochs since improvement 8
  7%|▋         | 35/500 [21:07<4:29:19, 34.75s/it]  7%|▋         | 36/500 [21:52<4:52:32, 37.83s/it]Epoch: 36 done with learning rate 4.90E-03, Train loss: 8.07E+06, Train scatter: [0.664  0.1549 0.4971 0.9832]
L1 regularization loss: 6.61E-01, L2 regularization loss: 2.06E-01
Test scatter: [0.6874 0.1578 0.4913 0.9737], Lowest was [0.4952 0.0889 0.3939 0.5508]
Median for last 10 epochs: [0.7008 0.1605 0.5004 0.9792], Epochs since improvement 10
  7%|▋         | 37/500 [22:19<4:27:24, 34.65s/it]  8%|▊         | 38/500 [23:05<4:51:47, 37.89s/it]Epoch: 38 done with learning rate 5.30E-03, Train loss: 7.11E+06, Train scatter: [0.6589 0.1477 0.4941 0.9738]
L1 regularization loss: 6.64E-01, L2 regularization loss: 2.13E-01
Test scatter: [0.7191 0.1502 0.49   0.9653], Lowest was [0.4952 0.0889 0.3939 0.5508]
Median for last 10 epochs: [0.7008 0.1594 0.5    0.979 ], Epochs since improvement 12
  8%|▊         | 39/500 [23:32<4:26:36, 34.70s/it]Epoch: 40 done with learning rate 5.70E-03, Train loss: 6.44E+06, Train scatter: [0.612  0.1414 0.5016 0.959 ]
L1 regularization loss: 6.67E-01, L2 regularization loss: 2.20E-01
Test scatter: [0.6574 0.1425 0.4959 0.952 ], Lowest was [0.4952 0.0889 0.3939 0.5508]
Median for last 10 epochs: [0.6874 0.1578 0.4959 0.9737], Epochs since improvement 14
  8%|▊         | 40/500 [24:23<5:03:51, 39.63s/it]  8%|▊         | 41/500 [24:50<4:34:55, 35.94s/it]  8%|▊         | 42/500 [25:37<4:57:35, 38.99s/it]Epoch: 42 done with learning rate 6.10E-03, Train loss: 5.83E+06, Train scatter: [0.6321 0.1364 0.4883 0.9456]
L1 regularization loss: 6.71E-01, L2 regularization loss: 2.29E-01
Test scatter: [0.6333 0.1359 0.481  0.94  ], Lowest was [0.4952 0.0889 0.3939 0.5508]
Median for last 10 epochs: [0.6829 0.1502 0.4913 0.9653], Epochs since improvement 16
  9%|▊         | 43/500 [26:04<4:30:10, 35.47s/it]  9%|▉         | 44/500 [26:50<4:54:13, 38.71s/it]Epoch: 44 done with learning rate 6.49E-03, Train loss: 5.62E+06, Train scatter: [0.8689 0.1327 0.4718 0.9393]
L1 regularization loss: 6.78E-01, L2 regularization loss: 2.37E-01
Test scatter: [0.8571 0.1306 0.4647 0.9346], Lowest was [0.4952 0.0889 0.3939 0.5508]
Median for last 10 epochs: [0.6874 0.1425 0.49   0.952 ], Epochs since improvement 18
  9%|▉         | 45/500 [27:17<4:27:32, 35.28s/it]  9%|▉         | 46/500 [28:02<4:49:04, 38.20s/it]Epoch: 46 done with learning rate 6.87E-03, Train loss: 4.64E+06, Train scatter: [0.6851 0.1274 0.4485 0.9094]
L1 regularization loss: 6.82E-01, L2 regularization loss: 2.48E-01
Test scatter: [0.6889 0.1261 0.4453 0.9063], Lowest was [0.4952 0.0889 0.3939 0.5508]
Median for last 10 epochs: [0.6889 0.1359 0.481  0.94  ], Epochs since improvement 20
  9%|▉         | 47/500 [28:30<4:23:45, 34.93s/it]  9%|▉         | 47/500 [29:15<4:41:57, 37.35s/it]
Epoch: 48 done with learning rate 7.24E-03, Train loss: 3.92E+06, Train scatter: [0.6182 0.1225 0.4366 0.8828]
L1 regularization loss: 6.87E-01, L2 regularization loss: 2.56E-01
Test scatter: [0.6251 0.1222 0.4345 0.8802], Lowest was [0.4952 0.0889 0.3939 0.5508]
Median for last 10 epochs: [0.6574 0.1306 0.4647 0.9346], Epochs since improvement 22
Exited after 48 epochs due to early stopping
1755.27 seconds spent training, 3.511 seconds per epoch. Processed 19836 trees per second
[0.62506324 0.12224194 0.4344648  0.8801663 ]
{'epoch_exit': 47, 'scatter_m_star': 0.62506324, 'lowest_m_star': 0.49517405, 'last20_m_star': 0.68812823, 'last10_m_star': 0.65743244, 'scatter_v_disk': 0.122241944, 'lowest_v_disk': 0.0888838, 'last20_v_disk': 0.14637572, 'last10_v_disk': 0.13061905, 'scatter_m_cold': 0.4344648, 'lowest_m_cold': 0.39394087, 'last20_m_cold': 0.49060896, 'last10_m_cold': 0.46468538, 'scatter_sfr_100': 0.8801663, 'lowest_sfr_100': 0.5508039, 'last20_sfr_100': 0.9586635, 'last10_sfr_100': 0.93457043}
Finished 1/1
Exited training after None epochs
Experiment 2 done: 2 / 6
Exploring ['conv_layers', 'batch_size']
Currently doing [  2 256]
{'experiment': 'GraphMerge', 'group': 'meta_pysr', 'move': False, 'model': 'MetaEdge', 'log': True, 'run_params': {'n_epochs': 500, 'n_trials': 1, 'batch_size': 256, 'val_epoch': 2, 'early_stopping': True, 'patience': 20, 'l1_lambda': 3e-05, 'l2_lambda': 3e-05, 'loss_func': 'GaussNd', 'metrics': 'test_multi_varrho', 'performance_plot': 'multi_base', 'save': True, 'seed': True, 'num_workers': 4}, 'learn_params': {'learning_rate': 0.01, 'schedule': 'onecycle', 'g_up': 1, 'g_down': 0.95, 'warmup': 4, 'period': 5, 'eta_min': 1e-05}, 'hyper_params': {'hidden_channels': 128, 'conv_layers': 2, 'conv_activation': 'relu', 'decode_activation': 'leakyrelu', 'decode_layers': 2, 'layernorm': True, 'agg': 'sum', 'variance': True, 'rho': 0, 'in_channels': 43, 'out_channels': 4}, 'data_params': {'case': 'vlarge_all_4t_z0.0_quantile_raw', 'targets': [0, 1, 2, 3], 'del_feats': [], 'split': 0.8, 'test': 0, 'scale': 0}, 'experiment_name': 'con2bat256'}
/home/cj1223/../../scratch/gpfs/cj1223/GraphResults/results_meta_pysr_120322/MetaEdge_vlarge_all_4t_z0.0_quantile_raw_liiimn
RelU conv activation
LeakyRelU decode activation
N_params 422280
Made folder for saving model
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:47<6:37:09, 47.75s/it]  0%|          | 2/500 [01:58<8:27:35, 61.15s/it]Epoch: 2 done with learning rate 4.17E-04, Train loss: 2.29E+07, Train scatter: [0.9351 0.1386 0.544  0.9954]
L1 regularization loss: 5.97E-01, L2 regularization loss: 1.15E-01
Test scatter: [0.9195 0.1361 0.5355 0.9851], Lowest was [0.9195 0.1361 0.5355 0.9851]
Median for last 10 epochs: [0.9195 0.1361 0.5355 0.9851], Epochs since improvement 2
  1%|          | 3/500 [02:44<7:31:37, 54.52s/it]  1%|          | 4/500 [03:54<8:20:46, 60.58s/it]Epoch: 4 done with learning rate 4.67E-04, Train loss: 1.49E+07, Train scatter: [0.9276 0.0905 0.5439 0.9938]
L1 regularization loss: 6.05E-01, L2 regularization loss: 1.24E-01
Test scatter: [0.9119 0.09   0.5354 0.9834], Lowest was [0.9119 0.09   0.5354 0.9834]
Median for last 10 epochs: [0.9119 0.09   0.5354 0.9834], Epochs since improvement 0
  1%|          | 5/500 [04:41<7:38:25, 55.57s/it]  1%|          | 6/500 [05:52<8:20:04, 60.74s/it]Epoch: 6 done with learning rate 5.51E-04, Train loss: 9.03E+06, Train scatter: [0.7247 0.0849 0.5439 0.6514]
L1 regularization loss: 6.17E-01, L2 regularization loss: 1.36E-01
Test scatter: [0.7135 0.0841 0.5353 0.6404], Lowest was [0.7135 0.0841 0.5353 0.6404]
Median for last 10 epochs: [0.7135 0.0841 0.5353 0.6404], Epochs since improvement 0
  1%|▏         | 7/500 [06:39<7:41:38, 56.18s/it]  2%|▏         | 8/500 [07:49<8:18:11, 60.75s/it]Epoch: 8 done with learning rate 6.67E-04, Train loss: 5.52E+06, Train scatter: [0.4755 0.0773 0.5438 0.5455]
L1 regularization loss: 6.21E-01, L2 regularization loss: 1.40E-01
Test scatter: [0.4727 0.0774 0.5353 0.542 ], Lowest was [0.4727 0.0774 0.5353 0.542 ]
Median for last 10 epochs: [0.5931 0.0808 0.5353 0.5912], Epochs since improvement 0
  2%|▏         | 9/500 [08:36<7:41:17, 56.37s/it]Epoch: 10 done with learning rate 8.15E-04, Train loss: 4.24E+06, Train scatter: [0.3718 0.0745 0.5438 0.5284]
L1 regularization loss: 6.24E-01, L2 regularization loss: 1.43E-01
Test scatter: [0.3755 0.0743 0.5353 0.5245], Lowest was [0.3755 0.0743 0.5353 0.5245]
Median for last 10 epochs: [0.4727 0.0774 0.5353 0.542 ], Epochs since improvement 0
  2%|▏         | 10/500 [09:54<8:35:07, 63.08s/it]  2%|▏         | 11/500 [10:41<7:53:30, 58.10s/it]  2%|▏         | 12/500 [11:51<8:21:52, 61.71s/it]Epoch: 12 done with learning rate 9.94E-04, Train loss: 3.76E+06, Train scatter: [0.3862 0.0768 0.5438 0.545 ]
L1 regularization loss: 6.26E-01, L2 regularization loss: 1.45E-01
Test scatter: [0.3908 0.077  0.5353 0.544 ], Lowest was [0.3755 0.0743 0.5353 0.5245]
Median for last 10 epochs: [0.4727 0.0774 0.5353 0.544 ], Epochs since improvement 0
  3%|▎         | 13/500 [12:37<7:44:13, 57.19s/it]  3%|▎         | 14/500 [13:49<8:18:35, 61.56s/it]Epoch: 14 done with learning rate 1.20E-03, Train loss: 3.52E+06, Train scatter: [0.2678 0.0713 0.5438 0.512 ]
L1 regularization loss: 6.29E-01, L2 regularization loss: 1.47E-01
Test scatter: [0.2751 0.0709 0.5352 0.5064], Lowest was [0.2751 0.0709 0.5352 0.5064]
Median for last 10 epochs: [0.3908 0.077  0.5353 0.542 ], Epochs since improvement 0
  3%|▎         | 15/500 [14:36<7:41:28, 57.09s/it]  3%|▎         | 16/500 [15:47<8:15:47, 61.46s/it]Epoch: 16 done with learning rate 1.44E-03, Train loss: 3.43E+06, Train scatter: [0.2196 0.069  0.5437 0.5076]
L1 regularization loss: 6.32E-01, L2 regularization loss: 1.48E-01
Test scatter: [0.2252 0.0688 0.5352 0.5004], Lowest was [0.2252 0.0688 0.5352 0.5004]
Median for last 10 epochs: [0.3755 0.0743 0.5353 0.5245], Epochs since improvement 0
  3%|▎         | 17/500 [16:34<7:39:22, 57.07s/it]  4%|▎         | 18/500 [17:45<8:12:21, 61.29s/it]Epoch: 18 done with learning rate 1.70E-03, Train loss: 3.41E+06, Train scatter: [0.3122 0.0692 0.5437 0.5364]
L1 regularization loss: 6.36E-01, L2 regularization loss: 1.49E-01
Test scatter: [0.3142 0.0683 0.5351 0.5304], Lowest was [0.2252 0.0683 0.5351 0.5004]
Median for last 10 epochs: [0.3142 0.0709 0.5352 0.5245], Epochs since improvement 0
  4%|▍         | 19/500 [18:32<7:36:30, 56.95s/it]Epoch: 20 done with learning rate 1.99E-03, Train loss: 3.32E+06, Train scatter: [0.2769 0.07   0.5437 0.5043]
L1 regularization loss: 6.39E-01, L2 regularization loss: 1.51E-01
Test scatter: [0.2808 0.0706 0.5351 0.497 ], Lowest was [0.2252 0.0683 0.5351 0.497 ]
Median for last 10 epochs: [0.2808 0.0706 0.5352 0.5064], Epochs since improvement 0
  4%|▍         | 20/500 [19:50<8:25:32, 63.19s/it]  4%|▍         | 21/500 [20:37<7:45:27, 58.30s/it]  4%|▍         | 22/500 [21:48<8:15:08, 62.15s/it]Epoch: 22 done with learning rate 2.30E-03, Train loss: 3.30E+06, Train scatter: [0.2246 0.0693 0.5436 0.5225]
L1 regularization loss: 6.44E-01, L2 regularization loss: 1.52E-01
Test scatter: [0.2273 0.0687 0.5351 0.5189], Lowest was [0.2252 0.0683 0.5351 0.497 ]
Median for last 10 epochs: [0.2751 0.0688 0.5351 0.5064], Epochs since improvement 0
  5%|▍         | 23/500 [22:35<7:37:38, 57.57s/it]  5%|▍         | 24/500 [23:46<8:09:38, 61.72s/it]Epoch: 24 done with learning rate 2.63E-03, Train loss: 3.26E+06, Train scatter: [0.2059 0.0644 0.5436 0.5028]
L1 regularization loss: 6.48E-01, L2 regularization loss: 1.54E-01
Test scatter: [0.2121 0.0644 0.5351 0.4999], Lowest was [0.2121 0.0644 0.5351 0.497 ]
Median for last 10 epochs: [0.2273 0.0687 0.5351 0.5004], Epochs since improvement 0
  5%|▌         | 25/500 [24:33<7:33:06, 57.23s/it]  5%|▌         | 26/500 [25:44<8:03:23, 61.19s/it]Epoch: 26 done with learning rate 2.98E-03, Train loss: 3.36E+06, Train scatter: [0.2534 0.0743 0.5436 0.5236]
L1 regularization loss: 6.60E-01, L2 regularization loss: 1.58E-01
Test scatter: [0.2582 0.0728 0.5351 0.5189], Lowest was [0.2121 0.0644 0.5351 0.497 ]
Median for last 10 epochs: [0.2582 0.0687 0.5351 0.5189], Epochs since improvement 2
  5%|▌         | 27/500 [26:30<7:28:37, 56.91s/it]  6%|▌         | 28/500 [27:40<7:58:39, 60.85s/it]Epoch: 28 done with learning rate 3.34E-03, Train loss: 3.17E+06, Train scatter: [0.2096 0.0693 0.5436 0.5137]
L1 regularization loss: 6.64E-01, L2 regularization loss: 1.60E-01
Test scatter: [0.2124 0.0687 0.5351 0.5095], Lowest was [0.2121 0.0644 0.5351 0.497 ]
Median for last 10 epochs: [0.2273 0.0687 0.5351 0.5095], Epochs since improvement 0
  6%|▌         | 29/500 [28:27<7:24:29, 56.62s/it]Epoch: 30 done with learning rate 3.72E-03, Train loss: 3.15E+06, Train scatter: [0.2038 0.0634 0.5436 0.4954]
L1 regularization loss: 6.67E-01, L2 regularization loss: 1.62E-01
Test scatter: [0.2106 0.0637 0.5351 0.4899], Lowest was [0.2106 0.0637 0.5351 0.4899]
Median for last 10 epochs: [0.2124 0.0687 0.5351 0.5095], Epochs since improvement 0
  6%|▌         | 30/500 [29:45<8:12:24, 62.86s/it]  6%|▌         | 31/500 [30:32<7:34:05, 58.09s/it]  6%|▋         | 32/500 [31:43<8:03:56, 62.04s/it]Epoch: 32 done with learning rate 4.10E-03, Train loss: 3.14E+06, Train scatter: [0.2025 0.0631 0.5436 0.5013]
L1 regularization loss: 6.73E-01, L2 regularization loss: 1.65E-01
Test scatter: [0.2091 0.0633 0.535  0.4957], Lowest was [0.2091 0.0633 0.535  0.4899]
Median for last 10 epochs: [0.2121 0.0644 0.5351 0.4999], Epochs since improvement 0
  7%|▋         | 33/500 [32:30<7:27:12, 57.46s/it]  7%|▋         | 34/500 [33:40<7:56:43, 61.38s/it]Epoch: 34 done with learning rate 4.50E-03, Train loss: 3.12E+06, Train scatter: [0.2504 0.0653 0.5435 0.515 ]
L1 regularization loss: 6.77E-01, L2 regularization loss: 1.67E-01
Test scatter: [0.2593 0.0654 0.535  0.5115], Lowest was [0.2091 0.0633 0.535  0.4899]
Median for last 10 epochs: [0.2124 0.0654 0.5351 0.5095], Epochs since improvement 0
  7%|▋         | 35/500 [34:27<7:21:42, 56.99s/it]  7%|▋         | 36/500 [35:37<7:50:11, 60.80s/it]Epoch: 36 done with learning rate 4.90E-03, Train loss: 3.13E+06, Train scatter: [0.3407 0.0942 0.5437 0.6247]
L1 regularization loss: 6.84E-01, L2 regularization loss: 1.70E-01
Test scatter: [0.3386 0.0948 0.5351 0.6239], Lowest was [0.2091 0.0633 0.535  0.4899]
Median for last 10 epochs: [0.2124 0.0654 0.5351 0.5095], Epochs since improvement 2
  7%|▋         | 37/500 [36:24<7:17:08, 56.65s/it]  8%|▊         | 38/500 [37:35<7:49:28, 60.97s/it]Epoch: 38 done with learning rate 5.30E-03, Train loss: 3.12E+06, Train scatter: [0.2161 0.0663 0.5435 0.5117]
L1 regularization loss: 6.94E-01, L2 regularization loss: 1.75E-01
Test scatter: [0.221  0.0669 0.5349 0.5049], Lowest was [0.2091 0.0633 0.5349 0.4899]
Median for last 10 epochs: [0.221  0.0654 0.535  0.5049], Epochs since improvement 0
  8%|▊         | 39/500 [38:21<7:15:47, 56.72s/it]Epoch: 40 done with learning rate 5.70E-03, Train loss: 3.14E+06, Train scatter: [0.4372 0.0778 0.5435 0.5469]
L1 regularization loss: 7.07E-01, L2 regularization loss: 1.81E-01
Test scatter: [0.4282 0.0763 0.5349 0.5404], Lowest was [0.2091 0.0633 0.5349 0.4899]
Median for last 10 epochs: [0.2593 0.0669 0.535  0.5115], Epochs since improvement 2
  8%|▊         | 40/500 [39:39<8:02:23, 62.92s/it]  8%|▊         | 41/500 [40:26<7:24:41, 58.13s/it]  8%|▊         | 42/500 [41:36<7:51:27, 61.76s/it]Epoch: 42 done with learning rate 6.10E-03, Train loss: 3.05E+06, Train scatter: [0.3975 0.0634 0.5434 0.4818]
L1 regularization loss: 7.14E-01, L2 regularization loss: 1.86E-01
Test scatter: [0.3892 0.0644 0.5349 0.4781], Lowest was [0.2091 0.0633 0.5349 0.4781]
Median for last 10 epochs: [0.3386 0.0669 0.5349 0.5115], Epochs since improvement 0
  9%|▊         | 43/500 [42:23<7:16:52, 57.36s/it]  9%|▉         | 44/500 [43:34<7:47:35, 61.52s/it]Epoch: 44 done with learning rate 6.49E-03, Train loss: 3.03E+06, Train scatter: [0.2194 0.068  0.5434 0.5127]
L1 regularization loss: 7.27E-01, L2 regularization loss: 1.93E-01
Test scatter: [0.2537 0.0681 0.5348 0.5057], Lowest was [0.2091 0.0633 0.5348 0.4781]
Median for last 10 epochs: [0.3386 0.0681 0.5349 0.5057], Epochs since improvement 0
  9%|▉         | 45/500 [44:21<7:12:49, 57.08s/it]  9%|▉         | 46/500 [45:32<7:43:20, 61.23s/it]Epoch: 46 done with learning rate 6.87E-03, Train loss: 2.98E+06, Train scatter: [0.3362 0.0828 0.5434 0.6208]
L1 regularization loss: 7.41E-01, L2 regularization loss: 2.01E-01
Test scatter: [0.3688 0.0834 0.5349 0.6192], Lowest was [0.2091 0.0633 0.5348 0.4781]
Median for last 10 epochs: [0.3688 0.0681 0.5349 0.5057], Epochs since improvement 2
  9%|▉         | 47/500 [46:19<7:09:48, 56.93s/it] 10%|▉         | 48/500 [47:30<7:39:53, 61.05s/it]Epoch: 48 done with learning rate 7.24E-03, Train loss: 6.72E+06, Train scatter: [0.93   0.1707 0.5441 0.9952]
L1 regularization loss: 1.01E+00, L2 regularization loss: 3.05E-01
Test scatter: [0.9143 0.1671 0.5355 0.9849], Lowest was [0.2091 0.0633 0.5348 0.4781]
Median for last 10 epochs: [0.3892 0.0763 0.5349 0.5404], Epochs since improvement 4
 10%|▉         | 49/500 [48:16<7:06:54, 56.80s/it]Epoch: 50 done with learning rate 7.60E-03, Train loss: 4.86E+06, Train scatter: [0.8938 0.1524 0.5441 0.9867]
L1 regularization loss: 1.02E+00, L2 regularization loss: 3.28E-01
Test scatter: [0.877  0.1495 0.5355 0.9774], Lowest was [0.2091 0.0633 0.5348 0.4781]
Median for last 10 epochs: [0.3892 0.0834 0.5349 0.6192], Epochs since improvement 6
 10%|█         | 50/500 [49:33<7:51:06, 62.81s/it] 10%|█         | 51/500 [50:20<7:14:23, 58.05s/it] 10%|█         | 52/500 [51:31<7:41:58, 61.87s/it]Epoch: 52 done with learning rate 7.94E-03, Train loss: 4.28E+06, Train scatter: [0.9206 0.1223 0.5441 0.7691]
L1 regularization loss: 1.04E+00, L2 regularization loss: 4.01E-01
Test scatter: [0.9049 0.1208 0.5355 0.7638], Lowest was [0.2091 0.0633 0.5348 0.4781]
Median for last 10 epochs: [0.877  0.1208 0.5355 0.7638], Epochs since improvement 8
 11%|█         | 53/500 [52:18<7:07:22, 57.37s/it] 11%|█         | 54/500 [53:28<7:35:15, 61.25s/it]Epoch: 54 done with learning rate 8.26E-03, Train loss: 3.78E+06, Train scatter: [0.8554 0.1095 0.5441 0.681 ]
L1 regularization loss: 1.05E+00, L2 regularization loss: 4.22E-01
Test scatter: [0.8382 0.1083 0.5355 0.6765], Lowest was [0.2091 0.0633 0.5348 0.4781]
Median for last 10 epochs: [0.877  0.1208 0.5355 0.7638], Epochs since improvement 10
 11%|█         | 55/500 [54:15<7:01:55, 56.89s/it] 11%|█         | 56/500 [55:25<7:30:46, 60.91s/it]Epoch: 56 done with learning rate 8.56E-03, Train loss: 3.64E+06, Train scatter: [0.9248 0.1181 0.5441 0.6903]
L1 regularization loss: 1.06E+00, L2 regularization loss: 4.30E-01
Test scatter: [0.9093 0.1158 0.5355 0.6851], Lowest was [0.2091 0.0633 0.5348 0.4781]
Median for last 10 epochs: [0.9049 0.1208 0.5355 0.7638], Epochs since improvement 12
 11%|█▏        | 57/500 [56:12<6:58:25, 56.67s/it] 12%|█▏        | 58/500 [57:23<7:28:56, 60.94s/it]Epoch: 58 done with learning rate 8.83E-03, Train loss: 3.39E+06, Train scatter: [0.8112 0.1037 0.5441 0.627 ]
L1 regularization loss: 1.07E+00, L2 regularization loss: 4.41E-01
Test scatter: [0.7906 0.103  0.5355 0.6261], Lowest was [0.2091 0.0633 0.5348 0.4781]
Median for last 10 epochs: [0.877  0.1158 0.5355 0.6851], Epochs since improvement 14
 12%|█▏        | 59/500 [58:10<6:56:35, 56.68s/it]Epoch: 60 done with learning rate 9.08E-03, Train loss: 3.29E+06, Train scatter: [0.5072 0.0993 0.5441 0.608 ]
L1 regularization loss: 1.08E+00, L2 regularization loss: 4.57E-01
Test scatter: [0.4883 0.0977 0.5355 0.6002], Lowest was [0.2091 0.0633 0.5348 0.4781]
Median for last 10 epochs: [0.8382 0.1083 0.5355 0.6765], Epochs since improvement 16
 12%|█▏        | 60/500 [59:27<7:41:07, 62.88s/it] 12%|█▏        | 61/500 [1:00:14<7:04:40, 58.04s/it] 12%|█▏        | 62/500 [1:01:24<7:31:26, 61.84s/it]Epoch: 62 done with learning rate 9.31E-03, Train loss: 3.13E+06, Train scatter: [0.5695 0.1009 0.5439 0.6149]
L1 regularization loss: 1.08E+00, L2 regularization loss: 4.65E-01
Test scatter: [0.535  0.1004 0.5354 0.6201], Lowest was [0.2091 0.0633 0.5348 0.4781]
Median for last 10 epochs: [0.7906 0.103  0.5355 0.6261], Epochs since improvement 18
 13%|█▎        | 63/500 [1:02:11<6:57:26, 57.32s/it] 13%|█▎        | 64/500 [1:03:21<7:23:32, 61.04s/it]Epoch: 64 done with learning rate 9.50E-03, Train loss: 3.07E+06, Train scatter: [0.4491 0.0894 0.5433 0.5579]
L1 regularization loss: 1.08E+00, L2 regularization loss: 4.73E-01
Test scatter: [0.4342 0.0888 0.5348 0.5577], Lowest was [0.2091 0.0633 0.5348 0.4781]
Median for last 10 epochs: [0.535  0.1004 0.5355 0.6201], Epochs since improvement 20
 13%|█▎        | 65/500 [1:04:08<6:51:36, 56.77s/it] 13%|█▎        | 66/500 [1:05:18<7:19:18, 60.73s/it]Epoch: 66 done with learning rate 9.66E-03, Train loss: 2.98E+06, Train scatter: [0.4736 0.0906 0.5405 0.5618]
L1 regularization loss: 1.09E+00, L2 regularization loss: 4.88E-01
Test scatter: [0.4588 0.0902 0.5321 0.5629], Lowest was [0.2091 0.0633 0.5321 0.4781]
Median for last 10 epochs: [0.4883 0.0977 0.5354 0.6002], Epochs since improvement 0
 13%|█▎        | 67/500 [1:06:04<6:47:58, 56.53s/it] 14%|█▎        | 68/500 [1:07:16<7:18:37, 60.92s/it]Epoch: 68 done with learning rate 9.80E-03, Train loss: 2.85E+06, Train scatter: [0.4874 0.1155 0.5382 0.66  ]
L1 regularization loss: 1.09E+00, L2 regularization loss: 5.01E-01
Test scatter: [0.4861 0.1143 0.5301 0.6579], Lowest was [0.2091 0.0633 0.5301 0.4781]
Median for last 10 epochs: [0.4861 0.0977 0.5348 0.6002], Epochs since improvement 0
 14%|█▍        | 69/500 [1:08:02<6:47:06, 56.67s/it]Epoch: 70 done with learning rate 9.90E-03, Train loss: 2.76E+06, Train scatter: [0.4118 0.0857 0.5312 0.5396]
L1 regularization loss: 1.10E+00, L2 regularization loss: 5.19E-01
Test scatter: [0.3982 0.0854 0.5234 0.5374], Lowest was [0.2091 0.0633 0.5234 0.4781]
Median for last 10 epochs: [0.4588 0.0902 0.5321 0.5629], Epochs since improvement 0
 14%|█▍        | 70/500 [1:09:21<7:32:49, 63.18s/it] 14%|█▍        | 71/500 [1:10:08<6:56:56, 58.31s/it] 14%|█▍        | 72/500 [1:11:19<7:23:20, 62.15s/it]Epoch: 72 done with learning rate 9.96E-03, Train loss: 2.73E+06, Train scatter: [0.4349 0.0787 0.5314 0.5531]
L1 regularization loss: 1.10E+00, L2 regularization loss: 5.41E-01
Test scatter: [0.4241 0.0795 0.5233 0.5497], Lowest was [0.2091 0.0633 0.5233 0.4781]
Median for last 10 epochs: [0.4342 0.0888 0.5301 0.5577], Epochs since improvement 0
 15%|█▍        | 73/500 [1:12:05<6:49:17, 57.51s/it] 15%|█▍        | 74/500 [1:13:17<7:17:42, 61.65s/it]Epoch: 74 done with learning rate 1.00E-02, Train loss: 2.66E+06, Train scatter: [0.5124 0.0725 0.526  0.5533]
L1 regularization loss: 1.11E+00, L2 regularization loss: 5.63E-01
Test scatter: [0.4902 0.0734 0.5179 0.5503], Lowest was [0.2091 0.0633 0.5179 0.4781]
Median for last 10 epochs: [0.4588 0.0854 0.5234 0.5503], Epochs since improvement 0
 15%|█▌        | 75/500 [1:14:04<6:45:06, 57.19s/it] 15%|█▌        | 76/500 [1:15:15<7:14:48, 61.53s/it]Epoch: 76 done with learning rate 1.00E-02, Train loss: 2.60E+06, Train scatter: [0.4389 0.0703 0.5247 0.5228]
L1 regularization loss: 1.11E+00, L2 regularization loss: 5.77E-01
Test scatter: [0.4295 0.0707 0.5165 0.5223], Lowest was [0.2091 0.0633 0.5165 0.4781]
Median for last 10 epochs: [0.4295 0.0795 0.5233 0.5497], Epochs since improvement 0
 15%|█▌        | 77/500 [1:16:02<6:42:36, 57.11s/it] 16%|█▌        | 78/500 [1:17:12<7:09:12, 61.02s/it]Epoch: 78 done with learning rate 1.00E-02, Train loss: 2.60E+06, Train scatter: [0.6967 0.1127 0.5123 0.9094]
L1 regularization loss: 1.13E+00, L2 regularization loss: 6.09E-01
Test scatter: [0.693  0.1129 0.5108 0.901 ], Lowest was [0.2091 0.0633 0.5108 0.4781]
Median for last 10 epochs: [0.4295 0.0795 0.5179 0.5497], Epochs since improvement 0
 16%|█▌        | 79/500 [1:17:59<6:38:11, 56.75s/it]Epoch: 80 done with learning rate 1.00E-02, Train loss: 2.74E+06, Train scatter: [0.4497 0.095  0.5368 0.644 ]
L1 regularization loss: 1.14E+00, L2 regularization loss: 6.27E-01
Test scatter: [0.4505 0.0978 0.5287 0.6462], Lowest was [0.2091 0.0633 0.5108 0.4781]
Median for last 10 epochs: [0.4505 0.0795 0.5179 0.5503], Epochs since improvement 2
 16%|█▌        | 80/500 [1:19:16<7:21:02, 63.01s/it] 16%|█▌        | 81/500 [1:20:03<6:45:53, 58.12s/it] 16%|█▋        | 82/500 [1:21:15<7:12:55, 62.14s/it]Epoch: 82 done with learning rate 9.99E-03, Train loss: 2.66E+06, Train scatter: [0.3537 0.0862 0.5406 0.5993]
L1 regularization loss: 1.15E+00, L2 regularization loss: 6.56E-01
Test scatter: [0.3653 0.0874 0.5324 0.6008], Lowest was [0.2091 0.0633 0.5108 0.4781]
Median for last 10 epochs: [0.4505 0.0874 0.5179 0.6008], Epochs since improvement 4
 17%|█▋        | 83/500 [1:22:01<6:39:45, 57.52s/it] 17%|█▋        | 84/500 [1:23:12<7:05:43, 61.40s/it]Epoch: 84 done with learning rate 9.99E-03, Train loss: 2.28E+06, Train scatter: [0.4141 0.0876 0.4312 0.6002]
L1 regularization loss: 1.18E+00, L2 regularization loss: 6.95E-01
Test scatter: [0.4115 0.0835 0.4349 0.5994], Lowest was [0.2091 0.0633 0.4349 0.4781]
Median for last 10 epochs: [0.4295 0.0874 0.5165 0.6008], Epochs since improvement 0
 17%|█▋        | 85/500 [1:23:59<6:34:14, 57.00s/it] 17%|█▋        | 86/500 [1:25:09<7:00:23, 60.93s/it]Epoch: 86 done with learning rate 9.98E-03, Train loss: 1.42E+06, Train scatter: [0.4509 0.0747 0.4421 0.5609]
L1 regularization loss: 1.23E+00, L2 regularization loss: 7.51E-01
Test scatter: [0.4463 0.0761 0.447  0.5664], Lowest was [0.2091 0.0633 0.4349 0.4781]
Median for last 10 epochs: [0.4463 0.0874 0.5108 0.6008], Epochs since improvement 2
 17%|█▋        | 87/500 [1:25:56<6:30:17, 56.70s/it] 18%|█▊        | 88/500 [1:27:06<6:57:45, 60.84s/it]Epoch: 88 done with learning rate 9.98E-03, Train loss: 6.50E+05, Train scatter: [0.2974 0.0702 0.3204 0.54  ]
L1 regularization loss: 1.24E+00, L2 regularization loss: 7.80E-01
Test scatter: [0.3006 0.072  0.3252 0.5402], Lowest was [0.2091 0.0633 0.3252 0.4781]
Median for last 10 epochs: [0.4115 0.0835 0.447  0.5994], Epochs since improvement 0
 18%|█▊        | 89/500 [1:27:53<6:28:14, 56.68s/it]Epoch: 90 done with learning rate 9.97E-03, Train loss: 6.56E+05, Train scatter: [0.4304 0.0722 0.2917 0.5357]
L1 regularization loss: 1.32E+00, L2 regularization loss: 8.53E-01
Test scatter: [0.4186 0.0737 0.2982 0.5362], Lowest was [0.2091 0.0633 0.2982 0.4781]
Median for last 10 epochs: [0.4115 0.0761 0.4349 0.5664], Epochs since improvement 0
 18%|█▊        | 90/500 [1:29:11<7:10:33, 63.01s/it] 18%|█▊        | 91/500 [1:29:58<6:36:36, 58.18s/it] 18%|█▊        | 92/500 [1:31:09<7:02:40, 62.16s/it]Epoch: 92 done with learning rate 9.96E-03, Train loss: 5.48E+05, Train scatter: [0.2752 0.0731 0.2755 0.5239]
L1 regularization loss: 1.34E+00, L2 regularization loss: 8.96E-01
Test scatter: [0.2783 0.0745 0.2838 0.5252], Lowest was [0.2091 0.0633 0.2838 0.4781]
Median for last 10 epochs: [0.4115 0.0745 0.3252 0.5402], Epochs since improvement 0
 19%|█▊        | 93/500 [1:31:56<6:30:00, 57.50s/it] 19%|█▉        | 94/500 [1:33:07<6:57:17, 61.67s/it]Epoch: 94 done with learning rate 9.95E-03, Train loss: 4.20E+05, Train scatter: [0.2706 0.0621 0.2771 0.522 ]
L1 regularization loss: 1.35E+00, L2 regularization loss: 9.47E-01
Test scatter: [0.2759 0.0639 0.2869 0.5254], Lowest was [0.2091 0.0633 0.2838 0.4781]
Median for last 10 epochs: [0.3006 0.0737 0.2982 0.5362], Epochs since improvement 2
 19%|█▉        | 95/500 [1:33:54<6:26:01, 57.19s/it] 19%|█▉        | 96/500 [1:35:04<6:51:10, 61.07s/it]Epoch: 96 done with learning rate 9.94E-03, Train loss: 3.98E+05, Train scatter: [0.2613 0.0587 0.262  0.5099]
L1 regularization loss: 1.37E+00, L2 regularization loss: 9.94E-01
Test scatter: [0.2676 0.0609 0.2695 0.5095], Lowest was [0.2091 0.0609 0.2695 0.4781]
Median for last 10 epochs: [0.2783 0.072  0.2869 0.5254], Epochs since improvement 0
 19%|█▉        | 97/500 [1:35:51<6:21:20, 56.78s/it] 20%|█▉        | 98/500 [1:37:01<6:47:40, 60.85s/it]Epoch: 98 done with learning rate 9.93E-03, Train loss: 3.37E+05, Train scatter: [0.2681 0.0582 0.2563 0.5116]
L1 regularization loss: 1.37E+00, L2 regularization loss: 1.03E+00
Test scatter: [0.2703 0.0594 0.2658 0.5096], Lowest was [0.2091 0.0594 0.2658 0.4781]
Median for last 10 epochs: [0.2759 0.0639 0.2838 0.5252], Epochs since improvement 0
 20%|█▉        | 99/500 [1:37:48<6:18:49, 56.68s/it]Epoch: 100 done with learning rate 9.91E-03, Train loss: 3.28E+05, Train scatter: [0.2408 0.0605 0.2715 0.5103]
L1 regularization loss: 1.38E+00, L2 regularization loss: 1.06E+00
Test scatter: [0.2457 0.0614 0.2786 0.508 ], Lowest was [0.2091 0.0594 0.2658 0.4781]
Median for last 10 epochs: [0.2703 0.0614 0.2786 0.5096], Epochs since improvement 2
 20%|██        | 100/500 [1:39:05<6:58:22, 62.76s/it] 20%|██        | 101/500 [1:39:52<6:25:31, 57.97s/it] 20%|██        | 102/500 [1:41:02<6:48:51, 61.64s/it]Epoch: 102 done with learning rate 9.90E-03, Train loss: 4.17E+05, Train scatter: [0.2619 0.0604 0.2677 0.5142]
L1 regularization loss: 1.44E+00, L2 regularization loss: 1.13E+00
Test scatter: [0.2631 0.0614 0.2757 0.5131], Lowest was [0.2091 0.0594 0.2658 0.4781]
Median for last 10 epochs: [0.2676 0.0614 0.2757 0.5096], Epochs since improvement 4
 21%|██        | 103/500 [1:41:49<6:18:26, 57.20s/it] 21%|██        | 104/500 [1:42:59<6:43:38, 61.16s/it]Epoch: 104 done with learning rate 9.89E-03, Train loss: 2.66E+05, Train scatter: [0.2278 0.0536 0.2506 0.4959]
L1 regularization loss: 1.45E+00, L2 regularization loss: 1.17E+00
Test scatter: [0.2343 0.0547 0.259  0.4929], Lowest was [0.2091 0.0547 0.259  0.4781]
Median for last 10 epochs: [0.2631 0.0609 0.2695 0.5095], Epochs since improvement 0
 21%|██        | 105/500 [1:43:46<6:13:57, 56.80s/it] 21%|██        | 106/500 [1:44:56<6:39:18, 60.81s/it]Epoch: 106 done with learning rate 9.87E-03, Train loss: 2.08E+05, Train scatter: [0.2244 0.0539 0.2509 0.4872]
L1 regularization loss: 1.45E+00, L2 regularization loss: 1.20E+00
Test scatter: [0.2224 0.0547 0.2576 0.4863], Lowest was [0.2091 0.0547 0.2576 0.4781]
Median for last 10 epochs: [0.2457 0.0594 0.2658 0.508 ], Epochs since improvement 0
 21%|██▏       | 107/500 [1:45:43<6:10:33, 56.57s/it] 22%|██▏       | 108/500 [1:46:53<6:36:27, 60.68s/it]Epoch: 108 done with learning rate 9.85E-03, Train loss: 2.38E+05, Train scatter: [0.2219 0.0538 0.2513 0.4895]
L1 regularization loss: 1.46E+00, L2 regularization loss: 1.24E+00
Test scatter: [0.2311 0.0548 0.2622 0.4877], Lowest was [0.2091 0.0547 0.2576 0.4781]
Median for last 10 epochs: [0.2343 0.0548 0.2622 0.4929], Epochs since improvement 2
 22%|██▏       | 109/500 [1:47:40<6:08:02, 56.48s/it]Epoch: 110 done with learning rate 9.83E-03, Train loss: 2.39E+05, Train scatter: [0.2197 0.0531 0.2469 0.4676]
L1 regularization loss: 1.49E+00, L2 regularization loss: 1.30E+00
Test scatter: [0.2255 0.0538 0.2546 0.4657], Lowest was [0.2091 0.0538 0.2546 0.4657]
Median for last 10 epochs: [0.2311 0.0547 0.259  0.4877], Epochs since improvement 0
 22%|██▏       | 110/500 [1:48:57<6:47:48, 62.74s/it] 22%|██▏       | 111/500 [1:49:44<6:15:36, 57.93s/it] 22%|██▏       | 112/500 [1:50:54<6:38:29, 61.62s/it]Epoch: 112 done with learning rate 9.81E-03, Train loss: 1.78E+05, Train scatter: [0.2333 0.0502 0.2399 0.4654]
L1 regularization loss: 1.49E+00, L2 regularization loss: 1.34E+00
Test scatter: [0.2359 0.0505 0.2494 0.4684], Lowest was [0.2091 0.0505 0.2494 0.4657]
Median for last 10 epochs: [0.2311 0.0547 0.2576 0.4863], Epochs since improvement 0
 23%|██▎       | 113/500 [1:51:41<6:08:42, 57.16s/it] 23%|██▎       | 114/500 [1:52:52<6:34:40, 61.35s/it]Epoch: 114 done with learning rate 9.79E-03, Train loss: 1.22E+05, Train scatter: [0.2058 0.0534 0.2461 0.4588]
L1 regularization loss: 1.52E+00, L2 regularization loss: 1.39E+00
Test scatter: [0.2128 0.0537 0.255  0.4562], Lowest was [0.2091 0.0505 0.2494 0.4562]
Median for last 10 epochs: [0.2255 0.0538 0.255  0.4684], Epochs since improvement 0
 23%|██▎       | 115/500 [1:53:39<6:05:38, 56.98s/it] 23%|██▎       | 116/500 [1:54:49<6:30:08, 60.96s/it]Epoch: 116 done with learning rate 9.77E-03, Train loss: 3.53E+05, Train scatter: [0.2155 0.0507 0.2557 0.4566]
L1 regularization loss: 1.54E+00, L2 regularization loss: 1.44E+00
Test scatter: [0.2152 0.0508 0.2599 0.4491], Lowest was [0.2091 0.0505 0.2494 0.4491]
Median for last 10 epochs: [0.2255 0.0537 0.255  0.4657], Epochs since improvement 0
 23%|██▎       | 117/500 [1:55:36<6:01:45, 56.67s/it] 24%|██▎       | 118/500 [1:56:46<6:26:24, 60.69s/it]Epoch: 118 done with learning rate 9.75E-03, Train loss: 4.79E+05, Train scatter: [0.2417 0.0499 0.2535 0.4532]
L1 regularization loss: 1.56E+00, L2 regularization loss: 1.50E+00
Test scatter: [0.2319 0.0491 0.2528 0.4469], Lowest was [0.2091 0.0491 0.2494 0.4469]
Median for last 10 epochs: [0.2255 0.0508 0.2546 0.4562], Epochs since improvement 0
 24%|██▍       | 119/500 [1:57:32<5:58:55, 56.52s/it]Epoch: 120 done with learning rate 9.73E-03, Train loss: 1.44E+05, Train scatter: [0.2354 0.0516 0.2479 0.4528]
L1 regularization loss: 1.57E+00, L2 regularization loss: 1.54E+00
Test scatter: [0.2381 0.0514 0.2548 0.4509], Lowest was [0.2091 0.0491 0.2494 0.4469]
Median for last 10 epochs: [0.2319 0.0508 0.2548 0.4509], Epochs since improvement 2
 24%|██▍       | 120/500 [1:58:49<6:35:06, 62.39s/it] 24%|██▍       | 121/500 [1:59:35<6:04:18, 57.67s/it] 24%|██▍       | 122/500 [2:00:46<6:27:21, 61.48s/it]Epoch: 122 done with learning rate 9.70E-03, Train loss: 3.05E+03, Train scatter: [0.1857 0.0476 0.2446 0.4394]
L1 regularization loss: 1.54E+00, L2 regularization loss: 1.55E+00
Test scatter: [0.1874 0.0481 0.2484 0.4352], Lowest was [0.1874 0.0481 0.2484 0.4352]
Median for last 10 epochs: [0.2152 0.0508 0.2548 0.4491], Epochs since improvement 0
 25%|██▍       | 123/500 [2:01:32<5:58:27, 57.05s/it] 25%|██▍       | 124/500 [2:02:43<6:23:19, 61.17s/it]Epoch: 124 done with learning rate 9.68E-03, Train loss: -2.37E+03, Train scatter: [0.1754 0.0462 0.2365 0.4362]
L1 regularization loss: 1.54E+00, L2 regularization loss: 1.58E+00
Test scatter: [0.1766 0.0466 0.2398 0.4309], Lowest was [0.1766 0.0466 0.2398 0.4309]
Median for last 10 epochs: [0.2152 0.0491 0.2528 0.4469], Epochs since improvement 0
 25%|██▌       | 125/500 [2:03:30<5:55:14, 56.84s/it] 25%|██▌       | 126/500 [2:04:40<6:19:37, 60.90s/it]Epoch: 126 done with learning rate 9.65E-03, Train loss: 2.29E+05, Train scatter: [0.176  0.0477 0.2353 0.4513]
L1 regularization loss: 1.72E+00, L2 regularization loss: 1.76E+00
Test scatter: [0.1796 0.0473 0.2414 0.4437], Lowest was [0.1766 0.0466 0.2398 0.4309]
Median for last 10 epochs: [0.1874 0.0481 0.2484 0.4437], Epochs since improvement 2
 25%|██▌       | 127/500 [2:05:27<5:52:14, 56.66s/it] 26%|██▌       | 128/500 [2:06:37<6:17:02, 60.81s/it]Epoch: 128 done with learning rate 9.62E-03, Train loss: -1.94E+05, Train scatter: [0.1599 0.0447 0.2271 0.4376]
L1 regularization loss: 1.69E+00, L2 regularization loss: 1.76E+00
Test scatter: [0.1627 0.0447 0.2333 0.4302], Lowest was [0.1627 0.0447 0.2333 0.4302]
Median for last 10 epochs: [0.1796 0.0473 0.2414 0.4352], Epochs since improvement 0
 26%|██▌       | 129/500 [2:07:24<5:50:02, 56.61s/it]Epoch: 130 done with learning rate 9.59E-03, Train loss: -2.68E+05, Train scatter: [0.1608 0.0473 0.2386 0.438 ]
L1 regularization loss: 1.68E+00, L2 regularization loss: 1.78E+00
Test scatter: [0.1655 0.0474 0.2475 0.4356], Lowest was [0.1627 0.0447 0.2333 0.4302]
Median for last 10 epochs: [0.1766 0.0473 0.2414 0.4352], Epochs since improvement 2
 26%|██▌       | 130/500 [2:08:41<6:26:52, 62.74s/it] 26%|██▌       | 131/500 [2:09:28<5:56:09, 57.91s/it] 26%|██▋       | 132/500 [2:10:39<6:19:09, 61.82s/it]Epoch: 132 done with learning rate 9.56E-03, Train loss: -3.29E+05, Train scatter: [0.1644 0.0447 0.2339 0.4449]
L1 regularization loss: 1.65E+00, L2 regularization loss: 1.79E+00
Test scatter: [0.1628 0.0447 0.2374 0.4322], Lowest was [0.1627 0.0447 0.2333 0.4302]
Median for last 10 epochs: [0.1655 0.0466 0.2398 0.4322], Epochs since improvement 4
 27%|██▋       | 133/500 [2:11:26<5:50:20, 57.28s/it] 27%|██▋       | 134/500 [2:12:36<6:13:10, 61.18s/it]Epoch: 134 done with learning rate 9.53E-03, Train loss: -3.65E+05, Train scatter: [0.207  0.0438 0.2205 0.4311]
L1 regularization loss: 1.66E+00, L2 regularization loss: 1.81E+00
Test scatter: [0.1953 0.0439 0.2269 0.4248], Lowest was [0.1627 0.0439 0.2269 0.4248]
Median for last 10 epochs: [0.1655 0.0447 0.2374 0.4322], Epochs since improvement 0
 27%|██▋       | 135/500 [2:13:23<5:46:04, 56.89s/it] 27%|██▋       | 136/500 [2:14:33<6:09:08, 60.85s/it]Epoch: 136 done with learning rate 9.50E-03, Train loss: -3.85E+05, Train scatter: [0.146  0.0452 0.2325 0.4203]
L1 regularization loss: 1.65E+00, L2 regularization loss: 1.83E+00
Test scatter: [0.1512 0.0446 0.2387 0.4146], Lowest was [0.1512 0.0439 0.2269 0.4146]
Median for last 10 epochs: [0.1628 0.0447 0.2374 0.4302], Epochs since improvement 0
 27%|██▋       | 137/500 [2:15:19<5:42:22, 56.59s/it] 28%|██▊       | 138/500 [2:16:31<6:08:18, 61.05s/it]Epoch: 138 done with learning rate 9.47E-03, Train loss: -3.90E+05, Train scatter: [0.1412 0.0427 0.2254 0.4237]
L1 regularization loss: 1.71E+00, L2 regularization loss: 1.87E+00
Test scatter: [0.1432 0.0427 0.2285 0.4178], Lowest was [0.1432 0.0427 0.2269 0.4146]
Median for last 10 epochs: [0.1628 0.0446 0.2374 0.4248], Epochs since improvement 0
 28%|██▊       | 139/500 [2:17:18<5:41:30, 56.76s/it]Epoch: 140 done with learning rate 9.43E-03, Train loss: -4.01E+05, Train scatter: [0.1453 0.0422 0.2219 0.4229]
L1 regularization loss: 1.68E+00, L2 regularization loss: 1.90E+00
Test scatter: [0.1523 0.0419 0.2249 0.4193], Lowest was [0.1432 0.0419 0.2249 0.4146]
Median for last 10 epochs: [0.1523 0.0439 0.2285 0.4193], Epochs since improvement 0
 28%|██▊       | 140/500 [2:18:36<6:19:12, 63.20s/it] 28%|██▊       | 141/500 [2:19:23<5:48:58, 58.32s/it] 28%|██▊       | 142/500 [2:20:33<6:08:16, 61.72s/it]Epoch: 142 done with learning rate 9.40E-03, Train loss: -4.05E+05, Train scatter: [0.1401 0.0471 0.2318 0.433 ]
L1 regularization loss: 1.70E+00, L2 regularization loss: 1.94E+00
Test scatter: [0.146  0.0466 0.2373 0.4306], Lowest was [0.1432 0.0419 0.2249 0.4146]
Median for last 10 epochs: [0.1512 0.0439 0.2285 0.4193], Epochs since improvement 2
 29%|██▊       | 143/500 [2:21:19<5:40:35, 57.24s/it] 29%|██▉       | 144/500 [2:22:30<6:03:22, 61.24s/it]Epoch: 144 done with learning rate 9.36E-03, Train loss: -4.08E+05, Train scatter: [0.1373 0.0453 0.2253 0.4292]
L1 regularization loss: 1.69E+00, L2 regularization loss: 1.97E+00
Test scatter: [0.1431 0.045  0.2284 0.422 ], Lowest was [0.1431 0.0419 0.2249 0.4146]
Median for last 10 epochs: [0.146  0.0446 0.2285 0.4193], Epochs since improvement 0
 29%|██▉       | 145/500 [2:23:17<5:36:37, 56.90s/it] 29%|██▉       | 146/500 [2:24:27<6:00:22, 61.08s/it]Epoch: 146 done with learning rate 9.33E-03, Train loss: -4.08E+05, Train scatter: [0.1357 0.0419 0.2201 0.421 ]
L1 regularization loss: 1.71E+00, L2 regularization loss: 2.02E+00
Test scatter: [0.1426 0.0414 0.2231 0.4172], Lowest was [0.1426 0.0414 0.2231 0.4146]
Median for last 10 epochs: [0.1432 0.0427 0.2284 0.4193], Epochs since improvement 0
 29%|██▉       | 147/500 [2:25:14<5:34:12, 56.81s/it] 30%|██▉       | 148/500 [2:26:25<5:56:56, 60.84s/it]Epoch: 148 done with learning rate 9.29E-03, Train loss: -4.09E+05, Train scatter: [0.1942 0.0439 0.2278 0.4388]
L1 regularization loss: 1.74E+00, L2 regularization loss: 2.08E+00
Test scatter: [0.1869 0.0437 0.231  0.4294], Lowest was [0.1426 0.0414 0.2231 0.4146]
Median for last 10 epochs: [0.146  0.0437 0.2284 0.422 ], Epochs since improvement 2
 30%|██▉       | 149/500 [2:27:11<5:31:05, 56.60s/it]Epoch: 150 done with learning rate 9.25E-03, Train loss: -4.13E+05, Train scatter: [0.1841 0.0511 0.3841 0.4367]
L1 regularization loss: 1.74E+00, L2 regularization loss: 2.12E+00
Test scatter: [0.1775 0.0506 0.3773 0.4318], Lowest was [0.1426 0.0414 0.2231 0.4146]
Median for last 10 epochs: [0.146  0.045  0.231  0.4294], Epochs since improvement 4
 30%|███       | 150/500 [2:28:29<6:07:26, 62.99s/it] 30%|███       | 151/500 [2:29:16<5:38:10, 58.14s/it] 30%|███       | 152/500 [2:30:26<5:58:06, 61.74s/it]Epoch: 152 done with learning rate 9.21E-03, Train loss: -4.02E+05, Train scatter: [0.2464 0.0619 0.2551 0.4625]
L1 regularization loss: 1.81E+00, L2 regularization loss: 2.19E+00
Test scatter: [0.25   0.0622 0.2586 0.4672], Lowest was [0.1426 0.0414 0.2231 0.4146]
Median for last 10 epochs: [0.1775 0.045  0.231  0.4294], Epochs since improvement 6
 31%|███       | 153/500 [2:31:13<5:31:09, 57.26s/it] 31%|███       | 154/500 [2:32:24<5:54:36, 61.49s/it]Epoch: 154 done with learning rate 9.17E-03, Train loss: -3.94E+05, Train scatter: [0.1721 0.06   0.3305 0.4699]
L1 regularization loss: 1.95E+00, L2 regularization loss: 2.35E+00
Test scatter: [0.1742 0.0594 0.3275 0.4656], Lowest was [0.1426 0.0414 0.2231 0.4146]
Median for last 10 epochs: [0.1775 0.0506 0.2586 0.4318], Epochs since improvement 8
 31%|███       | 155/500 [2:33:11<5:28:16, 57.09s/it] 31%|███       | 156/500 [2:34:22<5:50:41, 61.17s/it]Epoch: 156 done with learning rate 9.13E-03, Train loss: -4.04E+05, Train scatter: [0.1415 0.0428 0.2354 0.4234]
L1 regularization loss: 1.96E+00, L2 regularization loss: 2.44E+00
Test scatter: [0.1555 0.0426 0.2373 0.4198], Lowest was [0.1426 0.0414 0.2231 0.4146]
Median for last 10 epochs: [0.1775 0.0506 0.2586 0.4318], Epochs since improvement 10
 31%|███▏      | 157/500 [2:35:09<5:24:55, 56.84s/it] 32%|███▏      | 158/500 [2:36:19<5:47:45, 61.01s/it]Epoch: 158 done with learning rate 9.09E-03, Train loss: -3.40E+05, Train scatter: [0.1333 0.0434 0.2559 0.4462]
L1 regularization loss: 2.10E+00, L2 regularization loss: 2.60E+00
Test scatter: [0.1459 0.0428 0.2583 0.4402], Lowest was [0.1426 0.0414 0.2231 0.4146]
Median for last 10 epochs: [0.1742 0.0506 0.2586 0.4402], Epochs since improvement 12
 32%|███▏      | 159/500 [2:37:06<5:22:29, 56.74s/it]Epoch: 160 done with learning rate 9.05E-03, Train loss: -4.07E+05, Train scatter: [0.2378 0.0642 0.2771 0.4677]
L1 regularization loss: 2.07E+00, L2 regularization loss: 2.62E+00
Test scatter: [0.2306 0.0619 0.2758 0.4633], Lowest was [0.1426 0.0414 0.2231 0.4146]
Median for last 10 epochs: [0.1742 0.0594 0.2586 0.4633], Epochs since improvement 14
 32%|███▏      | 160/500 [2:38:24<5:57:08, 63.03s/it] 32%|███▏      | 161/500 [2:39:11<5:28:42, 58.18s/it] 32%|███▏      | 162/500 [2:40:22<5:50:18, 62.18s/it]Epoch: 162 done with learning rate 9.00E-03, Train loss: -4.19E+05, Train scatter: [0.13   0.0407 0.2178 0.4204]
L1 regularization loss: 2.11E+00, L2 regularization loss: 2.69E+00
Test scatter: [0.1311 0.0401 0.2214 0.4126], Lowest was [0.1311 0.0401 0.2214 0.4126]
Median for last 10 epochs: [0.1555 0.0428 0.2583 0.4402], Epochs since improvement 0
 33%|███▎      | 163/500 [2:41:09<5:23:23, 57.58s/it] 33%|███▎      | 164/500 [2:42:20<5:44:29, 61.52s/it]Epoch: 164 done with learning rate 8.96E-03, Train loss: -1.02E+05, Train scatter: [0.8369 0.0768 0.531  0.5048]
L1 regularization loss: 2.43E+00, L2 regularization loss: 3.06E+00
Test scatter: [0.82   0.0739 0.5227 0.4987], Lowest was [0.1311 0.0401 0.2214 0.4126]
Median for last 10 epochs: [0.1555 0.0428 0.2583 0.4402], Epochs since improvement 2
 33%|███▎      | 165/500 [2:43:07<5:18:50, 57.11s/it] 33%|███▎      | 166/500 [2:44:17<5:40:02, 61.08s/it]Epoch: 166 done with learning rate 8.91E-03, Train loss: -2.81E+05, Train scatter: [0.3894 0.0722 0.462  0.5375]
L1 regularization loss: 2.62E+00, L2 regularization loss: 3.58E+00
Test scatter: [0.3791 0.0717 0.4558 0.5386], Lowest was [0.1311 0.0401 0.2214 0.4126]
Median for last 10 epochs: [0.2306 0.0619 0.2758 0.4633], Epochs since improvement 4
 33%|███▎      | 167/500 [2:45:04<5:15:16, 56.81s/it] 34%|███▎      | 168/500 [2:46:15<5:37:48, 61.05s/it]Epoch: 168 done with learning rate 8.86E-03, Train loss: -3.61E+05, Train scatter: [0.2308 0.0445 0.2594 0.4522]
L1 regularization loss: 2.57E+00, L2 regularization loss: 3.62E+00
Test scatter: [0.2361 0.0439 0.2592 0.4436], Lowest was [0.1311 0.0401 0.2214 0.4126]
Median for last 10 epochs: [0.2361 0.0619 0.2758 0.4633], Epochs since improvement 6
 34%|███▍      | 169/500 [2:47:01<5:13:09, 56.77s/it]Epoch: 170 done with learning rate 8.82E-03, Train loss: -3.73E+05, Train scatter: [0.239  0.0462 0.2564 0.4559]
L1 regularization loss: 2.54E+00, L2 regularization loss: 3.63E+00
Test scatter: [0.2362 0.0455 0.2564 0.4483], Lowest was [0.1311 0.0401 0.2214 0.4126]
Median for last 10 epochs: [0.2362 0.0455 0.2592 0.4483], Epochs since improvement 8
 34%|███▍      | 170/500 [2:48:20<5:47:39, 63.21s/it] 34%|███▍      | 171/500 [2:49:07<5:19:52, 58.34s/it] 34%|███▍      | 172/500 [2:50:17<5:38:48, 61.98s/it]Epoch: 172 done with learning rate 8.77E-03, Train loss: -3.72E+05, Train scatter: [0.1972 0.0423 0.2691 0.4495]
L1 regularization loss: 2.58E+00, L2 regularization loss: 3.70E+00
Test scatter: [0.1932 0.0418 0.2674 0.4399], Lowest was [0.1311 0.0401 0.2214 0.4126]
Median for last 10 epochs: [0.2362 0.0455 0.2674 0.4483], Epochs since improvement 10
 35%|███▍      | 173/500 [2:51:04<5:12:48, 57.40s/it] 35%|███▍      | 174/500 [2:52:14<5:32:55, 61.27s/it]Epoch: 174 done with learning rate 8.72E-03, Train loss: -4.04E+05, Train scatter: [0.1468 0.0447 0.2351 0.439 ]
L1 regularization loss: 2.57E+00, L2 regularization loss: 3.69E+00
Test scatter: [0.1388 0.0442 0.2352 0.4318], Lowest was [0.1311 0.0401 0.2214 0.4126]
Median for last 10 epochs: [0.2361 0.0442 0.2592 0.4436], Epochs since improvement 12
 35%|███▌      | 175/500 [2:53:01<5:08:23, 56.93s/it] 35%|███▌      | 176/500 [2:54:12<5:29:47, 61.07s/it]Epoch: 176 done with learning rate 8.67E-03, Train loss: -4.08E+05, Train scatter: [0.2977 0.0417 0.2314 0.4282]
L1 regularization loss: 2.56E+00, L2 regularization loss: 3.70E+00
Test scatter: [0.2895 0.0414 0.2333 0.4203], Lowest was [0.1311 0.0401 0.2214 0.4126]
Median for last 10 epochs: [0.2361 0.0439 0.2564 0.4399], Epochs since improvement 14
 35%|███▌      | 177/500 [2:54:58<5:05:33, 56.76s/it] 36%|███▌      | 178/500 [2:56:09<5:27:38, 61.05s/it]Epoch: 178 done with learning rate 8.62E-03, Train loss: -2.97E+04, Train scatter: [0.9281 0.1635 0.5439 0.9924]
L1 regularization loss: 3.71E+00, L2 regularization loss: 4.24E+00
Test scatter: [0.913  0.1604 0.5353 0.9822], Lowest was [0.1311 0.0401 0.2214 0.4126]
Median for last 10 epochs: [0.2362 0.0442 0.2564 0.4399], Epochs since improvement 16
 36%|███▌      | 179/500 [2:56:56<5:03:49, 56.79s/it]Epoch: 180 done with learning rate 8.57E-03, Train loss: -4.48E+04, Train scatter: [0.9187 0.1378 0.5437 0.9875]
L1 regularization loss: 3.66E+00, L2 regularization loss: 4.21E+00
Test scatter: [0.904  0.1372 0.5351 0.9776], Lowest was [0.1311 0.0401 0.2214 0.4126]
Median for last 10 epochs: [0.2895 0.0442 0.2674 0.4399], Epochs since improvement 18
 36%|███▌      | 180/500 [2:58:13<5:34:55, 62.80s/it] 36%|███▌      | 181/500 [2:59:00<5:08:16, 57.98s/it] 36%|███▋      | 182/500 [3:00:11<5:27:35, 61.81s/it]Epoch: 182 done with learning rate 8.52E-03, Train loss: -5.56E+04, Train scatter: [0.7128 0.135  0.543  0.9793]
L1 regularization loss: 3.64E+00, L2 regularization loss: 4.21E+00
Test scatter: [0.7119 0.1338 0.5344 0.9695], Lowest was [0.1311 0.0401 0.2214 0.4126]
Median for last 10 epochs: [0.7119 0.1338 0.5344 0.9695], Epochs since improvement 20
 37%|███▋      | 183/500 [3:00:57<5:02:44, 57.30s/it] 37%|███▋      | 183/500 [3:02:08<5:15:31, 59.72s/it]
Epoch: 184 done with learning rate 8.46E-03, Train loss: -5.47E+04, Train scatter: [0.8968 0.1318 0.5415 0.9411]
L1 regularization loss: 3.66E+00, L2 regularization loss: 4.30E+00
Test scatter: [0.883  0.1303 0.533  0.9327], Lowest was [0.1311 0.0401 0.2214 0.4126]
Median for last 10 epochs: [0.883  0.1338 0.5344 0.9695], Epochs since improvement 22
Exited after 184 epochs due to early stopping
10928.99 seconds spent training, 21.858 seconds per epoch. Processed 3186 trees per second
[0.8829416  0.13031127 0.5329644  0.9326975 ]
{'epoch_exit': 183, 'scatter_m_star': 0.8829416, 'lowest_m_star': 0.1311091, 'last20_m_star': 0.3343253, 'last10_m_star': 0.88296694, 'scatter_v_disk': 0.13031127, 'lowest_v_disk': 0.040125888, 'last20_v_disk': 0.058613934, 'last10_v_disk': 0.13384736, 'scatter_m_cold': 0.5329644, 'lowest_m_cold': 0.22141689, 'last20_m_cold': 0.3616289, 'last10_m_cold': 0.5344344, 'scatter_sfr_100': 0.9326975, 'lowest_sfr_100': 0.41256857, 'last20_sfr_100': 0.49347967, 'last10_sfr_100': 0.96954846}
Finished 1/1
Exited training after None epochs
Experiment 3 done: 3 / 6
Exploring ['conv_layers', 'batch_size']
Currently doing [  2 512]
{'experiment': 'GraphMerge', 'group': 'meta_pysr', 'move': False, 'model': 'MetaEdge', 'log': True, 'run_params': {'n_epochs': 500, 'n_trials': 1, 'batch_size': 512, 'val_epoch': 2, 'early_stopping': True, 'patience': 20, 'l1_lambda': 3e-05, 'l2_lambda': 3e-05, 'loss_func': 'GaussNd', 'metrics': 'test_multi_varrho', 'performance_plot': 'multi_base', 'save': True, 'seed': True, 'num_workers': 4}, 'learn_params': {'learning_rate': 0.01, 'schedule': 'onecycle', 'g_up': 1, 'g_down': 0.95, 'warmup': 4, 'period': 5, 'eta_min': 1e-05}, 'hyper_params': {'hidden_channels': 128, 'conv_layers': 2, 'conv_activation': 'relu', 'decode_activation': 'leakyrelu', 'decode_layers': 2, 'layernorm': True, 'agg': 'sum', 'variance': True, 'rho': 0, 'in_channels': 43, 'out_channels': 4}, 'data_params': {'case': 'vlarge_all_4t_z0.0_quantile_raw', 'targets': [0, 1, 2, 3], 'del_feats': [], 'split': 0.8, 'test': 0, 'scale': 0}, 'experiment_name': 'con2bat512'}
/home/cj1223/../../scratch/gpfs/cj1223/GraphResults/results_meta_pysr_120322/MetaEdge_vlarge_all_4t_z0.0_quantile_raw_gdxeec
RelU conv activation
LeakyRelU decode activation
N_params 422280
Made folder for saving model
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:41<5:41:58, 41.12s/it]  0%|          | 2/500 [01:44<7:27:38, 53.93s/it]Epoch: 2 done with learning rate 4.17E-04, Train loss: 3.03E+07, Train scatter: [0.9352 0.1708 0.5441 0.9954]
L1 regularization loss: 5.94E-01, L2 regularization loss: 1.13E-01
Test scatter: [0.9196 0.1675 0.5356 0.9851], Lowest was [0.9196 0.1675 0.5356 0.9851]
Median for last 10 epochs: [0.9196 0.1675 0.5356 0.9851], Epochs since improvement 2
  1%|          | 3/500 [02:24<6:35:48, 47.78s/it]  1%|          | 4/500 [03:27<7:23:23, 53.64s/it]Epoch: 4 done with learning rate 4.67E-04, Train loss: 2.21E+07, Train scatter: [0.9351 0.144  0.5441 0.9954]
L1 regularization loss: 5.98E-01, L2 regularization loss: 1.16E-01
Test scatter: [0.9195 0.1396 0.5355 0.9851], Lowest was [0.9195 0.1396 0.5355 0.9851]
Median for last 10 epochs: [0.9195 0.1396 0.5355 0.9851], Epochs since improvement 0
  1%|          | 5/500 [04:07<6:42:43, 48.81s/it]  1%|          | 6/500 [05:09<7:20:22, 53.49s/it]Epoch: 6 done with learning rate 5.51E-04, Train loss: 1.69E+07, Train scatter: [0.9349 0.1105 0.544  0.9954]
L1 regularization loss: 6.03E-01, L2 regularization loss: 1.20E-01
Test scatter: [0.9192 0.1086 0.5355 0.985 ], Lowest was [0.9192 0.1086 0.5355 0.985 ]
Median for last 10 epochs: [0.9192 0.1086 0.5355 0.985 ], Epochs since improvement 0
  1%|▏         | 7/500 [05:50<6:44:14, 49.20s/it]  2%|▏         | 8/500 [06:53<7:18:56, 53.53s/it]Epoch: 8 done with learning rate 6.67E-04, Train loss: 1.32E+07, Train scatter: [0.9271 0.0925 0.5441 0.8272]
L1 regularization loss: 6.11E-01, L2 regularization loss: 1.28E-01
Test scatter: [0.9115 0.0932 0.5355 0.8187], Lowest was [0.9115 0.0932 0.5355 0.8187]
Median for last 10 epochs: [0.9154 0.1009 0.5355 0.9018], Epochs since improvement 0
  2%|▏         | 9/500 [07:33<6:44:08, 49.39s/it]Epoch: 10 done with learning rate 8.15E-04, Train loss: 9.66E+06, Train scatter: [0.6716 0.0889 0.544  0.606 ]
L1 regularization loss: 6.20E-01, L2 regularization loss: 1.37E-01
Test scatter: [0.6644 0.0895 0.5355 0.6084], Lowest was [0.6644 0.0895 0.5355 0.6084]
Median for last 10 epochs: [0.9115 0.0932 0.5355 0.8187], Epochs since improvement 0
  2%|▏         | 10/500 [08:43<7:36:53, 55.95s/it]  2%|▏         | 11/500 [09:24<6:57:23, 51.21s/it]  2%|▏         | 12/500 [10:27<7:26:37, 54.91s/it]Epoch: 12 done with learning rate 9.94E-04, Train loss: 6.85E+06, Train scatter: [0.6747 0.0866 0.5441 0.5752]
L1 regularization loss: 6.26E-01, L2 regularization loss: 1.43E-01
Test scatter: [0.6689 0.087  0.5355 0.5709], Lowest was [0.6644 0.087  0.5355 0.5709]
Median for last 10 epochs: [0.9115 0.0932 0.5355 0.8187], Epochs since improvement 0
  3%|▎         | 13/500 [11:08<6:50:21, 50.56s/it]  3%|▎         | 14/500 [12:11<7:21:09, 54.46s/it]Epoch: 14 done with learning rate 1.20E-03, Train loss: 5.28E+06, Train scatter: [0.5294 0.0842 0.5441 0.5524]
L1 regularization loss: 6.28E-01, L2 regularization loss: 1.46E-01
Test scatter: [0.5234 0.0832 0.5355 0.5437], Lowest was [0.5234 0.0832 0.5355 0.5437]
Median for last 10 epochs: [0.6689 0.0895 0.5355 0.6084], Epochs since improvement 0
  3%|▎         | 15/500 [12:52<6:46:11, 50.25s/it]  3%|▎         | 16/500 [13:55<7:17:15, 54.21s/it]Epoch: 16 done with learning rate 1.44E-03, Train loss: 4.70E+06, Train scatter: [0.4787 0.0808 0.544  0.5384]
L1 regularization loss: 6.32E-01, L2 regularization loss: 1.48E-01
Test scatter: [0.4729 0.0808 0.5354 0.533 ], Lowest was [0.4729 0.0808 0.5354 0.533 ]
Median for last 10 epochs: [0.6644 0.087  0.5355 0.5709], Epochs since improvement 0
  3%|▎         | 17/500 [14:36<6:43:08, 50.08s/it]  4%|▎         | 18/500 [15:39<7:14:22, 54.07s/it]Epoch: 18 done with learning rate 1.70E-03, Train loss: 4.45E+06, Train scatter: [0.3025 0.0805 0.5439 0.5417]
L1 regularization loss: 6.35E-01, L2 regularization loss: 1.50E-01
Test scatter: [0.308  0.0797 0.5354 0.5341], Lowest was [0.308  0.0797 0.5354 0.533 ]
Median for last 10 epochs: [0.5234 0.0832 0.5355 0.5437], Epochs since improvement 0
  4%|▍         | 19/500 [16:20<6:40:36, 49.97s/it]Epoch: 20 done with learning rate 1.99E-03, Train loss: 3.90E+06, Train scatter: [0.2715 0.077  0.5438 0.5326]
L1 regularization loss: 6.36E-01, L2 regularization loss: 1.51E-01
Test scatter: [0.2785 0.0777 0.5352 0.5295], Lowest was [0.2785 0.0777 0.5352 0.5295]
Median for last 10 epochs: [0.4729 0.0808 0.5354 0.5341], Epochs since improvement 0
  4%|▍         | 20/500 [17:29<7:26:33, 55.82s/it]  4%|▍         | 21/500 [18:09<6:48:44, 51.20s/it]  4%|▍         | 22/500 [19:13<7:17:56, 54.97s/it]Epoch: 22 done with learning rate 2.30E-03, Train loss: 3.72E+06, Train scatter: [0.2671 0.0767 0.5437 0.5255]
L1 regularization loss: 6.39E-01, L2 regularization loss: 1.52E-01
Test scatter: [0.2704 0.0766 0.5352 0.5188], Lowest was [0.2704 0.0766 0.5352 0.5188]
Median for last 10 epochs: [0.308  0.0797 0.5354 0.533 ], Epochs since improvement 0
  5%|▍         | 23/500 [19:54<6:42:18, 50.60s/it]  5%|▍         | 24/500 [20:57<7:12:57, 54.57s/it]Epoch: 24 done with learning rate 2.63E-03, Train loss: 3.50E+06, Train scatter: [0.3671 0.0745 0.5436 0.55  ]
L1 regularization loss: 6.42E-01, L2 regularization loss: 1.54E-01
Test scatter: [0.3672 0.0763 0.5351 0.5531], Lowest was [0.2704 0.0763 0.5351 0.5188]
Median for last 10 epochs: [0.308  0.0777 0.5352 0.533 ], Epochs since improvement 0
  5%|▌         | 25/500 [21:38<6:38:51, 50.38s/it]  5%|▌         | 26/500 [22:41<7:08:03, 54.19s/it]Epoch: 26 done with learning rate 2.98E-03, Train loss: 3.46E+06, Train scatter: [0.3642 0.0744 0.5436 0.5681]
L1 regularization loss: 6.45E-01, L2 regularization loss: 1.55E-01
Test scatter: [0.3635 0.0749 0.5351 0.5715], Lowest was [0.2704 0.0749 0.5351 0.5188]
Median for last 10 epochs: [0.308  0.0766 0.5352 0.5341], Epochs since improvement 0
  5%|▌         | 27/500 [23:22<6:34:42, 50.07s/it]  6%|▌         | 28/500 [24:24<7:04:08, 53.92s/it]Epoch: 28 done with learning rate 3.34E-03, Train loss: 3.32E+06, Train scatter: [0.2346 0.0733 0.5436 0.5088]
L1 regularization loss: 6.48E-01, L2 regularization loss: 1.56E-01
Test scatter: [0.2412 0.0735 0.5351 0.5056], Lowest was [0.2412 0.0735 0.5351 0.5056]
Median for last 10 epochs: [0.2785 0.0763 0.5351 0.5295], Epochs since improvement 0
  6%|▌         | 29/500 [25:05<6:31:34, 49.88s/it]Epoch: 30 done with learning rate 3.72E-03, Train loss: 3.32E+06, Train scatter: [0.2416 0.0738 0.5436 0.517 ]
L1 regularization loss: 6.51E-01, L2 regularization loss: 1.58E-01
Test scatter: [0.2456 0.075  0.5351 0.5207], Lowest was [0.2412 0.0735 0.5351 0.5056]
Median for last 10 epochs: [0.2704 0.075  0.5351 0.5207], Epochs since improvement 2
  6%|▌         | 30/500 [26:15<7:17:25, 55.84s/it]  6%|▌         | 31/500 [26:55<6:40:36, 51.25s/it]  6%|▋         | 32/500 [27:59<7:08:00, 54.87s/it]Epoch: 32 done with learning rate 4.10E-03, Train loss: 3.26E+06, Train scatter: [0.2492 0.0715 0.5436 0.5187]
L1 regularization loss: 6.55E-01, L2 regularization loss: 1.59E-01
Test scatter: [0.254  0.0722 0.535  0.5193], Lowest was [0.2412 0.0722 0.535  0.5056]
Median for last 10 epochs: [0.254  0.0749 0.5351 0.5207], Epochs since improvement 0
  7%|▋         | 33/500 [28:39<6:33:35, 50.57s/it]  7%|▋         | 34/500 [29:43<7:02:54, 54.45s/it]Epoch: 34 done with learning rate 4.50E-03, Train loss: 3.25E+06, Train scatter: [0.2677 0.0743 0.5435 0.5627]
L1 regularization loss: 6.59E-01, L2 regularization loss: 1.61E-01
Test scatter: [0.2701 0.0747 0.5349 0.5644], Lowest was [0.2412 0.0722 0.5349 0.5056]
Median for last 10 epochs: [0.254  0.0747 0.5351 0.5207], Epochs since improvement 0
  7%|▋         | 35/500 [30:23<6:29:49, 50.30s/it]  7%|▋         | 36/500 [31:26<6:57:10, 53.95s/it]Epoch: 36 done with learning rate 4.90E-03, Train loss: 3.19E+06, Train scatter: [0.2219 0.0693 0.5435 0.5185]
L1 regularization loss: 6.63E-01, L2 regularization loss: 1.63E-01
Test scatter: [0.2259 0.0706 0.5349 0.5112], Lowest was [0.2259 0.0706 0.5349 0.5056]
Median for last 10 epochs: [0.2456 0.0735 0.535  0.5193], Epochs since improvement 0
  7%|▋         | 37/500 [32:06<6:25:17, 49.93s/it]  8%|▊         | 38/500 [33:09<6:55:10, 53.92s/it]Epoch: 38 done with learning rate 5.30E-03, Train loss: 3.16E+06, Train scatter: [0.1961 0.0685 0.5435 0.4967]
L1 regularization loss: 6.69E-01, L2 regularization loss: 1.65E-01
Test scatter: [0.2062 0.0696 0.5349 0.4956], Lowest was [0.2062 0.0696 0.5349 0.4956]
Median for last 10 epochs: [0.2456 0.0722 0.5349 0.5193], Epochs since improvement 0
  8%|▊         | 39/500 [33:50<6:23:06, 49.86s/it]Epoch: 40 done with learning rate 5.70E-03, Train loss: 3.15E+06, Train scatter: [0.2319 0.0761 0.5434 0.5173]
L1 regularization loss: 6.73E-01, L2 regularization loss: 1.67E-01
Test scatter: [0.2357 0.0762 0.5348 0.5098], Lowest was [0.2062 0.0696 0.5348 0.4956]
Median for last 10 epochs: [0.2357 0.0722 0.5349 0.5112], Epochs since improvement 0
  8%|▊         | 40/500 [34:59<7:07:12, 55.72s/it]  8%|▊         | 41/500 [35:40<6:31:26, 51.17s/it]  8%|▊         | 42/500 [36:43<6:58:48, 54.86s/it]Epoch: 42 done with learning rate 6.10E-03, Train loss: 3.13E+06, Train scatter: [0.2041 0.0684 0.5434 0.5011]
L1 regularization loss: 6.78E-01, L2 regularization loss: 1.70E-01
Test scatter: [0.2188 0.0697 0.5348 0.5021], Lowest was [0.2062 0.0696 0.5348 0.4956]
Median for last 10 epochs: [0.2259 0.0706 0.5349 0.5098], Epochs since improvement 0
  9%|▊         | 43/500 [37:24<6:25:04, 50.56s/it]  9%|▉         | 44/500 [38:27<6:54:01, 54.48s/it]Epoch: 44 done with learning rate 6.49E-03, Train loss: 3.12E+06, Train scatter: [0.2797 0.0684 0.5433 0.4994]
L1 regularization loss: 6.82E-01, L2 regularization loss: 1.72E-01
Test scatter: [0.2736 0.0678 0.5348 0.4951], Lowest was [0.2062 0.0678 0.5348 0.4951]
Median for last 10 epochs: [0.2259 0.0697 0.5348 0.5021], Epochs since improvement 0
  9%|▉         | 45/500 [39:08<6:21:16, 50.28s/it]  9%|▉         | 46/500 [40:13<6:53:09, 54.60s/it]Epoch: 46 done with learning rate 6.87E-03, Train loss: 5.36E+06, Train scatter: [0.6922 0.1751 0.5441 0.9844]
L1 regularization loss: 8.60E-01, L2 regularization loss: 2.33E-01
Test scatter: [0.7072 0.1712 0.5355 0.9751], Lowest was [0.2062 0.0678 0.5348 0.4951]
Median for last 10 epochs: [0.2357 0.0697 0.5348 0.5021], Epochs since improvement 2
  9%|▉         | 47/500 [40:53<6:20:01, 50.33s/it] 10%|▉         | 48/500 [41:57<6:49:12, 54.32s/it]Epoch: 48 done with learning rate 7.24E-03, Train loss: 3.86E+06, Train scatter: [0.5658 0.1696 0.544  0.9855]
L1 regularization loss: 8.62E-01, L2 regularization loss: 2.36E-01
Test scatter: [0.5524 0.1656 0.5354 0.976 ], Lowest was [0.2062 0.0678 0.5348 0.4951]
Median for last 10 epochs: [0.2736 0.0762 0.5348 0.5098], Epochs since improvement 4
 10%|▉         | 49/500 [42:37<6:16:47, 50.13s/it]Epoch: 50 done with learning rate 7.60E-03, Train loss: 3.68E+06, Train scatter: [0.5286 0.1601 0.5441 0.9757]
L1 regularization loss: 8.65E-01, L2 regularization loss: 2.40E-01
Test scatter: [0.5324 0.1561 0.5355 0.9666], Lowest was [0.2062 0.0678 0.5348 0.4951]
Median for last 10 epochs: [0.5324 0.1561 0.5354 0.9666], Epochs since improvement 6
 10%|█         | 50/500 [43:47<7:00:39, 56.09s/it] 10%|█         | 51/500 [44:28<6:25:00, 51.45s/it] 10%|█         | 52/500 [45:31<6:51:24, 55.10s/it]Epoch: 52 done with learning rate 7.94E-03, Train loss: 3.61E+06, Train scatter: [0.5213 0.1303 0.5441 0.9516]
L1 regularization loss: 8.73E-01, L2 regularization loss: 2.49E-01
Test scatter: [0.5087 0.1275 0.5355 0.9198], Lowest was [0.2062 0.0678 0.5348 0.4951]
Median for last 10 epochs: [0.5324 0.1561 0.5355 0.9666], Epochs since improvement 8
 11%|█         | 53/500 [46:12<6:17:53, 50.72s/it] 11%|█         | 54/500 [47:16<6:46:42, 54.72s/it]Epoch: 54 done with learning rate 8.26E-03, Train loss: 3.45E+06, Train scatter: [0.4874 0.1038 0.544  0.7178]
L1 regularization loss: 8.81E-01, L2 regularization loss: 2.58E-01
Test scatter: [0.4853 0.1035 0.5354 0.7093], Lowest was [0.2062 0.0678 0.5348 0.4951]
Median for last 10 epochs: [0.5324 0.1561 0.5355 0.9666], Epochs since improvement 10
 11%|█         | 55/500 [47:56<6:14:12, 50.45s/it] 11%|█         | 56/500 [49:00<6:42:59, 54.46s/it]Epoch: 56 done with learning rate 8.56E-03, Train loss: 3.43E+06, Train scatter: [0.5749 0.1027 0.5439 0.7043]
L1 regularization loss: 8.90E-01, L2 regularization loss: 2.71E-01
Test scatter: [0.5717 0.1052 0.5354 0.6979], Lowest was [0.2062 0.0678 0.5348 0.4951]
Median for last 10 epochs: [0.5324 0.1275 0.5354 0.9198], Epochs since improvement 12
 11%|█▏        | 57/500 [49:41<6:11:15, 50.28s/it] 12%|█▏        | 58/500 [50:45<6:41:06, 54.45s/it]Epoch: 58 done with learning rate 8.83E-03, Train loss: 3.45E+06, Train scatter: [0.8459 0.0904 0.5439 0.6604]
L1 regularization loss: 9.01E-01, L2 regularization loss: 2.85E-01
Test scatter: [0.8314 0.0916 0.5354 0.6575], Lowest was [0.2062 0.0678 0.5348 0.4951]
Median for last 10 epochs: [0.5324 0.1052 0.5354 0.7093], Epochs since improvement 14
 12%|█▏        | 59/500 [51:25<6:09:41, 50.30s/it]Epoch: 60 done with learning rate 9.08E-03, Train loss: 3.39E+06, Train scatter: [0.4669 0.092  0.5439 0.6531]
L1 regularization loss: 9.08E-01, L2 regularization loss: 3.00E-01
Test scatter: [0.4607 0.0924 0.5353 0.6491], Lowest was [0.2062 0.0678 0.5348 0.4951]
Median for last 10 epochs: [0.5087 0.1035 0.5354 0.6979], Epochs since improvement 16
 12%|█▏        | 60/500 [52:34<6:49:48, 55.88s/it] 12%|█▏        | 61/500 [53:15<6:15:35, 51.33s/it] 12%|█▏        | 62/500 [54:18<6:40:17, 54.83s/it]Epoch: 62 done with learning rate 9.31E-03, Train loss: 3.37E+06, Train scatter: [0.353  0.0902 0.5439 0.6542]
L1 regularization loss: 9.13E-01, L2 regularization loss: 3.13E-01
Test scatter: [0.3573 0.0908 0.5353 0.6519], Lowest was [0.2062 0.0678 0.5348 0.4951]
Median for last 10 epochs: [0.4853 0.0924 0.5354 0.6575], Epochs since improvement 18
 13%|█▎        | 63/500 [54:59<6:08:14, 50.56s/it] 13%|█▎        | 64/500 [56:01<6:34:08, 54.24s/it]Epoch: 64 done with learning rate 9.50E-03, Train loss: 3.34E+06, Train scatter: [0.4768 0.1069 0.5439 0.6963]
L1 regularization loss: 9.20E-01, L2 regularization loss: 3.24E-01
Test scatter: [0.472  0.1054 0.5353 0.6862], Lowest was [0.2062 0.0678 0.5348 0.4951]
Median for last 10 epochs: [0.472  0.0924 0.5353 0.6575], Epochs since improvement 20
 13%|█▎        | 65/500 [56:42<6:03:06, 50.08s/it] 13%|█▎        | 65/500 [57:46<6:26:41, 53.34s/it]
Epoch: 66 done with learning rate 9.66E-03, Train loss: 3.28E+06, Train scatter: [0.4305 0.0952 0.5439 0.5968]
L1 regularization loss: 9.28E-01, L2 regularization loss: 3.36E-01
Test scatter: [0.4258 0.094  0.5353 0.5938], Lowest was [0.2062 0.0678 0.5348 0.4951]
Median for last 10 epochs: [0.4607 0.0924 0.5353 0.6519], Epochs since improvement 22
Exited after 66 epochs due to early stopping
3466.86 seconds spent training, 6.934 seconds per epoch. Processed 10043 trees per second
[0.42575005 0.09398703 0.53526443 0.593791  ]
{'epoch_exit': 65, 'scatter_m_star': 0.42575005, 'lowest_m_star': 0.20615622, 'last20_m_star': 0.4970094, 'last10_m_star': 0.46074158, 'scatter_v_disk': 0.09398703, 'lowest_v_disk': 0.06775095, 'last20_v_disk': 0.104371876, 'last10_v_disk': 0.092370085, 'scatter_m_cold': 0.53526443, 'lowest_m_cold': 0.5347627, 'last20_m_cold': 0.5353581, 'last10_m_cold': 0.5353062, 'scatter_sfr_100': 0.593791, 'lowest_sfr_100': 0.49510464, 'last20_sfr_100': 0.69204754, 'last10_sfr_100': 0.65187645}
Finished 1/1
Exited training after None epochs
Experiment 4 done: 4 / 6
Exploring ['conv_layers', 'batch_size']
Currently doing [  3 256]
{'experiment': 'GraphMerge', 'group': 'meta_pysr', 'move': False, 'model': 'MetaEdge', 'log': True, 'run_params': {'n_epochs': 500, 'n_trials': 1, 'batch_size': 256, 'val_epoch': 2, 'early_stopping': True, 'patience': 20, 'l1_lambda': 3e-05, 'l2_lambda': 3e-05, 'loss_func': 'GaussNd', 'metrics': 'test_multi_varrho', 'performance_plot': 'multi_base', 'save': True, 'seed': True, 'num_workers': 4}, 'learn_params': {'learning_rate': 0.01, 'schedule': 'onecycle', 'g_up': 1, 'g_down': 0.95, 'warmup': 4, 'period': 5, 'eta_min': 1e-05}, 'hyper_params': {'hidden_channels': 128, 'conv_layers': 3, 'conv_activation': 'relu', 'decode_activation': 'leakyrelu', 'decode_layers': 2, 'layernorm': True, 'agg': 'sum', 'variance': True, 'rho': 0, 'in_channels': 43, 'out_channels': 4}, 'data_params': {'case': 'vlarge_all_4t_z0.0_quantile_raw', 'targets': [0, 1, 2, 3], 'del_feats': [], 'split': 0.8, 'test': 0, 'scale': 0}, 'experiment_name': 'con3bat256'}
/home/cj1223/../../scratch/gpfs/cj1223/GraphResults/results_meta_pysr_120322/MetaEdge_vlarge_all_4t_z0.0_quantile_raw_zstahg
RelU conv activation
LeakyRelU decode activation
N_params 546504
Made folder for saving model
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [01:01<8:27:45, 61.05s/it]  0%|          | 2/500 [02:30<10:44:01, 77.59s/it]Epoch: 2 done with learning rate 4.17E-04, Train loss: 9.35E+07, Train scatter: [0.9351 0.1359 0.5441 0.9955]
L1 regularization loss: 7.36E-01, L2 regularization loss: 1.27E-01
Test scatter: [0.9195 0.1311 0.5355 0.9851], Lowest was [0.9195 0.1311 0.5355 0.9851]
Median for last 10 epochs: [0.9195 0.1311 0.5355 0.9851], Epochs since improvement 2
  1%|          | 3/500 [03:31<9:40:58, 70.14s/it]   1%|          | 4/500 [05:01<10:46:11, 78.17s/it]Epoch: 4 done with learning rate 4.67E-04, Train loss: 7.98E+07, Train scatter: [0.9321 0.0996 0.5437 0.9954]
L1 regularization loss: 7.41E-01, L2 regularization loss: 1.29E-01
Test scatter: [0.9166 0.0982 0.5351 0.9851], Lowest was [0.9166 0.0982 0.5351 0.9851]
Median for last 10 epochs: [0.9166 0.0982 0.5351 0.9851], Epochs since improvement 0
  1%|          | 5/500 [06:03<9:54:56, 72.11s/it]   1%|          | 6/500 [07:34<10:46:21, 78.51s/it]Epoch: 6 done with learning rate 5.51E-04, Train loss: 1.00E+08, Train scatter: [0.9354 0.1496 0.544  0.9955]
L1 regularization loss: 7.50E-01, L2 regularization loss: 1.31E-01
Test scatter: [0.9198 0.1424 0.5354 0.9851], Lowest was [0.9166 0.0982 0.5351 0.9851]
Median for last 10 epochs: [0.9166 0.0982 0.5351 0.9851], Epochs since improvement 2
  1%|▏         | 7/500 [08:35<9:58:50, 72.88s/it]   2%|▏         | 8/500 [10:05<10:43:18, 78.45s/it]Epoch: 8 done with learning rate 6.67E-04, Train loss: 8.82E+07, Train scatter: [0.9341 0.115  0.5418 0.9955]
L1 regularization loss: 7.54E-01, L2 regularization loss: 1.34E-01
Test scatter: [0.9185 0.1124 0.5331 0.9851], Lowest was [0.9166 0.0982 0.5331 0.9851]
Median for last 10 epochs: [0.9176 0.1053 0.5341 0.9851], Epochs since improvement 0
  2%|▏         | 9/500 [11:07<9:58:17, 73.11s/it] Epoch: 10 done with learning rate 8.15E-04, Train loss: 8.22E+07, Train scatter: [0.8544 0.1014 0.4502 0.9954]
L1 regularization loss: 7.64E-01, L2 regularization loss: 1.41E-01
Test scatter: [0.8417 0.1004 0.4465 0.985 ], Lowest was [0.8417 0.0982 0.4465 0.985 ]
Median for last 10 epochs: [0.9166 0.1004 0.5331 0.9851], Epochs since improvement 0
  2%|▏         | 10/500 [12:45<10:59:28, 80.75s/it]  2%|▏         | 11/500 [13:46<10:09:50, 74.83s/it]  2%|▏         | 12/500 [15:16<10:47:04, 79.56s/it]Epoch: 12 done with learning rate 9.94E-04, Train loss: 7.57E+07, Train scatter: [0.5285 0.0916 0.4084 0.9954]
L1 regularization loss: 7.72E-01, L2 regularization loss: 1.47E-01
Test scatter: [0.531  0.0914 0.4052 0.985 ], Lowest was [0.531  0.0914 0.4052 0.985 ]
Median for last 10 epochs: [0.9166 0.1004 0.5331 0.9851], Epochs since improvement 0
  3%|▎         | 13/500 [16:18<10:01:09, 74.07s/it]  3%|▎         | 14/500 [17:48<10:38:27, 78.82s/it]Epoch: 14 done with learning rate 1.20E-03, Train loss: 7.35E+07, Train scatter: [0.3551 0.0845 0.4034 0.9954]
L1 regularization loss: 7.74E-01, L2 regularization loss: 1.49E-01
Test scatter: [0.3686 0.086  0.4007 0.985 ], Lowest was [0.3686 0.086  0.4007 0.985 ]
Median for last 10 epochs: [0.8417 0.1004 0.4465 0.985 ], Epochs since improvement 0
  3%|▎         | 15/500 [18:49<9:54:26, 73.54s/it]   3%|▎         | 16/500 [20:19<10:32:11, 78.37s/it]Epoch: 16 done with learning rate 1.44E-03, Train loss: 7.04E+07, Train scatter: [0.3282 0.0808 0.3919 0.9954]
L1 regularization loss: 7.76E-01, L2 regularization loss: 1.51E-01
Test scatter: [0.3399 0.082  0.3931 0.9851], Lowest was [0.3399 0.082  0.3931 0.985 ]
Median for last 10 epochs: [0.531  0.0914 0.4052 0.985 ], Epochs since improvement 0
  3%|▎         | 17/500 [21:20<9:49:51, 73.28s/it]   4%|▎         | 18/500 [22:51<10:30:44, 78.51s/it]Epoch: 18 done with learning rate 1.70E-03, Train loss: 4.61E+07, Train scatter: [0.2906 0.0784 0.3556 0.9769]
L1 regularization loss: 7.79E-01, L2 regularization loss: 1.54E-01
Test scatter: [0.3053 0.0786 0.3564 0.9692], Lowest was [0.3053 0.0786 0.3564 0.9692]
Median for last 10 epochs: [0.3686 0.086  0.4007 0.985 ], Epochs since improvement 0
  4%|▍         | 19/500 [23:52<9:48:07, 73.36s/it] Epoch: 20 done with learning rate 1.99E-03, Train loss: 5.83E+06, Train scatter: [0.2737 0.084  0.3645 0.5584]
L1 regularization loss: 7.86E-01, L2 regularization loss: 1.58E-01
Test scatter: [0.2809 0.083  0.3605 0.5615], Lowest was [0.2809 0.0786 0.3564 0.5615]
Median for last 10 epochs: [0.3399 0.083  0.3931 0.985 ], Epochs since improvement 0
  4%|▍         | 20/500 [25:30<10:46:14, 80.78s/it]  4%|▍         | 21/500 [26:31<9:58:11, 74.93s/it]   4%|▍         | 22/500 [28:01<10:31:53, 79.32s/it]Epoch: 22 done with learning rate 2.30E-03, Train loss: 4.56E+06, Train scatter: [0.2406 0.0777 0.3494 0.5317]
L1 regularization loss: 7.91E-01, L2 regularization loss: 1.61E-01
Test scatter: [0.2496 0.0767 0.3493 0.5325], Lowest was [0.2496 0.0767 0.3493 0.5325]
Median for last 10 epochs: [0.3053 0.082  0.3605 0.9692], Epochs since improvement 0
  5%|▍         | 23/500 [29:02<9:47:39, 73.92s/it]   5%|▍         | 24/500 [30:32<10:22:49, 78.51s/it]Epoch: 24 done with learning rate 2.63E-03, Train loss: 4.00E+06, Train scatter: [0.2372 0.0757 0.3509 0.5196]
L1 regularization loss: 7.95E-01, L2 regularization loss: 1.63E-01
Test scatter: [0.2452 0.0752 0.3539 0.521 ], Lowest was [0.2452 0.0752 0.3493 0.521 ]
Median for last 10 epochs: [0.2809 0.0786 0.3564 0.5615], Epochs since improvement 0
  5%|▌         | 25/500 [31:33<9:40:56, 73.38s/it]   5%|▌         | 26/500 [33:03<10:19:47, 78.45s/it]Epoch: 26 done with learning rate 2.98E-03, Train loss: 3.90E+06, Train scatter: [0.228  0.0743 0.3437 0.5221]
L1 regularization loss: 8.00E-01, L2 regularization loss: 1.67E-01
Test scatter: [0.2316 0.0735 0.3447 0.5225], Lowest was [0.2316 0.0735 0.3447 0.521 ]
Median for last 10 epochs: [0.2496 0.0767 0.3539 0.5325], Epochs since improvement 0
  5%|▌         | 27/500 [34:05<9:37:59, 73.32s/it]   6%|▌         | 28/500 [35:35<10:16:21, 78.35s/it]Epoch: 28 done with learning rate 3.34E-03, Train loss: 3.77E+06, Train scatter: [0.2193 0.0721 0.3286 0.5079]
L1 regularization loss: 8.07E-01, L2 regularization loss: 1.71E-01
Test scatter: [0.2305 0.0721 0.3329 0.5129], Lowest was [0.2305 0.0721 0.3329 0.5129]
Median for last 10 epochs: [0.2452 0.0752 0.3493 0.5225], Epochs since improvement 0
  6%|▌         | 29/500 [36:36<9:34:55, 73.24s/it] Epoch: 30 done with learning rate 3.72E-03, Train loss: 3.68E+06, Train scatter: [0.2259 0.0723 0.3216 0.4998]
L1 regularization loss: 8.15E-01, L2 regularization loss: 1.76E-01
Test scatter: [0.2276 0.0718 0.3272 0.5004], Lowest was [0.2276 0.0718 0.3272 0.5004]
Median for last 10 epochs: [0.2316 0.0735 0.3447 0.521 ], Epochs since improvement 0
  6%|▌         | 30/500 [38:13<10:28:46, 80.27s/it]  6%|▌         | 31/500 [39:14<9:43:02, 74.59s/it]   6%|▋         | 32/500 [40:43<10:16:34, 79.05s/it]Epoch: 32 done with learning rate 4.10E-03, Train loss: 3.35E+06, Train scatter: [0.2667 0.0745 0.3294 0.5283]
L1 regularization loss: 8.22E-01, L2 regularization loss: 1.80E-01
Test scatter: [0.2671 0.0754 0.3345 0.5247], Lowest was [0.2276 0.0718 0.3272 0.5004]
Median for last 10 epochs: [0.2316 0.0735 0.3345 0.521 ], Epochs since improvement 2
  7%|▋         | 33/500 [41:45<9:34:01, 73.75s/it]   7%|▋         | 34/500 [43:15<10:11:13, 78.70s/it]Epoch: 34 done with learning rate 4.50E-03, Train loss: 3.62E+06, Train scatter: [0.2197 0.0706 0.3223 0.5092]
L1 regularization loss: 8.35E-01, L2 regularization loss: 1.87E-01
Test scatter: [0.2295 0.0701 0.3281 0.5127], Lowest was [0.2276 0.0701 0.3272 0.5004]
Median for last 10 epochs: [0.2305 0.0721 0.3329 0.5129], Epochs since improvement 0
  7%|▋         | 35/500 [44:16<9:29:36, 73.50s/it]   7%|▋         | 36/500 [45:46<10:06:26, 78.42s/it]Epoch: 36 done with learning rate 4.90E-03, Train loss: 3.13E+06, Train scatter: [0.2208 0.0673 0.3149 0.4981]
L1 regularization loss: 8.43E-01, L2 regularization loss: 1.93E-01
Test scatter: [0.2269 0.0683 0.3197 0.5004], Lowest was [0.2269 0.0683 0.3197 0.5004]
Median for last 10 epochs: [0.2295 0.0718 0.3281 0.5127], Epochs since improvement 0
  7%|▋         | 37/500 [46:48<9:25:56, 73.34s/it]   8%|▊         | 38/500 [48:18<10:03:45, 78.41s/it]Epoch: 38 done with learning rate 5.30E-03, Train loss: 3.01E+06, Train scatter: [0.2011 0.0653 0.3078 0.4858]
L1 regularization loss: 8.54E-01, L2 regularization loss: 2.01E-01
Test scatter: [0.2071 0.0651 0.3114 0.4838], Lowest was [0.2071 0.0651 0.3114 0.4838]
Median for last 10 epochs: [0.2276 0.0701 0.3272 0.5004], Epochs since improvement 0
  8%|▊         | 39/500 [49:19<9:23:17, 73.31s/it] Epoch: 40 done with learning rate 5.70E-03, Train loss: 2.88E+06, Train scatter: [0.2258 0.0655 0.3054 0.4921]
L1 regularization loss: 8.68E-01, L2 regularization loss: 2.11E-01
Test scatter: [0.2371 0.0667 0.3169 0.5002], Lowest was [0.2071 0.0651 0.3114 0.4838]
Median for last 10 epochs: [0.2295 0.0683 0.3197 0.5004], Epochs since improvement 2
  8%|▊         | 40/500 [50:57<10:17:56, 80.60s/it]  8%|▊         | 41/500 [51:58<9:32:20, 74.82s/it]   8%|▊         | 42/500 [53:28<10:04:55, 79.25s/it]Epoch: 42 done with learning rate 6.10E-03, Train loss: 2.76E+06, Train scatter: [0.2026 0.0605 0.3034 0.4809]
L1 regularization loss: 8.82E-01, L2 regularization loss: 2.21E-01
Test scatter: [0.2116 0.0607 0.3084 0.481 ], Lowest was [0.2071 0.0607 0.3084 0.481 ]
Median for last 10 epochs: [0.2269 0.0667 0.3169 0.5002], Epochs since improvement 0
  9%|▊         | 43/500 [54:29<9:22:11, 73.81s/it]   9%|▉         | 44/500 [55:59<9:56:43, 78.52s/it]Epoch: 44 done with learning rate 6.49E-03, Train loss: 2.54E+06, Train scatter: [0.2548 0.0614 0.2933 0.4889]
L1 regularization loss: 8.93E-01, L2 regularization loss: 2.32E-01
Test scatter: [0.2577 0.0614 0.2976 0.4867], Lowest was [0.2071 0.0607 0.2976 0.481 ]
Median for last 10 epochs: [0.2269 0.0651 0.3114 0.4867], Epochs since improvement 0
  9%|▉         | 45/500 [57:00<9:16:10, 73.34s/it]  9%|▉         | 46/500 [58:30<9:52:16, 78.27s/it]Epoch: 46 done with learning rate 6.87E-03, Train loss: 2.48E+06, Train scatter: [0.2289 0.0629 0.2841 0.4704]
L1 regularization loss: 9.07E-01, L2 regularization loss: 2.46E-01
Test scatter: [0.2354 0.0626 0.2887 0.4693], Lowest was [0.2071 0.0607 0.2887 0.4693]
Median for last 10 epochs: [0.2354 0.0626 0.3084 0.4838], Epochs since improvement 0
  9%|▉         | 47/500 [59:31<9:12:43, 73.21s/it] 10%|▉         | 48/500 [1:01:01<9:48:49, 78.16s/it]Epoch: 48 done with learning rate 7.24E-03, Train loss: 2.18E+06, Train scatter: [0.2351 0.0602 0.2904 0.4756]
L1 regularization loss: 9.20E-01, L2 regularization loss: 2.60E-01
Test scatter: [0.2324 0.0608 0.2905 0.4764], Lowest was [0.2071 0.0607 0.2887 0.4693]
Median for last 10 epochs: [0.2354 0.0614 0.2976 0.481 ], Epochs since improvement 2
 10%|▉         | 49/500 [1:02:02<9:09:50, 73.15s/it]Epoch: 50 done with learning rate 7.60E-03, Train loss: 8.93E+06, Train scatter: [0.898  0.1721 0.5434 0.9002]
L1 regularization loss: 9.93E-01, L2 regularization loss: 3.01E-01
Test scatter: [0.8853 0.1684 0.5349 0.8944], Lowest was [0.2071 0.0607 0.2887 0.4693]
Median for last 10 epochs: [0.2354 0.0614 0.2976 0.481 ], Epochs since improvement 4
 10%|█         | 50/500 [1:03:40<10:03:21, 80.45s/it] 10%|█         | 51/500 [1:04:41<9:19:30, 74.77s/it]  10%|█         | 52/500 [1:06:11<9:52:07, 79.30s/it]Epoch: 52 done with learning rate 7.94E-03, Train loss: 5.69E+06, Train scatter: [0.6284 0.1021 0.4982 0.6899]
L1 regularization loss: 1.02E+00, L2 regularization loss: 3.46E-01
Test scatter: [0.6257 0.1021 0.4964 0.6862], Lowest was [0.2071 0.0607 0.2887 0.4693]
Median for last 10 epochs: [0.2577 0.0626 0.2976 0.4867], Epochs since improvement 6
 11%|█         | 53/500 [1:07:13<9:10:58, 73.96s/it] 11%|█         | 54/500 [1:08:42<9:44:35, 78.64s/it]Epoch: 54 done with learning rate 8.26E-03, Train loss: 3.95E+06, Train scatter: [0.4756 0.0936 0.4863 0.6145]
L1 regularization loss: 1.03E+00, L2 regularization loss: 3.70E-01
Test scatter: [0.4658 0.0939 0.4767 0.6095], Lowest was [0.2071 0.0607 0.2887 0.4693]
Median for last 10 epochs: [0.4658 0.0939 0.4767 0.6095], Epochs since improvement 8
 11%|█         | 55/500 [1:09:44<9:04:51, 73.46s/it] 11%|█         | 56/500 [1:11:14<9:40:46, 78.48s/it]Epoch: 56 done with learning rate 8.56E-03, Train loss: 3.25E+06, Train scatter: [0.4635 0.0898 0.4776 0.5788]
L1 regularization loss: 1.04E+00, L2 regularization loss: 3.92E-01
Test scatter: [0.4601 0.0902 0.4688 0.5736], Lowest was [0.2071 0.0607 0.2887 0.4693]
Median for last 10 epochs: [0.4658 0.0939 0.4767 0.6095], Epochs since improvement 10
 11%|█▏        | 57/500 [1:12:15<9:01:45, 73.37s/it] 12%|█▏        | 58/500 [1:13:45<9:37:09, 78.35s/it]Epoch: 58 done with learning rate 8.83E-03, Train loss: 2.66E+06, Train scatter: [0.4453 0.0833 0.4675 0.5521]
L1 regularization loss: 1.05E+00, L2 regularization loss: 4.14E-01
Test scatter: [0.4361 0.0838 0.4568 0.5488], Lowest was [0.2071 0.0607 0.2887 0.4693]
Median for last 10 epochs: [0.4658 0.0939 0.4767 0.6095], Epochs since improvement 12
 12%|█▏        | 59/500 [1:14:47<8:58:40, 73.29s/it]Epoch: 60 done with learning rate 9.08E-03, Train loss: 2.36E+06, Train scatter: [0.4204 0.0841 0.4442 0.5715]
L1 regularization loss: 1.06E+00, L2 regularization loss: 4.35E-01
Test scatter: [0.4169 0.0846 0.4394 0.5668], Lowest was [0.2071 0.0607 0.2887 0.4693]
Median for last 10 epochs: [0.4601 0.0902 0.4688 0.5736], Epochs since improvement 14
 12%|█▏        | 60/500 [1:16:25<9:51:38, 80.68s/it] 12%|█▏        | 61/500 [1:17:26<9:08:28, 74.96s/it] 12%|█▏        | 62/500 [1:18:56<9:40:27, 79.51s/it]Epoch: 62 done with learning rate 9.31E-03, Train loss: 2.12E+06, Train scatter: [0.3254 0.0766 0.3968 0.5242]
L1 regularization loss: 1.07E+00, L2 regularization loss: 4.53E-01
Test scatter: [0.3232 0.0768 0.3947 0.5224], Lowest was [0.2071 0.0607 0.2887 0.4693]
Median for last 10 epochs: [0.4361 0.0846 0.4568 0.5668], Epochs since improvement 16
 13%|█▎        | 63/500 [1:19:58<8:59:34, 74.08s/it] 13%|█▎        | 64/500 [1:21:28<9:33:51, 78.97s/it]Epoch: 64 done with learning rate 9.50E-03, Train loss: 1.90E+06, Train scatter: [0.339  0.0738 0.3802 0.5235]
L1 regularization loss: 1.08E+00, L2 regularization loss: 4.65E-01
Test scatter: [0.3431 0.0753 0.3849 0.5253], Lowest was [0.2071 0.0607 0.2887 0.4693]
Median for last 10 epochs: [0.4169 0.0838 0.4394 0.5488], Epochs since improvement 18
 13%|█▎        | 65/500 [1:22:30<8:54:27, 73.72s/it] 13%|█▎        | 66/500 [1:23:59<9:28:22, 78.58s/it]Epoch: 66 done with learning rate 9.66E-03, Train loss: 1.75E+06, Train scatter: [0.319  0.0765 0.3729 0.5096]
L1 regularization loss: 1.08E+00, L2 regularization loss: 4.77E-01
Test scatter: [0.3111 0.0769 0.375  0.5011], Lowest was [0.2071 0.0607 0.2887 0.4693]
Median for last 10 epochs: [0.3431 0.0769 0.3947 0.5253], Epochs since improvement 20
 13%|█▎        | 67/500 [1:25:01<8:49:47, 73.41s/it] 13%|█▎        | 67/500 [1:26:31<9:19:08, 77.48s/it]
Epoch: 68 done with learning rate 9.80E-03, Train loss: 1.61E+06, Train scatter: [0.3095 0.0684 0.3512 0.4889]
L1 regularization loss: 1.09E+00, L2 regularization loss: 4.90E-01
Test scatter: [0.3132 0.0682 0.3533 0.4842], Lowest was [0.2071 0.0607 0.2887 0.4693]
Median for last 10 epochs: [0.3232 0.0768 0.3849 0.5224], Epochs since improvement 22
Exited after 68 epochs due to early stopping
5191.14 seconds spent training, 10.382 seconds per epoch. Processed 6707 trees per second
[0.31323504 0.06824549 0.3532818  0.48422888]
{'epoch_exit': 67, 'scatter_m_star': 0.31323504, 'lowest_m_star': 0.20710239, 'last20_m_star': 0.42650354, 'last10_m_star': 0.3231999, 'scatter_v_disk': 0.068245485, 'lowest_v_disk': 0.060737375, 'last20_v_disk': 0.08418061, 'last10_v_disk': 0.076835774, 'scatter_m_cold': 0.3532818, 'lowest_m_cold': 0.28870994, 'last20_m_cold': 0.4480748, 'last10_m_cold': 0.38490617, 'scatter_sfr_100': 0.48422888, 'lowest_sfr_100': 0.46928298, 'last20_sfr_100': 0.5577996, 'last10_sfr_100': 0.5223644}
Finished 1/1
Exited training after None epochs
Experiment 5 done: 5 / 6
Exploring ['conv_layers', 'batch_size']
Currently doing [  3 512]
{'experiment': 'GraphMerge', 'group': 'meta_pysr', 'move': False, 'model': 'MetaEdge', 'log': True, 'run_params': {'n_epochs': 500, 'n_trials': 1, 'batch_size': 512, 'val_epoch': 2, 'early_stopping': True, 'patience': 20, 'l1_lambda': 3e-05, 'l2_lambda': 3e-05, 'loss_func': 'GaussNd', 'metrics': 'test_multi_varrho', 'performance_plot': 'multi_base', 'save': True, 'seed': True, 'num_workers': 4}, 'learn_params': {'learning_rate': 0.01, 'schedule': 'onecycle', 'g_up': 1, 'g_down': 0.95, 'warmup': 4, 'period': 5, 'eta_min': 1e-05}, 'hyper_params': {'hidden_channels': 128, 'conv_layers': 3, 'conv_activation': 'relu', 'decode_activation': 'leakyrelu', 'decode_layers': 2, 'layernorm': True, 'agg': 'sum', 'variance': True, 'rho': 0, 'in_channels': 43, 'out_channels': 4}, 'data_params': {'case': 'vlarge_all_4t_z0.0_quantile_raw', 'targets': [0, 1, 2, 3], 'del_feats': [], 'split': 0.8, 'test': 0, 'scale': 0}, 'experiment_name': 'con3bat512'}
/home/cj1223/../../scratch/gpfs/cj1223/GraphResults/results_meta_pysr_120322/MetaEdge_vlarge_all_4t_z0.0_quantile_raw_eoclso
RelU conv activation
LeakyRelU decode activation
N_params 546504
Made folder for saving model
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:53<7:24:39, 53.46s/it]  0%|          | 2/500 [02:13<9:32:13, 68.94s/it]Epoch: 2 done with learning rate 4.17E-04, Train loss: 1.19E+08, Train scatter: [0.9352 0.1729 0.5441 0.9953]
L1 regularization loss: 7.33E-01, L2 regularization loss: 1.27E-01
Test scatter: [0.9196 0.168  0.5355 0.985 ], Lowest was [0.9196 0.168  0.5355 0.985 ]
Median for last 10 epochs: [0.9196 0.168  0.5355 0.985 ], Epochs since improvement 2
  1%|          | 3/500 [03:06<8:32:44, 61.90s/it]  1%|          | 4/500 [04:27<9:34:01, 69.44s/it]Epoch: 4 done with learning rate 4.67E-04, Train loss: 8.85E+07, Train scatter: [0.9352 0.1345 0.5441 0.9954]
L1 regularization loss: 7.37E-01, L2 regularization loss: 1.28E-01
Test scatter: [0.9196 0.1297 0.5355 0.9851], Lowest was [0.9196 0.1297 0.5355 0.985 ]
Median for last 10 epochs: [0.9196 0.1297 0.5355 0.985 ], Epochs since improvement 0
  1%|          | 5/500 [05:21<8:45:42, 63.72s/it]  1%|          | 6/500 [06:41<9:31:49, 69.45s/it]Epoch: 6 done with learning rate 5.51E-04, Train loss: 8.34E+07, Train scatter: [0.9349 0.1142 0.5441 0.9954]
L1 regularization loss: 7.39E-01, L2 regularization loss: 1.28E-01
Test scatter: [0.9193 0.112  0.5355 0.9851], Lowest was [0.9193 0.112  0.5355 0.985 ]
Median for last 10 epochs: [0.9193 0.112  0.5355 0.9851], Epochs since improvement 0
  1%|▏         | 7/500 [07:35<8:47:36, 64.21s/it]  2%|▏         | 8/500 [08:55<9:28:39, 69.35s/it]Epoch: 8 done with learning rate 6.67E-04, Train loss: 8.10E+07, Train scatter: [0.9317 0.1025 0.5439 0.9954]
L1 regularization loss: 7.42E-01, L2 regularization loss: 1.30E-01
Test scatter: [0.9162 0.1013 0.5353 0.9851], Lowest was [0.9162 0.1013 0.5353 0.985 ]
Median for last 10 epochs: [0.9177 0.1066 0.5354 0.9851], Epochs since improvement 0
  2%|▏         | 9/500 [09:49<8:46:56, 64.39s/it]Epoch: 10 done with learning rate 8.15E-04, Train loss: 1.48E+08, Train scatter: [0.9352 0.1574 0.5441 0.9954]
L1 regularization loss: 7.52E-01, L2 regularization loss: 1.32E-01
Test scatter: [0.9196 0.1523 0.5355 0.985 ], Lowest was [0.9162 0.1013 0.5353 0.985 ]
Median for last 10 epochs: [0.9193 0.112  0.5355 0.9851], Epochs since improvement 2
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
  2%|▏         | 10/500 [11:17<9:46:16, 71.79s/it]  2%|▏         | 11/500 [12:11<8:59:24, 66.18s/it]  2%|▏         | 12/500 [13:32<9:35:01, 70.70s/it]Epoch: 12 done with learning rate 9.94E-04, Train loss: 9.10E+07, Train scatter: [0.9347 0.1377 0.544  0.9954]
L1 regularization loss: 7.56E-01, L2 regularization loss: 1.34E-01
Test scatter: [0.9192 0.1342 0.5355 0.9851], Lowest was [0.9162 0.1013 0.5353 0.985 ]
Median for last 10 epochs: [0.9193 0.1297 0.5355 0.9851], Epochs since improvement 4
  3%|▎         | 13/500 [14:25<8:51:39, 65.50s/it]  3%|▎         | 14/500 [15:46<9:27:20, 70.04s/it]Epoch: 14 done with learning rate 1.20E-03, Train loss: 8.66E+07, Train scatter: [0.9328 0.1237 0.544  0.9954]
L1 regularization loss: 7.59E-01, L2 regularization loss: 1.37E-01
Test scatter: [0.9174 0.123  0.5354 0.9851], Lowest was [0.9162 0.1013 0.5353 0.985 ]
Median for last 10 epochs: [0.9192 0.123  0.5355 0.9851], Epochs since improvement 6
  3%|▎         | 15/500 [16:39<8:45:57, 65.07s/it]  3%|▎         | 16/500 [18:00<9:22:33, 69.74s/it]Epoch: 16 done with learning rate 1.44E-03, Train loss: 8.33E+07, Train scatter: [0.8458 0.1075 0.5435 0.9954]
L1 regularization loss: 7.65E-01, L2 regularization loss: 1.41E-01
Test scatter: [0.834  0.1073 0.535  0.985 ], Lowest was [0.834  0.1013 0.535  0.985 ]
Median for last 10 epochs: [0.9174 0.123  0.5354 0.9851], Epochs since improvement 0
  3%|▎         | 17/500 [18:53<8:42:05, 64.86s/it]  4%|▎         | 18/500 [20:15<9:21:26, 69.89s/it]Epoch: 18 done with learning rate 1.70E-03, Train loss: 7.90E+07, Train scatter: [0.5862 0.0975 0.5032 0.9953]
L1 regularization loss: 7.76E-01, L2 regularization loss: 1.49E-01
Test scatter: [0.5821 0.096  0.5016 0.985 ], Lowest was [0.5821 0.096  0.5016 0.985 ]
Median for last 10 epochs: [0.9174 0.123  0.5354 0.985 ], Epochs since improvement 0
  4%|▍         | 19/500 [21:08<8:40:39, 64.95s/it]Epoch: 20 done with learning rate 1.99E-03, Train loss: 7.64E+07, Train scatter: [0.3918 0.0883 0.433  0.9953]
L1 regularization loss: 7.81E-01, L2 regularization loss: 1.53E-01
Test scatter: [0.3971 0.0884 0.4295 0.985 ], Lowest was [0.3971 0.0884 0.4295 0.985 ]
Median for last 10 epochs: [0.834  0.1073 0.535  0.985 ], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
  4%|▍         | 20/500 [22:36<9:34:34, 71.82s/it]  4%|▍         | 21/500 [23:30<8:50:08, 66.41s/it]  4%|▍         | 22/500 [24:52<9:25:23, 70.97s/it]Epoch: 22 done with learning rate 2.30E-03, Train loss: 7.47E+07, Train scatter: [0.352  0.0863 0.4035 0.9953]
L1 regularization loss: 7.84E-01, L2 regularization loss: 1.55E-01
Test scatter: [0.3654 0.0862 0.4029 0.985 ], Lowest was [0.3654 0.0862 0.4029 0.985 ]
Median for last 10 epochs: [0.5821 0.096  0.5016 0.985 ], Epochs since improvement 0
  5%|▍         | 23/500 [25:45<8:42:54, 65.77s/it]  5%|▍         | 24/500 [27:06<9:17:41, 70.30s/it]Epoch: 24 done with learning rate 2.63E-03, Train loss: 7.38E+07, Train scatter: [0.319  0.082  0.3873 0.9953]
L1 regularization loss: 7.86E-01, L2 regularization loss: 1.57E-01
Test scatter: [0.3254 0.0818 0.3819 0.985 ], Lowest was [0.3254 0.0818 0.3819 0.985 ]
Median for last 10 epochs: [0.3971 0.0884 0.4295 0.985 ], Epochs since improvement 0
  5%|▌         | 25/500 [28:00<8:37:08, 65.32s/it]  5%|▌         | 26/500 [29:21<9:12:46, 69.97s/it]Epoch: 26 done with learning rate 2.98E-03, Train loss: 7.32E+07, Train scatter: [0.3353 0.0808 0.3677 0.9953]
L1 regularization loss: 7.88E-01, L2 regularization loss: 1.59E-01
Test scatter: [0.3388 0.0814 0.3739 0.985 ], Lowest was [0.3254 0.0814 0.3739 0.985 ]
Median for last 10 epochs: [0.3654 0.0862 0.4029 0.985 ], Epochs since improvement 0
  5%|▌         | 27/500 [30:14<8:32:58, 65.07s/it]  6%|▌         | 28/500 [31:35<9:09:33, 69.86s/it]Epoch: 28 done with learning rate 3.34E-03, Train loss: 7.29E+07, Train scatter: [0.3046 0.0783 0.3426 0.9953]
L1 regularization loss: 7.91E-01, L2 regularization loss: 1.60E-01
Test scatter: [0.3077 0.0784 0.3459 0.985 ], Lowest was [0.3077 0.0784 0.3459 0.985 ]
Median for last 10 epochs: [0.3388 0.0818 0.3819 0.985 ], Epochs since improvement 0
  6%|▌         | 29/500 [32:29<8:30:17, 65.01s/it]Epoch: 30 done with learning rate 3.72E-03, Train loss: 7.24E+07, Train scatter: [0.2781 0.0764 0.3371 0.9953]
L1 regularization loss: 7.93E-01, L2 regularization loss: 1.63E-01
Test scatter: [0.2861 0.0759 0.3392 0.985 ], Lowest was [0.2861 0.0759 0.3392 0.985 ]
Median for last 10 epochs: [0.3254 0.0814 0.3739 0.985 ], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
  6%|▌         | 30/500 [33:57<9:23:09, 71.89s/it]  6%|▌         | 31/500 [34:51<8:39:21, 66.44s/it]  6%|▋         | 32/500 [36:12<9:12:26, 70.83s/it]Epoch: 32 done with learning rate 4.10E-03, Train loss: 7.19E+07, Train scatter: [0.2694 0.0784 0.3373 0.9953]
L1 regularization loss: 7.97E-01, L2 regularization loss: 1.65E-01
Test scatter: [0.2862 0.0786 0.347  0.985 ], Lowest was [0.2861 0.0759 0.3392 0.985 ]
Median for last 10 epochs: [0.3077 0.0786 0.347  0.985 ], Epochs since improvement 2
  7%|▋         | 33/500 [37:05<8:31:27, 65.71s/it]  7%|▋         | 34/500 [38:26<9:04:45, 70.14s/it]Epoch: 34 done with learning rate 4.50E-03, Train loss: 7.15E+07, Train scatter: [0.2612 0.0745 0.3237 0.9954]
L1 regularization loss: 8.01E-01, L2 regularization loss: 1.67E-01
Test scatter: [0.269  0.0744 0.3286 0.985 ], Lowest was [0.269  0.0744 0.3286 0.985 ]
Median for last 10 epochs: [0.2862 0.0784 0.3459 0.985 ], Epochs since improvement 0
  7%|▋         | 35/500 [39:20<8:25:52, 65.27s/it]  7%|▋         | 36/500 [40:40<8:59:07, 69.71s/it]Epoch: 36 done with learning rate 4.90E-03, Train loss: 7.10E+07, Train scatter: [0.2561 0.0742 0.3394 0.9953]
L1 regularization loss: 8.03E-01, L2 regularization loss: 1.70E-01
Test scatter: [0.2688 0.074  0.341  0.985 ], Lowest was [0.2688 0.074  0.3286 0.985 ]
Median for last 10 epochs: [0.2861 0.0759 0.341  0.985 ], Epochs since improvement 0
  7%|▋         | 37/500 [41:34<8:20:51, 64.91s/it]  8%|▊         | 38/500 [42:54<8:56:02, 69.62s/it]Epoch: 38 done with learning rate 5.30E-03, Train loss: 6.99E+07, Train scatter: [0.288  0.0762 0.3385 0.9952]
L1 regularization loss: 8.08E-01, L2 regularization loss: 1.72E-01
Test scatter: [0.3032 0.0761 0.3459 0.9849], Lowest was [0.2688 0.074  0.3286 0.9849]
Median for last 10 epochs: [0.2861 0.0759 0.341  0.985 ], Epochs since improvement 0
  8%|▊         | 39/500 [43:48<8:18:24, 64.87s/it]Epoch: 40 done with learning rate 5.70E-03, Train loss: 3.99E+07, Train scatter: [0.2847 0.073  0.3338 0.9952]
L1 regularization loss: 8.15E-01, L2 regularization loss: 1.78E-01
Test scatter: [0.2947 0.0727 0.3347 0.9849], Lowest was [0.2688 0.0727 0.3286 0.9849]
Median for last 10 epochs: [0.2862 0.0744 0.341  0.985 ], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
  8%|▊         | 40/500 [45:16<9:10:00, 71.74s/it]  8%|▊         | 41/500 [46:09<8:27:19, 66.32s/it]  8%|▊         | 42/500 [47:30<8:58:48, 70.59s/it]Epoch: 42 done with learning rate 6.10E-03, Train loss: 8.89E+06, Train scatter: [0.3029 0.0827 0.3382 0.5881]
L1 regularization loss: 8.25E-01, L2 regularization loss: 1.85E-01
Test scatter: [0.3251 0.0823 0.3419 0.5895], Lowest was [0.2688 0.0727 0.3286 0.5895]
Median for last 10 epochs: [0.2947 0.0744 0.341  0.9849], Epochs since improvement 0
  9%|▊         | 43/500 [48:24<8:19:09, 65.54s/it]  9%|▉         | 44/500 [49:44<8:52:27, 70.06s/it]Epoch: 44 done with learning rate 6.49E-03, Train loss: 4.51E+06, Train scatter: [0.2419 0.0743 0.3222 0.5324]
L1 regularization loss: 8.32E-01, L2 regularization loss: 1.89E-01
Test scatter: [0.2493 0.0745 0.3286 0.5316], Lowest was [0.2493 0.0727 0.3286 0.5316]
Median for last 10 epochs: [0.2947 0.0745 0.341  0.9849], Epochs since improvement 0
  9%|▉         | 45/500 [50:38<8:13:53, 65.13s/it]  9%|▉         | 46/500 [51:59<8:48:53, 69.90s/it]Epoch: 46 done with learning rate 6.87E-03, Train loss: 4.00E+06, Train scatter: [0.2398 0.0716 0.3176 0.5166]
L1 regularization loss: 8.36E-01, L2 regularization loss: 1.92E-01
Test scatter: [0.2445 0.0719 0.3219 0.5149], Lowest was [0.2445 0.0719 0.3219 0.5149]
Median for last 10 epochs: [0.2947 0.0745 0.3347 0.5895], Epochs since improvement 0
  9%|▉         | 47/500 [52:53<8:11:10, 65.06s/it] 10%|▉         | 48/500 [54:14<8:47:15, 69.99s/it]Epoch: 48 done with learning rate 7.24E-03, Train loss: 4.17E+06, Train scatter: [0.2948 0.0833 0.3451 0.6188]
L1 regularization loss: 8.43E-01, L2 regularization loss: 1.96E-01
Test scatter: [0.3003 0.083  0.3409 0.6181], Lowest was [0.2445 0.0719 0.3219 0.5149]
Median for last 10 epochs: [0.2947 0.0745 0.3347 0.5895], Epochs since improvement 2
 10%|▉         | 49/500 [55:08<8:09:14, 65.09s/it]Epoch: 50 done with learning rate 7.60E-03, Train loss: 3.52E+06, Train scatter: [0.2722 0.0734 0.3227 0.5524]
L1 regularization loss: 8.46E-01, L2 regularization loss: 2.00E-01
Test scatter: [0.2774 0.0731 0.3229 0.5502], Lowest was [0.2445 0.0719 0.3219 0.5149]
Median for last 10 epochs: [0.2774 0.0745 0.3286 0.5502], Epochs since improvement 4
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 10%|█         | 50/500 [56:36<8:59:53, 71.99s/it] 10%|█         | 51/500 [57:30<8:17:31, 66.49s/it] 10%|█         | 52/500 [58:50<8:48:14, 70.75s/it]Epoch: 52 done with learning rate 7.94E-03, Train loss: 3.43E+06, Train scatter: [0.2796 0.0709 0.3169 0.5201]
L1 regularization loss: 8.53E-01, L2 regularization loss: 2.04E-01
Test scatter: [0.2918 0.0701 0.3166 0.5152], Lowest was [0.2445 0.0701 0.3166 0.5149]
Median for last 10 epochs: [0.2774 0.0731 0.3229 0.5316], Epochs since improvement 0
 11%|█         | 53/500 [59:44<8:08:55, 65.63s/it] 11%|█         | 54/500 [1:01:05<8:41:24, 70.14s/it]Epoch: 54 done with learning rate 8.26E-03, Train loss: 3.25E+06, Train scatter: [0.2322 0.0705 0.3079 0.504 ]
L1 regularization loss: 8.60E-01, L2 regularization loss: 2.09E-01
Test scatter: [0.2444 0.0691 0.3093 0.5007], Lowest was [0.2444 0.0691 0.3093 0.5007]
Median for last 10 epochs: [0.2774 0.0719 0.3219 0.5152], Epochs since improvement 0
 11%|█         | 55/500 [1:01:58<8:03:33, 65.20s/it] 11%|█         | 56/500 [1:03:19<8:37:30, 69.93s/it]Epoch: 56 done with learning rate 8.56E-03, Train loss: 3.14E+06, Train scatter: [0.2214 0.0665 0.303  0.485 ]
L1 regularization loss: 8.69E-01, L2 regularization loss: 2.15E-01
Test scatter: [0.2419 0.0676 0.3081 0.4879], Lowest was [0.2419 0.0676 0.3081 0.4879]
Median for last 10 epochs: [0.2774 0.0701 0.3166 0.5152], Epochs since improvement 0
 11%|█▏        | 57/500 [1:04:13<8:00:19, 65.06s/it] 12%|█▏        | 58/500 [1:05:34<8:34:56, 69.90s/it]Epoch: 58 done with learning rate 8.83E-03, Train loss: 3.15E+06, Train scatter: [0.3669 0.0666 0.321  0.4891]
L1 regularization loss: 8.76E-01, L2 regularization loss: 2.20E-01
Test scatter: [0.3649 0.0666 0.3216 0.4847], Lowest was [0.2419 0.0666 0.3081 0.4847]
Median for last 10 epochs: [0.2774 0.0691 0.3166 0.5007], Epochs since improvement 0
 12%|█▏        | 59/500 [1:06:28<7:57:50, 65.01s/it]Epoch: 60 done with learning rate 9.08E-03, Train loss: 5.14E+06, Train scatter: [0.2922 0.074  0.371  0.5381]
L1 regularization loss: 8.87E-01, L2 regularization loss: 2.28E-01
Test scatter: [0.2945 0.0738 0.3652 0.5298], Lowest was [0.2419 0.0666 0.3081 0.4847]
Median for last 10 epochs: [0.2918 0.0691 0.3166 0.5007], Epochs since improvement 2
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 12%|█▏        | 60/500 [1:07:56<8:47:30, 71.93s/it] 12%|█▏        | 61/500 [1:08:50<8:06:23, 66.48s/it] 12%|█▏        | 62/500 [1:10:11<8:37:17, 70.86s/it]Epoch: 62 done with learning rate 9.31E-03, Train loss: 3.48E+06, Train scatter: [0.2277 0.0684 0.3081 0.4926]
L1 regularization loss: 8.94E-01, L2 regularization loss: 2.33E-01
Test scatter: [0.238  0.0692 0.3117 0.4939], Lowest was [0.238  0.0666 0.3081 0.4847]
Median for last 10 epochs: [0.2444 0.0691 0.3117 0.4939], Epochs since improvement 0
 13%|█▎        | 63/500 [1:11:04<7:58:40, 65.72s/it] 13%|█▎        | 64/500 [1:12:25<8:30:56, 70.31s/it]Epoch: 64 done with learning rate 9.50E-03, Train loss: 2.90E+06, Train scatter: [0.285  0.0729 0.3576 0.5113]
L1 regularization loss: 8.99E-01, L2 regularization loss: 2.38E-01
Test scatter: [0.2881 0.0732 0.3518 0.5075], Lowest was [0.238  0.0666 0.3081 0.4847]
Median for last 10 epochs: [0.2881 0.0692 0.3216 0.4939], Epochs since improvement 2
 13%|█▎        | 65/500 [1:13:19<7:53:22, 65.29s/it] 13%|█▎        | 66/500 [1:14:40<8:25:35, 69.90s/it]Epoch: 66 done with learning rate 9.66E-03, Train loss: 2.83E+06, Train scatter: [0.2218 0.0688 0.3044 0.4836]
L1 regularization loss: 9.06E-01, L2 regularization loss: 2.45E-01
Test scatter: [0.2332 0.0687 0.3064 0.4845], Lowest was [0.2332 0.0666 0.3064 0.4845]
Median for last 10 epochs: [0.2881 0.0692 0.3216 0.4939], Epochs since improvement 0
 13%|█▎        | 67/500 [1:15:33<7:49:05, 65.00s/it] 14%|█▎        | 68/500 [1:16:54<8:22:15, 69.76s/it]Epoch: 68 done with learning rate 9.80E-03, Train loss: 2.83E+06, Train scatter: [0.2294 0.0683 0.304  0.4778]
L1 regularization loss: 9.12E-01, L2 regularization loss: 2.51E-01
Test scatter: [0.2402 0.0678 0.3068 0.4772], Lowest was [0.2332 0.0666 0.3064 0.4772]
Median for last 10 epochs: [0.2402 0.0692 0.3117 0.4939], Epochs since improvement 0
 14%|█▍        | 69/500 [1:17:48<7:46:18, 64.91s/it]Epoch: 70 done with learning rate 9.90E-03, Train loss: 2.77E+06, Train scatter: [0.228  0.0664 0.3024 0.4935]
L1 regularization loss: 9.18E-01, L2 regularization loss: 2.57E-01
Test scatter: [0.2355 0.0663 0.3011 0.4837], Lowest was [0.2332 0.0663 0.3011 0.4772]
Median for last 10 epochs: [0.238  0.0687 0.3068 0.4845], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 14%|█▍        | 70/500 [1:19:16<8:34:21, 71.77s/it] 14%|█▍        | 71/500 [1:20:09<7:54:22, 66.35s/it] 14%|█▍        | 72/500 [1:21:29<8:22:52, 70.50s/it]Epoch: 72 done with learning rate 9.96E-03, Train loss: 2.65E+06, Train scatter: [0.2559 0.0657 0.2988 0.4828]
L1 regularization loss: 9.24E-01, L2 regularization loss: 2.64E-01
Test scatter: [0.2632 0.0656 0.299  0.4824], Lowest was [0.2332 0.0656 0.299  0.4772]
Median for last 10 epochs: [0.2402 0.0678 0.3064 0.4837], Epochs since improvement 0
 15%|█▍        | 73/500 [1:22:23<7:45:35, 65.42s/it] 15%|█▍        | 74/500 [1:23:44<8:17:05, 70.01s/it]Epoch: 74 done with learning rate 1.00E-02, Train loss: 2.52E+06, Train scatter: [0.2301 0.0664 0.2941 0.4726]
L1 regularization loss: 9.29E-01, L2 regularization loss: 2.70E-01
Test scatter: [0.236  0.0666 0.2966 0.4756], Lowest was [0.2332 0.0656 0.2966 0.4756]
Median for last 10 epochs: [0.236  0.0666 0.3011 0.4824], Epochs since improvement 0
 15%|█▌        | 75/500 [1:24:37<7:41:08, 65.10s/it] 15%|█▌        | 76/500 [1:25:58<8:12:17, 69.66s/it]Epoch: 76 done with learning rate 1.00E-02, Train loss: 2.54E+06, Train scatter: [0.2155 0.0644 0.2893 0.4733]
L1 regularization loss: 9.36E-01, L2 regularization loss: 2.77E-01
Test scatter: [0.2252 0.0643 0.2909 0.4764], Lowest was [0.2252 0.0643 0.2909 0.4756]
Median for last 10 epochs: [0.236  0.0663 0.299  0.4772], Epochs since improvement 0
 15%|█▌        | 77/500 [1:26:51<7:37:09, 64.85s/it] 16%|█▌        | 78/500 [1:28:12<8:10:01, 69.67s/it]Epoch: 78 done with learning rate 1.00E-02, Train loss: 2.52E+06, Train scatter: [0.4362 0.0624 0.2849 0.4667]
L1 regularization loss: 9.43E-01, L2 regularization loss: 2.84E-01
Test scatter: [0.4373 0.0632 0.2884 0.4675], Lowest was [0.2252 0.0632 0.2884 0.4675]
Median for last 10 epochs: [0.236  0.0656 0.2966 0.4764], Epochs since improvement 0
 16%|█▌        | 79/500 [1:29:06<7:35:08, 64.87s/it]Epoch: 80 done with learning rate 1.00E-02, Train loss: 2.51E+06, Train scatter: [0.2627 0.0631 0.2857 0.453 ]
L1 regularization loss: 9.53E-01, L2 regularization loss: 2.92E-01
Test scatter: [0.2638 0.0632 0.2867 0.4547], Lowest was [0.2252 0.0632 0.2867 0.4547]
Median for last 10 epochs: [0.2632 0.0643 0.2909 0.4756], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 16%|█▌        | 80/500 [1:30:34<8:23:32, 71.93s/it] 16%|█▌        | 81/500 [1:31:28<7:44:05, 66.46s/it] 16%|█▋        | 82/500 [1:32:48<8:11:40, 70.57s/it]Epoch: 82 done with learning rate 9.99E-03, Train loss: 2.32E+06, Train scatter: [0.21   0.0626 0.2754 0.4495]
L1 regularization loss: 9.58E-01, L2 regularization loss: 2.97E-01
Test scatter: [0.22   0.0633 0.2784 0.4514], Lowest was [0.22   0.0632 0.2784 0.4514]
Median for last 10 epochs: [0.236  0.0633 0.2884 0.4675], Epochs since improvement 0
 17%|█▋        | 83/500 [1:33:42<7:35:23, 65.52s/it] 17%|█▋        | 84/500 [1:35:03<8:06:11, 70.12s/it]Epoch: 84 done with learning rate 9.99E-03, Train loss: 2.81E+06, Train scatter: [0.4113 0.0909 0.4503 0.5537]
L1 regularization loss: 9.69E-01, L2 regularization loss: 3.07E-01
Test scatter: [0.4056 0.0872 0.4413 0.5467], Lowest was [0.22   0.0632 0.2784 0.4514]
Median for last 10 epochs: [0.2638 0.0633 0.2884 0.4675], Epochs since improvement 2
 17%|█▋        | 85/500 [1:35:56<7:30:52, 65.19s/it] 17%|█▋        | 86/500 [1:37:17<8:01:08, 69.73s/it]Epoch: 86 done with learning rate 9.98E-03, Train loss: 2.58E+06, Train scatter: [0.2232 0.065  0.2867 0.4816]
L1 regularization loss: 9.74E-01, L2 regularization loss: 3.12E-01
Test scatter: [0.2319 0.0652 0.2908 0.4816], Lowest was [0.22   0.0632 0.2784 0.4514]
Median for last 10 epochs: [0.2638 0.0633 0.2884 0.4675], Epochs since improvement 4
 17%|█▋        | 87/500 [1:38:11<7:27:06, 64.96s/it] 18%|█▊        | 88/500 [1:39:31<7:57:43, 69.57s/it]Epoch: 88 done with learning rate 9.98E-03, Train loss: 2.33E+06, Train scatter: [0.26   0.0688 0.2853 0.4889]
L1 regularization loss: 9.85E-01, L2 regularization loss: 3.21E-01
Test scatter: [0.2676 0.0686 0.2867 0.4952], Lowest was [0.22   0.0632 0.2784 0.4514]
Median for last 10 epochs: [0.2638 0.0652 0.2867 0.4816], Epochs since improvement 6
 18%|█▊        | 89/500 [1:40:24<7:23:46, 64.78s/it]Epoch: 90 done with learning rate 9.97E-03, Train loss: 2.21E+06, Train scatter: [0.2082 0.0587 0.2721 0.4544]
L1 regularization loss: 9.91E-01, L2 regularization loss: 3.29E-01
Test scatter: [0.2219 0.0596 0.2796 0.4566], Lowest was [0.22   0.0596 0.2784 0.4514]
Median for last 10 epochs: [0.2319 0.0652 0.2867 0.4816], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 18%|█▊        | 90/500 [1:41:53<8:12:01, 72.00s/it] 18%|█▊        | 91/500 [1:42:47<7:33:15, 66.49s/it] 18%|█▊        | 92/500 [1:44:08<8:01:50, 70.86s/it]Epoch: 92 done with learning rate 9.96E-03, Train loss: 2.10E+06, Train scatter: [0.2293 0.0638 0.2882 0.4682]
L1 regularization loss: 9.96E-01, L2 regularization loss: 3.34E-01
Test scatter: [0.2436 0.0659 0.2913 0.4677], Lowest was [0.22   0.0596 0.2784 0.4514]
Median for last 10 epochs: [0.2436 0.0659 0.2908 0.4816], Epochs since improvement 2
 19%|█▊        | 93/500 [1:45:02<7:25:58, 65.75s/it] 19%|█▉        | 94/500 [1:46:23<7:56:40, 70.44s/it]Epoch: 94 done with learning rate 9.95E-03, Train loss: 2.27E+06, Train scatter: [0.2251 0.0562 0.2754 0.4576]
L1 regularization loss: 1.00E+00, L2 regularization loss: 3.43E-01
Test scatter: [0.2287 0.0567 0.2766 0.4532], Lowest was [0.22   0.0567 0.2766 0.4514]
Median for last 10 epochs: [0.2319 0.0652 0.2867 0.4677], Epochs since improvement 0
 19%|█▉        | 95/500 [1:47:17<7:21:22, 65.39s/it] 19%|█▉        | 96/500 [1:48:38<7:51:36, 70.04s/it]Epoch: 96 done with learning rate 9.94E-03, Train loss: 2.10E+06, Train scatter: [0.2061 0.0567 0.2676 0.4505]
L1 regularization loss: 1.01E+00, L2 regularization loss: 3.49E-01
Test scatter: [0.216  0.0572 0.2716 0.4541], Lowest was [0.216  0.0567 0.2716 0.4514]
Median for last 10 epochs: [0.2287 0.0596 0.2796 0.4566], Epochs since improvement 0
 19%|█▉        | 97/500 [1:49:31<7:17:22, 65.12s/it] 20%|█▉        | 98/500 [1:50:52<7:47:14, 69.74s/it]Epoch: 98 done with learning rate 9.93E-03, Train loss: 2.02E+06, Train scatter: [0.2196 0.059  0.2793 0.4604]
L1 regularization loss: 1.02E+00, L2 regularization loss: 3.57E-01
Test scatter: [0.2259 0.0589 0.2804 0.4562], Lowest was [0.216  0.0567 0.2716 0.4514]
Median for last 10 epochs: [0.2259 0.0589 0.2796 0.4562], Epochs since improvement 2
 20%|█▉        | 99/500 [1:51:45<7:13:37, 64.88s/it]Epoch: 100 done with learning rate 9.91E-03, Train loss: 2.01E+06, Train scatter: [0.3164 0.0583 0.2797 0.4545]
L1 regularization loss: 1.02E+00, L2 regularization loss: 3.63E-01
Test scatter: [0.3159 0.0585 0.2806 0.4535], Lowest was [0.216  0.0567 0.2716 0.4514]
Median for last 10 epochs: [0.2287 0.0585 0.2804 0.4541], Epochs since improvement 4
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 20%|██        | 100/500 [1:53:16<8:03:14, 72.49s/it] 20%|██        | 101/500 [1:54:09<7:24:09, 66.79s/it] 20%|██        | 102/500 [1:55:29<7:49:34, 70.79s/it]Epoch: 102 done with learning rate 9.90E-03, Train loss: 1.87E+06, Train scatter: [0.2116 0.0558 0.2731 0.4401]
L1 regularization loss: 1.03E+00, L2 regularization loss: 3.71E-01
Test scatter: [0.2187 0.056  0.2759 0.4442], Lowest was [0.216  0.056  0.2716 0.4442]
Median for last 10 epochs: [0.2259 0.0572 0.2766 0.4535], Epochs since improvement 0
 21%|██        | 103/500 [1:56:23<7:14:09, 65.62s/it] 21%|██        | 104/500 [1:57:43<7:41:28, 69.92s/it]Epoch: 104 done with learning rate 9.89E-03, Train loss: 1.90E+06, Train scatter: [0.3186 0.0568 0.2739 0.4514]
L1 regularization loss: 1.03E+00, L2 regularization loss: 3.77E-01
Test scatter: [0.3134 0.0586 0.2767 0.4498], Lowest was [0.216  0.056  0.2716 0.4442]
Median for last 10 epochs: [0.2259 0.0585 0.2767 0.4535], Epochs since improvement 2
 21%|██        | 105/500 [1:58:36<7:08:10, 65.04s/it] 21%|██        | 106/500 [1:59:57<7:38:37, 69.84s/it]Epoch: 106 done with learning rate 9.87E-03, Train loss: 1.73E+06, Train scatter: [0.2257 0.055  0.2692 0.4393]
L1 regularization loss: 1.04E+00, L2 regularization loss: 3.86E-01
Test scatter: [0.2358 0.0562 0.2704 0.4424], Lowest was [0.216  0.056  0.2704 0.4424]
Median for last 10 epochs: [0.2358 0.0585 0.2767 0.4498], Epochs since improvement 0
 21%|██▏       | 107/500 [2:00:51<7:05:42, 64.99s/it] 22%|██▏       | 108/500 [2:02:13<7:37:03, 69.96s/it]Epoch: 108 done with learning rate 9.85E-03, Train loss: 1.68E+06, Train scatter: [0.335  0.0598 0.2635 0.4335]
L1 regularization loss: 1.05E+00, L2 regularization loss: 3.94E-01
Test scatter: [0.3351 0.0602 0.2659 0.4333], Lowest was [0.216  0.056  0.2659 0.4333]
Median for last 10 epochs: [0.3134 0.0585 0.2759 0.4442], Epochs since improvement 0
 22%|██▏       | 109/500 [2:03:06<7:03:56, 65.06s/it]Epoch: 110 done with learning rate 9.83E-03, Train loss: 1.67E+06, Train scatter: [0.2008 0.0596 0.2627 0.4406]
L1 regularization loss: 1.06E+00, L2 regularization loss: 4.05E-01
Test scatter: [0.2075 0.0603 0.267  0.4428], Lowest was [0.2075 0.056  0.2659 0.4333]
Median for last 10 epochs: [0.2358 0.0586 0.2704 0.4428], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 22%|██▏       | 110/500 [2:04:35<7:48:32, 72.08s/it] 22%|██▏       | 111/500 [2:05:29<7:11:36, 66.57s/it] 22%|██▏       | 112/500 [2:06:50<7:38:54, 70.96s/it]Epoch: 112 done with learning rate 9.81E-03, Train loss: 1.80E+06, Train scatter: [0.1891 0.0543 0.2593 0.4314]
L1 regularization loss: 1.07E+00, L2 regularization loss: 4.14E-01
Test scatter: [0.1987 0.0551 0.2623 0.4314], Lowest was [0.1987 0.0551 0.2623 0.4314]
Median for last 10 epochs: [0.2358 0.0586 0.267  0.4424], Epochs since improvement 0
 23%|██▎       | 113/500 [2:07:43<7:04:21, 65.79s/it] 23%|██▎       | 114/500 [2:09:05<7:33:22, 70.47s/it]Epoch: 114 done with learning rate 9.79E-03, Train loss: 1.51E+06, Train scatter: [0.1998 0.0529 0.2548 0.4267]
L1 regularization loss: 1.08E+00, L2 regularization loss: 4.22E-01
Test scatter: [0.2149 0.0538 0.2592 0.4293], Lowest was [0.1987 0.0538 0.2592 0.4293]
Median for last 10 epochs: [0.2149 0.0562 0.2659 0.4333], Epochs since improvement 0
 23%|██▎       | 115/500 [2:09:59<6:59:50, 65.43s/it] 23%|██▎       | 116/500 [2:11:19<7:27:59, 70.00s/it]Epoch: 116 done with learning rate 9.77E-03, Train loss: 1.48E+06, Train scatter: [0.1908 0.0536 0.2601 0.4274]
L1 regularization loss: 1.08E+00, L2 regularization loss: 4.30E-01
Test scatter: [0.1991 0.0541 0.2626 0.4309], Lowest was [0.1987 0.0538 0.2592 0.4293]
Median for last 10 epochs: [0.2075 0.0551 0.2626 0.4314], Epochs since improvement 2
 23%|██▎       | 117/500 [2:12:13<6:55:38, 65.11s/it] 24%|██▎       | 118/500 [2:13:34<7:25:58, 70.05s/it]Epoch: 118 done with learning rate 9.75E-03, Train loss: 1.39E+06, Train scatter: [0.209  0.0513 0.2571 0.4212]
L1 regularization loss: 1.09E+00, L2 regularization loss: 4.39E-01
Test scatter: [0.2207 0.0512 0.2589 0.4217], Lowest was [0.1987 0.0512 0.2589 0.4217]
Median for last 10 epochs: [0.2075 0.0541 0.2623 0.4309], Epochs since improvement 0
 24%|██▍       | 119/500 [2:14:28<6:53:34, 65.13s/it]Epoch: 120 done with learning rate 9.73E-03, Train loss: 1.37E+06, Train scatter: [0.2237 0.0575 0.2739 0.452 ]
L1 regularization loss: 1.10E+00, L2 regularization loss: 4.47E-01
Test scatter: [0.2307 0.0573 0.2799 0.4525], Lowest was [0.1987 0.0512 0.2589 0.4217]
Median for last 10 epochs: [0.2149 0.0541 0.2623 0.4309], Epochs since improvement 2
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 24%|██▍       | 120/500 [2:15:56<7:35:47, 71.97s/it] 24%|██▍       | 121/500 [2:16:50<7:00:07, 66.51s/it] 24%|██▍       | 122/500 [2:18:11<7:26:05, 70.81s/it]Epoch: 122 done with learning rate 9.70E-03, Train loss: 1.24E+06, Train scatter: [0.1813 0.0568 0.2538 0.4173]
L1 regularization loss: 1.10E+00, L2 regularization loss: 4.55E-01
Test scatter: [0.1911 0.0573 0.256  0.4196], Lowest was [0.1911 0.0512 0.256  0.4196]
Median for last 10 epochs: [0.2149 0.0541 0.2592 0.4293], Epochs since improvement 0
 25%|██▍       | 123/500 [2:19:04<6:52:18, 65.62s/it] 25%|██▍       | 124/500 [2:20:25<7:19:05, 70.07s/it]Epoch: 124 done with learning rate 9.68E-03, Train loss: 1.22E+06, Train scatter: [0.178  0.0499 0.2573 0.4226]
L1 regularization loss: 1.11E+00, L2 regularization loss: 4.64E-01
Test scatter: [0.1899 0.0508 0.2613 0.4248], Lowest was [0.1899 0.0508 0.256  0.4196]
Median for last 10 epochs: [0.1991 0.0541 0.2613 0.4248], Epochs since improvement 0
 25%|██▌       | 125/500 [2:21:18<6:47:18, 65.17s/it] 25%|██▌       | 126/500 [2:22:39<7:14:52, 69.77s/it]Epoch: 126 done with learning rate 9.65E-03, Train loss: 1.12E+06, Train scatter: [0.2008 0.0521 0.2704 0.4443]
L1 regularization loss: 1.12E+00, L2 regularization loss: 4.73E-01
Test scatter: [0.2093 0.0524 0.2748 0.4478], Lowest was [0.1899 0.0508 0.256  0.4196]
Median for last 10 epochs: [0.2093 0.0524 0.2613 0.4248], Epochs since improvement 2
 25%|██▌       | 127/500 [2:23:32<6:43:30, 64.91s/it] 26%|██▌       | 128/500 [2:24:53<7:10:42, 69.47s/it]Epoch: 128 done with learning rate 9.62E-03, Train loss: 1.03E+06, Train scatter: [0.1847 0.05   0.2557 0.417 ]
L1 regularization loss: 1.12E+00, L2 regularization loss: 4.80E-01
Test scatter: [0.2062 0.0502 0.2572 0.417 ], Lowest was [0.1899 0.0502 0.256  0.417 ]
Median for last 10 epochs: [0.2062 0.0524 0.2613 0.4248], Epochs since improvement 0
 26%|██▌       | 129/500 [2:25:46<6:40:16, 64.73s/it]Epoch: 130 done with learning rate 9.59E-03, Train loss: 9.92E+05, Train scatter: [0.1797 0.0493 0.2615 0.4155]
L1 regularization loss: 1.13E+00, L2 regularization loss: 4.88E-01
Test scatter: [0.1964 0.0498 0.2619 0.4153], Lowest was [0.1899 0.0498 0.256  0.4153]
Median for last 10 epochs: [0.1964 0.0508 0.2613 0.4196], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 26%|██▌       | 130/500 [2:27:15<7:23:03, 71.85s/it] 26%|██▌       | 131/500 [2:28:08<6:48:29, 66.42s/it] 26%|██▋       | 132/500 [2:29:30<7:14:25, 70.83s/it]Epoch: 132 done with learning rate 9.56E-03, Train loss: 9.51E+05, Train scatter: [0.2174 0.0506 0.2614 0.4218]
L1 regularization loss: 1.13E+00, L2 regularization loss: 4.97E-01
Test scatter: [0.2252 0.051  0.2658 0.4206], Lowest was [0.1899 0.0498 0.256  0.4153]
Median for last 10 epochs: [0.2062 0.0508 0.2619 0.4206], Epochs since improvement 2
 27%|██▋       | 133/500 [2:30:23<6:42:02, 65.73s/it] 27%|██▋       | 134/500 [2:31:45<7:09:47, 70.46s/it]Epoch: 134 done with learning rate 9.53E-03, Train loss: 9.36E+05, Train scatter: [0.1997 0.0542 0.2609 0.4161]
L1 regularization loss: 1.14E+00, L2 regularization loss: 5.06E-01
Test scatter: [0.2141 0.0547 0.2628 0.416 ], Lowest was [0.1899 0.0498 0.256  0.4153]
Median for last 10 epochs: [0.2093 0.051  0.2628 0.417 ], Epochs since improvement 4
 27%|██▋       | 135/500 [2:32:39<6:38:11, 65.46s/it] 27%|██▋       | 136/500 [2:34:01<7:07:41, 70.50s/it]Epoch: 136 done with learning rate 9.50E-03, Train loss: 8.92E+05, Train scatter: [0.1736 0.0597 0.2543 0.4149]
L1 regularization loss: 1.15E+00, L2 regularization loss: 5.15E-01
Test scatter: [0.1919 0.0592 0.2551 0.4136], Lowest was [0.1899 0.0498 0.2551 0.4136]
Median for last 10 epochs: [0.2062 0.051  0.2619 0.416 ], Epochs since improvement 0
 27%|██▋       | 137/500 [2:34:55<6:36:02, 65.46s/it] 28%|██▊       | 138/500 [2:36:15<7:02:10, 69.97s/it]Epoch: 138 done with learning rate 9.47E-03, Train loss: 8.43E+05, Train scatter: [0.1928 0.0516 0.2611 0.4142]
L1 regularization loss: 1.15E+00, L2 regularization loss: 5.25E-01
Test scatter: [0.2093 0.0513 0.261  0.4125], Lowest was [0.1899 0.0498 0.2551 0.4125]
Median for last 10 epochs: [0.2093 0.0513 0.2619 0.4153], Epochs since improvement 0
 28%|██▊       | 139/500 [2:37:09<6:31:25, 65.06s/it]Epoch: 140 done with learning rate 9.43E-03, Train loss: 8.35E+05, Train scatter: [0.1798 0.0512 0.2513 0.4072]
L1 regularization loss: 1.17E+00, L2 regularization loss: 5.36E-01
Test scatter: [0.1889 0.0516 0.2525 0.4078], Lowest was [0.1889 0.0498 0.2525 0.4078]
Median for last 10 epochs: [0.2093 0.0516 0.261  0.4136], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 28%|██▊       | 140/500 [2:38:37<7:12:17, 72.05s/it] 28%|██▊       | 141/500 [2:39:31<6:38:06, 66.54s/it] 28%|██▊       | 142/500 [2:40:51<7:02:23, 70.79s/it]Epoch: 142 done with learning rate 9.40E-03, Train loss: 8.10E+05, Train scatter: [0.1626 0.0557 0.2554 0.4266]
L1 regularization loss: 1.17E+00, L2 regularization loss: 5.47E-01
Test scatter: [0.1785 0.0555 0.2579 0.4193], Lowest was [0.1785 0.0498 0.2525 0.4078]
Median for last 10 epochs: [0.1919 0.0547 0.2579 0.4136], Epochs since improvement 0
 29%|██▊       | 143/500 [2:41:45<6:30:40, 65.66s/it] 29%|██▉       | 144/500 [2:43:07<6:58:19, 70.50s/it]Epoch: 144 done with learning rate 9.36E-03, Train loss: 7.81E+05, Train scatter: [0.174  0.0518 0.2505 0.4267]
L1 regularization loss: 1.18E+00, L2 regularization loss: 5.56E-01
Test scatter: [0.205  0.0524 0.2553 0.4348], Lowest was [0.1785 0.0498 0.2525 0.4078]
Median for last 10 epochs: [0.1919 0.0524 0.2553 0.4136], Epochs since improvement 2
 29%|██▉       | 145/500 [2:44:01<6:27:05, 65.42s/it] 29%|██▉       | 146/500 [2:45:21<6:53:30, 70.09s/it]Epoch: 146 done with learning rate 9.33E-03, Train loss: 7.13E+05, Train scatter: [0.165  0.0518 0.2574 0.4183]
L1 regularization loss: 1.20E+00, L2 regularization loss: 5.71E-01
Test scatter: [0.1826 0.0513 0.259  0.4122], Lowest was [0.1785 0.0498 0.2525 0.4078]
Median for last 10 epochs: [0.1889 0.0516 0.2579 0.4125], Epochs since improvement 4
 29%|██▉       | 147/500 [2:46:15<6:23:20, 65.16s/it] 30%|██▉       | 148/500 [2:47:36<6:49:22, 69.78s/it]Epoch: 148 done with learning rate 9.29E-03, Train loss: 6.75E+05, Train scatter: [0.185  0.0526 0.2496 0.4086]
L1 regularization loss: 1.21E+00, L2 regularization loss: 5.81E-01
Test scatter: [0.1963 0.0529 0.2512 0.4114], Lowest was [0.1785 0.0498 0.2512 0.4078]
Median for last 10 epochs: [0.1889 0.0524 0.2553 0.4122], Epochs since improvement 0
 30%|██▉       | 149/500 [2:48:29<6:19:48, 64.93s/it]Epoch: 150 done with learning rate 9.25E-03, Train loss: 5.89E+05, Train scatter: [0.1759 0.0515 0.2725 0.4232]
L1 regularization loss: 1.22E+00, L2 regularization loss: 5.92E-01
Test scatter: [0.1906 0.0516 0.2738 0.4192], Lowest was [0.1785 0.0498 0.2512 0.4078]
Median for last 10 epochs: [0.1906 0.0524 0.2579 0.4192], Epochs since improvement 2
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 30%|███       | 150/500 [2:49:58<6:59:33, 71.92s/it] 30%|███       | 151/500 [2:50:51<6:26:21, 66.42s/it] 30%|███       | 152/500 [2:52:13<6:51:24, 70.93s/it]Epoch: 152 done with learning rate 9.21E-03, Train loss: 5.79E+05, Train scatter: [0.1579 0.0508 0.2597 0.424 ]
L1 regularization loss: 1.23E+00, L2 regularization loss: 6.06E-01
Test scatter: [0.1709 0.0509 0.2616 0.4273], Lowest was [0.1709 0.0498 0.2512 0.4078]
Median for last 10 epochs: [0.1906 0.0516 0.259  0.4192], Epochs since improvement 0
 31%|███       | 153/500 [2:53:06<6:20:08, 65.73s/it] 31%|███       | 154/500 [2:54:27<6:44:20, 70.12s/it]Epoch: 154 done with learning rate 9.17E-03, Train loss: 5.55E+05, Train scatter: [0.2075 0.0509 0.2396 0.4088]
L1 regularization loss: 1.24E+00, L2 regularization loss: 6.16E-01
Test scatter: [0.2081 0.0505 0.2426 0.4007], Lowest was [0.1709 0.0498 0.2426 0.4007]
Median for last 10 epochs: [0.1906 0.0513 0.259  0.4122], Epochs since improvement 0
 31%|███       | 155/500 [2:55:20<6:14:33, 65.14s/it] 31%|███       | 156/500 [2:56:41<6:40:13, 69.81s/it]Epoch: 156 done with learning rate 9.13E-03, Train loss: 4.98E+05, Train scatter: [0.2729 0.0521 0.25   0.4022]
L1 regularization loss: 1.24E+00, L2 regularization loss: 6.25E-01
Test scatter: [0.262  0.0515 0.2511 0.3987], Lowest was [0.1709 0.0498 0.2426 0.3987]
Median for last 10 epochs: [0.1963 0.0515 0.2512 0.4114], Epochs since improvement 0
 31%|███▏      | 157/500 [2:57:35<6:11:30, 64.99s/it] 32%|███▏      | 158/500 [2:58:55<6:36:17, 69.52s/it]Epoch: 158 done with learning rate 9.09E-03, Train loss: 4.52E+05, Train scatter: [0.1612 0.0468 0.2549 0.4046]
L1 regularization loss: 1.25E+00, L2 regularization loss: 6.35E-01
Test scatter: [0.1649 0.0467 0.2571 0.4059], Lowest was [0.1649 0.0467 0.2426 0.3987]
Median for last 10 epochs: [0.1906 0.0509 0.2571 0.4059], Epochs since improvement 0
 32%|███▏      | 159/500 [2:59:48<6:07:55, 64.74s/it]Epoch: 160 done with learning rate 9.04E-03, Train loss: 3.72E+05, Train scatter: [0.1727 0.0453 0.2417 0.4   ]
L1 regularization loss: 1.26E+00, L2 regularization loss: 6.45E-01
Test scatter: [0.1758 0.0452 0.2426 0.3948], Lowest was [0.1649 0.0452 0.2426 0.3948]
Median for last 10 epochs: [0.1758 0.0505 0.2511 0.4007], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 32%|███▏      | 160/500 [3:01:16<6:46:14, 71.69s/it] 32%|███▏      | 161/500 [3:02:10<6:14:31, 66.29s/it] 32%|███▏      | 162/500 [3:03:30<6:37:44, 70.61s/it]Epoch: 162 done with learning rate 9.00E-03, Train loss: 2.73E+05, Train scatter: [0.1735 0.0461 0.266  0.3993]
L1 regularization loss: 1.26E+00, L2 regularization loss: 6.55E-01
Test scatter: [0.1854 0.0466 0.2692 0.402 ], Lowest was [0.1649 0.0452 0.2426 0.3948]
Median for last 10 epochs: [0.1854 0.0467 0.2511 0.4007], Epochs since improvement 2
 33%|███▎      | 163/500 [3:04:24<6:07:48, 65.48s/it] 33%|███▎      | 164/500 [3:05:45<6:32:30, 70.09s/it]Epoch: 164 done with learning rate 8.96E-03, Train loss: 2.42E+05, Train scatter: [0.1592 0.0501 0.2429 0.3948]
L1 regularization loss: 1.28E+00, L2 regularization loss: 6.70E-01
Test scatter: [0.1748 0.0507 0.245  0.3953], Lowest was [0.1649 0.0452 0.2426 0.3948]
Median for last 10 epochs: [0.1758 0.0467 0.2511 0.3987], Epochs since improvement 4
 33%|███▎      | 165/500 [3:06:39<6:03:55, 65.18s/it] 33%|███▎      | 166/500 [3:08:00<6:30:10, 70.09s/it]Epoch: 166 done with learning rate 8.91E-03, Train loss: 1.76E+05, Train scatter: [0.1465 0.0448 0.2381 0.3856]
L1 regularization loss: 1.29E+00, L2 regularization loss: 6.79E-01
Test scatter: [0.1611 0.0447 0.2406 0.3861], Lowest was [0.1611 0.0447 0.2406 0.3861]
Median for last 10 epochs: [0.1748 0.0466 0.245  0.3953], Epochs since improvement 0
 33%|███▎      | 167/500 [3:08:54<6:01:51, 65.20s/it] 34%|███▎      | 168/500 [3:10:15<6:26:46, 69.90s/it]Epoch: 168 done with learning rate 8.86E-03, Train loss: 7.57E+04, Train scatter: [0.1345 0.0438 0.241  0.3895]
L1 regularization loss: 1.30E+00, L2 regularization loss: 6.90E-01
Test scatter: [0.1423 0.0439 0.2457 0.3915], Lowest was [0.1423 0.0439 0.2406 0.3861]
Median for last 10 epochs: [0.1748 0.0452 0.245  0.3948], Epochs since improvement 0
 34%|███▍      | 169/500 [3:11:08<5:58:37, 65.01s/it]Epoch: 170 done with learning rate 8.82E-03, Train loss: 3.83E+03, Train scatter: [0.1578 0.0443 0.2282 0.383 ]
L1 regularization loss: 1.31E+00, L2 regularization loss: 7.01E-01
Test scatter: [0.1595 0.0439 0.2309 0.3824], Lowest was [0.1423 0.0439 0.2309 0.3824]
Median for last 10 epochs: [0.1611 0.0447 0.245  0.3915], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 34%|███▍      | 170/500 [3:12:36<6:35:03, 71.83s/it] 34%|███▍      | 171/500 [3:13:30<6:04:15, 66.43s/it] 34%|███▍      | 172/500 [3:14:51<6:27:38, 70.91s/it]Epoch: 172 done with learning rate 8.77E-03, Train loss: -6.32E+04, Train scatter: [0.1469 0.0439 0.2417 0.3853]
L1 regularization loss: 1.31E+00, L2 regularization loss: 7.12E-01
Test scatter: [0.1545 0.044  0.2447 0.3852], Lowest was [0.1423 0.0439 0.2309 0.3824]
Median for last 10 epochs: [0.1595 0.044  0.2447 0.3861], Epochs since improvement 2
 35%|███▍      | 173/500 [3:15:45<5:58:10, 65.72s/it] 35%|███▍      | 174/500 [3:17:07<6:23:14, 70.53s/it]Epoch: 174 done with learning rate 8.72E-03, Train loss: -1.15E+05, Train scatter: [0.1292 0.0435 0.2391 0.3806]
L1 regularization loss: 1.32E+00, L2 regularization loss: 7.22E-01
Test scatter: [0.144  0.0437 0.244  0.3813], Lowest was [0.1423 0.0437 0.2309 0.3813]
Median for last 10 epochs: [0.1545 0.0439 0.244  0.3852], Epochs since improvement 0
 35%|███▌      | 175/500 [3:18:00<5:54:45, 65.49s/it] 35%|███▌      | 176/500 [3:19:22<6:18:58, 70.18s/it]Epoch: 176 done with learning rate 8.67E-03, Train loss: -1.64E+05, Train scatter: [0.1886 0.0527 0.2822 0.3957]
L1 regularization loss: 1.33E+00, L2 regularization loss: 7.33E-01
Test scatter: [0.1872 0.052  0.2875 0.3946], Lowest was [0.1423 0.0437 0.2309 0.3813]
Median for last 10 epochs: [0.1545 0.0439 0.2447 0.3852], Epochs since improvement 2
 35%|███▌      | 177/500 [3:20:15<5:51:03, 65.21s/it] 36%|███▌      | 178/500 [3:21:37<6:16:36, 70.18s/it]Epoch: 178 done with learning rate 8.62E-03, Train loss: -2.17E+05, Train scatter: [0.1307 0.0459 0.2329 0.3842]
L1 regularization loss: 1.36E+00, L2 regularization loss: 7.57E-01
Test scatter: [0.1494 0.0452 0.2366 0.3843], Lowest was [0.1423 0.0437 0.2309 0.3813]
Median for last 10 epochs: [0.1545 0.044  0.244  0.3843], Epochs since improvement 4
 36%|███▌      | 179/500 [3:22:31<5:48:55, 65.22s/it]Epoch: 180 done with learning rate 8.57E-03, Train loss: -2.52E+05, Train scatter: [0.1333 0.0429 0.2217 0.3798]
L1 regularization loss: 1.37E+00, L2 regularization loss: 7.69E-01
Test scatter: [0.1678 0.0428 0.2245 0.3799], Lowest was [0.1423 0.0428 0.2245 0.3799]
Median for last 10 epochs: [0.1545 0.044  0.244  0.3843], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 36%|███▌      | 180/500 [3:24:01<6:28:03, 72.76s/it] 36%|███▌      | 181/500 [3:24:55<5:56:24, 67.03s/it] 36%|███▋      | 182/500 [3:26:16<6:18:39, 71.44s/it]Epoch: 182 done with learning rate 8.52E-03, Train loss: -2.97E+05, Train scatter: [0.1426 0.0523 0.2584 0.4117]
L1 regularization loss: 1.38E+00, L2 regularization loss: 7.81E-01
Test scatter: [0.1515 0.0516 0.2625 0.4084], Lowest was [0.1423 0.0428 0.2245 0.3799]
Median for last 10 epochs: [0.1515 0.0452 0.244  0.3843], Epochs since improvement 2
 37%|███▋      | 183/500 [3:27:10<5:49:23, 66.13s/it] 37%|███▋      | 184/500 [3:28:32<6:13:11, 70.86s/it]Epoch: 184 done with learning rate 8.46E-03, Train loss: -3.17E+05, Train scatter: [0.1223 0.0476 0.2297 0.3845]
L1 regularization loss: 1.38E+00, L2 regularization loss: 7.94E-01
Test scatter: [0.156  0.0465 0.2325 0.3835], Lowest was [0.1423 0.0428 0.2245 0.3799]
Median for last 10 epochs: [0.156  0.0465 0.2366 0.3843], Epochs since improvement 4
 37%|███▋      | 185/500 [3:29:26<5:45:09, 65.74s/it] 37%|███▋      | 186/500 [3:30:47<6:08:38, 70.44s/it]Epoch: 186 done with learning rate 8.41E-03, Train loss: -3.50E+05, Train scatter: [0.1185 0.0414 0.218  0.3747]
L1 regularization loss: 1.39E+00, L2 regularization loss: 8.06E-01
Test scatter: [0.1435 0.0411 0.2212 0.3756], Lowest was [0.1423 0.0411 0.2212 0.3756]
Median for last 10 epochs: [0.1515 0.0452 0.2325 0.3835], Epochs since improvement 0
 37%|███▋      | 187/500 [3:31:41<5:41:26, 65.45s/it] 38%|███▊      | 188/500 [3:33:03<6:06:26, 70.47s/it]Epoch: 188 done with learning rate 8.35E-03, Train loss: -3.59E+05, Train scatter: [0.1433 0.0438 0.2312 0.3803]
L1 regularization loss: 1.40E+00, L2 regularization loss: 8.21E-01
Test scatter: [0.1571 0.0436 0.2337 0.3826], Lowest was [0.1423 0.0411 0.2212 0.3756]
Median for last 10 epochs: [0.156  0.0436 0.2325 0.3826], Epochs since improvement 2
 38%|███▊      | 189/500 [3:33:57<5:39:07, 65.43s/it]Epoch: 190 done with learning rate 8.30E-03, Train loss: -3.72E+05, Train scatter: [0.2023 0.0476 0.2407 0.3953]
L1 regularization loss: 1.41E+00, L2 regularization loss: 8.35E-01
Test scatter: [0.2116 0.0476 0.2479 0.3995], Lowest was [0.1423 0.0411 0.2212 0.3756]
Median for last 10 epochs: [0.156  0.0465 0.2337 0.3835], Epochs since improvement 4
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 38%|███▊      | 190/500 [3:35:25<6:13:28, 72.29s/it] 38%|███▊      | 191/500 [3:36:19<5:43:41, 66.74s/it] 38%|███▊      | 192/500 [3:37:41<6:05:49, 71.27s/it]Epoch: 192 done with learning rate 8.24E-03, Train loss: -3.71E+05, Train scatter: [0.1406 0.0429 0.2188 0.3868]
L1 regularization loss: 1.42E+00, L2 regularization loss: 8.52E-01
Test scatter: [0.1746 0.0425 0.2218 0.3849], Lowest was [0.1423 0.0411 0.2212 0.3756]
Median for last 10 epochs: [0.1571 0.0436 0.2325 0.3835], Epochs since improvement 6
 39%|███▊      | 193/500 [3:38:34<5:37:33, 65.97s/it] 39%|███▉      | 194/500 [3:39:56<6:01:02, 70.79s/it]Epoch: 194 done with learning rate 8.19E-03, Train loss: -3.78E+05, Train scatter: [0.1316 0.0419 0.2343 0.3808]
L1 regularization loss: 1.43E+00, L2 regularization loss: 8.67E-01
Test scatter: [0.1868 0.042  0.2373 0.3796], Lowest was [0.1423 0.0411 0.2212 0.3756]
Median for last 10 epochs: [0.1746 0.0425 0.2337 0.3826], Epochs since improvement 8
 39%|███▉      | 195/500 [3:40:50<5:33:41, 65.65s/it] 39%|███▉      | 196/500 [3:42:11<5:56:05, 70.28s/it]Epoch: 196 done with learning rate 8.13E-03, Train loss: -3.99E+05, Train scatter: [0.1149 0.0427 0.2307 0.3787]
L1 regularization loss: 1.44E+00, L2 regularization loss: 8.84E-01
Test scatter: [0.1226 0.0423 0.2352 0.38  ], Lowest was [0.1226 0.0411 0.2212 0.3756]
Median for last 10 epochs: [0.1746 0.0425 0.2352 0.3826], Epochs since improvement 0
 39%|███▉      | 197/500 [3:43:05<5:29:58, 65.34s/it] 40%|███▉      | 198/500 [3:44:25<5:51:47, 69.89s/it]Epoch: 198 done with learning rate 8.07E-03, Train loss: -2.72E+05, Train scatter: [0.1487 0.0488 0.2424 0.4166]
L1 regularization loss: 1.50E+00, L2 regularization loss: 9.30E-01
Test scatter: [0.1711 0.0493 0.2437 0.4103], Lowest was [0.1226 0.0411 0.2212 0.3756]
Median for last 10 epochs: [0.1746 0.0425 0.2373 0.3849], Epochs since improvement 2
 40%|███▉      | 199/500 [3:45:19<5:26:35, 65.10s/it]Epoch: 200 done with learning rate 8.01E-03, Train loss: -3.43E+05, Train scatter: [0.1266 0.0406 0.2195 0.3926]
L1 regularization loss: 1.52E+00, L2 regularization loss: 9.55E-01
Test scatter: [0.1319 0.0405 0.2222 0.3879], Lowest was [0.1226 0.0405 0.2212 0.3756]
Median for last 10 epochs: [0.1711 0.0423 0.2352 0.3849], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 40%|████      | 200/500 [3:46:48<6:00:27, 72.09s/it] 40%|████      | 201/500 [3:47:41<5:31:42, 66.56s/it] 40%|████      | 202/500 [3:49:03<5:52:21, 70.94s/it]Epoch: 202 done with learning rate 7.95E-03, Train loss: -3.96E+05, Train scatter: [0.1265 0.042  0.2307 0.392 ]
L1 regularization loss: 1.52E+00, L2 regularization loss: 9.68E-01
Test scatter: [0.132  0.0417 0.2315 0.3873], Lowest was [0.1226 0.0405 0.2212 0.3756]
Median for last 10 epochs: [0.132  0.042  0.2352 0.3873], Epochs since improvement 2
 41%|████      | 203/500 [3:49:56<5:25:24, 65.74s/it] 41%|████      | 204/500 [3:51:17<5:46:14, 70.18s/it]Epoch: 204 done with learning rate 7.89E-03, Train loss: -4.02E+05, Train scatter: [0.1221 0.0421 0.2217 0.3917]
L1 regularization loss: 1.53E+00, L2 regularization loss: 9.84E-01
Test scatter: [0.1262 0.0419 0.2263 0.3899], Lowest was [0.1226 0.0405 0.2212 0.3756]
Median for last 10 epochs: [0.1319 0.0419 0.2315 0.3879], Epochs since improvement 4
 41%|████      | 205/500 [3:52:11<5:20:57, 65.28s/it] 41%|████      | 206/500 [3:53:31<5:42:35, 69.92s/it]Epoch: 206 done with learning rate 7.83E-03, Train loss: -4.17E+05, Train scatter: [0.1292 0.0489 0.2411 0.4169]
L1 regularization loss: 1.54E+00, L2 regularization loss: 9.94E-01
Test scatter: [0.1327 0.0481 0.2441 0.4154], Lowest was [0.1226 0.0405 0.2212 0.3756]
Median for last 10 epochs: [0.132  0.0419 0.2315 0.3899], Epochs since improvement 6
 41%|████▏     | 207/500 [3:54:25<5:17:41, 65.06s/it] 42%|████▏     | 208/500 [3:55:46<5:39:30, 69.76s/it]Epoch: 208 done with learning rate 7.77E-03, Train loss: -4.26E+05, Train scatter: [0.1261 0.0407 0.215  0.3899]
L1 regularization loss: 1.55E+00, L2 regularization loss: 1.01E+00
Test scatter: [0.1293 0.0402 0.217  0.3851], Lowest was [0.1226 0.0402 0.217  0.3756]
Median for last 10 epochs: [0.1319 0.0417 0.2263 0.3879], Epochs since improvement 0
 42%|████▏     | 209/500 [3:56:39<5:14:52, 64.92s/it]Epoch: 210 done with learning rate 7.71E-03, Train loss: -4.28E+05, Train scatter: [0.1366 0.0454 0.2409 0.4048]
L1 regularization loss: 1.55E+00, L2 regularization loss: 1.02E+00
Test scatter: [0.1403 0.0452 0.2406 0.399 ], Lowest was [0.1226 0.0402 0.217  0.3756]
Median for last 10 epochs: [0.132  0.0419 0.2315 0.3899], Epochs since improvement 2
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 42%|████▏     | 210/500 [3:58:08<5:47:43, 71.94s/it] 42%|████▏     | 211/500 [3:59:01<5:20:07, 66.46s/it] 42%|████▏     | 212/500 [4:00:22<5:40:00, 70.83s/it]Epoch: 212 done with learning rate 7.65E-03, Train loss: -4.19E+05, Train scatter: [0.1218 0.0405 0.2113 0.3826]
L1 regularization loss: 1.57E+00, L2 regularization loss: 1.04E+00
Test scatter: [0.1248 0.0404 0.2148 0.3807], Lowest was [0.1226 0.0402 0.2148 0.3756]
Median for last 10 epochs: [0.1293 0.0419 0.2263 0.3899], Epochs since improvement 0
 43%|████▎     | 213/500 [4:01:16<5:14:06, 65.67s/it] 43%|████▎     | 214/500 [4:02:37<5:35:25, 70.37s/it]Epoch: 214 done with learning rate 7.58E-03, Train loss: -4.18E+05, Train scatter: [0.12   0.0408 0.2157 0.3838]
L1 regularization loss: 1.58E+00, L2 regularization loss: 1.05E+00
Test scatter: [0.1251 0.0403 0.2177 0.3803], Lowest was [0.1226 0.0402 0.2148 0.3756]
Median for last 10 epochs: [0.1293 0.0404 0.2177 0.3851], Epochs since improvement 2
 43%|████▎     | 215/500 [4:03:31<5:10:31, 65.37s/it] 43%|████▎     | 216/500 [4:04:53<5:32:27, 70.24s/it]Epoch: 216 done with learning rate 7.52E-03, Train loss: -4.28E+05, Train scatter: [0.119  0.0411 0.2109 0.389 ]
L1 regularization loss: 1.61E+00, L2 regularization loss: 1.08E+00
Test scatter: [0.1235 0.0407 0.2136 0.3846], Lowest was [0.1226 0.0402 0.2136 0.3756]
Median for last 10 epochs: [0.1251 0.0404 0.217  0.3846], Epochs since improvement 0
 43%|████▎     | 217/500 [4:05:46<5:07:52, 65.27s/it] 44%|████▎     | 218/500 [4:07:08<5:29:56, 70.20s/it]Epoch: 218 done with learning rate 7.46E-03, Train loss: -4.25E+05, Train scatter: [0.1212 0.0405 0.2143 0.3926]
L1 regularization loss: 1.65E+00, L2 regularization loss: 1.12E+00
Test scatter: [0.1255 0.0403 0.2157 0.3889], Lowest was [0.1226 0.0402 0.2136 0.3756]
Median for last 10 epochs: [0.1251 0.0404 0.2157 0.3846], Epochs since improvement 2
 44%|████▍     | 219/500 [4:08:02<5:05:33, 65.24s/it]Epoch: 220 done with learning rate 7.39E-03, Train loss: -4.29E+05, Train scatter: [0.1283 0.0525 0.2629 0.409 ]
L1 regularization loss: 1.65E+00, L2 regularization loss: 1.13E+00
Test scatter: [0.1328 0.0518 0.2637 0.4066], Lowest was [0.1226 0.0402 0.2136 0.3756]
Median for last 10 epochs: [0.1251 0.0404 0.2157 0.3846], Epochs since improvement 4
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 44%|████▍     | 220/500 [4:09:29<5:35:49, 71.96s/it] 44%|████▍     | 221/500 [4:10:23<5:09:17, 66.51s/it] 44%|████▍     | 222/500 [4:11:45<5:29:07, 71.03s/it]Epoch: 222 done with learning rate 7.33E-03, Train loss: -4.26E+05, Train scatter: [0.1249 0.0408 0.2141 0.3997]
L1 regularization loss: 1.66E+00, L2 regularization loss: 1.14E+00
Test scatter: [0.1275 0.0404 0.2166 0.3939], Lowest was [0.1226 0.0402 0.2136 0.3756]
Median for last 10 epochs: [0.1255 0.0404 0.2166 0.3889], Epochs since improvement 6
 45%|████▍     | 223/500 [4:12:39<5:04:14, 65.90s/it] 45%|████▍     | 224/500 [4:14:00<5:24:55, 70.64s/it]Epoch: 224 done with learning rate 7.26E-03, Train loss: -4.23E+05, Train scatter: [0.152  0.0419 0.2174 0.4016]
L1 regularization loss: 1.68E+00, L2 regularization loss: 1.16E+00
Test scatter: [0.1542 0.0418 0.2199 0.3945], Lowest was [0.1226 0.0402 0.2136 0.3756]
Median for last 10 epochs: [0.1275 0.0407 0.2166 0.3939], Epochs since improvement 8
 45%|████▌     | 225/500 [4:14:54<5:00:23, 65.54s/it] 45%|████▌     | 226/500 [4:16:15<5:20:50, 70.26s/it]Epoch: 226 done with learning rate 7.20E-03, Train loss: -4.41E+05, Train scatter: [0.1281 0.0404 0.2151 0.3852]
L1 regularization loss: 1.68E+00, L2 regularization loss: 1.18E+00
Test scatter: [0.1313 0.0403 0.2187 0.3817], Lowest was [0.1226 0.0402 0.2136 0.3756]
Median for last 10 epochs: [0.1313 0.0404 0.2187 0.3939], Epochs since improvement 10
 45%|████▌     | 227/500 [4:17:09<4:57:12, 65.32s/it] 46%|████▌     | 228/500 [4:18:29<5:16:00, 69.71s/it]Epoch: 228 done with learning rate 7.13E-03, Train loss: -4.38E+05, Train scatter: [0.1472 0.0419 0.2186 0.398 ]
L1 regularization loss: 1.70E+00, L2 regularization loss: 1.19E+00
Test scatter: [0.1532 0.0418 0.2241 0.3968], Lowest was [0.1226 0.0402 0.2136 0.3756]
Median for last 10 epochs: [0.1328 0.0418 0.2199 0.3945], Epochs since improvement 12
 46%|████▌     | 229/500 [4:19:23<4:53:25, 64.96s/it]Epoch: 230 done with learning rate 7.06E-03, Train loss: -4.32E+05, Train scatter: [0.1395 0.0522 0.2655 0.4249]
L1 regularization loss: 1.71E+00, L2 regularization loss: 1.21E+00
Test scatter: [0.1436 0.0523 0.2688 0.4238], Lowest was [0.1226 0.0402 0.2136 0.3756]
Median for last 10 epochs: [0.1436 0.0418 0.2199 0.3945], Epochs since improvement 14
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 46%|████▌     | 230/500 [4:20:51<5:23:54, 71.98s/it] 46%|████▌     | 231/500 [4:21:45<4:58:21, 66.55s/it] 46%|████▋     | 232/500 [4:23:06<5:16:48, 70.93s/it]Epoch: 232 done with learning rate 6.99E-03, Train loss: -4.32E+05, Train scatter: [0.1233 0.0409 0.2175 0.4007]
L1 regularization loss: 1.72E+00, L2 regularization loss: 1.22E+00
Test scatter: [0.1269 0.0411 0.2194 0.3931], Lowest was [0.1226 0.0402 0.2136 0.3756]
Median for last 10 epochs: [0.1436 0.0418 0.2199 0.3945], Epochs since improvement 16
 47%|████▋     | 233/500 [4:24:00<4:52:39, 65.77s/it] 47%|████▋     | 234/500 [4:25:22<5:12:48, 70.56s/it]Epoch: 234 done with learning rate 6.93E-03, Train loss: -4.50E+05, Train scatter: [0.1246 0.0459 0.2306 0.3926]
L1 regularization loss: 1.73E+00, L2 regularization loss: 1.23E+00
Test scatter: [0.1267 0.0449 0.2325 0.387 ], Lowest was [0.1226 0.0402 0.2136 0.3756]
Median for last 10 epochs: [0.1313 0.0418 0.2241 0.3931], Epochs since improvement 18
 47%|████▋     | 235/500 [4:26:15<4:49:10, 65.47s/it] 47%|████▋     | 236/500 [4:27:36<5:08:03, 70.01s/it]Epoch: 236 done with learning rate 6.86E-03, Train loss: -4.35E+05, Train scatter: [0.1675 0.0409 0.2152 0.393 ]
L1 regularization loss: 1.75E+00, L2 regularization loss: 1.25E+00
Test scatter: [0.1626 0.0404 0.2167 0.3865], Lowest was [0.1226 0.0402 0.2136 0.3756]
Median for last 10 epochs: [0.1436 0.0418 0.2241 0.3931], Epochs since improvement 20
 47%|████▋     | 237/500 [4:28:30<4:45:23, 65.11s/it] 48%|████▊     | 238/500 [4:29:50<5:04:39, 69.77s/it]Epoch: 238 done with learning rate 6.79E-03, Train loss: -4.52E+05, Train scatter: [0.1179 0.0395 0.2093 0.3921]
L1 regularization loss: 1.75E+00, L2 regularization loss: 1.27E+00
Test scatter: [0.1207 0.0395 0.2123 0.3893], Lowest was [0.1207 0.0395 0.2123 0.3756]
Median for last 10 epochs: [0.1269 0.0411 0.2194 0.3893], Epochs since improvement 0
 48%|████▊     | 239/500 [4:30:44<4:42:24, 64.92s/it]Epoch: 240 done with learning rate 6.72E-03, Train loss: -4.45E+05, Train scatter: [0.1174 0.0399 0.211  0.4003]
L1 regularization loss: 1.77E+00, L2 regularization loss: 1.28E+00
Test scatter: [0.118  0.0397 0.2134 0.392 ], Lowest was [0.118  0.0395 0.2123 0.3756]
Median for last 10 epochs: [0.1267 0.0404 0.2167 0.3893], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 48%|████▊     | 240/500 [4:32:12<5:11:28, 71.88s/it] 48%|████▊     | 241/500 [4:33:06<4:47:05, 66.51s/it] 48%|████▊     | 242/500 [4:34:28<5:05:31, 71.05s/it]Epoch: 242 done with learning rate 6.65E-03, Train loss: -4.46E+05, Train scatter: [0.1353 0.0455 0.2313 0.4134]
L1 regularization loss: 1.78E+00, L2 regularization loss: 1.30E+00
Test scatter: [0.1364 0.0453 0.2357 0.4125], Lowest was [0.118  0.0395 0.2123 0.3756]
Median for last 10 epochs: [0.1267 0.0404 0.2167 0.3893], Epochs since improvement 2
 49%|████▊     | 243/500 [4:35:21<4:42:01, 65.84s/it] 49%|████▉     | 244/500 [4:36:42<4:59:54, 70.29s/it]Epoch: 244 done with learning rate 6.58E-03, Train loss: -4.33E+05, Train scatter: [0.1151 0.0399 0.2135 0.3947]
L1 regularization loss: 1.82E+00, L2 regularization loss: 1.33E+00
Test scatter: [0.1191 0.04   0.2189 0.3908], Lowest was [0.118  0.0395 0.2123 0.3756]
Median for last 10 epochs: [0.1207 0.04   0.2167 0.3908], Epochs since improvement 4
 49%|████▉     | 245/500 [4:37:36<4:37:43, 65.35s/it] 49%|████▉     | 246/500 [4:38:57<4:57:21, 70.24s/it]Epoch: 246 done with learning rate 6.51E-03, Train loss: -4.52E+05, Train scatter: [0.1191 0.0396 0.215  0.4035]
L1 regularization loss: 1.82E+00, L2 regularization loss: 1.34E+00
Test scatter: [0.1231 0.0392 0.2177 0.4011], Lowest was [0.118  0.0392 0.2123 0.3756]
Median for last 10 epochs: [0.1207 0.0397 0.2177 0.392 ], Epochs since improvement 0
 49%|████▉     | 247/500 [4:39:51<4:35:08, 65.25s/it] 50%|████▉     | 248/500 [4:41:12<4:54:09, 70.04s/it]Epoch: 248 done with learning rate 6.44E-03, Train loss: -4.56E+05, Train scatter: [0.1348 0.0424 0.2153 0.4146]
L1 regularization loss: 1.83E+00, L2 regularization loss: 1.36E+00
Test scatter: [0.136  0.0417 0.2202 0.4105], Lowest was [0.118  0.0392 0.2123 0.3756]
Median for last 10 epochs: [0.1231 0.04   0.2189 0.4011], Epochs since improvement 2
 50%|████▉     | 249/500 [4:42:06<4:32:22, 65.11s/it]Epoch: 250 done with learning rate 6.37E-03, Train loss: -4.53E+05, Train scatter: [0.1244 0.0419 0.2159 0.3997]
L1 regularization loss: 1.84E+00, L2 regularization loss: 1.37E+00
Test scatter: [0.1262 0.0412 0.2165 0.3931], Lowest was [0.118  0.0392 0.2123 0.3756]
Median for last 10 epochs: [0.1262 0.0412 0.2189 0.4011], Epochs since improvement 4
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 50%|█████     | 250/500 [4:43:35<5:01:53, 72.45s/it] 50%|█████     | 251/500 [4:44:29<4:37:28, 66.86s/it] 50%|█████     | 252/500 [4:45:50<4:54:04, 71.15s/it]Epoch: 252 done with learning rate 6.30E-03, Train loss: -4.56E+05, Train scatter: [0.1392 0.0437 0.2134 0.4154]
L1 regularization loss: 1.85E+00, L2 regularization loss: 1.38E+00
Test scatter: [0.1424 0.0436 0.2167 0.4124], Lowest was [0.118  0.0392 0.2123 0.3756]
Median for last 10 epochs: [0.1262 0.0412 0.2177 0.4011], Epochs since improvement 6
 51%|█████     | 253/500 [4:46:44<4:31:24, 65.93s/it] 51%|█████     | 254/500 [4:48:06<4:50:06, 70.76s/it]Epoch: 254 done with learning rate 6.23E-03, Train loss: -4.70E+05, Train scatter: [0.126  0.0404 0.2205 0.3933]
L1 regularization loss: 1.86E+00, L2 regularization loss: 1.40E+00
Test scatter: [0.1257 0.0401 0.2228 0.3865], Lowest was [0.118  0.0392 0.2123 0.3756]
Median for last 10 epochs: [0.1262 0.0412 0.2177 0.4011], Epochs since improvement 8
 51%|█████     | 255/500 [4:49:00<4:28:15, 65.69s/it] 51%|█████     | 256/500 [4:50:20<4:44:59, 70.08s/it]Epoch: 256 done with learning rate 6.15E-03, Train loss: -4.54E+05, Train scatter: [0.1185 0.0453 0.2154 0.403 ]
L1 regularization loss: 1.88E+00, L2 regularization loss: 1.42E+00
Test scatter: [0.1197 0.0439 0.216  0.3946], Lowest was [0.118  0.0392 0.2123 0.3756]
Median for last 10 epochs: [0.1262 0.0417 0.2167 0.3946], Epochs since improvement 10
 51%|█████▏    | 257/500 [4:51:14<4:24:05, 65.21s/it] 52%|█████▏    | 258/500 [4:52:36<4:43:03, 70.18s/it]Epoch: 258 done with learning rate 6.08E-03, Train loss: -4.79E+05, Train scatter: [0.1237 0.0402 0.2108 0.3895]
L1 regularization loss: 1.88E+00, L2 regularization loss: 1.43E+00
Test scatter: [0.1246 0.0399 0.2125 0.3868], Lowest was [0.118  0.0392 0.2123 0.3756]
Median for last 10 epochs: [0.1257 0.0412 0.2165 0.3931], Epochs since improvement 12
 52%|█████▏    | 259/500 [4:53:30<4:22:14, 65.29s/it]Epoch: 260 done with learning rate 6.01E-03, Train loss: -4.55E+05, Train scatter: [0.1295 0.04   0.2198 0.3926]
L1 regularization loss: 1.91E+00, L2 regularization loss: 1.45E+00
Test scatter: [0.1267 0.0395 0.2198 0.3829], Lowest was [0.118  0.0392 0.2123 0.3756]
Median for last 10 epochs: [0.1257 0.0401 0.2167 0.3868], Epochs since improvement 14
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 52%|█████▏    | 260/500 [4:54:58<4:48:59, 72.25s/it] 52%|█████▏    | 261/500 [4:55:52<4:25:45, 66.72s/it] 52%|█████▏    | 262/500 [4:57:13<4:41:04, 70.86s/it]Epoch: 262 done with learning rate 5.94E-03, Train loss: -4.74E+05, Train scatter: [0.1098 0.0406 0.2163 0.391 ]
L1 regularization loss: 1.92E+00, L2 regularization loss: 1.46E+00
Test scatter: [0.1119 0.0406 0.2204 0.3852], Lowest was [0.1119 0.0392 0.2123 0.3756]
Median for last 10 epochs: [0.1246 0.0401 0.2198 0.3865], Epochs since improvement 0
 53%|█████▎    | 263/500 [4:58:07<4:19:41, 65.75s/it] 53%|█████▎    | 264/500 [4:59:27<4:36:10, 70.21s/it]Epoch: 264 done with learning rate 5.86E-03, Train loss: -4.61E+05, Train scatter: [0.1209 0.0408 0.216  0.391 ]
L1 regularization loss: 1.94E+00, L2 regularization loss: 1.48E+00
Test scatter: [0.1217 0.0399 0.2164 0.3822], Lowest was [0.1119 0.0392 0.2123 0.3756]
Median for last 10 epochs: [0.1217 0.0399 0.2164 0.3852], Epochs since improvement 2
 53%|█████▎    | 265/500 [5:00:21<4:15:51, 65.32s/it] 53%|█████▎    | 266/500 [5:01:42<4:33:07, 70.03s/it]Epoch: 266 done with learning rate 5.79E-03, Train loss: -4.85E+05, Train scatter: [0.1544 0.049  0.241  0.4196]
L1 regularization loss: 1.95E+00, L2 regularization loss: 1.49E+00
Test scatter: [0.1537 0.0481 0.2421 0.4128], Lowest was [0.1119 0.0392 0.2123 0.3756]
Median for last 10 epochs: [0.1246 0.0399 0.2198 0.3852], Epochs since improvement 4
 53%|█████▎    | 267/500 [5:02:36<4:12:59, 65.15s/it] 54%|█████▎    | 268/500 [5:03:57<4:30:14, 69.89s/it]Epoch: 268 done with learning rate 5.72E-03, Train loss: -4.81E+05, Train scatter: [0.1238 0.0392 0.2153 0.4056]
L1 regularization loss: 1.97E+00, L2 regularization loss: 1.51E+00
Test scatter: [0.124  0.0387 0.2154 0.3968], Lowest was [0.1119 0.0387 0.2123 0.3756]
Median for last 10 epochs: [0.124  0.0399 0.2198 0.3852], Epochs since improvement 0
 54%|█████▍    | 269/500 [5:04:51<4:10:26, 65.05s/it]Epoch: 270 done with learning rate 5.64E-03, Train loss: -4.85E+05, Train scatter: [0.1224 0.0427 0.211  0.3963]
L1 regularization loss: 1.98E+00, L2 regularization loss: 1.53E+00
Test scatter: [0.1238 0.0424 0.2137 0.3884], Lowest was [0.1119 0.0387 0.2123 0.3756]
Median for last 10 epochs: [0.1238 0.0406 0.2164 0.3884], Epochs since improvement 2
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 54%|█████▍    | 270/500 [5:06:19<4:35:52, 71.97s/it] 54%|█████▍    | 271/500 [5:07:13<4:13:57, 66.54s/it] 54%|█████▍    | 272/500 [5:08:33<4:29:02, 70.80s/it]Epoch: 272 done with learning rate 5.57E-03, Train loss: -4.83E+05, Train scatter: [0.1086 0.0386 0.203  0.3815]
L1 regularization loss: 2.00E+00, L2 regularization loss: 1.54E+00
Test scatter: [0.1123 0.0383 0.2064 0.3761], Lowest was [0.1119 0.0383 0.2064 0.3756]
Median for last 10 epochs: [0.1238 0.0399 0.2154 0.3884], Epochs since improvement 0
 55%|█████▍    | 273/500 [5:09:27<4:08:22, 65.65s/it] 55%|█████▍    | 274/500 [5:10:48<4:25:07, 70.39s/it]Epoch: 274 done with learning rate 5.50E-03, Train loss: -4.78E+05, Train scatter: [0.1371 0.0414 0.2202 0.4064]
L1 regularization loss: 2.02E+00, L2 regularization loss: 1.55E+00
Test scatter: [0.1373 0.0408 0.2214 0.399 ], Lowest was [0.1119 0.0383 0.2064 0.3756]
Median for last 10 epochs: [0.124  0.0408 0.2154 0.3968], Epochs since improvement 2
 55%|█████▌    | 275/500 [5:11:42<4:05:22, 65.43s/it] 55%|█████▌    | 276/500 [5:13:04<4:22:05, 70.20s/it]Epoch: 276 done with learning rate 5.42E-03, Train loss: -4.85E+05, Train scatter: [0.1243 0.0392 0.2035 0.3886]
L1 regularization loss: 2.04E+00, L2 regularization loss: 1.57E+00
Test scatter: [0.1283 0.0387 0.2063 0.3798], Lowest was [0.1119 0.0383 0.2063 0.3756]
Median for last 10 epochs: [0.124  0.0387 0.2137 0.3884], Epochs since improvement 0
 55%|█████▌    | 277/500 [5:13:57<4:02:33, 65.26s/it] 56%|█████▌    | 278/500 [5:15:18<4:18:45, 69.93s/it]Epoch: 278 done with learning rate 5.35E-03, Train loss: -4.87E+05, Train scatter: [0.125  0.0409 0.2133 0.3938]
L1 regularization loss: 2.05E+00, L2 regularization loss: 1.59E+00
Test scatter: [0.1282 0.0405 0.2158 0.3863], Lowest was [0.1119 0.0383 0.2063 0.3756]
Median for last 10 epochs: [0.1282 0.0405 0.2137 0.3863], Epochs since improvement 2
 56%|█████▌    | 279/500 [5:16:12<3:59:44, 65.09s/it]Epoch: 280 done with learning rate 5.28E-03, Train loss: -5.27E+04, Train scatter: [0.7028 0.1161 0.4821 0.7748]
L1 regularization loss: 2.94E+00, L2 regularization loss: 1.88E+00
Test scatter: [0.6905 0.1143 0.4772 0.7681], Lowest was [0.1119 0.0383 0.2063 0.3756]
Median for last 10 epochs: [0.1283 0.0405 0.2158 0.3863], Epochs since improvement 4
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 56%|█████▌    | 280/500 [5:17:41<4:24:38, 72.18s/it] 56%|█████▌    | 281/500 [5:18:34<4:03:14, 66.64s/it] 56%|█████▋    | 282/500 [5:19:56<4:18:18, 71.09s/it]Epoch: 282 done with learning rate 5.20E-03, Train loss: -1.48E+05, Train scatter: [0.6128 0.1123 0.4816 0.7351]
L1 regularization loss: 2.98E+00, L2 regularization loss: 1.94E+00
Test scatter: [0.6068 0.111  0.476  0.7298], Lowest was [0.1119 0.0383 0.2063 0.3756]
Median for last 10 epochs: [0.1373 0.0408 0.2214 0.399 ], Epochs since improvement 6
 57%|█████▋    | 283/500 [5:20:49<3:58:09, 65.85s/it] 57%|█████▋    | 284/500 [5:22:10<4:13:23, 70.39s/it]Epoch: 284 done with learning rate 5.13E-03, Train loss: -1.82E+05, Train scatter: [0.3201 0.0889 0.4439 0.609 ]
L1 regularization loss: 2.99E+00, L2 regularization loss: 1.97E+00
Test scatter: [0.3191 0.0871 0.4398 0.5974], Lowest was [0.1119 0.0383 0.2063 0.3756]
Median for last 10 epochs: [0.3191 0.0871 0.4398 0.5974], Epochs since improvement 8
 57%|█████▋    | 285/500 [5:23:04<3:54:15, 65.37s/it] 57%|█████▋    | 286/500 [5:24:26<4:11:17, 70.45s/it]Epoch: 286 done with learning rate 5.06E-03, Train loss: -2.26E+05, Train scatter: [0.2485 0.0742 0.3985 0.5471]
L1 regularization loss: 3.01E+00, L2 regularization loss: 2.00E+00
Test scatter: [0.2473 0.0729 0.3958 0.5364], Lowest was [0.1119 0.0383 0.2063 0.3756]
Median for last 10 epochs: [0.3191 0.0871 0.4398 0.5974], Epochs since improvement 10
 57%|█████▋    | 287/500 [5:25:20<3:52:29, 65.49s/it] 58%|█████▊    | 288/500 [5:26:43<4:09:27, 70.60s/it]Epoch: 288 done with learning rate 4.98E-03, Train loss: -2.45E+05, Train scatter: [0.3056 0.0795 0.3822 0.5348]
L1 regularization loss: 3.02E+00, L2 regularization loss: 2.01E+00
Test scatter: [0.3001 0.0784 0.3784 0.5243], Lowest was [0.1119 0.0383 0.2063 0.3756]
Median for last 10 epochs: [0.3191 0.0871 0.4398 0.5974], Epochs since improvement 12
 58%|█████▊    | 289/500 [5:27:37<3:50:33, 65.56s/it]Epoch: 290 done with learning rate 4.91E-03, Train loss: -1.49E+05, Train scatter: [0.279  0.0836 0.4631 0.5707]
L1 regularization loss: 3.06E+00, L2 regularization loss: 2.06E+00
Test scatter: [0.2795 0.0821 0.4555 0.5632], Lowest was [0.1119 0.0383 0.2063 0.3756]
Median for last 10 epochs: [0.3001 0.0821 0.4398 0.5632], Epochs since improvement 14
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 58%|█████▊    | 290/500 [5:29:04<4:12:43, 72.20s/it] 58%|█████▊    | 291/500 [5:29:58<3:52:16, 66.68s/it] 58%|█████▊    | 292/500 [5:31:19<4:05:55, 70.94s/it]Epoch: 292 done with learning rate 4.83E-03, Train loss: -2.37E+05, Train scatter: [0.266  0.0802 0.4125 0.5599]
L1 regularization loss: 3.06E+00, L2 regularization loss: 2.06E+00
Test scatter: [0.2619 0.0788 0.4121 0.5494], Lowest was [0.1119 0.0383 0.2063 0.3756]
Median for last 10 epochs: [0.2795 0.0788 0.4121 0.5494], Epochs since improvement 16
 59%|█████▊    | 293/500 [5:32:13<3:46:53, 65.77s/it] 59%|█████▉    | 294/500 [5:33:35<4:02:19, 70.58s/it]Epoch: 294 done with learning rate 4.76E-03, Train loss: -2.69E+05, Train scatter: [0.2075 0.0644 0.3407 0.5099]
L1 regularization loss: 3.06E+00, L2 regularization loss: 2.08E+00
Test scatter: [0.2052 0.0635 0.3405 0.5009], Lowest was [0.1119 0.0383 0.2063 0.3756]
Median for last 10 epochs: [0.2619 0.0784 0.3958 0.5364], Epochs since improvement 18
 59%|█████▉    | 295/500 [5:34:28<3:43:47, 65.50s/it] 59%|█████▉    | 296/500 [5:35:49<3:58:20, 70.10s/it]Epoch: 296 done with learning rate 4.69E-03, Train loss: 2.18E+06, Train scatter: [0.9347 0.1728 0.5442 0.996 ]
L1 regularization loss: 3.50E+00, L2 regularization loss: 2.33E+00
Test scatter: [0.9194 0.169  0.5357 0.9863], Lowest was [0.1119 0.0383 0.2063 0.3756]
Median for last 10 epochs: [0.2795 0.0788 0.4121 0.5494], Epochs since improvement 20
 59%|█████▉    | 297/500 [5:36:43<3:40:27, 65.16s/it] 59%|█████▉    | 297/500 [5:38:05<3:51:04, 68.30s/it]
Epoch: 298 done with learning rate 4.61E-03, Train loss: -2.43E+04, Train scatter: [0.9343 0.1728 0.5442 0.9951]
L1 regularization loss: 3.50E+00, L2 regularization loss: 2.33E+00
Test scatter: [0.919  0.169  0.5356 0.9854], Lowest was [0.1119 0.0383 0.2063 0.3756]
Median for last 10 epochs: [0.2795 0.0821 0.4555 0.5632], Epochs since improvement 22
Exited after 298 epochs due to early stopping
20285.11 seconds spent training, 40.570 seconds per epoch. Processed 1716 trees per second
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
[0.91897404 0.16896881 0.53559583 0.98534566]
{'epoch_exit': 297, 'scatter_m_star': 0.91897404, 'lowest_m_star': 0.11193289, 'last20_m_star': 0.30963027, 'last10_m_star': 0.27945468, 'scatter_v_disk': 0.16896881, 'lowest_v_disk': 0.038303263, 'last20_v_disk': 0.08458693, 'last10_v_disk': 0.082082875, 'scatter_m_cold': 0.53559583, 'lowest_m_cold': 0.20632933, 'last20_m_cold': 0.44762754, 'last10_m_cold': 0.45547166, 'scatter_sfr_100': 0.98534566, 'lowest_sfr_100': 0.37555584, 'last20_sfr_100': 0.5802972, 'last10_sfr_100': 0.56318176}
Finished 1/1
Exited training after None epochs
Experiment 6 done: 6 / 6
