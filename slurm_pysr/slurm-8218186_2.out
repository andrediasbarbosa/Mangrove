Starting process with 6 experiments
{'conv_layers': [1, 2, 3], 'batch_size': [256, 512]}
Exploring ['conv_layers', 'batch_size']
Currently doing [  1 256]
{'experiment': 'GraphMerge', 'group': 'meta_pysr', 'move': False, 'model': 'MetaEdge', 'log': True, 'run_params': {'n_epochs': 500, 'n_trials': 1, 'batch_size': 256, 'val_epoch': 2, 'early_stopping': True, 'patience': 20, 'l1_lambda': 3e-05, 'l2_lambda': 3e-05, 'loss_func': 'GaussNd', 'metrics': 'test_multi_varrho', 'performance_plot': 'multi_base', 'save': True, 'seed': True, 'num_workers': 4}, 'learn_params': {'learning_rate': 0.01, 'schedule': 'onecycle', 'g_up': 1, 'g_down': 0.95, 'warmup': 4, 'period': 5, 'eta_min': 1e-05}, 'hyper_params': {'hidden_channels': 128, 'conv_layers': 1, 'conv_activation': 'relu', 'decode_activation': 'leakyrelu', 'decode_layers': 2, 'layernorm': True, 'agg': 'sum', 'variance': True, 'rho': 0}, 'data_params': {'case': 'vlarge_all_4t_z0.0_quantile_raw', 'targets': [0, 1, 2, 3], 'del_feats': [], 'split': 0.8, 'test': 0, 'scale': 0}, 'experiment_name': 'con1bat256'}
/home/cj1223/../../scratch/gpfs/cj1223/GraphResults/results_meta_pysr_120322/MetaEdge_vlarge_all_4t_z0.0_quantile_raw_acxymy
RelU conv activation
LeakyRelU decode activation
N_params 298056
Made folder for saving model
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:31<4:19:20, 31.18s/it]  0%|          | 2/500 [01:18<5:39:25, 40.90s/it]Epoch: 2 done with learning rate 4.17E-04, Train loss: 1.27E+07, Train scatter: [0.9352 0.1645 0.5441 0.9954]
L1 regularization loss: 4.56E-01, L2 regularization loss: 9.93E-02
Test scatter: [0.9196 0.1644 0.5355 0.9851], Lowest was [0.9196 0.1644 0.5355 0.9851]
Median for last 10 epochs: [0.9196 0.1644 0.5355 0.9851], Epochs since improvement 2
  1%|          | 3/500 [01:49<5:01:27, 36.39s/it]  1%|          | 4/500 [02:38<5:40:15, 41.16s/it]Epoch: 4 done with learning rate 4.67E-04, Train loss: 6.58E+06, Train scatter: [0.9352 0.1472 0.5439 0.9954]
L1 regularization loss: 4.62E-01, L2 regularization loss: 1.06E-01
Test scatter: [0.9196 0.1437 0.5353 0.985 ], Lowest was [0.9196 0.1437 0.5353 0.985 ]
Median for last 10 epochs: [0.9196 0.1437 0.5353 0.985 ], Epochs since improvement 0
  1%|          | 5/500 [03:09<5:09:45, 37.55s/it]  1%|          | 6/500 [03:57<5:39:34, 41.24s/it]Epoch: 6 done with learning rate 5.51E-04, Train loss: 5.15E+06, Train scatter: [0.9341 0.1223 0.5415 0.6833]
L1 regularization loss: 4.73E-01, L2 regularization loss: 1.17E-01
Test scatter: [0.9183 0.1223 0.5328 0.6683], Lowest was [0.9183 0.1223 0.5328 0.6683]
Median for last 10 epochs: [0.9183 0.1223 0.5328 0.6683], Epochs since improvement 0
  1%|▏         | 7/500 [04:29<5:11:39, 37.93s/it]  2%|▏         | 8/500 [05:17<5:38:36, 41.29s/it]Epoch: 8 done with learning rate 6.67E-04, Train loss: 4.28E+06, Train scatter: [0.9141 0.1047 0.5331 0.6159]
L1 regularization loss: 4.79E-01, L2 regularization loss: 1.21E-01
Test scatter: [0.8983 0.1056 0.5241 0.6057], Lowest was [0.8983 0.1056 0.5241 0.6057]
Median for last 10 epochs: [0.9083 0.1139 0.5284 0.637 ], Epochs since improvement 0
  2%|▏         | 9/500 [05:48<5:12:11, 38.15s/it]Epoch: 10 done with learning rate 8.15E-04, Train loss: 3.51E+06, Train scatter: [0.7817 0.0983 0.4349 0.6083]
L1 regularization loss: 4.85E-01, L2 regularization loss: 1.24E-01
Test scatter: [0.7692 0.1026 0.427  0.5991], Lowest was [0.7692 0.1026 0.427  0.5991]
Median for last 10 epochs: [0.8983 0.1056 0.5241 0.6057], Epochs since improvement 0
  2%|▏         | 10/500 [06:42<5:51:01, 42.98s/it]  2%|▏         | 11/500 [07:13<5:20:46, 39.36s/it]  2%|▏         | 12/500 [08:02<5:42:35, 42.12s/it]Epoch: 12 done with learning rate 9.94E-04, Train loss: 2.50E+06, Train scatter: [0.581  0.0944 0.4329 0.6013]
L1 regularization loss: 4.90E-01, L2 regularization loss: 1.27E-01
Test scatter: [0.5889 0.0993 0.4315 0.6085], Lowest was [0.5889 0.0993 0.427  0.5991]
Median for last 10 epochs: [0.8983 0.1056 0.5241 0.6085], Epochs since improvement 0
  3%|▎         | 13/500 [08:33<5:14:45, 38.78s/it]  3%|▎         | 14/500 [09:21<5:38:02, 41.73s/it]Epoch: 14 done with learning rate 1.20E-03, Train loss: 1.68E+06, Train scatter: [0.5413 0.0908 0.3952 0.5968]
L1 regularization loss: 4.95E-01, L2 regularization loss: 1.30E-01
Test scatter: [0.5543 0.093  0.3943 0.6005], Lowest was [0.5543 0.093  0.3943 0.5991]
Median for last 10 epochs: [0.7692 0.1026 0.4315 0.6057], Epochs since improvement 0
  3%|▎         | 15/500 [09:53<5:11:38, 38.55s/it]  3%|▎         | 16/500 [10:41<5:35:04, 41.54s/it]Epoch: 16 done with learning rate 1.44E-03, Train loss: 1.31E+06, Train scatter: [0.6009 0.0888 0.3521 0.6013]
L1 regularization loss: 5.00E-01, L2 regularization loss: 1.32E-01
Test scatter: [0.5988 0.091  0.3599 0.6134], Lowest was [0.5543 0.091  0.3599 0.5991]
Median for last 10 epochs: [0.5988 0.0993 0.427  0.6057], Epochs since improvement 0
  3%|▎         | 17/500 [11:12<5:09:16, 38.42s/it]  4%|▎         | 18/500 [12:00<5:32:10, 41.35s/it]Epoch: 18 done with learning rate 1.70E-03, Train loss: 1.01E+06, Train scatter: [0.5168 0.086  0.3359 0.5894]
L1 regularization loss: 5.07E-01, L2 regularization loss: 1.35E-01
Test scatter: [0.5253 0.0891 0.3483 0.5976], Lowest was [0.5253 0.0891 0.3483 0.5976]
Median for last 10 epochs: [0.5889 0.093  0.3943 0.6005], Epochs since improvement 0
  4%|▍         | 19/500 [12:31<5:06:52, 38.28s/it]Epoch: 20 done with learning rate 1.99E-03, Train loss: 9.06E+05, Train scatter: [0.5235 0.0826 0.318  0.5649]
L1 regularization loss: 5.15E-01, L2 regularization loss: 1.38E-01
Test scatter: [0.5043 0.0841 0.3248 0.5615], Lowest was [0.5043 0.0841 0.3248 0.5615]
Median for last 10 epochs: [0.5543 0.091  0.3599 0.6005], Epochs since improvement 0
  4%|▍         | 20/500 [13:25<5:42:53, 42.86s/it]  4%|▍         | 21/500 [13:56<5:14:14, 39.36s/it]  4%|▍         | 22/500 [14:45<5:36:10, 42.20s/it]Epoch: 22 done with learning rate 2.30E-03, Train loss: 7.49E+05, Train scatter: [0.4788 0.0805 0.3075 0.5578]
L1 regularization loss: 5.23E-01, L2 regularization loss: 1.41E-01
Test scatter: [0.4682 0.0827 0.3174 0.5566], Lowest was [0.4682 0.0827 0.3174 0.5566]
Median for last 10 epochs: [0.5253 0.0891 0.3483 0.5976], Epochs since improvement 0
  5%|▍         | 23/500 [15:16<5:08:53, 38.85s/it]  5%|▍         | 24/500 [16:05<5:31:25, 41.78s/it]Epoch: 24 done with learning rate 2.63E-03, Train loss: 6.52E+05, Train scatter: [0.4548 0.08   0.3139 0.5684]
L1 regularization loss: 5.34E-01, L2 regularization loss: 1.46E-01
Test scatter: [0.4475 0.0805 0.3227 0.5603], Lowest was [0.4475 0.0805 0.3174 0.5566]
Median for last 10 epochs: [0.5043 0.0841 0.3248 0.5615], Epochs since improvement 0
  5%|▌         | 25/500 [16:36<5:05:42, 38.62s/it]  5%|▌         | 26/500 [17:24<5:28:07, 41.54s/it]Epoch: 26 done with learning rate 2.98E-03, Train loss: 1.16E+06, Train scatter: [0.5622 0.0778 0.3804 0.5444]
L1 regularization loss: 5.49E-01, L2 regularization loss: 1.52E-01
Test scatter: [0.6021 0.0788 0.3839 0.5441], Lowest was [0.4475 0.0788 0.3174 0.5441]
Median for last 10 epochs: [0.5043 0.0827 0.3248 0.5603], Epochs since improvement 0
  5%|▌         | 27/500 [17:55<5:02:55, 38.43s/it]  6%|▌         | 28/500 [18:44<5:26:20, 41.48s/it]Epoch: 28 done with learning rate 3.34E-03, Train loss: 5.30E+05, Train scatter: [0.4464 0.076  0.2844 0.5422]
L1 regularization loss: 5.63E-01, L2 regularization loss: 1.59E-01
Test scatter: [0.4421 0.0768 0.2905 0.5337], Lowest was [0.4421 0.0768 0.2905 0.5337]
Median for last 10 epochs: [0.4682 0.0805 0.3227 0.5566], Epochs since improvement 0
  6%|▌         | 29/500 [19:15<5:01:12, 38.37s/it]Epoch: 30 done with learning rate 3.72E-03, Train loss: 4.60E+05, Train scatter: [0.4498 0.0764 0.2905 0.5271]
L1 regularization loss: 5.78E-01, L2 regularization loss: 1.67E-01
Test scatter: [0.4485 0.079  0.3086 0.5345], Lowest was [0.4421 0.0768 0.2905 0.5337]
Median for last 10 epochs: [0.4485 0.079  0.3174 0.5441], Epochs since improvement 2
  6%|▌         | 30/500 [20:09<5:35:55, 42.88s/it]  6%|▌         | 31/500 [20:40<5:07:39, 39.36s/it]  6%|▋         | 32/500 [21:29<5:29:11, 42.20s/it]Epoch: 32 done with learning rate 4.10E-03, Train loss: 7.04E+06, Train scatter: [0.9339 0.1722 0.544  0.9945]
L1 regularization loss: 7.23E-01, L2 regularization loss: 2.22E-01
Test scatter: [0.9185 0.1683 0.5354 0.9843], Lowest was [0.4421 0.0768 0.2905 0.5337]
Median for last 10 epochs: [0.4485 0.079  0.3227 0.5441], Epochs since improvement 4
  7%|▋         | 33/500 [22:00<5:02:53, 38.92s/it]  7%|▋         | 34/500 [22:49<5:25:08, 41.86s/it]Epoch: 34 done with learning rate 4.50E-03, Train loss: 3.06E+06, Train scatter: [0.937  0.1697 0.5439 0.9944]
L1 regularization loss: 7.63E-01, L2 regularization loss: 2.54E-01
Test scatter: [0.921  0.1659 0.5353 0.9842], Lowest was [0.4421 0.0768 0.2905 0.5337]
Median for last 10 epochs: [0.6021 0.079  0.3839 0.5441], Epochs since improvement 6
  7%|▋         | 35/500 [23:20<4:59:35, 38.66s/it]  7%|▋         | 36/500 [24:08<5:22:15, 41.67s/it]Epoch: 36 done with learning rate 4.90E-03, Train loss: 2.33E+06, Train scatter: [0.9278 0.166  0.5439 0.9941]
L1 regularization loss: 7.68E-01, L2 regularization loss: 2.70E-01
Test scatter: [0.9123 0.1625 0.5353 0.9839], Lowest was [0.4421 0.0768 0.2905 0.5337]
Median for last 10 epochs: [0.9123 0.1625 0.5353 0.9839], Epochs since improvement 8
  7%|▋         | 37/500 [24:40<4:57:51, 38.60s/it]  8%|▊         | 38/500 [25:28<5:20:20, 41.60s/it]Epoch: 38 done with learning rate 5.30E-03, Train loss: 1.97E+06, Train scatter: [0.8502 0.1556 0.544  0.9928]
L1 regularization loss: 7.71E-01, L2 regularization loss: 2.78E-01
Test scatter: [0.8432 0.1531 0.5354 0.9827], Lowest was [0.4421 0.0768 0.2905 0.5337]
Median for last 10 epochs: [0.9123 0.1625 0.5353 0.9839], Epochs since improvement 10
  8%|▊         | 39/500 [26:00<4:55:37, 38.48s/it]Epoch: 40 done with learning rate 5.70E-03, Train loss: 1.55E+06, Train scatter: [0.6502 0.138  0.544  0.9916]
L1 regularization loss: 7.83E-01, L2 regularization loss: 3.03E-01
Test scatter: [0.6712 0.137  0.5354 0.9815], Lowest was [0.4421 0.0768 0.2905 0.5337]
Median for last 10 epochs: [0.9123 0.1625 0.5354 0.9839], Epochs since improvement 12
  8%|▊         | 40/500 [26:54<5:31:36, 43.25s/it]  8%|▊         | 41/500 [27:25<5:03:14, 39.64s/it]  8%|▊         | 42/500 [28:14<5:23:49, 42.42s/it]Epoch: 42 done with learning rate 6.10E-03, Train loss: 2.73E+06, Train scatter: [0.9286 0.1523 0.5441 0.98  ]
L1 regularization loss: 8.36E-01, L2 regularization loss: 3.51E-01
Test scatter: [0.9132 0.1498 0.5355 0.9708], Lowest was [0.4421 0.0768 0.2905 0.5337]
Median for last 10 epochs: [0.9123 0.1531 0.5354 0.9827], Epochs since improvement 14
  9%|▊         | 43/500 [28:45<4:57:35, 39.07s/it]  9%|▉         | 44/500 [29:33<5:17:15, 41.74s/it]Epoch: 44 done with learning rate 6.49E-03, Train loss: 2.07E+06, Train scatter: [0.9271 0.1364 0.5442 0.8379]
L1 regularization loss: 8.40E-01, L2 regularization loss: 3.69E-01
Test scatter: [0.9118 0.1371 0.5356 0.845 ], Lowest was [0.4421 0.0768 0.2905 0.5337]
Median for last 10 epochs: [0.9118 0.1498 0.5354 0.9815], Epochs since improvement 16
  9%|▉         | 45/500 [30:05<4:52:49, 38.62s/it]  9%|▉         | 46/500 [30:54<5:16:41, 41.85s/it]Epoch: 46 done with learning rate 6.87E-03, Train loss: 1.60E+06, Train scatter: [0.9248 0.1246 0.5441 0.7507]
L1 regularization loss: 8.43E-01, L2 regularization loss: 3.82E-01
Test scatter: [0.9096 0.1271 0.5355 0.7599], Lowest was [0.4421 0.0768 0.2905 0.5337]
Median for last 10 epochs: [0.9096 0.1371 0.5355 0.9708], Epochs since improvement 18
  9%|▉         | 47/500 [31:25<4:51:57, 38.67s/it] 10%|▉         | 48/500 [32:14<5:14:54, 41.80s/it]Epoch: 48 done with learning rate 7.24E-03, Train loss: 1.39E+06, Train scatter: [0.92   0.1205 0.5439 0.7183]
L1 regularization loss: 8.46E-01, L2 regularization loss: 3.91E-01
Test scatter: [0.9051 0.1217 0.5353 0.7298], Lowest was [0.4421 0.0768 0.2905 0.5337]
Median for last 10 epochs: [0.9096 0.137  0.5355 0.845 ], Epochs since improvement 20
 10%|▉         | 49/500 [32:46<4:50:22, 38.63s/it]Epoch: 50 done with learning rate 7.60E-03, Train loss: 1.26E+06, Train scatter: [0.9094 0.1173 0.5434 0.6924]
L1 regularization loss: 8.48E-01, L2 regularization loss: 3.99E-01
Test scatter: [0.8951 0.1185 0.5348 0.7047], Lowest was [0.4421 0.0768 0.2905 0.5337]
Median for last 10 epochs: [0.9096 0.1271 0.5355 0.7599], Epochs since improvement 22
 10%|▉         | 49/500 [33:39<5:09:51, 41.22s/it]
Exited after 50 epochs due to early stopping
2020.12 seconds spent training, 4.040 seconds per epoch. Processed 17236 trees per second
[0.8950606  0.11847831 0.5348172  0.7046575 ]
{'epoch_exit': 49, 'scatter_m_star': 0.8950606, 'lowest_m_star': 0.4421162, 'last20_m_star': 0.9107092, 'last10_m_star': 0.90964943, 'scatter_v_disk': 0.118478306, 'lowest_v_disk': 0.0767567, 'last20_v_disk': 0.14342442, 'last10_v_disk': 0.12707596, 'scatter_m_cold': 0.5348172, 'lowest_m_cold': 0.29051834, 'last20_m_cold': 0.53539383, 'last10_m_cold': 0.5354834, 'scatter_sfr_100': 0.7046575, 'lowest_sfr_100': 0.53370297, 'last20_sfr_100': 0.97613984, 'last10_sfr_100': 0.7598846}
Finished 1/1
Exited training after None epochs
Experiment 1 done: 1 / 6
Exploring ['conv_layers', 'batch_size']
Currently doing [  1 512]
{'experiment': 'GraphMerge', 'group': 'meta_pysr', 'move': False, 'model': 'MetaEdge', 'log': True, 'run_params': {'n_epochs': 500, 'n_trials': 1, 'batch_size': 512, 'val_epoch': 2, 'early_stopping': True, 'patience': 20, 'l1_lambda': 3e-05, 'l2_lambda': 3e-05, 'loss_func': 'GaussNd', 'metrics': 'test_multi_varrho', 'performance_plot': 'multi_base', 'save': True, 'seed': True, 'num_workers': 4}, 'learn_params': {'learning_rate': 0.01, 'schedule': 'onecycle', 'g_up': 1, 'g_down': 0.95, 'warmup': 4, 'period': 5, 'eta_min': 1e-05}, 'hyper_params': {'hidden_channels': 128, 'conv_layers': 1, 'conv_activation': 'relu', 'decode_activation': 'leakyrelu', 'decode_layers': 2, 'layernorm': True, 'agg': 'sum', 'variance': True, 'rho': 0, 'in_channels': 43, 'out_channels': 4}, 'data_params': {'case': 'vlarge_all_4t_z0.0_quantile_raw', 'targets': [0, 1, 2, 3], 'del_feats': [], 'split': 0.8, 'test': 0, 'scale': 0}, 'experiment_name': 'con1bat512'}
/home/cj1223/../../scratch/gpfs/cj1223/GraphResults/results_meta_pysr_120322/MetaEdge_vlarge_all_4t_z0.0_quantile_raw_sjenws
RelU conv activation
LeakyRelU decode activation
N_params 298056
Made folder for saving model
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:27<3:51:25, 27.83s/it]  0%|          | 2/500 [01:11<5:08:33, 37.18s/it]Epoch: 2 done with learning rate 4.17E-04, Train loss: 2.37E+07, Train scatter: [0.9353 0.1636 0.5442 0.9954]
L1 regularization loss: 4.54E-01, L2 regularization loss: 9.77E-02
Test scatter: [0.9197 0.1658 0.5356 0.9851], Lowest was [0.9197 0.1658 0.5356 0.9851]
Median for last 10 epochs: [0.9197 0.1658 0.5356 0.9851], Epochs since improvement 2
  1%|          | 3/500 [01:38<4:30:34, 32.66s/it]  1%|          | 4/500 [02:23<5:08:45, 37.35s/it]Epoch: 4 done with learning rate 4.67E-04, Train loss: 1.23E+07, Train scatter: [0.9353 0.1737 0.5441 0.9954]
L1 regularization loss: 4.57E-01, L2 regularization loss: 9.95E-02
Test scatter: [0.9197 0.1743 0.5355 0.9851], Lowest was [0.9197 0.1658 0.5355 0.9851]
Median for last 10 epochs: [0.9197 0.17   0.5355 0.9851], Epochs since improvement 0
  1%|          | 5/500 [02:50<4:38:00, 33.70s/it]  1%|          | 6/500 [03:36<5:10:12, 37.68s/it]Epoch: 6 done with learning rate 5.51E-04, Train loss: 9.87E+06, Train scatter: [0.9352 0.1736 0.5441 0.9954]
L1 regularization loss: 4.61E-01, L2 regularization loss: 1.03E-01
Test scatter: [0.9196 0.1698 0.5356 0.9851], Lowest was [0.9196 0.1658 0.5355 0.9851]
Median for last 10 epochs: [0.9196 0.1698 0.5355 0.9851], Epochs since improvement 0
  1%|▏         | 7/500 [04:03<4:41:48, 34.30s/it]  2%|▏         | 8/500 [04:48<5:10:46, 37.90s/it]Epoch: 8 done with learning rate 6.67E-04, Train loss: 8.67E+06, Train scatter: [0.9352 0.152  0.5441 0.9952]
L1 regularization loss: 4.66E-01, L2 regularization loss: 1.09E-01
Test scatter: [0.9196 0.1472 0.5355 0.9848], Lowest was [0.9196 0.1472 0.5355 0.9848]
Median for last 10 epochs: [0.9196 0.1585 0.5355 0.9849], Epochs since improvement 0
  2%|▏         | 9/500 [05:16<4:43:24, 34.63s/it]Epoch: 10 done with learning rate 8.15E-04, Train loss: 8.10E+06, Train scatter: [0.9351 0.1368 0.5441 0.7151]
L1 regularization loss: 4.76E-01, L2 regularization loss: 1.18E-01
Test scatter: [0.9195 0.1344 0.5355 0.7225], Lowest was [0.9195 0.1344 0.5355 0.7225]
Median for last 10 epochs: [0.9196 0.1472 0.5355 0.9848], Epochs since improvement 0
  2%|▏         | 10/500 [06:07<5:23:13, 39.58s/it]  2%|▏         | 11/500 [06:34<4:52:01, 35.83s/it]  2%|▏         | 12/500 [07:19<5:14:53, 38.72s/it]Epoch: 12 done with learning rate 9.94E-04, Train loss: 7.65E+06, Train scatter: [0.9333 0.1268 0.544  0.6965]
L1 regularization loss: 4.80E-01, L2 regularization loss: 1.21E-01
Test scatter: [0.9177 0.1229 0.5354 0.6837], Lowest was [0.9177 0.1229 0.5354 0.6837]
Median for last 10 epochs: [0.9196 0.1472 0.5355 0.9848], Epochs since improvement 0
  3%|▎         | 13/500 [07:47<4:46:20, 35.28s/it]  3%|▎         | 14/500 [08:32<5:11:01, 38.40s/it]Epoch: 14 done with learning rate 1.20E-03, Train loss: 6.73E+06, Train scatter: [0.9036 0.1206 0.543  0.6186]
L1 regularization loss: 4.84E-01, L2 regularization loss: 1.24E-01
Test scatter: [0.8886 0.1179 0.5344 0.6115], Lowest was [0.8886 0.1179 0.5344 0.6115]
Median for last 10 epochs: [0.9195 0.1344 0.5355 0.7225], Epochs since improvement 0
  3%|▎         | 15/500 [08:59<4:43:03, 35.02s/it]  3%|▎         | 16/500 [09:45<5:08:17, 38.22s/it]Epoch: 16 done with learning rate 1.44E-03, Train loss: 6.26E+06, Train scatter: [0.9317 0.1124 0.5411 0.6129]
L1 regularization loss: 4.92E-01, L2 regularization loss: 1.28E-01
Test scatter: [0.9159 0.1108 0.5325 0.6146], Lowest was [0.8886 0.1108 0.5325 0.6115]
Median for last 10 epochs: [0.9177 0.1229 0.5354 0.6837], Epochs since improvement 0
  3%|▎         | 17/500 [10:12<4:41:06, 34.92s/it]  4%|▎         | 18/500 [10:58<5:07:38, 38.29s/it]Epoch: 18 done with learning rate 1.70E-03, Train loss: 5.77E+06, Train scatter: [0.8229 0.1044 0.5382 0.5869]
L1 regularization loss: 4.97E-01, L2 regularization loss: 1.32E-01
Test scatter: [0.8122 0.1032 0.5295 0.5782], Lowest was [0.8122 0.1032 0.5295 0.5782]
Median for last 10 epochs: [0.9159 0.1179 0.5344 0.6146], Epochs since improvement 0
  4%|▍         | 19/500 [11:26<4:40:55, 35.04s/it]Epoch: 20 done with learning rate 1.99E-03, Train loss: 5.43E+06, Train scatter: [0.4995 0.1001 0.5337 0.5697]
L1 regularization loss: 5.03E-01, L2 regularization loss: 1.36E-01
Test scatter: [0.501  0.1003 0.5251 0.568 ], Lowest was [0.501  0.1003 0.5251 0.568 ]
Median for last 10 epochs: [0.8886 0.1108 0.5325 0.6115], Epochs since improvement 0
  4%|▍         | 20/500 [12:16<5:17:20, 39.67s/it]  4%|▍         | 21/500 [12:44<4:47:09, 35.97s/it]  4%|▍         | 22/500 [13:29<5:08:16, 38.70s/it]Epoch: 22 done with learning rate 2.30E-03, Train loss: 5.13E+06, Train scatter: [0.5037 0.0974 0.5274 0.5668]
L1 regularization loss: 5.07E-01, L2 regularization loss: 1.40E-01
Test scatter: [0.5047 0.0978 0.5195 0.5636], Lowest was [0.501  0.0978 0.5195 0.5636]
Median for last 10 epochs: [0.8122 0.1032 0.5295 0.5782], Epochs since improvement 0
  5%|▍         | 23/500 [13:56<4:40:34, 35.29s/it]  5%|▍         | 24/500 [14:41<5:03:40, 38.28s/it]Epoch: 24 done with learning rate 2.63E-03, Train loss: 4.64E+06, Train scatter: [0.4318 0.0945 0.5038 0.5834]
L1 regularization loss: 5.11E-01, L2 regularization loss: 1.44E-01
Test scatter: [0.4388 0.0955 0.4954 0.5841], Lowest was [0.4388 0.0955 0.4954 0.5636]
Median for last 10 epochs: [0.5047 0.1003 0.5251 0.5782], Epochs since improvement 0
  5%|▌         | 25/500 [15:09<4:36:50, 34.97s/it]  5%|▌         | 26/500 [15:54<5:00:45, 38.07s/it]Epoch: 26 done with learning rate 2.98E-03, Train loss: 3.48E+06, Train scatter: [0.5229 0.1009 0.4358 0.6129]
L1 regularization loss: 5.18E-01, L2 regularization loss: 1.49E-01
Test scatter: [0.5315 0.104  0.451  0.6163], Lowest was [0.4388 0.0955 0.451  0.5636]
Median for last 10 epochs: [0.5047 0.1003 0.5195 0.5782], Epochs since improvement 0
  5%|▌         | 27/500 [16:21<4:34:36, 34.84s/it]  6%|▌         | 28/500 [17:07<5:00:40, 38.22s/it]Epoch: 28 done with learning rate 3.34E-03, Train loss: 3.48E+06, Train scatter: [0.5276 0.1042 0.4448 0.6905]
L1 regularization loss: 5.24E-01, L2 regularization loss: 1.52E-01
Test scatter: [0.5169 0.1015 0.4291 0.6677], Lowest was [0.4388 0.0955 0.4291 0.5636]
Median for last 10 epochs: [0.5047 0.1003 0.4954 0.5841], Epochs since improvement 0
  6%|▌         | 29/500 [17:35<4:34:11, 34.93s/it]Epoch: 30 done with learning rate 3.72E-03, Train loss: 2.12E+06, Train scatter: [0.4954 0.0967 0.3599 0.5773]
L1 regularization loss: 5.30E-01, L2 regularization loss: 1.56E-01
Test scatter: [0.4981 0.0957 0.3619 0.5799], Lowest was [0.4388 0.0955 0.3619 0.5636]
Median for last 10 epochs: [0.5047 0.0978 0.451  0.5841], Epochs since improvement 0
  6%|▌         | 30/500 [18:27<5:14:05, 40.10s/it]  6%|▌         | 31/500 [18:54<4:44:19, 36.37s/it]  6%|▋         | 32/500 [19:40<5:05:42, 39.19s/it]Epoch: 32 done with learning rate 4.10E-03, Train loss: 1.86E+06, Train scatter: [0.3769 0.0895 0.3385 0.5609]
L1 regularization loss: 5.35E-01, L2 regularization loss: 1.59E-01
Test scatter: [0.4129 0.0897 0.3438 0.5682], Lowest was [0.4129 0.0897 0.3438 0.5636]
Median for last 10 epochs: [0.4981 0.0957 0.4291 0.5841], Epochs since improvement 0
  7%|▋         | 33/500 [20:08<4:37:42, 35.68s/it]  7%|▋         | 34/500 [20:53<5:00:25, 38.68s/it]Epoch: 34 done with learning rate 4.50E-03, Train loss: 1.70E+06, Train scatter: [0.3633 0.0895 0.3436 0.5853]
L1 regularization loss: 5.41E-01, L2 regularization loss: 1.62E-01
Test scatter: [0.3735 0.0881 0.3456 0.5832], Lowest was [0.3735 0.0881 0.3438 0.5636]
Median for last 10 epochs: [0.4981 0.0957 0.3619 0.5832], Epochs since improvement 0
  7%|▋         | 35/500 [21:21<4:33:25, 35.28s/it]  7%|▋         | 36/500 [22:06<4:57:08, 38.42s/it]Epoch: 36 done with learning rate 4.90E-03, Train loss: 1.54E+06, Train scatter: [0.3697 0.0861 0.331  0.5829]
L1 regularization loss: 5.49E-01, L2 regularization loss: 1.66E-01
Test scatter: [0.3826 0.0861 0.3367 0.5842], Lowest was [0.3735 0.0861 0.3367 0.5636]
Median for last 10 epochs: [0.4129 0.0897 0.3456 0.5832], Epochs since improvement 0
  7%|▋         | 37/500 [22:34<4:30:38, 35.07s/it]  8%|▊         | 38/500 [23:19<4:54:43, 38.28s/it]Epoch: 38 done with learning rate 5.30E-03, Train loss: 1.32E+06, Train scatter: [0.3687 0.084  0.3149 0.5384]
L1 regularization loss: 5.58E-01, L2 regularization loss: 1.71E-01
Test scatter: [0.3821 0.0842 0.3226 0.5414], Lowest was [0.3735 0.0842 0.3226 0.5414]
Median for last 10 epochs: [0.3826 0.0881 0.3438 0.5799], Epochs since improvement 0
  8%|▊         | 39/500 [23:47<4:29:02, 35.02s/it]Epoch: 40 done with learning rate 5.70E-03, Train loss: 1.43E+06, Train scatter: [0.3766 0.0903 0.326  0.5768]
L1 regularization loss: 5.70E-01, L2 regularization loss: 1.78E-01
Test scatter: [0.4129 0.0918 0.3305 0.5815], Lowest was [0.3735 0.0842 0.3226 0.5414]
Median for last 10 epochs: [0.3826 0.0881 0.3367 0.5815], Epochs since improvement 2
  8%|▊         | 40/500 [24:38<5:06:20, 39.96s/it]  8%|▊         | 41/500 [25:06<4:36:40, 36.17s/it]  8%|▊         | 42/500 [25:51<4:57:58, 39.04s/it]Epoch: 42 done with learning rate 6.10E-03, Train loss: 1.82E+06, Train scatter: [0.3239 0.0799 0.3281 0.535 ]
L1 regularization loss: 5.77E-01, L2 regularization loss: 1.82E-01
Test scatter: [0.3355 0.0789 0.3306 0.5389], Lowest was [0.3355 0.0789 0.3226 0.5389]
Median for last 10 epochs: [0.3821 0.0861 0.3306 0.5815], Epochs since improvement 0
  9%|▊         | 43/500 [26:19<4:30:33, 35.52s/it]  9%|▉         | 44/500 [27:05<4:53:28, 38.61s/it]Epoch: 44 done with learning rate 6.49E-03, Train loss: 1.31E+06, Train scatter: [0.3872 0.0808 0.4211 0.5581]
L1 regularization loss: 5.87E-01, L2 regularization loss: 1.87E-01
Test scatter: [0.3905 0.0789 0.4216 0.5591], Lowest was [0.3355 0.0789 0.3226 0.5389]
Median for last 10 epochs: [0.3826 0.0842 0.3306 0.5591], Epochs since improvement 2
  9%|▉         | 45/500 [27:32<4:27:12, 35.24s/it]  9%|▉         | 46/500 [28:17<4:49:38, 38.28s/it]Epoch: 46 done with learning rate 6.87E-03, Train loss: 1.17E+06, Train scatter: [0.4081 0.0804 0.3476 0.5961]
L1 regularization loss: 5.93E-01, L2 regularization loss: 1.92E-01
Test scatter: [0.4118 0.0799 0.35   0.5924], Lowest was [0.3355 0.0789 0.3226 0.5389]
Median for last 10 epochs: [0.3905 0.0799 0.3306 0.5591], Epochs since improvement 4
  9%|▉         | 47/500 [28:45<4:24:30, 35.03s/it] 10%|▉         | 48/500 [29:30<4:47:08, 38.12s/it]Epoch: 48 done with learning rate 7.24E-03, Train loss: 1.10E+06, Train scatter: [0.3148 0.0777 0.3026 0.5369]
L1 regularization loss: 6.00E-01, L2 regularization loss: 1.97E-01
Test scatter: [0.3364 0.0779 0.3075 0.539 ], Lowest was [0.3355 0.0779 0.3075 0.5389]
Median for last 10 epochs: [0.3905 0.0789 0.3306 0.5591], Epochs since improvement 0
 10%|▉         | 49/500 [29:57<4:22:23, 34.91s/it]Epoch: 50 done with learning rate 7.60E-03, Train loss: 1.11E+06, Train scatter: [0.328  0.0766 0.3006 0.5297]
L1 regularization loss: 6.09E-01, L2 regularization loss: 2.04E-01
Test scatter: [0.3426 0.0772 0.3093 0.5354], Lowest was [0.3355 0.0772 0.3075 0.5354]
Median for last 10 epochs: [0.3426 0.0789 0.3306 0.539 ], Epochs since improvement 0
 10%|█         | 50/500 [30:49<5:00:17, 40.04s/it] 10%|█         | 51/500 [31:17<4:31:12, 36.24s/it] 10%|█         | 52/500 [32:03<4:52:21, 39.16s/it]Epoch: 52 done with learning rate 7.94E-03, Train loss: 9.54E+05, Train scatter: [0.2785 0.0736 0.2794 0.5082]
L1 regularization loss: 6.15E-01, L2 regularization loss: 2.09E-01
Test scatter: [0.2873 0.0742 0.2883 0.5088], Lowest was [0.2873 0.0742 0.2883 0.5088]
Median for last 10 epochs: [0.3426 0.0779 0.3093 0.539 ], Epochs since improvement 0
 11%|█         | 53/500 [32:30<4:25:13, 35.60s/it] 11%|█         | 54/500 [33:17<4:49:33, 38.95s/it]Epoch: 54 done with learning rate 8.26E-03, Train loss: 9.66E+05, Train scatter: [0.4079 0.0741 0.3289 0.5371]
L1 regularization loss: 6.21E-01, L2 regularization loss: 2.15E-01
Test scatter: [0.4147 0.0737 0.3255 0.5328], Lowest was [0.2873 0.0737 0.2883 0.5088]
Median for last 10 epochs: [0.3426 0.0772 0.3093 0.5354], Epochs since improvement 0
 11%|█         | 55/500 [33:44<4:23:10, 35.49s/it] 11%|█         | 56/500 [34:31<4:47:14, 38.82s/it]Epoch: 56 done with learning rate 8.56E-03, Train loss: 1.17E+06, Train scatter: [0.3243 0.0798 0.3804 0.5855]
L1 regularization loss: 6.33E-01, L2 regularization loss: 2.22E-01
Test scatter: [0.3605 0.0811 0.3937 0.5931], Lowest was [0.2873 0.0737 0.2883 0.5088]
Median for last 10 epochs: [0.3426 0.0772 0.3093 0.5354], Epochs since improvement 2
 11%|█▏        | 57/500 [34:58<4:21:03, 35.36s/it] 12%|█▏        | 58/500 [35:45<4:45:41, 38.78s/it]Epoch: 58 done with learning rate 8.83E-03, Train loss: 2.11E+06, Train scatter: [0.2843 0.0738 0.4109 0.5123]
L1 regularization loss: 6.42E-01, L2 regularization loss: 2.29E-01
Test scatter: [0.2911 0.074  0.4061 0.5135], Lowest was [0.2873 0.0737 0.2883 0.5088]
Median for last 10 epochs: [0.3426 0.0742 0.3255 0.5328], Epochs since improvement 4
 12%|█▏        | 59/500 [36:12<4:19:48, 35.35s/it]Epoch: 60 done with learning rate 9.08E-03, Train loss: 2.30E+06, Train scatter: [0.3188 0.0761 0.4323 0.5308]
L1 regularization loss: 6.60E-01, L2 regularization loss: 2.40E-01
Test scatter: [0.3335 0.0758 0.4289 0.5345], Lowest was [0.2873 0.0737 0.2883 0.5088]
Median for last 10 epochs: [0.3335 0.0742 0.3937 0.5328], Epochs since improvement 6
 12%|█▏        | 60/500 [37:04<4:55:35, 40.31s/it] 12%|█▏        | 61/500 [37:31<4:26:29, 36.42s/it] 12%|█▏        | 62/500 [38:18<4:47:06, 39.33s/it]Epoch: 62 done with learning rate 9.31E-03, Train loss: 1.32E+06, Train scatter: [0.2955 0.0721 0.3238 0.5132]
L1 regularization loss: 6.62E-01, L2 regularization loss: 2.44E-01
Test scatter: [0.3103 0.0721 0.3295 0.5154], Lowest was [0.2873 0.0721 0.2883 0.5088]
Median for last 10 epochs: [0.3335 0.074  0.3937 0.5328], Epochs since improvement 0
 13%|█▎        | 63/500 [38:45<4:20:13, 35.73s/it] 13%|█▎        | 64/500 [39:32<4:43:39, 39.03s/it]Epoch: 64 done with learning rate 9.50E-03, Train loss: 1.89E+06, Train scatter: [0.3276 0.0744 0.3396 0.5468]
L1 regularization loss: 6.66E-01, L2 regularization loss: 2.50E-01
Test scatter: [0.3303 0.0742 0.3379 0.5452], Lowest was [0.2873 0.0721 0.2883 0.5088]
Median for last 10 epochs: [0.3303 0.0742 0.3937 0.5345], Epochs since improvement 2
 13%|█▎        | 65/500 [39:59<4:17:32, 35.52s/it] 13%|█▎        | 66/500 [40:45<4:40:09, 38.73s/it]Epoch: 66 done with learning rate 9.66E-03, Train loss: 9.80E+05, Train scatter: [0.2927 0.0703 0.3244 0.5225]
L1 regularization loss: 6.72E-01, L2 regularization loss: 2.56E-01
Test scatter: [0.2984 0.0705 0.3266 0.5224], Lowest was [0.2873 0.0705 0.2883 0.5088]
Median for last 10 epochs: [0.3103 0.074  0.3379 0.5224], Epochs since improvement 0
 13%|█▎        | 67/500 [41:13<4:15:02, 35.34s/it] 14%|█▎        | 68/500 [41:59<4:38:17, 38.65s/it]Epoch: 68 done with learning rate 9.80E-03, Train loss: 7.69E+05, Train scatter: [0.2742 0.0681 0.3037 0.5242]
L1 regularization loss: 6.75E-01, L2 regularization loss: 2.61E-01
Test scatter: [0.2801 0.0682 0.3111 0.5221], Lowest was [0.2801 0.0682 0.2883 0.5088]
Median for last 10 epochs: [0.3103 0.0721 0.3295 0.5224], Epochs since improvement 0
 14%|█▍        | 69/500 [42:26<4:13:16, 35.26s/it]Epoch: 70 done with learning rate 9.90E-03, Train loss: 6.61E+05, Train scatter: [0.283  0.0677 0.3449 0.5299]
L1 regularization loss: 6.78E-01, L2 regularization loss: 2.66E-01
Test scatter: [0.2898 0.0684 0.3529 0.5271], Lowest was [0.2801 0.0682 0.2883 0.5088]
Median for last 10 epochs: [0.2984 0.0705 0.3295 0.5224], Epochs since improvement 2
 14%|█▍        | 70/500 [43:18<4:47:18, 40.09s/it] 14%|█▍        | 71/500 [43:45<4:19:36, 36.31s/it] 14%|█▍        | 72/500 [44:32<4:41:13, 39.42s/it]Epoch: 72 done with learning rate 9.96E-03, Train loss: 6.22E+05, Train scatter: [0.304  0.0693 0.3167 0.5843]
L1 regularization loss: 6.90E-01, L2 regularization loss: 2.76E-01
Test scatter: [0.314  0.0691 0.3134 0.5828], Lowest was [0.2801 0.0682 0.2883 0.5088]
Median for last 10 epochs: [0.2984 0.0691 0.3266 0.5271], Epochs since improvement 4
 15%|█▍        | 73/500 [44:59<4:14:49, 35.81s/it] 15%|█▍        | 74/500 [45:45<4:36:16, 38.91s/it]Epoch: 74 done with learning rate 1.00E-02, Train loss: 1.16E+06, Train scatter: [0.251  0.0671 0.3022 0.5029]
L1 regularization loss: 7.21E-01, L2 regularization loss: 2.93E-01
Test scatter: [0.2639 0.0667 0.304  0.5011], Lowest was [0.2639 0.0667 0.2883 0.5011]
Median for last 10 epochs: [0.2898 0.0684 0.3134 0.5224], Epochs since improvement 0
 15%|█▌        | 75/500 [46:13<4:11:10, 35.46s/it] 15%|█▌        | 76/500 [46:59<4:32:57, 38.63s/it]Epoch: 76 done with learning rate 1.00E-02, Train loss: 4.76E+05, Train scatter: [0.247  0.0649 0.2868 0.497 ]
L1 regularization loss: 7.21E-01, L2 regularization loss: 2.98E-01
Test scatter: [0.2592 0.0657 0.2964 0.4988], Lowest was [0.2592 0.0657 0.2883 0.4988]
Median for last 10 epochs: [0.2801 0.0682 0.3111 0.5221], Epochs since improvement 0
 15%|█▌        | 77/500 [47:26<4:08:42, 35.28s/it] 16%|█▌        | 78/500 [48:12<4:30:42, 38.49s/it]Epoch: 78 done with learning rate 1.00E-02, Train loss: 4.83E+05, Train scatter: [0.281  0.0698 0.2924 0.5314]
L1 regularization loss: 7.27E-01, L2 regularization loss: 3.06E-01
Test scatter: [0.298  0.0708 0.3052 0.532 ], Lowest was [0.2592 0.0657 0.2883 0.4988]
Median for last 10 epochs: [0.2898 0.0684 0.3052 0.5271], Epochs since improvement 2
 16%|█▌        | 79/500 [48:40<4:06:37, 35.15s/it]Epoch: 80 done with learning rate 1.00E-02, Train loss: 4.46E+05, Train scatter: [0.3044 0.0691 0.2873 0.5191]
L1 regularization loss: 7.35E-01, L2 regularization loss: 3.15E-01
Test scatter: [0.3097 0.0705 0.2957 0.5156], Lowest was [0.2592 0.0657 0.2883 0.4988]
Median for last 10 epochs: [0.298  0.0691 0.304  0.5156], Epochs since improvement 4
 16%|█▌        | 80/500 [49:32<4:42:29, 40.36s/it] 16%|█▌        | 81/500 [50:00<4:14:45, 36.48s/it] 16%|█▋        | 82/500 [50:46<4:33:54, 39.32s/it]Epoch: 82 done with learning rate 9.99E-03, Train loss: 3.96E+05, Train scatter: [0.2526 0.0647 0.2658 0.4856]
L1 regularization loss: 7.42E-01, L2 regularization loss: 3.24E-01
Test scatter: [0.2622 0.0657 0.2783 0.4863], Lowest was [0.2592 0.0657 0.2783 0.4863]
Median for last 10 epochs: [0.2639 0.0667 0.2964 0.5011], Epochs since improvement 0
 17%|█▋        | 83/500 [51:13<4:08:12, 35.71s/it] 17%|█▋        | 84/500 [51:59<4:28:54, 38.79s/it]Epoch: 84 done with learning rate 9.99E-03, Train loss: 2.94E+05, Train scatter: [0.2418 0.0641 0.2747 0.4853]
L1 regularization loss: 7.47E-01, L2 regularization loss: 3.31E-01
Test scatter: [0.2563 0.0649 0.281  0.487 ], Lowest was [0.2563 0.0649 0.2783 0.4863]
Median for last 10 epochs: [0.2622 0.0657 0.2957 0.4988], Epochs since improvement 0
 17%|█▋        | 85/500 [52:26<4:04:39, 35.37s/it] 17%|█▋        | 86/500 [53:12<4:26:36, 38.64s/it]Epoch: 86 done with learning rate 9.98E-03, Train loss: 2.80E+05, Train scatter: [0.2923 0.0682 0.303  0.5205]
L1 regularization loss: 7.59E-01, L2 regularization loss: 3.43E-01
Test scatter: [0.2997 0.0693 0.3126 0.5262], Lowest was [0.2563 0.0649 0.2783 0.4863]
Median for last 10 epochs: [0.298  0.0693 0.2957 0.5156], Epochs since improvement 2
 17%|█▋        | 87/500 [53:40<4:02:43, 35.26s/it] 18%|█▊        | 88/500 [54:26<4:25:03, 38.60s/it]Epoch: 88 done with learning rate 9.98E-03, Train loss: 2.52E+05, Train scatter: [0.2765 0.0639 0.2695 0.4903]
L1 regularization loss: 7.69E-01, L2 regularization loss: 3.53E-01
Test scatter: [0.292  0.065  0.2799 0.4902], Lowest was [0.2563 0.0649 0.2783 0.4863]
Median for last 10 epochs: [0.292  0.0657 0.281  0.4902], Epochs since improvement 4
 18%|█▊        | 89/500 [54:54<4:01:33, 35.26s/it]Epoch: 90 done with learning rate 9.97E-03, Train loss: 2.00E+05, Train scatter: [0.2401 0.0627 0.2662 0.4765]
L1 regularization loss: 7.74E-01, L2 regularization loss: 3.62E-01
Test scatter: [0.2544 0.064  0.2778 0.4791], Lowest was [0.2544 0.064  0.2778 0.4791]
Median for last 10 epochs: [0.2622 0.065  0.2799 0.487 ], Epochs since improvement 0
 18%|█▊        | 90/500 [55:46<4:35:42, 40.35s/it] 18%|█▊        | 91/500 [56:13<4:08:31, 36.46s/it] 18%|█▊        | 92/500 [57:00<4:27:56, 39.40s/it]Epoch: 92 done with learning rate 9.96E-03, Train loss: 1.47E+05, Train scatter: [0.2469 0.0612 0.2575 0.4954]
L1 regularization loss: 7.77E-01, L2 regularization loss: 3.70E-01
Test scatter: [0.2614 0.0627 0.2717 0.4956], Lowest was [0.2544 0.0627 0.2717 0.4791]
Median for last 10 epochs: [0.2614 0.0649 0.2799 0.4902], Epochs since improvement 0
 19%|█▊        | 93/500 [57:27<4:03:07, 35.84s/it] 19%|█▉        | 94/500 [58:14<4:24:49, 39.14s/it]Epoch: 94 done with learning rate 9.95E-03, Train loss: 2.09E+07, Train scatter: [0.9236 0.1741 0.5436 0.9891]
L1 regularization loss: 1.18E+00, L2 regularization loss: 6.09E-01
Test scatter: [0.9069 0.1701 0.5351 0.9783], Lowest was [0.2544 0.0627 0.2717 0.4791]
Median for last 10 epochs: [0.292  0.065  0.2799 0.4956], Epochs since improvement 2
 19%|█▉        | 95/500 [58:41<4:00:26, 35.62s/it] 19%|█▉        | 96/500 [59:27<4:20:32, 38.69s/it]Epoch: 96 done with learning rate 9.94E-03, Train loss: 1.68E+06, Train scatter: [0.9065 0.1363 0.5141 0.982 ]
L1 regularization loss: 1.26E+00, L2 regularization loss: 7.10E-01
Test scatter: [0.89   0.1364 0.5101 0.9715], Lowest was [0.2544 0.0627 0.2717 0.4791]
Median for last 10 epochs: [0.292  0.065  0.2799 0.4956], Epochs since improvement 4
 19%|█▉        | 97/500 [59:55<3:57:15, 35.32s/it] 20%|█▉        | 98/500 [1:00:41<4:18:38, 38.60s/it]Epoch: 98 done with learning rate 9.93E-03, Train loss: 1.09E+06, Train scatter: [0.8735 0.1245 0.5311 0.9763]
L1 regularization loss: 1.28E+00, L2 regularization loss: 7.46E-01
Test scatter: [0.8578 0.1254 0.5236 0.966 ], Lowest was [0.2544 0.0627 0.2717 0.4791]
Median for last 10 epochs: [0.8578 0.1254 0.5101 0.966 ], Epochs since improvement 6
 20%|█▉        | 99/500 [1:01:08<3:55:33, 35.24s/it]Epoch: 100 done with learning rate 9.91E-03, Train loss: 7.91E+05, Train scatter: [0.7688 0.1185 0.5108 0.97  ]
L1 regularization loss: 1.29E+00, L2 regularization loss: 8.01E-01
Test scatter: [0.7614 0.1177 0.505  0.96  ], Lowest was [0.2544 0.0627 0.2717 0.4791]
Median for last 10 epochs: [0.8578 0.1254 0.5101 0.966 ], Epochs since improvement 8
 20%|██        | 100/500 [1:02:02<4:32:00, 40.80s/it] 20%|██        | 101/500 [1:02:30<4:04:35, 36.78s/it] 20%|██        | 102/500 [1:03:15<4:21:39, 39.45s/it]Epoch: 102 done with learning rate 9.90E-03, Train loss: 6.06E+05, Train scatter: [0.6917 0.1133 0.4845 0.9622]
L1 regularization loss: 1.30E+00, L2 regularization loss: 8.50E-01
Test scatter: [0.7328 0.1128 0.484  0.9524], Lowest was [0.2544 0.0627 0.2717 0.4791]
Median for last 10 epochs: [0.8578 0.1254 0.5101 0.966 ], Epochs since improvement 10
 21%|██        | 103/500 [1:03:43<3:57:28, 35.89s/it] 21%|██        | 104/500 [1:04:29<4:17:04, 38.95s/it]Epoch: 104 done with learning rate 9.89E-03, Train loss: 5.13E+05, Train scatter: [0.6624 0.1101 0.491  0.9526]
L1 regularization loss: 1.31E+00, L2 regularization loss: 8.76E-01
Test scatter: [0.7059 0.1097 0.4897 0.9434], Lowest was [0.2544 0.0627 0.2717 0.4791]
Median for last 10 epochs: [0.7614 0.1177 0.505  0.96  ], Epochs since improvement 12
 21%|██        | 105/500 [1:04:56<3:53:38, 35.49s/it] 21%|██        | 106/500 [1:05:42<4:13:28, 38.60s/it]Epoch: 106 done with learning rate 9.87E-03, Train loss: 4.45E+05, Train scatter: [0.6685 0.1071 0.4811 0.9399]
L1 regularization loss: 1.31E+00, L2 regularization loss: 9.01E-01
Test scatter: [0.711  0.1069 0.4789 0.9315], Lowest was [0.2544 0.0627 0.2717 0.4791]
Median for last 10 epochs: [0.7328 0.1128 0.4897 0.9524], Epochs since improvement 14
 21%|██▏       | 107/500 [1:06:10<3:51:11, 35.30s/it] 22%|██▏       | 108/500 [1:06:56<4:12:36, 38.67s/it]Epoch: 108 done with learning rate 9.85E-03, Train loss: 3.73E+05, Train scatter: [0.6523 0.105  0.4776 0.9222]
L1 regularization loss: 1.32E+00, L2 regularization loss: 9.25E-01
Test scatter: [0.6939 0.1051 0.4755 0.9151], Lowest was [0.2544 0.0627 0.2717 0.4791]
Median for last 10 epochs: [0.711  0.1097 0.484  0.9434], Epochs since improvement 16
 22%|██▏       | 109/500 [1:07:24<3:49:58, 35.29s/it]Epoch: 110 done with learning rate 9.83E-03, Train loss: 3.10E+05, Train scatter: [0.6335 0.1031 0.4442 0.8915]
L1 regularization loss: 1.32E+00, L2 regularization loss: 9.53E-01
Test scatter: [0.6664 0.1035 0.4436 0.8869], Lowest was [0.2544 0.0627 0.2717 0.4791]
Median for last 10 epochs: [0.7059 0.1069 0.4789 0.9315], Epochs since improvement 18
 22%|██▏       | 110/500 [1:08:15<4:20:47, 40.12s/it] 22%|██▏       | 111/500 [1:08:42<3:55:19, 36.30s/it] 22%|██▏       | 112/500 [1:09:28<4:13:04, 39.14s/it]Epoch: 112 done with learning rate 9.81E-03, Train loss: 2.66E+05, Train scatter: [0.6202 0.1026 0.457  0.8378]
L1 regularization loss: 1.33E+00, L2 regularization loss: 9.85E-01
Test scatter: [0.6501 0.104  0.4581 0.8372], Lowest was [0.2544 0.0627 0.2717 0.4791]
Median for last 10 epochs: [0.6939 0.1051 0.4755 0.9151], Epochs since improvement 20
 23%|██▎       | 113/500 [1:09:56<3:49:46, 35.62s/it] 23%|██▎       | 113/500 [1:10:42<4:02:08, 37.54s/it]
Epoch: 114 done with learning rate 9.79E-03, Train loss: 2.09E+05, Train scatter: [0.6168 0.1019 0.4395 0.7907]
L1 regularization loss: 1.33E+00, L2 regularization loss: 1.01E+00
Test scatter: [0.6374 0.1022 0.4411 0.7934], Lowest was [0.2544 0.0627 0.2717 0.4791]
Median for last 10 epochs: [0.6664 0.104  0.4581 0.8869], Epochs since improvement 22
Exited after 114 epochs due to early stopping
4242.34 seconds spent training, 8.485 seconds per epoch. Processed 8207 trees per second
[0.6373665  0.10217912 0.44105133 0.79340225]
{'epoch_exit': 113, 'scatter_m_star': 0.6373665, 'lowest_m_star': 0.25441086, 'last20_m_star': 0.7084769, 'last10_m_star': 0.6663706, 'scatter_v_disk': 0.102179125, 'lowest_v_disk': 0.06266446, 'last20_v_disk': 0.10831397, 'last10_v_disk': 0.10400728, 'scatter_m_cold': 0.44105133, 'lowest_m_cold': 0.2717239, 'last20_m_cold': 0.48148268, 'last10_m_cold': 0.45813435, 'scatter_sfr_100': 0.79340225, 'lowest_sfr_100': 0.47906885, 'last20_sfr_100': 0.93748176, 'last10_sfr_100': 0.8868597}
Finished 1/1
Exited training after None epochs
Experiment 2 done: 2 / 6
Exploring ['conv_layers', 'batch_size']
Currently doing [  2 256]
{'experiment': 'GraphMerge', 'group': 'meta_pysr', 'move': False, 'model': 'MetaEdge', 'log': True, 'run_params': {'n_epochs': 500, 'n_trials': 1, 'batch_size': 256, 'val_epoch': 2, 'early_stopping': True, 'patience': 20, 'l1_lambda': 3e-05, 'l2_lambda': 3e-05, 'loss_func': 'GaussNd', 'metrics': 'test_multi_varrho', 'performance_plot': 'multi_base', 'save': True, 'seed': True, 'num_workers': 4}, 'learn_params': {'learning_rate': 0.01, 'schedule': 'onecycle', 'g_up': 1, 'g_down': 0.95, 'warmup': 4, 'period': 5, 'eta_min': 1e-05}, 'hyper_params': {'hidden_channels': 128, 'conv_layers': 2, 'conv_activation': 'relu', 'decode_activation': 'leakyrelu', 'decode_layers': 2, 'layernorm': True, 'agg': 'sum', 'variance': True, 'rho': 0, 'in_channels': 43, 'out_channels': 4}, 'data_params': {'case': 'vlarge_all_4t_z0.0_quantile_raw', 'targets': [0, 1, 2, 3], 'del_feats': [], 'split': 0.8, 'test': 0, 'scale': 0}, 'experiment_name': 'con2bat256'}
/home/cj1223/../../scratch/gpfs/cj1223/GraphResults/results_meta_pysr_120322/MetaEdge_vlarge_all_4t_z0.0_quantile_raw_lphpuv
RelU conv activation
LeakyRelU decode activation
N_params 422280
Made folder for saving model
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:48<6:40:15, 48.13s/it]  0%|          | 2/500 [01:59<8:31:26, 61.62s/it]Epoch: 2 done with learning rate 4.17E-04, Train loss: 2.28E+07, Train scatter: [0.9351 0.1341 0.544  0.9954]
L1 regularization loss: 5.97E-01, L2 regularization loss: 1.15E-01
Test scatter: [0.9195 0.1316 0.5355 0.9851], Lowest was [0.9195 0.1316 0.5355 0.9851]
Median for last 10 epochs: [0.9195 0.1316 0.5355 0.9851], Epochs since improvement 2
  1%|          | 3/500 [02:45<7:33:40, 54.77s/it]  1%|          | 4/500 [03:56<8:25:14, 61.12s/it]Epoch: 4 done with learning rate 4.67E-04, Train loss: 1.48E+07, Train scatter: [0.9263 0.0938 0.5439 0.9939]
L1 regularization loss: 6.05E-01, L2 regularization loss: 1.24E-01
Test scatter: [0.9104 0.0936 0.5354 0.9834], Lowest was [0.9104 0.0936 0.5354 0.9834]
Median for last 10 epochs: [0.9104 0.0936 0.5354 0.9834], Epochs since improvement 0
  1%|          | 5/500 [04:43<7:40:47, 55.85s/it]  1%|          | 6/500 [05:53<8:20:35, 60.80s/it]Epoch: 6 done with learning rate 5.51E-04, Train loss: 9.22E+06, Train scatter: [0.7747 0.0879 0.5439 0.6394]
L1 regularization loss: 6.18E-01, L2 regularization loss: 1.36E-01
Test scatter: [0.7638 0.0867 0.5353 0.6343], Lowest was [0.7638 0.0867 0.5353 0.6343]
Median for last 10 epochs: [0.7638 0.0867 0.5353 0.6343], Epochs since improvement 0
  1%|▏         | 7/500 [06:40<7:41:31, 56.17s/it]  2%|▏         | 8/500 [07:50<8:18:11, 60.75s/it]Epoch: 8 done with learning rate 6.67E-04, Train loss: 5.55E+06, Train scatter: [0.5873 0.0808 0.5439 0.584 ]
L1 regularization loss: 6.22E-01, L2 regularization loss: 1.41E-01
Test scatter: [0.5832 0.0803 0.5353 0.5822], Lowest was [0.5832 0.0803 0.5353 0.5822]
Median for last 10 epochs: [0.6735 0.0835 0.5353 0.6082], Epochs since improvement 0
  2%|▏         | 9/500 [08:37<7:40:59, 56.33s/it]Epoch: 10 done with learning rate 8.15E-04, Train loss: 4.14E+06, Train scatter: [0.3763 0.0768 0.5438 0.5531]
L1 regularization loss: 6.25E-01, L2 regularization loss: 1.43E-01
Test scatter: [0.3737 0.0764 0.5353 0.5444], Lowest was [0.3737 0.0764 0.5353 0.5444]
Median for last 10 epochs: [0.5832 0.0803 0.5353 0.5822], Epochs since improvement 0
  2%|▏         | 10/500 [09:55<8:33:48, 62.92s/it]  2%|▏         | 11/500 [10:41<7:52:31, 57.98s/it]  2%|▏         | 12/500 [11:52<8:22:26, 61.78s/it]Epoch: 12 done with learning rate 9.94E-04, Train loss: 3.68E+06, Train scatter: [0.2566 0.0745 0.5438 0.5349]
L1 regularization loss: 6.27E-01, L2 regularization loss: 1.45E-01
Test scatter: [0.2605 0.0745 0.5352 0.5275], Lowest was [0.2605 0.0745 0.5352 0.5275]
Median for last 10 epochs: [0.5832 0.0803 0.5353 0.5822], Epochs since improvement 0
  3%|▎         | 13/500 [12:39<7:44:31, 57.23s/it]  3%|▎         | 14/500 [13:50<8:19:12, 61.63s/it]Epoch: 14 done with learning rate 1.20E-03, Train loss: 3.52E+06, Train scatter: [0.2936 0.0777 0.5438 0.5319]
L1 regularization loss: 6.29E-01, L2 regularization loss: 1.46E-01
Test scatter: [0.3006 0.0772 0.5352 0.5237], Lowest was [0.2605 0.0745 0.5352 0.5237]
Median for last 10 epochs: [0.3737 0.0772 0.5353 0.5444], Epochs since improvement 0
  3%|▎         | 15/500 [14:37<7:41:46, 57.13s/it]  3%|▎         | 16/500 [15:49<8:16:21, 61.53s/it]Epoch: 16 done with learning rate 1.44E-03, Train loss: 3.39E+06, Train scatter: [0.2643 0.0705 0.5438 0.5213]
L1 regularization loss: 6.33E-01, L2 regularization loss: 1.48E-01
Test scatter: [0.2704 0.0704 0.5352 0.5182], Lowest was [0.2605 0.0704 0.5352 0.5182]
Median for last 10 epochs: [0.3006 0.0764 0.5352 0.5275], Epochs since improvement 0
  3%|▎         | 17/500 [16:36<7:39:38, 57.10s/it]  4%|▎         | 18/500 [17:47<8:14:14, 61.52s/it]Epoch: 18 done with learning rate 1.70E-03, Train loss: 3.32E+06, Train scatter: [0.3775 0.0778 0.5437 0.5585]
L1 regularization loss: 6.37E-01, L2 regularization loss: 1.49E-01
Test scatter: [0.3815 0.0769 0.5352 0.558 ], Lowest was [0.2605 0.0704 0.5352 0.5182]
Median for last 10 epochs: [0.3006 0.0764 0.5352 0.5275], Epochs since improvement 0
  4%|▍         | 19/500 [18:34<7:37:40, 57.09s/it]Epoch: 20 done with learning rate 1.99E-03, Train loss: 3.27E+06, Train scatter: [0.231  0.0712 0.5438 0.5199]
L1 regularization loss: 6.41E-01, L2 regularization loss: 1.51E-01
Test scatter: [0.24   0.0706 0.5352 0.5141], Lowest was [0.24   0.0704 0.5352 0.5141]
Median for last 10 epochs: [0.2704 0.0745 0.5352 0.5237], Epochs since improvement 0
  4%|▍         | 20/500 [19:52<8:27:39, 63.46s/it]  4%|▍         | 21/500 [20:39<7:46:42, 58.46s/it]  4%|▍         | 22/500 [21:50<8:15:59, 62.26s/it]Epoch: 22 done with learning rate 2.30E-03, Train loss: 3.11E+06, Train scatter: [0.3753 0.0687 0.5437 0.514 ]
L1 regularization loss: 6.45E-01, L2 regularization loss: 1.52E-01
Test scatter: [0.3714 0.0685 0.5352 0.5076], Lowest was [0.24   0.0685 0.5352 0.5076]
Median for last 10 epochs: [0.3006 0.0706 0.5352 0.5182], Epochs since improvement 0
  5%|▍         | 23/500 [22:37<7:37:52, 57.60s/it]  5%|▍         | 24/500 [23:48<8:08:50, 61.62s/it]Epoch: 24 done with learning rate 2.63E-03, Train loss: 2.99E+06, Train scatter: [0.2056 0.0643 0.5437 0.5101]
L1 regularization loss: 6.49E-01, L2 regularization loss: 1.54E-01
Test scatter: [0.2107 0.0644 0.5352 0.505 ], Lowest was [0.2107 0.0644 0.5352 0.505 ]
Median for last 10 epochs: [0.2704 0.0704 0.5352 0.5141], Epochs since improvement 0
  5%|▌         | 25/500 [24:35<7:32:26, 57.15s/it]  5%|▌         | 26/500 [25:46<8:04:12, 61.29s/it]Epoch: 26 done with learning rate 2.98E-03, Train loss: 3.04E+06, Train scatter: [0.2396 0.0704 0.5438 0.5162]
L1 regularization loss: 6.55E-01, L2 regularization loss: 1.56E-01
Test scatter: [0.2422 0.0696 0.5352 0.51  ], Lowest was [0.2107 0.0644 0.5352 0.505 ]
Median for last 10 epochs: [0.2422 0.0696 0.5352 0.51  ], Epochs since improvement 2
  5%|▌         | 27/500 [26:33<7:28:50, 56.93s/it]  6%|▌         | 28/500 [27:44<8:01:39, 61.23s/it]Epoch: 28 done with learning rate 3.34E-03, Train loss: 2.95E+06, Train scatter: [0.2711 0.0755 0.5437 0.5454]
L1 regularization loss: 6.58E-01, L2 regularization loss: 1.57E-01
Test scatter: [0.2756 0.0755 0.5352 0.5402], Lowest was [0.2107 0.0644 0.5352 0.505 ]
Median for last 10 epochs: [0.2422 0.0696 0.5352 0.51  ], Epochs since improvement 0
  6%|▌         | 29/500 [28:30<7:26:16, 56.85s/it]Epoch: 30 done with learning rate 3.72E-03, Train loss: 2.93E+06, Train scatter: [0.1999 0.0626 0.5437 0.4919]
L1 regularization loss: 6.65E-01, L2 regularization loss: 1.60E-01
Test scatter: [0.206  0.0621 0.5351 0.4834], Lowest was [0.206  0.0621 0.5351 0.4834]
Median for last 10 epochs: [0.2422 0.0685 0.5352 0.5076], Epochs since improvement 0
  6%|▌         | 30/500 [29:48<8:13:59, 63.06s/it]  6%|▌         | 31/500 [30:35<7:34:25, 58.14s/it]  6%|▋         | 32/500 [31:47<8:05:51, 62.29s/it]Epoch: 32 done with learning rate 4.10E-03, Train loss: 2.94E+06, Train scatter: [0.2315 0.0638 0.5436 0.5015]
L1 regularization loss: 6.73E-01, L2 regularization loss: 1.64E-01
Test scatter: [0.2336 0.0641 0.5351 0.493 ], Lowest was [0.206  0.0621 0.5351 0.4834]
Median for last 10 epochs: [0.2336 0.0644 0.5352 0.505 ], Epochs since improvement 0
  7%|▋         | 33/500 [32:33<7:28:33, 57.63s/it]  7%|▋         | 34/500 [33:45<7:59:10, 61.70s/it]Epoch: 34 done with learning rate 4.50E-03, Train loss: 2.94E+06, Train scatter: [0.2321 0.0716 0.5434 0.5147]
L1 regularization loss: 6.84E-01, L2 regularization loss: 1.69E-01
Test scatter: [0.2333 0.0716 0.5349 0.5095], Lowest was [0.206  0.0621 0.5349 0.4834]
Median for last 10 epochs: [0.2336 0.0696 0.5351 0.5095], Epochs since improvement 0
  7%|▋         | 35/500 [34:31<7:23:15, 57.19s/it]  7%|▋         | 36/500 [35:42<7:54:41, 61.38s/it]Epoch: 36 done with learning rate 4.90E-03, Train loss: 2.94E+06, Train scatter: [0.3585 0.0947 0.5435 0.5584]
L1 regularization loss: 6.93E-01, L2 regularization loss: 1.73E-01
Test scatter: [0.3663 0.0934 0.535  0.556 ], Lowest was [0.206  0.0621 0.5349 0.4834]
Median for last 10 epochs: [0.2336 0.0716 0.5351 0.5095], Epochs since improvement 2
  7%|▋         | 37/500 [36:29<7:19:28, 56.95s/it]  8%|▊         | 38/500 [37:41<7:52:42, 61.39s/it]Epoch: 38 done with learning rate 5.30E-03, Train loss: 2.92E+06, Train scatter: [0.2154 0.0655 0.5433 0.4996]
L1 regularization loss: 7.02E-01, L2 regularization loss: 1.77E-01
Test scatter: [0.2201 0.0654 0.5347 0.4953], Lowest was [0.206  0.0621 0.5347 0.4834]
Median for last 10 epochs: [0.2333 0.0654 0.535  0.4953], Epochs since improvement 0
  8%|▊         | 39/500 [38:27<7:17:48, 56.98s/it]Epoch: 40 done with learning rate 5.70E-03, Train loss: 2.97E+06, Train scatter: [0.2463 0.0654 0.5432 0.5016]
L1 regularization loss: 7.20E-01, L2 regularization loss: 1.84E-01
Test scatter: [0.2482 0.0651 0.5347 0.4947], Lowest was [0.206  0.0621 0.5347 0.4834]
Median for last 10 epochs: [0.2336 0.0654 0.5349 0.4953], Epochs since improvement 0
  8%|▊         | 40/500 [39:46<8:06:02, 63.40s/it]  8%|▊         | 41/500 [40:33<7:26:51, 58.41s/it]  8%|▊         | 42/500 [41:44<7:56:01, 62.36s/it]Epoch: 42 done with learning rate 6.10E-03, Train loss: 5.58E+06, Train scatter: [0.9243 0.155  0.5439 0.9954]
L1 regularization loss: 9.19E-01, L2 regularization loss: 2.56E-01
Test scatter: [0.9104 0.1505 0.5353 0.985 ], Lowest was [0.206  0.0621 0.5347 0.4834]
Median for last 10 epochs: [0.2482 0.0716 0.5349 0.5095], Epochs since improvement 2
  9%|▊         | 43/500 [42:31<7:19:04, 57.65s/it]  9%|▉         | 44/500 [43:42<7:49:39, 61.80s/it]Epoch: 44 done with learning rate 6.49E-03, Train loss: 4.19E+06, Train scatter: [0.9114 0.1408 0.544  0.9955]
L1 regularization loss: 9.29E-01, L2 regularization loss: 2.72E-01
Test scatter: [0.8981 0.1366 0.5354 0.9851], Lowest was [0.206  0.0621 0.5347 0.4834]
Median for last 10 epochs: [0.3663 0.0934 0.535  0.556 ], Epochs since improvement 4
  9%|▉         | 45/500 [44:29<7:14:28, 57.29s/it]  9%|▉         | 46/500 [45:41<7:46:12, 61.61s/it]Epoch: 46 done with learning rate 6.87E-03, Train loss: 3.92E+06, Train scatter: [0.7231 0.1298 0.544  0.9953]
L1 regularization loss: 9.38E-01, L2 regularization loss: 2.85E-01
Test scatter: [0.7053 0.1258 0.5354 0.9849], Lowest was [0.206  0.0621 0.5347 0.4834]
Median for last 10 epochs: [0.7053 0.1258 0.5353 0.9849], Epochs since improvement 6
  9%|▉         | 47/500 [46:28<7:11:27, 57.15s/it] 10%|▉         | 48/500 [47:38<7:41:21, 61.24s/it]Epoch: 48 done with learning rate 7.24E-03, Train loss: 3.84E+06, Train scatter: [0.5467 0.1198 0.544  0.9942]
L1 regularization loss: 9.43E-01, L2 regularization loss: 2.93E-01
Test scatter: [0.533  0.1151 0.5354 0.9839], Lowest was [0.206  0.0621 0.5347 0.4834]
Median for last 10 epochs: [0.7053 0.1258 0.5354 0.9849], Epochs since improvement 8
 10%|▉         | 49/500 [48:25<7:07:46, 56.91s/it]Epoch: 50 done with learning rate 7.60E-03, Train loss: 3.54E+06, Train scatter: [0.614  0.1088 0.5441 0.9909]
L1 regularization loss: 9.51E-01, L2 regularization loss: 3.01E-01
Test scatter: [0.5974 0.1072 0.5355 0.9807], Lowest was [0.206  0.0621 0.5347 0.4834]
Median for last 10 epochs: [0.7053 0.1258 0.5354 0.9849], Epochs since improvement 10
 10%|█         | 50/500 [49:42<7:52:10, 62.96s/it] 10%|█         | 51/500 [50:29<7:14:47, 58.10s/it] 10%|█         | 52/500 [51:41<7:44:37, 62.23s/it]Epoch: 52 done with learning rate 7.94E-03, Train loss: 3.52E+06, Train scatter: [0.8448 0.1095 0.5441 0.9897]
L1 regularization loss: 9.54E-01, L2 regularization loss: 3.07E-01
Test scatter: [0.8295 0.1089 0.5355 0.9795], Lowest was [0.206  0.0621 0.5347 0.4834]
Median for last 10 epochs: [0.7053 0.1151 0.5354 0.9839], Epochs since improvement 12
 11%|█         | 53/500 [52:28<7:08:58, 57.58s/it] 11%|█         | 54/500 [53:38<7:37:06, 61.49s/it]Epoch: 54 done with learning rate 8.26E-03, Train loss: 3.45E+06, Train scatter: [0.463  0.1    0.5441 0.9752]
L1 regularization loss: 9.57E-01, L2 regularization loss: 3.15E-01
Test scatter: [0.4573 0.0988 0.5355 0.9653], Lowest was [0.206  0.0621 0.5347 0.4834]
Median for last 10 epochs: [0.5974 0.1089 0.5355 0.9807], Epochs since improvement 14
 11%|█         | 55/500 [54:25<7:03:01, 57.04s/it] 11%|█         | 56/500 [55:37<7:34:48, 61.46s/it]Epoch: 56 done with learning rate 8.56E-03, Train loss: 3.48E+06, Train scatter: [0.8987 0.1022 0.544  0.8171]
L1 regularization loss: 9.77E-01, L2 regularization loss: 3.40E-01
Test scatter: [0.884  0.1012 0.5355 0.8018], Lowest was [0.206  0.0621 0.5347 0.4834]
Median for last 10 epochs: [0.5974 0.1072 0.5355 0.9795], Epochs since improvement 16
 11%|█▏        | 57/500 [56:23<7:01:04, 57.03s/it] 12%|█▏        | 58/500 [57:35<7:32:29, 61.42s/it]Epoch: 58 done with learning rate 8.83E-03, Train loss: 3.38E+06, Train scatter: [0.9097 0.1112 0.544  0.9935]
L1 regularization loss: 1.01E+00, L2 regularization loss: 3.69E-01
Test scatter: [0.8961 0.1105 0.5354 0.9832], Lowest was [0.206  0.0621 0.5347 0.4834]
Median for last 10 epochs: [0.8295 0.1072 0.5355 0.9795], Epochs since improvement 18
 12%|█▏        | 59/500 [58:22<6:58:58, 57.00s/it]Epoch: 60 done with learning rate 9.08E-03, Train loss: 3.31E+06, Train scatter: [0.6365 0.1069 0.544  0.992 ]
L1 regularization loss: 1.01E+00, L2 regularization loss: 3.77E-01
Test scatter: [0.6443 0.106  0.5354 0.9818], Lowest was [0.206  0.0621 0.5347 0.4834]
Median for last 10 epochs: [0.8295 0.106  0.5355 0.9795], Epochs since improvement 20
 12%|█▏        | 60/500 [59:40<7:44:51, 63.39s/it] 12%|█▏        | 61/500 [1:00:27<7:07:27, 58.42s/it] 12%|█▏        | 61/500 [1:01:39<7:23:45, 60.65s/it]
Epoch: 62 done with learning rate 9.31E-03, Train loss: 3.28E+06, Train scatter: [0.3994 0.0966 0.544  0.9894]
L1 regularization loss: 1.02E+00, L2 regularization loss: 3.91E-01
Test scatter: [0.3971 0.0954 0.5354 0.9793], Lowest was [0.206  0.0621 0.5347 0.4834]
Median for last 10 epochs: [0.6443 0.1012 0.5354 0.9793], Epochs since improvement 22
Exited after 62 epochs due to early stopping
3699.68 seconds spent training, 7.399 seconds per epoch. Processed 9411 trees per second
[0.39711636 0.09536052 0.53539765 0.9792588 ]
{'epoch_exit': 61, 'scatter_m_star': 0.39711636, 'lowest_m_star': 0.2059828, 'last20_m_star': 0.6747732, 'last10_m_star': 0.6442849, 'scatter_v_disk': 0.095360525, 'lowest_v_disk': 0.0621481, 'last20_v_disk': 0.10807456, 'last10_v_disk': 0.10120297, 'scatter_m_cold': 0.53539765, 'lowest_m_cold': 0.53465563, 'last20_m_cold': 0.5354415, 'last10_m_cold': 0.5354449, 'scatter_sfr_100': 0.9792588, 'lowest_sfr_100': 0.48342863, 'last20_sfr_100': 0.9812257, 'last10_sfr_100': 0.9792867}
Finished 1/1
Exited training after None epochs
Experiment 3 done: 3 / 6
Exploring ['conv_layers', 'batch_size']
Currently doing [  2 512]
{'experiment': 'GraphMerge', 'group': 'meta_pysr', 'move': False, 'model': 'MetaEdge', 'log': True, 'run_params': {'n_epochs': 500, 'n_trials': 1, 'batch_size': 512, 'val_epoch': 2, 'early_stopping': True, 'patience': 20, 'l1_lambda': 3e-05, 'l2_lambda': 3e-05, 'loss_func': 'GaussNd', 'metrics': 'test_multi_varrho', 'performance_plot': 'multi_base', 'save': True, 'seed': True, 'num_workers': 4}, 'learn_params': {'learning_rate': 0.01, 'schedule': 'onecycle', 'g_up': 1, 'g_down': 0.95, 'warmup': 4, 'period': 5, 'eta_min': 1e-05}, 'hyper_params': {'hidden_channels': 128, 'conv_layers': 2, 'conv_activation': 'relu', 'decode_activation': 'leakyrelu', 'decode_layers': 2, 'layernorm': True, 'agg': 'sum', 'variance': True, 'rho': 0, 'in_channels': 43, 'out_channels': 4}, 'data_params': {'case': 'vlarge_all_4t_z0.0_quantile_raw', 'targets': [0, 1, 2, 3], 'del_feats': [], 'split': 0.8, 'test': 0, 'scale': 0}, 'experiment_name': 'con2bat512'}
/home/cj1223/../../scratch/gpfs/cj1223/GraphResults/results_meta_pysr_120322/MetaEdge_vlarge_all_4t_z0.0_quantile_raw_yaomph
RelU conv activation
LeakyRelU decode activation
N_params 422280
Made folder for saving model
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:41<5:43:25, 41.29s/it]  0%|          | 2/500 [01:43<7:25:18, 53.65s/it]Epoch: 2 done with learning rate 4.17E-04, Train loss: 3.03E+07, Train scatter: [0.9352 0.1712 0.5441 0.9954]
L1 regularization loss: 5.94E-01, L2 regularization loss: 1.13E-01
Test scatter: [0.9196 0.1676 0.5356 0.9851], Lowest was [0.9196 0.1676 0.5356 0.9851]
Median for last 10 epochs: [0.9196 0.1676 0.5356 0.9851], Epochs since improvement 2
  1%|          | 3/500 [02:23<6:34:10, 47.59s/it]  1%|          | 4/500 [03:26<7:20:49, 53.33s/it]Epoch: 4 done with learning rate 4.67E-04, Train loss: 2.23E+07, Train scatter: [0.9352 0.1581 0.5441 0.9954]
L1 regularization loss: 5.98E-01, L2 regularization loss: 1.15E-01
Test scatter: [0.9196 0.154  0.5355 0.9851], Lowest was [0.9196 0.154  0.5355 0.9851]
Median for last 10 epochs: [0.9196 0.154  0.5355 0.9851], Epochs since improvement 0
  1%|          | 5/500 [04:06<6:41:52, 48.71s/it]  1%|          | 6/500 [05:08<7:18:27, 53.25s/it]Epoch: 6 done with learning rate 5.51E-04, Train loss: 1.77E+07, Train scatter: [0.9348 0.114  0.5441 0.9954]
L1 regularization loss: 6.04E-01, L2 regularization loss: 1.20E-01
Test scatter: [0.9192 0.1125 0.5355 0.985 ], Lowest was [0.9192 0.1125 0.5355 0.985 ]
Median for last 10 epochs: [0.9192 0.1125 0.5355 0.985 ], Epochs since improvement 0
  1%|▏         | 7/500 [05:49<6:43:00, 49.05s/it]  2%|▏         | 8/500 [06:52<7:19:15, 53.57s/it]Epoch: 8 done with learning rate 6.67E-04, Train loss: 1.36E+07, Train scatter: [0.9277 0.0977 0.5441 0.8751]
L1 regularization loss: 6.11E-01, L2 regularization loss: 1.28E-01
Test scatter: [0.9125 0.0965 0.5355 0.8686], Lowest was [0.9125 0.0965 0.5355 0.8686]
Median for last 10 epochs: [0.9159 0.1045 0.5355 0.9268], Epochs since improvement 0
  2%|▏         | 9/500 [07:32<6:44:27, 49.42s/it]Epoch: 10 done with learning rate 8.15E-04, Train loss: 1.06E+07, Train scatter: [0.9314 0.1381 0.5441 0.9832]
L1 regularization loss: 6.21E-01, L2 regularization loss: 1.37E-01
Test scatter: [0.9157 0.1343 0.5355 0.9731], Lowest was [0.9125 0.0965 0.5355 0.8686]
Median for last 10 epochs: [0.9157 0.1125 0.5355 0.9731], Epochs since improvement 0
  2%|▏         | 10/500 [08:42<7:34:59, 55.71s/it]  2%|▏         | 11/500 [09:22<6:55:23, 50.97s/it]  2%|▏         | 12/500 [10:25<7:23:18, 54.51s/it]Epoch: 12 done with learning rate 9.94E-04, Train loss: 8.49E+06, Train scatter: [0.8764 0.1113 0.544  0.7153]
L1 regularization loss: 6.25E-01, L2 regularization loss: 1.41E-01
Test scatter: [0.8633 0.111  0.5354 0.7155], Lowest was [0.8633 0.0965 0.5354 0.7155]
Median for last 10 epochs: [0.9157 0.1125 0.5355 0.9731], Epochs since improvement 0
  3%|▎         | 13/500 [11:05<6:47:11, 50.17s/it]  3%|▎         | 14/500 [12:09<7:19:26, 54.25s/it]Epoch: 14 done with learning rate 1.20E-03, Train loss: 6.13E+06, Train scatter: [0.621  0.0971 0.5439 0.6432]
L1 regularization loss: 6.29E-01, L2 regularization loss: 1.45E-01
Test scatter: [0.615  0.0961 0.5353 0.6393], Lowest was [0.615  0.0961 0.5353 0.6393]
Median for last 10 epochs: [0.9125 0.111  0.5355 0.8686], Epochs since improvement 0
  3%|▎         | 15/500 [12:49<6:44:21, 50.02s/it]  3%|▎         | 16/500 [13:52<7:16:29, 54.11s/it]Epoch: 16 done with learning rate 1.44E-03, Train loss: 5.12E+06, Train scatter: [0.5359 0.0948 0.5439 0.606 ]
L1 regularization loss: 6.32E-01, L2 regularization loss: 1.47E-01
Test scatter: [0.5281 0.0933 0.5353 0.5969], Lowest was [0.5281 0.0933 0.5353 0.5969]
Median for last 10 epochs: [0.8633 0.0965 0.5354 0.7155], Epochs since improvement 0
  3%|▎         | 17/500 [14:33<6:42:11, 49.96s/it]  4%|▎         | 18/500 [15:37<7:15:03, 54.16s/it]Epoch: 18 done with learning rate 1.70E-03, Train loss: 4.74E+06, Train scatter: [0.4849 0.0964 0.5439 0.5945]
L1 regularization loss: 6.34E-01, L2 regularization loss: 1.49E-01
Test scatter: [0.4776 0.0951 0.5353 0.5885], Lowest was [0.4776 0.0933 0.5353 0.5885]
Median for last 10 epochs: [0.615  0.0961 0.5353 0.6393], Epochs since improvement 0
  4%|▍         | 19/500 [16:17<6:40:50, 50.00s/it]Epoch: 20 done with learning rate 1.99E-03, Train loss: 4.41E+06, Train scatter: [0.3196 0.0845 0.5439 0.5588]
L1 regularization loss: 6.37E-01, L2 regularization loss: 1.51E-01
Test scatter: [0.3221 0.085  0.5353 0.554 ], Lowest was [0.3221 0.085  0.5353 0.554 ]
Median for last 10 epochs: [0.5281 0.0951 0.5353 0.5969], Epochs since improvement 0
  4%|▍         | 20/500 [17:27<7:27:20, 55.92s/it]  4%|▍         | 21/500 [18:07<6:49:43, 51.32s/it]  4%|▍         | 22/500 [19:11<7:18:36, 55.06s/it]Epoch: 22 done with learning rate 2.30E-03, Train loss: 3.97E+06, Train scatter: [0.4907 0.0903 0.5438 0.6558]
L1 regularization loss: 6.40E-01, L2 regularization loss: 1.52E-01
Test scatter: [0.4894 0.091  0.5353 0.6604], Lowest was [0.3221 0.085  0.5353 0.554 ]
Median for last 10 epochs: [0.4894 0.0933 0.5353 0.5969], Epochs since improvement 0
  5%|▍         | 23/500 [19:52<6:43:16, 50.73s/it]  5%|▍         | 24/500 [20:56<7:13:45, 54.68s/it]Epoch: 24 done with learning rate 2.63E-03, Train loss: 3.79E+06, Train scatter: [0.2883 0.0803 0.5438 0.5375]
L1 regularization loss: 6.43E-01, L2 regularization loss: 1.54E-01
Test scatter: [0.2889 0.0801 0.5352 0.5345], Lowest was [0.2889 0.0801 0.5352 0.5345]
Median for last 10 epochs: [0.4776 0.091  0.5353 0.5885], Epochs since improvement 0
  5%|▌         | 25/500 [21:36<6:39:19, 50.44s/it]  5%|▌         | 26/500 [22:39<7:08:52, 54.29s/it]Epoch: 26 done with learning rate 2.98E-03, Train loss: 4.10E+06, Train scatter: [0.4008 0.0888 0.5437 0.55  ]
L1 regularization loss: 6.50E-01, L2 regularization loss: 1.57E-01
Test scatter: [0.3999 0.088  0.5352 0.545 ], Lowest was [0.2889 0.0801 0.5352 0.5345]
Median for last 10 epochs: [0.3999 0.088  0.5353 0.554 ], Epochs since improvement 0
  5%|▌         | 27/500 [23:20<6:35:14, 50.14s/it]  6%|▌         | 28/500 [24:25<7:09:12, 54.56s/it]Epoch: 28 done with learning rate 3.34E-03, Train loss: 3.42E+06, Train scatter: [0.2455 0.0773 0.5437 0.5291]
L1 regularization loss: 6.53E-01, L2 regularization loss: 1.59E-01
Test scatter: [0.2514 0.0773 0.5351 0.5253], Lowest was [0.2514 0.0773 0.5351 0.5253]
Median for last 10 epochs: [0.3221 0.085  0.5352 0.545 ], Epochs since improvement 0
  6%|▌         | 29/500 [25:05<6:34:38, 50.27s/it]Epoch: 30 done with learning rate 3.72E-03, Train loss: 3.39E+06, Train scatter: [0.2429 0.0743 0.5437 0.5186]
L1 regularization loss: 6.56E-01, L2 regularization loss: 1.61E-01
Test scatter: [0.2521 0.0749 0.5351 0.518 ], Lowest was [0.2514 0.0749 0.5351 0.518 ]
Median for last 10 epochs: [0.2889 0.0801 0.5352 0.5345], Epochs since improvement 0
  6%|▌         | 30/500 [26:15<7:21:00, 56.30s/it]  6%|▌         | 31/500 [26:56<6:43:16, 51.59s/it]  6%|▋         | 32/500 [28:00<7:12:10, 55.41s/it]Epoch: 32 done with learning rate 4.10E-03, Train loss: 3.32E+06, Train scatter: [0.2349 0.0738 0.5437 0.5098]
L1 regularization loss: 6.58E-01, L2 regularization loss: 1.63E-01
Test scatter: [0.2423 0.0738 0.5351 0.5049], Lowest was [0.2423 0.0738 0.5351 0.5049]
Median for last 10 epochs: [0.2521 0.0773 0.5351 0.5253], Epochs since improvement 0
  7%|▋         | 33/500 [28:41<6:36:11, 50.90s/it]  7%|▋         | 34/500 [29:45<7:05:34, 54.80s/it]Epoch: 34 done with learning rate 4.50E-03, Train loss: 3.28E+06, Train scatter: [0.2196 0.0764 0.5436 0.5149]
L1 regularization loss: 6.61E-01, L2 regularization loss: 1.65E-01
Test scatter: [0.2252 0.0759 0.535  0.5079], Lowest was [0.2252 0.0738 0.535  0.5049]
Median for last 10 epochs: [0.2514 0.0759 0.5351 0.518 ], Epochs since improvement 0
  7%|▋         | 35/500 [30:25<6:31:14, 50.48s/it]  7%|▋         | 36/500 [31:28<7:00:32, 54.38s/it]Epoch: 36 done with learning rate 4.90E-03, Train loss: 3.26E+06, Train scatter: [0.2485 0.076  0.5435 0.5145]
L1 regularization loss: 6.66E-01, L2 regularization loss: 1.68E-01
Test scatter: [0.2522 0.0755 0.535  0.51  ], Lowest was [0.2252 0.0738 0.535  0.5049]
Median for last 10 epochs: [0.2514 0.0755 0.5351 0.51  ], Epochs since improvement 0
  7%|▋         | 37/500 [32:09<6:27:32, 50.22s/it]  8%|▊         | 38/500 [33:12<6:55:36, 53.97s/it]Epoch: 38 done with learning rate 5.30E-03, Train loss: 3.20E+06, Train scatter: [0.251  0.073  0.5435 0.5013]
L1 regularization loss: 6.70E-01, L2 regularization loss: 1.70E-01
Test scatter: [0.2529 0.0734 0.535  0.4983], Lowest was [0.2252 0.0734 0.535  0.4983]
Median for last 10 epochs: [0.2521 0.0749 0.535  0.5079], Epochs since improvement 0
  8%|▊         | 39/500 [33:52<6:23:41, 49.94s/it]Epoch: 40 done with learning rate 5.70E-03, Train loss: 3.17E+06, Train scatter: [0.2323 0.0739 0.5434 0.5038]
L1 regularization loss: 6.73E-01, L2 regularization loss: 1.72E-01
Test scatter: [0.2389 0.0736 0.5349 0.4991], Lowest was [0.2252 0.0734 0.5349 0.4983]
Median for last 10 epochs: [0.2423 0.0738 0.535  0.5049], Epochs since improvement 0
  8%|▊         | 40/500 [35:03<7:11:13, 56.25s/it]  8%|▊         | 41/500 [35:44<6:34:21, 51.55s/it]  8%|▊         | 42/500 [36:48<7:02:43, 55.38s/it]Epoch: 42 done with learning rate 6.10E-03, Train loss: 3.20E+06, Train scatter: [0.4653 0.0963 0.5433 0.5602]
L1 regularization loss: 6.81E-01, L2 regularization loss: 1.75E-01
Test scatter: [0.4649 0.0955 0.5348 0.5557], Lowest was [0.2252 0.0734 0.5348 0.4983]
Median for last 10 epochs: [0.2522 0.0755 0.535  0.5079], Epochs since improvement 0
  9%|▊         | 43/500 [37:29<6:28:16, 50.98s/it]  9%|▉         | 44/500 [38:33<6:56:50, 54.85s/it]Epoch: 44 done with learning rate 6.49E-03, Train loss: 3.10E+06, Train scatter: [0.2094 0.0743 0.5433 0.5015]
L1 regularization loss: 6.86E-01, L2 regularization loss: 1.77E-01
Test scatter: [0.2521 0.074  0.5348 0.4954], Lowest was [0.2252 0.0734 0.5348 0.4954]
Median for last 10 epochs: [0.2522 0.074  0.5349 0.4991], Epochs since improvement 0
  9%|▉         | 45/500 [39:13<6:23:38, 50.59s/it]  9%|▉         | 46/500 [40:17<6:53:04, 54.59s/it]Epoch: 46 done with learning rate 6.87E-03, Train loss: 3.08E+06, Train scatter: [0.2561 0.073  0.5434 0.5425]
L1 regularization loss: 6.90E-01, L2 regularization loss: 1.78E-01
Test scatter: [0.2589 0.0731 0.5349 0.5401], Lowest was [0.2252 0.0731 0.5348 0.4954]
Median for last 10 epochs: [0.2529 0.0736 0.5349 0.4991], Epochs since improvement 0
  9%|▉         | 47/500 [40:58<6:20:00, 50.33s/it] 10%|▉         | 48/500 [42:01<6:49:09, 54.31s/it]Epoch: 48 done with learning rate 7.24E-03, Train loss: 3.05E+06, Train scatter: [0.2128 0.0698 0.5433 0.5054]
L1 regularization loss: 6.92E-01, L2 regularization loss: 1.80E-01
Test scatter: [0.2366 0.0702 0.5348 0.4984], Lowest was [0.2252 0.0702 0.5348 0.4954]
Median for last 10 epochs: [0.2521 0.0736 0.5348 0.4991], Epochs since improvement 0
 10%|▉         | 49/500 [42:42<6:16:56, 50.15s/it]Epoch: 50 done with learning rate 7.60E-03, Train loss: 3.46E+06, Train scatter: [0.463  0.1356 0.5439 0.7636]
L1 regularization loss: 7.20E-01, L2 regularization loss: 1.94E-01
Test scatter: [0.4604 0.133  0.5353 0.7622], Lowest was [0.2252 0.0702 0.5348 0.4954]
Median for last 10 epochs: [0.2589 0.074  0.5348 0.5401], Epochs since improvement 2
 10%|█         | 50/500 [43:53<7:03:11, 56.43s/it] 10%|█         | 51/500 [44:33<6:26:08, 51.60s/it] 10%|█         | 52/500 [45:37<6:53:21, 55.36s/it]Epoch: 52 done with learning rate 7.94E-03, Train loss: 3.14E+06, Train scatter: [0.4055 0.0958 0.5438 0.551 ]
L1 regularization loss: 7.21E-01, L2 regularization loss: 1.97E-01
Test scatter: [0.4035 0.0938 0.5352 0.5411], Lowest was [0.2252 0.0702 0.5348 0.4954]
Median for last 10 epochs: [0.2589 0.074  0.5349 0.5401], Epochs since improvement 4
 11%|█         | 53/500 [46:17<6:18:37, 50.82s/it] 11%|█         | 54/500 [47:21<6:46:22, 54.67s/it]Epoch: 54 done with learning rate 8.26E-03, Train loss: 3.43E+06, Train scatter: [0.8698 0.1501 0.544  0.7283]
L1 regularization loss: 7.47E-01, L2 regularization loss: 2.10E-01
Test scatter: [0.8574 0.1469 0.5354 0.723 ], Lowest was [0.2252 0.0702 0.5348 0.4954]
Median for last 10 epochs: [0.4035 0.0938 0.5352 0.5411], Epochs since improvement 6
 11%|█         | 55/500 [48:01<6:13:25, 50.35s/it] 11%|█         | 56/500 [49:04<6:40:28, 54.12s/it]Epoch: 56 done with learning rate 8.56E-03, Train loss: 3.21E+06, Train scatter: [0.7508 0.0971 0.5439 0.6394]
L1 regularization loss: 7.52E-01, L2 regularization loss: 2.16E-01
Test scatter: [0.7407 0.0958 0.5354 0.6234], Lowest was [0.2252 0.0702 0.5348 0.4954]
Median for last 10 epochs: [0.4604 0.0958 0.5353 0.6234], Epochs since improvement 8
 11%|█▏        | 57/500 [49:45<6:09:11, 50.00s/it] 12%|█▏        | 58/500 [50:48<6:37:38, 53.98s/it]Epoch: 58 done with learning rate 8.83E-03, Train loss: 3.15E+06, Train scatter: [0.6214 0.0846 0.5438 0.5551]
L1 regularization loss: 7.56E-01, L2 regularization loss: 2.20E-01
Test scatter: [0.612  0.0849 0.5352 0.5528], Lowest was [0.2252 0.0702 0.5348 0.4954]
Median for last 10 epochs: [0.612  0.0958 0.5353 0.6234], Epochs since improvement 10
 12%|█▏        | 59/500 [51:29<6:07:07, 49.95s/it]Epoch: 60 done with learning rate 9.08E-03, Train loss: 3.11E+06, Train scatter: [0.553  0.0807 0.5436 0.5599]
L1 regularization loss: 7.61E-01, L2 regularization loss: 2.26E-01
Test scatter: [0.5425 0.0809 0.5351 0.5557], Lowest was [0.2252 0.0702 0.5348 0.4954]
Median for last 10 epochs: [0.612  0.0938 0.5352 0.5557], Epochs since improvement 12
 12%|█▏        | 60/500 [52:40<6:52:53, 56.30s/it] 12%|█▏        | 61/500 [53:20<6:17:51, 51.64s/it] 12%|█▏        | 62/500 [54:24<6:43:04, 55.22s/it]Epoch: 62 done with learning rate 9.31E-03, Train loss: 3.10E+06, Train scatter: [0.4365 0.0846 0.5435 0.5286]
L1 regularization loss: 7.65E-01, L2 regularization loss: 2.32E-01
Test scatter: [0.4291 0.0835 0.535  0.5243], Lowest was [0.2252 0.0702 0.5348 0.4954]
Median for last 10 epochs: [0.612  0.0849 0.5352 0.5557], Epochs since improvement 14
 13%|█▎        | 63/500 [55:05<6:10:27, 50.86s/it] 13%|█▎        | 64/500 [56:09<6:38:29, 54.84s/it]Epoch: 64 done with learning rate 9.50E-03, Train loss: 3.07E+06, Train scatter: [0.4286 0.0816 0.5434 0.5211]
L1 regularization loss: 7.69E-01, L2 regularization loss: 2.38E-01
Test scatter: [0.42   0.081  0.5349 0.5166], Lowest was [0.2252 0.0702 0.5348 0.4954]
Median for last 10 epochs: [0.5425 0.0835 0.5351 0.5528], Epochs since improvement 16
 13%|█▎        | 65/500 [56:49<6:06:22, 50.53s/it] 13%|█▎        | 66/500 [57:54<6:36:00, 54.75s/it]Epoch: 66 done with learning rate 9.66E-03, Train loss: 3.05E+06, Train scatter: [0.3166 0.0807 0.5434 0.5418]
L1 regularization loss: 7.75E-01, L2 regularization loss: 2.43E-01
Test scatter: [0.3162 0.0799 0.5349 0.5329], Lowest was [0.2252 0.0702 0.5348 0.4954]
Median for last 10 epochs: [0.4291 0.081  0.535  0.5329], Epochs since improvement 18
 13%|█▎        | 67/500 [58:34<6:04:09, 50.46s/it] 14%|█▎        | 68/500 [59:37<6:30:07, 54.18s/it]Epoch: 68 done with learning rate 9.80E-03, Train loss: 3.08E+06, Train scatter: [0.4168 0.1019 0.5433 0.5235]
L1 regularization loss: 7.91E-01, L2 regularization loss: 2.52E-01
Test scatter: [0.4133 0.0998 0.5348 0.5198], Lowest was [0.2252 0.0702 0.5348 0.4954]
Median for last 10 epochs: [0.42   0.081  0.5349 0.5243], Epochs since improvement 20
 14%|█▍        | 69/500 [1:00:18<5:59:49, 50.09s/it]Epoch: 70 done with learning rate 9.90E-03, Train loss: 3.03E+06, Train scatter: [0.2578 0.0791 0.5433 0.5148]
L1 regularization loss: 7.93E-01, L2 regularization loss: 2.56E-01
Test scatter: [0.2629 0.0787 0.5348 0.5097], Lowest was [0.2252 0.0702 0.5348 0.4954]
Median for last 10 epochs: [0.4133 0.081  0.5349 0.5198], Epochs since improvement 22
 14%|█▍        | 69/500 [1:01:29<6:24:04, 53.47s/it]
Exited after 70 epochs due to early stopping
3689.25 seconds spent training, 7.378 seconds per epoch. Processed 9438 trees per second
[0.2629324  0.07872883 0.53475523 0.5096427 ]
{'epoch_exit': 69, 'scatter_m_star': 0.2629324, 'lowest_m_star': 0.22519878, 'last20_m_star': 0.42457092, 'last10_m_star': 0.41325852, 'scatter_v_disk': 0.07872883, 'lowest_v_disk': 0.07021652, 'last20_v_disk': 0.08421543, 'last10_v_disk': 0.08098286, 'scatter_m_cold': 0.53475523, 'lowest_m_cold': 0.5347627, 'last20_m_cold': 0.5350294, 'last10_m_cold': 0.5348717, 'scatter_sfr_100': 0.5096427, 'lowest_sfr_100': 0.4953864, 'last20_sfr_100': 0.5369643, 'last10_sfr_100': 0.5198375}
Finished 1/1
Exited training after None epochs
Experiment 4 done: 4 / 6
Exploring ['conv_layers', 'batch_size']
Currently doing [  3 256]
{'experiment': 'GraphMerge', 'group': 'meta_pysr', 'move': False, 'model': 'MetaEdge', 'log': True, 'run_params': {'n_epochs': 500, 'n_trials': 1, 'batch_size': 256, 'val_epoch': 2, 'early_stopping': True, 'patience': 20, 'l1_lambda': 3e-05, 'l2_lambda': 3e-05, 'loss_func': 'GaussNd', 'metrics': 'test_multi_varrho', 'performance_plot': 'multi_base', 'save': True, 'seed': True, 'num_workers': 4}, 'learn_params': {'learning_rate': 0.01, 'schedule': 'onecycle', 'g_up': 1, 'g_down': 0.95, 'warmup': 4, 'period': 5, 'eta_min': 1e-05}, 'hyper_params': {'hidden_channels': 128, 'conv_layers': 3, 'conv_activation': 'relu', 'decode_activation': 'leakyrelu', 'decode_layers': 2, 'layernorm': True, 'agg': 'sum', 'variance': True, 'rho': 0, 'in_channels': 43, 'out_channels': 4}, 'data_params': {'case': 'vlarge_all_4t_z0.0_quantile_raw', 'targets': [0, 1, 2, 3], 'del_feats': [], 'split': 0.8, 'test': 0, 'scale': 0}, 'experiment_name': 'con3bat256'}
/home/cj1223/../../scratch/gpfs/cj1223/GraphResults/results_meta_pysr_120322/MetaEdge_vlarge_all_4t_z0.0_quantile_raw_twnltq
RelU conv activation
LeakyRelU decode activation
N_params 546504
Made folder for saving model
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [01:00<8:26:47, 60.94s/it]  0%|          | 2/500 [02:30<10:45:38, 77.79s/it]Epoch: 2 done with learning rate 4.17E-04, Train loss: 9.39E+07, Train scatter: [0.9351 0.1371 0.5441 0.9954]
L1 regularization loss: 7.36E-01, L2 regularization loss: 1.27E-01
Test scatter: [0.9195 0.1325 0.5355 0.9851], Lowest was [0.9195 0.1325 0.5355 0.9851]
Median for last 10 epochs: [0.9195 0.1325 0.5355 0.9851], Epochs since improvement 2
  1%|          | 3/500 [03:31<9:41:59, 70.26s/it]   1%|          | 4/500 [05:03<10:49:44, 78.60s/it]Epoch: 4 done with learning rate 4.67E-04, Train loss: 7.94E+07, Train scatter: [0.9332 0.0995 0.5439 0.9954]
L1 regularization loss: 7.41E-01, L2 regularization loss: 1.29E-01
Test scatter: [0.9176 0.0986 0.5353 0.9851], Lowest was [0.9176 0.0986 0.5353 0.9851]
Median for last 10 epochs: [0.9176 0.0986 0.5353 0.9851], Epochs since improvement 0
  1%|          | 5/500 [06:04<9:57:27, 72.42s/it]   1%|          | 6/500 [07:35<10:48:48, 78.80s/it]Epoch: 6 done with learning rate 5.51E-04, Train loss: 2.47E+08, Train scatter: [0.9354 0.1672 0.5441 0.9954]
L1 regularization loss: 7.51E-01, L2 regularization loss: 1.31E-01
Test scatter: [0.9198 0.1612 0.5355 0.9851], Lowest was [0.9176 0.0986 0.5353 0.9851]
Median for last 10 epochs: [0.9176 0.0986 0.5353 0.9851], Epochs since improvement 0
  1%|▏         | 7/500 [08:37<10:01:13, 73.17s/it]  2%|▏         | 8/500 [10:08<10:47:15, 78.93s/it]Epoch: 8 done with learning rate 6.67E-04, Train loss: 9.25E+07, Train scatter: [0.9356 0.15   0.5434 0.9954]
L1 regularization loss: 7.53E-01, L2 regularization loss: 1.32E-01
Test scatter: [0.9201 0.146  0.5348 0.9851], Lowest was [0.9176 0.0986 0.5348 0.9851]
Median for last 10 epochs: [0.9187 0.1223 0.535  0.9851], Epochs since improvement 0
  2%|▏         | 9/500 [11:10<10:00:49, 73.42s/it]Epoch: 10 done with learning rate 8.15E-04, Train loss: 8.78E+07, Train scatter: [0.9352 0.1335 0.538  0.9954]
L1 regularization loss: 7.57E-01, L2 regularization loss: 1.35E-01
Test scatter: [0.9196 0.1311 0.5294 0.9851], Lowest was [0.9176 0.0986 0.5294 0.9851]
Median for last 10 epochs: [0.9196 0.1311 0.5348 0.9851], Epochs since improvement 0
  2%|▏         | 10/500 [12:48<11:01:46, 81.03s/it]  2%|▏         | 11/500 [13:49<10:11:20, 75.01s/it]  2%|▏         | 12/500 [15:20<10:49:08, 79.81s/it]Epoch: 12 done with learning rate 9.94E-04, Train loss: 8.31E+07, Train scatter: [0.935  0.1182 0.5051 0.9954]
L1 regularization loss: 7.61E-01, L2 regularization loss: 1.38E-01
Test scatter: [0.9195 0.1181 0.4985 0.9851], Lowest was [0.9176 0.0986 0.4985 0.9851]
Median for last 10 epochs: [0.9196 0.1311 0.5348 0.9851], Epochs since improvement 0
  3%|▎         | 13/500 [16:21<10:02:59, 74.29s/it]  3%|▎         | 14/500 [17:52<10:41:06, 79.15s/it]Epoch: 14 done with learning rate 1.20E-03, Train loss: 7.72E+07, Train scatter: [0.8781 0.1042 0.4545 0.9954]
L1 regularization loss: 7.67E-01, L2 regularization loss: 1.42E-01
Test scatter: [0.862  0.1034 0.4416 0.985 ], Lowest was [0.862  0.0986 0.4416 0.985 ]
Median for last 10 epochs: [0.9196 0.1311 0.5294 0.9851], Epochs since improvement 0
  3%|▎         | 15/500 [18:53<9:56:28, 73.79s/it]   3%|▎         | 16/500 [20:24<10:36:20, 78.89s/it]Epoch: 16 done with learning rate 1.44E-03, Train loss: 7.02E+07, Train scatter: [0.5528 0.1004 0.4247 0.9953]
L1 regularization loss: 7.75E-01, L2 regularization loss: 1.49E-01
Test scatter: [0.5455 0.0997 0.4167 0.985 ], Lowest was [0.5455 0.0986 0.4167 0.985 ]
Median for last 10 epochs: [0.9195 0.1181 0.4985 0.9851], Epochs since improvement 0
  3%|▎         | 17/500 [21:25<9:52:05, 73.55s/it]   4%|▎         | 18/500 [22:56<10:33:12, 78.82s/it]Epoch: 18 done with learning rate 1.70E-03, Train loss: 5.25E+07, Train scatter: [0.437  0.0934 0.404  0.9757]
L1 regularization loss: 7.78E-01, L2 regularization loss: 1.53E-01
Test scatter: [0.4438 0.0933 0.4002 0.9673], Lowest was [0.4438 0.0933 0.4002 0.9673]
Median for last 10 epochs: [0.862  0.1034 0.4416 0.985 ], Epochs since improvement 0
  4%|▍         | 19/500 [23:58<9:50:29, 73.66s/it] Epoch: 20 done with learning rate 1.99E-03, Train loss: 7.32E+06, Train scatter: [0.3531 0.0873 0.395  0.6455]
L1 regularization loss: 7.84E-01, L2 regularization loss: 1.56E-01
Test scatter: [0.3624 0.0871 0.3912 0.6436], Lowest was [0.3624 0.0871 0.3912 0.6436]
Median for last 10 epochs: [0.5455 0.0997 0.4167 0.985 ], Epochs since improvement 0
  4%|▍         | 20/500 [25:35<10:47:05, 80.89s/it]  4%|▍         | 21/500 [26:37<9:59:17, 75.07s/it]   4%|▍         | 22/500 [28:08<10:35:59, 79.83s/it]Epoch: 22 done with learning rate 2.30E-03, Train loss: 5.08E+06, Train scatter: [0.3061 0.0802 0.3975 0.5768]
L1 regularization loss: 7.89E-01, L2 regularization loss: 1.59E-01
Test scatter: [0.3188 0.0802 0.3954 0.5819], Lowest was [0.3188 0.0802 0.3912 0.5819]
Median for last 10 epochs: [0.4438 0.0933 0.4002 0.9673], Epochs since improvement 0
  5%|▍         | 23/500 [29:09<9:50:00, 74.22s/it]   5%|▍         | 24/500 [30:39<10:27:20, 79.08s/it]Epoch: 24 done with learning rate 2.63E-03, Train loss: 4.50E+06, Train scatter: [0.2937 0.0766 0.3753 0.571 ]
L1 regularization loss: 7.94E-01, L2 regularization loss: 1.62E-01
Test scatter: [0.2991 0.0767 0.3763 0.5707], Lowest was [0.2991 0.0767 0.3763 0.5707]
Median for last 10 epochs: [0.3624 0.0871 0.3954 0.6436], Epochs since improvement 0
  5%|▌         | 25/500 [31:41<9:44:20, 73.81s/it]   5%|▌         | 26/500 [33:12<10:23:33, 78.93s/it]Epoch: 26 done with learning rate 2.98E-03, Train loss: 4.19E+06, Train scatter: [0.2573 0.0762 0.3609 0.5469]
L1 regularization loss: 7.99E-01, L2 regularization loss: 1.65E-01
Test scatter: [0.2627 0.0763 0.3613 0.546 ], Lowest was [0.2627 0.0763 0.3613 0.546 ]
Median for last 10 epochs: [0.3188 0.0802 0.3912 0.5819], Epochs since improvement 0
  5%|▌         | 27/500 [34:14<9:41:41, 73.79s/it]   6%|▌         | 28/500 [35:44<10:19:40, 78.77s/it]Epoch: 28 done with learning rate 3.34E-03, Train loss: 4.12E+06, Train scatter: [0.2575 0.0736 0.359  0.5935]
L1 regularization loss: 8.05E-01, L2 regularization loss: 1.69E-01
Test scatter: [0.2634 0.0745 0.3582 0.594 ], Lowest was [0.2627 0.0745 0.3582 0.546 ]
Median for last 10 epochs: [0.2991 0.0767 0.3763 0.5819], Epochs since improvement 0
  6%|▌         | 29/500 [36:45<9:37:11, 73.53s/it] Epoch: 30 done with learning rate 3.72E-03, Train loss: 3.82E+06, Train scatter: [0.2818 0.074  0.3484 0.5354]
L1 regularization loss: 8.12E-01, L2 regularization loss: 1.73E-01
Test scatter: [0.2866 0.0743 0.3486 0.5363], Lowest was [0.2627 0.0743 0.3486 0.5363]
Median for last 10 epochs: [0.2866 0.0763 0.3613 0.5707], Epochs since improvement 0
  6%|▌         | 30/500 [38:23<10:32:08, 80.70s/it]  6%|▌         | 31/500 [39:24<9:45:43, 74.93s/it]   6%|▋         | 32/500 [40:55<10:21:52, 79.73s/it]Epoch: 32 done with learning rate 4.10E-03, Train loss: 3.81E+06, Train scatter: [0.2423 0.0755 0.3455 0.5336]
L1 regularization loss: 8.18E-01, L2 regularization loss: 1.76E-01
Test scatter: [0.2481 0.0763 0.3501 0.5354], Lowest was [0.2481 0.0743 0.3486 0.5354]
Median for last 10 epochs: [0.2634 0.0763 0.3582 0.546 ], Epochs since improvement 0
  7%|▋         | 33/500 [41:57<9:38:06, 74.27s/it]   7%|▋         | 34/500 [43:27<10:14:59, 79.18s/it]Epoch: 34 done with learning rate 4.50E-03, Train loss: 3.53E+06, Train scatter: [0.2298 0.0708 0.3314 0.5088]
L1 regularization loss: 8.25E-01, L2 regularization loss: 1.81E-01
Test scatter: [0.2358 0.0697 0.3328 0.509 ], Lowest was [0.2358 0.0697 0.3328 0.509 ]
Median for last 10 epochs: [0.2627 0.0745 0.3501 0.5363], Epochs since improvement 0
  7%|▋         | 35/500 [44:29<9:32:40, 73.89s/it]   7%|▋         | 36/500 [45:59<10:10:12, 78.91s/it]Epoch: 36 done with learning rate 4.90E-03, Train loss: 3.47E+06, Train scatter: [0.2525 0.0697 0.3529 0.5036]
L1 regularization loss: 8.31E-01, L2 regularization loss: 1.85E-01
Test scatter: [0.2594 0.069  0.3547 0.5018], Lowest was [0.2358 0.069  0.3328 0.5018]
Median for last 10 epochs: [0.2594 0.0743 0.3501 0.5354], Epochs since improvement 0
  7%|▋         | 37/500 [47:01<9:28:23, 73.66s/it]   8%|▊         | 38/500 [48:32<10:08:30, 79.03s/it]Epoch: 38 done with learning rate 5.30E-03, Train loss: 3.24E+06, Train scatter: [0.2993 0.069  0.3558 0.5591]
L1 regularization loss: 8.38E-01, L2 regularization loss: 1.91E-01
Test scatter: [0.2999 0.0689 0.3563 0.5592], Lowest was [0.2358 0.0689 0.3328 0.5018]
Median for last 10 epochs: [0.2594 0.0697 0.3501 0.5354], Epochs since improvement 0
  8%|▊         | 39/500 [49:34<9:26:50, 73.77s/it] Epoch: 40 done with learning rate 5.70E-03, Train loss: 3.25E+06, Train scatter: [0.2652 0.0684 0.3332 0.5594]
L1 regularization loss: 8.47E-01, L2 regularization loss: 1.98E-01
Test scatter: [0.2727 0.069  0.3371 0.5677], Lowest was [0.2358 0.0689 0.3328 0.5018]
Median for last 10 epochs: [0.2594 0.069  0.3501 0.5354], Epochs since improvement 2
  8%|▊         | 40/500 [51:11<10:20:22, 80.92s/it]  8%|▊         | 41/500 [52:13<9:34:29, 75.10s/it]   8%|▊         | 42/500 [53:44<10:10:03, 79.92s/it]Epoch: 42 done with learning rate 6.10E-03, Train loss: 3.49E+06, Train scatter: [0.2683 0.0782 0.356  0.5213]
L1 regularization loss: 8.61E-01, L2 regularization loss: 2.07E-01
Test scatter: [0.2755 0.0778 0.3539 0.521 ], Lowest was [0.2358 0.0689 0.3328 0.5018]
Median for last 10 epochs: [0.2727 0.069  0.3539 0.521 ], Epochs since improvement 4
  9%|▊         | 43/500 [54:45<9:25:57, 74.30s/it]   9%|▉         | 44/500 [56:16<10:02:15, 79.24s/it]Epoch: 44 done with learning rate 6.49E-03, Train loss: 3.02E+06, Train scatter: [0.2227 0.0663 0.3268 0.4992]
L1 regularization loss: 8.72E-01, L2 regularization loss: 2.15E-01
Test scatter: [0.2319 0.0661 0.3326 0.4997], Lowest was [0.2319 0.0661 0.3326 0.4997]
Median for last 10 epochs: [0.2727 0.069  0.3539 0.521 ], Epochs since improvement 0
  9%|▉         | 45/500 [57:17<9:20:06, 73.86s/it]   9%|▉         | 46/500 [58:48<9:56:40, 78.86s/it]Epoch: 46 done with learning rate 6.87E-03, Train loss: 2.93E+06, Train scatter: [0.2086 0.0631 0.3171 0.4867]
L1 regularization loss: 8.80E-01, L2 regularization loss: 2.22E-01
Test scatter: [0.2176 0.0631 0.3232 0.4876], Lowest was [0.2176 0.0631 0.3232 0.4876]
Median for last 10 epochs: [0.2727 0.0689 0.3371 0.521 ], Epochs since improvement 0
  9%|▉         | 47/500 [59:50<9:16:49, 73.75s/it] 10%|▉         | 48/500 [1:01:21<9:55:26, 79.04s/it]Epoch: 48 done with learning rate 7.24E-03, Train loss: 2.58E+06, Train scatter: [0.2224 0.0663 0.3207 0.4891]
L1 regularization loss: 8.90E-01, L2 regularization loss: 2.29E-01
Test scatter: [0.2318 0.066  0.3242 0.489 ], Lowest was [0.2176 0.0631 0.3232 0.4876]
Median for last 10 epochs: [0.2319 0.0661 0.3326 0.4997], Epochs since improvement 2
 10%|▉         | 49/500 [1:02:23<9:14:20, 73.75s/it]Epoch: 50 done with learning rate 7.60E-03, Train loss: 2.58E+06, Train scatter: [0.2062 0.063  0.3227 0.5069]
L1 regularization loss: 9.03E-01, L2 regularization loss: 2.38E-01
Test scatter: [0.2149 0.0627 0.3256 0.5095], Lowest was [0.2149 0.0627 0.3232 0.4876]
Median for last 10 epochs: [0.2318 0.066  0.3256 0.4997], Epochs since improvement 0
 10%|█         | 50/500 [1:04:02<10:10:45, 81.43s/it] 10%|█         | 51/500 [1:05:03<9:24:40, 75.46s/it]  10%|█         | 52/500 [1:06:34<9:56:31, 79.89s/it]Epoch: 52 done with learning rate 7.94E-03, Train loss: 2.07E+06, Train scatter: [0.2026 0.0621 0.3131 0.4835]
L1 regularization loss: 9.14E-01, L2 regularization loss: 2.46E-01
Test scatter: [0.2132 0.0629 0.3188 0.4832], Lowest was [0.2132 0.0627 0.3188 0.4832]
Median for last 10 epochs: [0.2176 0.0631 0.3242 0.489 ], Epochs since improvement 0
 11%|█         | 53/500 [1:07:35<9:14:10, 74.39s/it] 11%|█         | 54/500 [1:09:05<9:47:55, 79.09s/it]Epoch: 54 done with learning rate 8.26E-03, Train loss: 1.90E+06, Train scatter: [0.2258 0.067  0.3181 0.48  ]
L1 regularization loss: 9.20E-01, L2 regularization loss: 2.53E-01
Test scatter: [0.2312 0.0665 0.3227 0.4787], Lowest was [0.2132 0.0627 0.3188 0.4787]
Median for last 10 epochs: [0.2176 0.0631 0.3232 0.4876], Epochs since improvement 0
 11%|█         | 55/500 [1:10:07<9:07:37, 73.84s/it] 11%|█         | 56/500 [1:11:37<9:43:20, 78.83s/it]Epoch: 56 done with learning rate 8.56E-03, Train loss: 1.83E+06, Train scatter: [0.3009 0.0662 0.3205 0.4852]
L1 regularization loss: 9.30E-01, L2 regularization loss: 2.61E-01
Test scatter: [0.2937 0.0663 0.3281 0.489 ], Lowest was [0.2132 0.0627 0.3188 0.4787]
Median for last 10 epochs: [0.2312 0.066  0.3242 0.489 ], Epochs since improvement 2
 11%|█▏        | 57/500 [1:12:39<9:03:47, 73.65s/it] 12%|█▏        | 58/500 [1:14:09<9:39:39, 78.69s/it]Epoch: 58 done with learning rate 8.83E-03, Train loss: 1.73E+06, Train scatter: [0.2022 0.0596 0.3102 0.4785]
L1 regularization loss: 9.43E-01, L2 regularization loss: 2.72E-01
Test scatter: [0.212  0.0602 0.314  0.4777], Lowest was [0.212  0.0602 0.314  0.4777]
Median for last 10 epochs: [0.2149 0.0629 0.3227 0.4832], Epochs since improvement 0
 12%|█▏        | 59/500 [1:15:11<9:00:30, 73.54s/it]Epoch: 60 done with learning rate 9.08E-03, Train loss: 1.72E+06, Train scatter: [0.2021 0.0673 0.3189 0.4847]
L1 regularization loss: 9.58E-01, L2 regularization loss: 2.85E-01
Test scatter: [0.2176 0.0678 0.3252 0.4876], Lowest was [0.212  0.0602 0.314  0.4777]
Median for last 10 epochs: [0.2176 0.0663 0.3227 0.4832], Epochs since improvement 2
 12%|█▏        | 60/500 [1:16:49<9:53:39, 80.95s/it] 12%|█▏        | 61/500 [1:17:51<9:09:43, 75.13s/it] 12%|█▏        | 62/500 [1:19:21<9:41:42, 79.69s/it]Epoch: 62 done with learning rate 9.31E-03, Train loss: 1.86E+06, Train scatter: [0.2115 0.0632 0.3303 0.4745]
L1 regularization loss: 9.74E-01, L2 regularization loss: 2.99E-01
Test scatter: [0.2232 0.0637 0.3329 0.4734], Lowest was [0.212  0.0602 0.314  0.4734]
Median for last 10 epochs: [0.2232 0.0663 0.3252 0.4787], Epochs since improvement 0
 13%|█▎        | 63/500 [1:20:23<9:00:58, 74.27s/it] 13%|█▎        | 64/500 [1:21:54<9:36:53, 79.39s/it]Epoch: 64 done with learning rate 9.50E-03, Train loss: 1.55E+06, Train scatter: [0.1971 0.0598 0.3098 0.4723]
L1 regularization loss: 9.85E-01, L2 regularization loss: 3.13E-01
Test scatter: [0.216  0.06   0.3143 0.4728], Lowest was [0.212  0.06   0.314  0.4728]
Median for last 10 epochs: [0.2176 0.0637 0.3252 0.4777], Epochs since improvement 0
 13%|█▎        | 65/500 [1:22:55<8:56:31, 74.00s/it] 13%|█▎        | 66/500 [1:24:26<9:31:01, 78.94s/it]Epoch: 66 done with learning rate 9.66E-03, Train loss: 1.47E+06, Train scatter: [0.2345 0.0675 0.3233 0.4876]
L1 regularization loss: 9.99E-01, L2 regularization loss: 3.27E-01
Test scatter: [0.2654 0.0681 0.3277 0.4823], Lowest was [0.212  0.06   0.314  0.4728]
Median for last 10 epochs: [0.2176 0.0637 0.3252 0.4777], Epochs since improvement 2
 13%|█▎        | 67/500 [1:25:27<8:52:08, 73.74s/it] 14%|█▎        | 68/500 [1:26:58<9:26:58, 78.75s/it]Epoch: 68 done with learning rate 9.80E-03, Train loss: 1.47E+06, Train scatter: [0.2256 0.0665 0.3167 0.4776]
L1 regularization loss: 1.01E+00, L2 regularization loss: 3.43E-01
Test scatter: [0.234  0.0651 0.322  0.479 ], Lowest was [0.212  0.06   0.314  0.4728]
Median for last 10 epochs: [0.2232 0.0651 0.3252 0.479 ], Epochs since improvement 4
 14%|█▍        | 69/500 [1:27:59<8:47:43, 73.46s/it]Epoch: 70 done with learning rate 9.90E-03, Train loss: 1.39E+06, Train scatter: [0.2028 0.0605 0.3141 0.4594]
L1 regularization loss: 1.04E+00, L2 regularization loss: 3.63E-01
Test scatter: [0.2302 0.0613 0.3229 0.4624], Lowest was [0.212  0.06   0.314  0.4624]
Median for last 10 epochs: [0.2302 0.0637 0.3229 0.4734], Epochs since improvement 0
 14%|█▍        | 70/500 [1:29:38<9:40:49, 81.05s/it] 14%|█▍        | 71/500 [1:30:40<8:58:05, 75.26s/it] 14%|█▍        | 72/500 [1:32:10<9:29:58, 79.90s/it]Epoch: 72 done with learning rate 9.96E-03, Train loss: 1.22E+06, Train scatter: [0.1929 0.0603 0.2977 0.4499]
L1 regularization loss: 1.10E+00, L2 regularization loss: 4.04E-01
Test scatter: [0.2246 0.0628 0.3027 0.4501], Lowest was [0.212  0.06   0.3027 0.4501]
Median for last 10 epochs: [0.2302 0.0628 0.322  0.4728], Epochs since improvement 0
 15%|█▍        | 73/500 [1:33:11<8:48:40, 74.29s/it] 15%|█▍        | 74/500 [1:34:43<9:23:07, 79.31s/it]Epoch: 74 done with learning rate 1.00E-02, Train loss: 1.23E+06, Train scatter: [0.2178 0.0608 0.3107 0.4611]
L1 regularization loss: 1.11E+00, L2 regularization loss: 4.20E-01
Test scatter: [0.2409 0.0608 0.3109 0.454 ], Lowest was [0.212  0.06   0.3027 0.4501]
Median for last 10 epochs: [0.234  0.0628 0.322  0.4624], Epochs since improvement 2
 15%|█▌        | 75/500 [1:35:44<8:44:23, 74.03s/it] 15%|█▌        | 76/500 [1:37:15<9:18:08, 78.98s/it]Epoch: 76 done with learning rate 1.00E-02, Train loss: 1.09E+06, Train scatter: [0.1987 0.0526 0.2912 0.4385]
L1 regularization loss: 1.13E+00, L2 regularization loss: 4.40E-01
Test scatter: [0.2026 0.0533 0.2954 0.438 ], Lowest was [0.2026 0.0533 0.2954 0.438 ]
Median for last 10 epochs: [0.2302 0.0613 0.3109 0.454 ], Epochs since improvement 0
 15%|█▌        | 77/500 [1:38:16<8:39:18, 73.66s/it] 16%|█▌        | 78/500 [1:39:47<9:15:04, 78.92s/it]Epoch: 78 done with learning rate 1.00E-02, Train loss: 8.99E+05, Train scatter: [0.1852 0.0512 0.2828 0.4345]
L1 regularization loss: 1.14E+00, L2 regularization loss: 4.51E-01
Test scatter: [0.2086 0.0519 0.2841 0.4362], Lowest was [0.2026 0.0519 0.2841 0.4362]
Median for last 10 epochs: [0.2246 0.0608 0.3027 0.4501], Epochs since improvement 0
 16%|█▌        | 79/500 [1:40:49<8:37:04, 73.69s/it]Epoch: 80 done with learning rate 1.00E-02, Train loss: 8.32E+05, Train scatter: [0.1925 0.0523 0.2802 0.4477]
L1 regularization loss: 1.16E+00, L2 regularization loss: 4.67E-01
Test scatter: [0.2076 0.0524 0.283  0.4509], Lowest was [0.2026 0.0519 0.283  0.4362]
Median for last 10 epochs: [0.2086 0.0533 0.2954 0.4501], Epochs since improvement 0
 16%|█▌        | 80/500 [1:42:27<9:28:13, 81.18s/it] 16%|█▌        | 81/500 [1:43:29<8:45:20, 75.23s/it] 16%|█▋        | 82/500 [1:44:59<9:16:02, 79.81s/it]Epoch: 82 done with learning rate 9.99E-03, Train loss: 6.67E+05, Train scatter: [0.2073 0.064  0.2849 0.4413]
L1 regularization loss: 1.17E+00, L2 regularization loss: 4.80E-01
Test scatter: [0.2116 0.0637 0.2859 0.4419], Lowest was [0.2026 0.0519 0.283  0.4362]
Median for last 10 epochs: [0.2086 0.0533 0.2859 0.4419], Epochs since improvement 2
 17%|█▋        | 83/500 [1:46:01<8:36:58, 74.39s/it] 17%|█▋        | 84/500 [1:47:32<9:11:24, 79.53s/it]Epoch: 84 done with learning rate 9.99E-03, Train loss: 5.66E+05, Train scatter: [0.1781 0.0549 0.2741 0.4252]
L1 regularization loss: 1.18E+00, L2 regularization loss: 4.99E-01
Test scatter: [0.1791 0.0556 0.2794 0.4241], Lowest was [0.1791 0.0519 0.2794 0.4241]
Median for last 10 epochs: [0.2076 0.0533 0.2841 0.438 ], Epochs since improvement 0
 17%|█▋        | 85/500 [1:48:34<8:32:40, 74.12s/it] 17%|█▋        | 86/500 [1:50:05<9:05:40, 79.08s/it]Epoch: 86 done with learning rate 9.98E-03, Train loss: 4.01E+05, Train scatter: [0.1696 0.05   0.2617 0.4128]
L1 regularization loss: 1.19E+00, L2 regularization loss: 5.11E-01
Test scatter: [0.1711 0.0505 0.2678 0.4083], Lowest was [0.1711 0.0505 0.2678 0.4083]
Median for last 10 epochs: [0.2076 0.0524 0.283  0.4362], Epochs since improvement 0
 17%|█▋        | 87/500 [1:51:06<8:27:38, 73.75s/it] 18%|█▊        | 88/500 [1:52:36<9:00:11, 78.67s/it]Epoch: 88 done with learning rate 9.98E-03, Train loss: 2.50E+05, Train scatter: [0.1622 0.0518 0.2611 0.4131]
L1 regularization loss: 1.21E+00, L2 regularization loss: 5.27E-01
Test scatter: [0.1639 0.0526 0.2651 0.4143], Lowest was [0.1639 0.0505 0.2651 0.4083]
Median for last 10 epochs: [0.1791 0.0526 0.2794 0.4241], Epochs since improvement 0
 18%|█▊        | 89/500 [1:53:38<8:23:44, 73.54s/it]Epoch: 90 done with learning rate 9.97E-03, Train loss: 1.04E+05, Train scatter: [0.2011 0.0591 0.2705 0.4391]
L1 regularization loss: 1.22E+00, L2 regularization loss: 5.43E-01
Test scatter: [0.2009 0.0597 0.2787 0.4351], Lowest was [0.1639 0.0505 0.2651 0.4083]
Median for last 10 epochs: [0.1791 0.0556 0.2787 0.4241], Epochs since improvement 2
 18%|█▊        | 90/500 [1:55:16<9:14:12, 81.10s/it] 18%|█▊        | 91/500 [1:56:18<8:32:31, 75.19s/it] 18%|█▊        | 92/500 [1:57:49<9:03:51, 79.98s/it]Epoch: 92 done with learning rate 9.96E-03, Train loss: 6.52E+04, Train scatter: [0.1718 0.0511 0.2689 0.4297]
L1 regularization loss: 1.27E+00, L2 regularization loss: 5.91E-01
Test scatter: [0.1889 0.0513 0.2715 0.4241], Lowest was [0.1639 0.0505 0.2651 0.4083]
Median for last 10 epochs: [0.1791 0.0526 0.2715 0.4241], Epochs since improvement 4
 19%|█▊        | 93/500 [1:58:51<8:25:35, 74.53s/it] 19%|█▉        | 94/500 [2:00:21<8:56:48, 79.33s/it]Epoch: 94 done with learning rate 9.95E-03, Train loss: -1.60E+05, Train scatter: [0.1489 0.0452 0.2573 0.4058]
L1 regularization loss: 1.27E+00, L2 regularization loss: 6.02E-01
Test scatter: [0.1745 0.045  0.2596 0.3984], Lowest was [0.1639 0.045  0.2596 0.3984]
Median for last 10 epochs: [0.1745 0.0513 0.2678 0.4143], Epochs since improvement 0
 19%|█▉        | 95/500 [2:01:23<8:18:53, 73.91s/it] 19%|█▉        | 96/500 [2:02:53<8:50:27, 78.78s/it]Epoch: 96 done with learning rate 9.94E-03, Train loss: -2.07E+05, Train scatter: [0.1485 0.0445 0.2529 0.4041]
L1 regularization loss: 1.29E+00, L2 regularization loss: 6.19E-01
Test scatter: [0.1615 0.0442 0.2519 0.4021], Lowest was [0.1615 0.0442 0.2519 0.3984]
Median for last 10 epochs: [0.1745 0.0513 0.2651 0.4143], Epochs since improvement 0
 19%|█▉        | 97/500 [2:03:54<8:13:39, 73.50s/it] 20%|█▉        | 98/500 [2:05:25<8:47:01, 78.66s/it]Epoch: 98 done with learning rate 9.93E-03, Train loss: -2.83E+05, Train scatter: [0.1552 0.0484 0.2811 0.4122]
L1 regularization loss: 1.30E+00, L2 regularization loss: 6.37E-01
Test scatter: [0.1867 0.0482 0.2802 0.4112], Lowest was [0.1615 0.0442 0.2519 0.3984]
Median for last 10 epochs: [0.1867 0.0482 0.2715 0.4112], Epochs since improvement 2
 20%|█▉        | 99/500 [2:06:26<8:10:37, 73.41s/it]Epoch: 100 done with learning rate 9.91E-03, Train loss: -3.20E+05, Train scatter: [0.1367 0.0456 0.2374 0.4033]
L1 regularization loss: 1.32E+00, L2 regularization loss: 6.57E-01
Test scatter: [0.1397 0.0453 0.2398 0.3967], Lowest was [0.1397 0.0442 0.2398 0.3967]
Median for last 10 epochs: [0.1745 0.0453 0.2596 0.4021], Epochs since improvement 0
 20%|██        | 100/500 [2:08:04<8:59:10, 80.88s/it] 20%|██        | 101/500 [2:09:05<8:18:48, 75.01s/it] 20%|██        | 102/500 [2:10:36<8:48:15, 79.64s/it]Epoch: 102 done with learning rate 9.90E-03, Train loss: -3.33E+05, Train scatter: [0.1892 0.0561 0.2805 0.4342]
L1 regularization loss: 1.34E+00, L2 regularization loss: 6.80E-01
Test scatter: [0.2105 0.0563 0.2811 0.4359], Lowest was [0.1397 0.0442 0.2398 0.3967]
Median for last 10 epochs: [0.1745 0.0453 0.2596 0.4021], Epochs since improvement 2
 21%|██        | 103/500 [2:11:37<8:10:25, 74.12s/it] 21%|██        | 104/500 [2:13:07<8:41:26, 79.01s/it]Epoch: 104 done with learning rate 9.89E-03, Train loss: -3.29E+05, Train scatter: [0.1547 0.0545 0.251  0.4258]
L1 regularization loss: 1.39E+00, L2 regularization loss: 7.32E-01
Test scatter: [0.1602 0.0535 0.2504 0.4218], Lowest was [0.1397 0.0442 0.2398 0.3967]
Median for last 10 epochs: [0.1615 0.0482 0.2519 0.4112], Epochs since improvement 4
 21%|██        | 105/500 [2:14:09<8:05:14, 73.71s/it] 21%|██        | 106/500 [2:15:39<8:37:18, 78.78s/it]Epoch: 106 done with learning rate 9.87E-03, Train loss: -3.66E+05, Train scatter: [0.1291 0.0438 0.237  0.4133]
L1 regularization loss: 1.40E+00, L2 regularization loss: 7.58E-01
Test scatter: [0.1383 0.0435 0.2411 0.4113], Lowest was [0.1383 0.0435 0.2398 0.3967]
Median for last 10 epochs: [0.1602 0.0482 0.2504 0.4113], Epochs since improvement 0
 21%|██▏       | 107/500 [2:16:41<8:01:18, 73.48s/it] 22%|██▏       | 108/500 [2:18:10<8:32:16, 78.41s/it]Epoch: 108 done with learning rate 9.85E-03, Train loss: -9.48E+04, Train scatter: [0.4311 0.0838 0.431  0.5802]
L1 regularization loss: 1.49E+00, L2 regularization loss: 8.62E-01
Test scatter: [0.4277 0.0833 0.4282 0.5769], Lowest was [0.1383 0.0435 0.2398 0.3967]
Median for last 10 epochs: [0.1602 0.0535 0.2504 0.4218], Epochs since improvement 2
 22%|██▏       | 109/500 [2:19:12<7:57:31, 73.28s/it]Epoch: 110 done with learning rate 9.83E-03, Train loss: -2.66E+05, Train scatter: [0.1821 0.0591 0.3287 0.4781]
L1 regularization loss: 1.53E+00, L2 regularization loss: 9.12E-01
Test scatter: [0.1896 0.0584 0.3256 0.4713], Lowest was [0.1383 0.0435 0.2398 0.3967]
Median for last 10 epochs: [0.1896 0.0563 0.2811 0.4359], Epochs since improvement 4
 22%|██▏       | 110/500 [2:20:50<8:45:50, 80.90s/it] 22%|██▏       | 111/500 [2:21:52<8:06:30, 75.04s/it] 22%|██▏       | 112/500 [2:23:22<8:34:41, 79.59s/it]Epoch: 112 done with learning rate 9.81E-03, Train loss: -3.31E+05, Train scatter: [0.1604 0.0487 0.261  0.45  ]
L1 regularization loss: 1.53E+00, L2 regularization loss: 9.25E-01
Test scatter: [0.1655 0.0481 0.2612 0.4404], Lowest was [0.1383 0.0435 0.2398 0.3967]
Median for last 10 epochs: [0.1655 0.0535 0.2612 0.4404], Epochs since improvement 6
 23%|██▎       | 113/500 [2:24:23<7:57:42, 74.06s/it] 23%|██▎       | 114/500 [2:25:54<8:29:18, 79.17s/it]Epoch: 114 done with learning rate 9.79E-03, Train loss: -3.50E+05, Train scatter: [0.1776 0.0521 0.2612 0.4452]
L1 regularization loss: 1.55E+00, L2 regularization loss: 9.46E-01
Test scatter: [0.1819 0.0514 0.2626 0.4361], Lowest was [0.1383 0.0435 0.2398 0.3967]
Median for last 10 epochs: [0.1819 0.0514 0.2626 0.4404], Epochs since improvement 8
 23%|██▎       | 115/500 [2:26:56<7:53:35, 73.81s/it] 23%|██▎       | 116/500 [2:28:26<8:23:31, 78.67s/it]Epoch: 116 done with learning rate 9.77E-03, Train loss: -3.69E+05, Train scatter: [0.1492 0.0496 0.3971 0.4423]
L1 regularization loss: 1.56E+00, L2 regularization loss: 9.67E-01
Test scatter: [0.1517 0.049  0.3874 0.4329], Lowest was [0.1383 0.0435 0.2398 0.3967]
Median for last 10 epochs: [0.1819 0.0514 0.3256 0.4404], Epochs since improvement 10
 23%|██▎       | 117/500 [2:29:27<7:48:46, 73.44s/it] 24%|██▎       | 118/500 [2:30:58<8:21:13, 78.73s/it]Epoch: 118 done with learning rate 9.75E-03, Train loss: -3.74E+05, Train scatter: [0.1492 0.047  0.2536 0.4335]
L1 regularization loss: 1.58E+00, L2 regularization loss: 9.96E-01
Test scatter: [0.1567 0.0462 0.2516 0.4245], Lowest was [0.1383 0.0435 0.2398 0.3967]
Median for last 10 epochs: [0.1655 0.049  0.2626 0.4361], Epochs since improvement 12
 24%|██▍       | 119/500 [2:31:59<7:46:49, 73.52s/it]Epoch: 120 done with learning rate 9.73E-03, Train loss: -3.85E+05, Train scatter: [0.1623 0.0524 0.243  0.4311]
L1 regularization loss: 1.61E+00, L2 regularization loss: 1.04E+00
Test scatter: [0.1908 0.0516 0.2451 0.426 ], Lowest was [0.1383 0.0435 0.2398 0.3967]
Median for last 10 epochs: [0.1655 0.049  0.2612 0.4329], Epochs since improvement 14
 24%|██▍       | 120/500 [2:33:38<8:34:10, 81.18s/it] 24%|██▍       | 121/500 [2:34:40<7:55:38, 75.30s/it] 24%|██▍       | 122/500 [2:36:10<8:23:19, 79.89s/it]Epoch: 122 done with learning rate 9.70E-03, Train loss: -5.41E+04, Train scatter: [0.7009 0.1113 0.4563 0.906 ]
L1 regularization loss: 1.83E+00, L2 regularization loss: 1.22E+00
Test scatter: [0.7173 0.1036 0.4481 0.881 ], Lowest was [0.1383 0.0435 0.2398 0.3967]
Median for last 10 epochs: [0.1819 0.0514 0.2626 0.4329], Epochs since improvement 16
 25%|██▍       | 123/500 [2:37:12<7:47:25, 74.39s/it] 25%|██▍       | 124/500 [2:38:43<8:18:17, 79.52s/it]Epoch: 124 done with learning rate 9.68E-03, Train loss: -2.50E+05, Train scatter: [0.4612 0.0592 0.306  0.5111]
L1 regularization loss: 1.87E+00, L2 regularization loss: 1.29E+00
Test scatter: [0.4515 0.0595 0.3074 0.9033], Lowest was [0.1383 0.0435 0.2398 0.3967]
Median for last 10 epochs: [0.1908 0.0516 0.3074 0.4329], Epochs since improvement 18
 25%|██▌       | 125/500 [2:39:45<7:42:40, 74.03s/it] 25%|██▌       | 126/500 [2:41:15<8:12:23, 78.99s/it]Epoch: 126 done with learning rate 9.65E-03, Train loss: -2.90E+05, Train scatter: [0.3805 0.0558 0.2807 0.4928]
L1 regularization loss: 1.92E+00, L2 regularization loss: 1.36E+00
Test scatter: [0.3681 0.0557 0.2821 0.4873], Lowest was [0.1383 0.0435 0.2398 0.3967]
Median for last 10 epochs: [0.3681 0.0557 0.2821 0.4873], Epochs since improvement 20
 25%|██▌       | 127/500 [2:42:17<7:38:21, 73.73s/it] 25%|██▌       | 127/500 [2:43:47<8:01:03, 77.38s/it]
Epoch: 128 done with learning rate 9.62E-03, Train loss: -3.40E+05, Train scatter: [0.1765 0.0544 0.2584 0.4636]
L1 regularization loss: 1.94E+00, L2 regularization loss: 1.41E+00
Test scatter: [0.1808 0.0536 0.2591 0.4564], Lowest was [0.1383 0.0435 0.2398 0.3967]
Median for last 10 epochs: [0.3681 0.0557 0.2821 0.4873], Epochs since improvement 22
Exited after 128 epochs due to early stopping
9827.68 seconds spent training, 19.655 seconds per epoch. Processed 3543 trees per second
[0.18075275 0.05360085 0.25907862 0.45639256]
{'epoch_exit': 127, 'scatter_m_star': 0.18075275, 'lowest_m_star': 0.13834734, 'last20_m_star': 0.18573442, 'last10_m_star': 0.3681205, 'scatter_v_disk': 0.053600848, 'lowest_v_disk': 0.04345484, 'last20_v_disk': 0.052618504, 'last10_v_disk': 0.055692367, 'scatter_m_cold': 0.25907862, 'lowest_m_cold': 0.23982887, 'last20_m_cold': 0.2723447, 'last10_m_cold': 0.28210866, 'scatter_sfr_100': 0.45639256, 'lowest_sfr_100': 0.39668205, 'last20_sfr_100': 0.44842649, 'last10_sfr_100': 0.48728472}
Finished 1/1
Exited training after None epochs
Experiment 5 done: 5 / 6
Exploring ['conv_layers', 'batch_size']
Currently doing [  3 512]
{'experiment': 'GraphMerge', 'group': 'meta_pysr', 'move': False, 'model': 'MetaEdge', 'log': True, 'run_params': {'n_epochs': 500, 'n_trials': 1, 'batch_size': 512, 'val_epoch': 2, 'early_stopping': True, 'patience': 20, 'l1_lambda': 3e-05, 'l2_lambda': 3e-05, 'loss_func': 'GaussNd', 'metrics': 'test_multi_varrho', 'performance_plot': 'multi_base', 'save': True, 'seed': True, 'num_workers': 4}, 'learn_params': {'learning_rate': 0.01, 'schedule': 'onecycle', 'g_up': 1, 'g_down': 0.95, 'warmup': 4, 'period': 5, 'eta_min': 1e-05}, 'hyper_params': {'hidden_channels': 128, 'conv_layers': 3, 'conv_activation': 'relu', 'decode_activation': 'leakyrelu', 'decode_layers': 2, 'layernorm': True, 'agg': 'sum', 'variance': True, 'rho': 0, 'in_channels': 43, 'out_channels': 4}, 'data_params': {'case': 'vlarge_all_4t_z0.0_quantile_raw', 'targets': [0, 1, 2, 3], 'del_feats': [], 'split': 0.8, 'test': 0, 'scale': 0}, 'experiment_name': 'con3bat512'}
/home/cj1223/../../scratch/gpfs/cj1223/GraphResults/results_meta_pysr_120322/MetaEdge_vlarge_all_4t_z0.0_quantile_raw_jiagvo
RelU conv activation
LeakyRelU decode activation
N_params 546504
Made folder for saving model
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:53<7:27:46, 53.84s/it]  0%|          | 2/500 [02:14<9:36:12, 69.42s/it]Epoch: 2 done with learning rate 4.17E-04, Train loss: 1.19E+08, Train scatter: [0.9352 0.1732 0.5441 0.9953]
L1 regularization loss: 7.33E-01, L2 regularization loss: 1.27E-01
Test scatter: [0.9196 0.1687 0.5355 0.985 ], Lowest was [0.9196 0.1687 0.5355 0.985 ]
Median for last 10 epochs: [0.9196 0.1687 0.5355 0.985 ], Epochs since improvement 2
  1%|          | 3/500 [03:07<8:34:58, 62.17s/it]  1%|          | 4/500 [04:28<9:35:21, 69.60s/it]Epoch: 4 done with learning rate 4.67E-04, Train loss: 8.96E+07, Train scatter: [0.9351 0.1353 0.5441 0.9954]
L1 regularization loss: 7.37E-01, L2 regularization loss: 1.28E-01
Test scatter: [0.9195 0.1314 0.5355 0.9851], Lowest was [0.9195 0.1314 0.5355 0.985 ]
Median for last 10 epochs: [0.9195 0.1314 0.5355 0.985 ], Epochs since improvement 0
  1%|          | 5/500 [05:22<8:46:49, 63.86s/it]  1%|          | 6/500 [06:42<9:32:34, 69.54s/it]Epoch: 6 done with learning rate 5.51E-04, Train loss: 8.28E+07, Train scatter: [0.9348 0.1132 0.5441 0.9954]
L1 regularization loss: 7.39E-01, L2 regularization loss: 1.28E-01
Test scatter: [0.9192 0.1112 0.5355 0.9851], Lowest was [0.9192 0.1112 0.5355 0.985 ]
Median for last 10 epochs: [0.9192 0.1112 0.5355 0.9851], Epochs since improvement 0
  1%|▏         | 7/500 [07:36<8:48:15, 64.29s/it]  2%|▏         | 8/500 [08:57<9:29:59, 69.51s/it]Epoch: 8 done with learning rate 6.67E-04, Train loss: 8.06E+07, Train scatter: [0.9302 0.1027 0.544  0.9955]
L1 regularization loss: 7.42E-01, L2 regularization loss: 1.30E-01
Test scatter: [0.9144 0.1018 0.5354 0.9851], Lowest was [0.9144 0.1018 0.5354 0.985 ]
Median for last 10 epochs: [0.9168 0.1065 0.5354 0.9851], Epochs since improvement 0
  2%|▏         | 9/500 [09:50<8:47:36, 64.47s/it]Epoch: 10 done with learning rate 8.15E-04, Train loss: 7.94E+07, Train scatter: [0.7964 0.0944 0.5426 0.9955]
L1 regularization loss: 7.46E-01, L2 regularization loss: 1.32E-01
Test scatter: [0.7855 0.0944 0.534  0.9851], Lowest was [0.7855 0.0944 0.534  0.985 ]
Median for last 10 epochs: [0.9144 0.1018 0.5354 0.9851], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
  2%|▏         | 10/500 [11:18<9:45:32, 71.70s/it]  2%|▏         | 11/500 [12:11<8:58:15, 66.04s/it]  2%|▏         | 12/500 [13:33<9:35:47, 70.79s/it]Epoch: 12 done with learning rate 9.94E-04, Train loss: 7.84E+07, Train scatter: [0.6403 0.0893 0.5363 0.9954]
L1 regularization loss: 7.50E-01, L2 regularization loss: 1.35E-01
Test scatter: [0.6325 0.0894 0.5283 0.9851], Lowest was [0.6325 0.0894 0.5283 0.985 ]
Median for last 10 epochs: [0.9144 0.1018 0.5354 0.9851], Epochs since improvement 0
  3%|▎         | 13/500 [14:26<8:51:54, 65.53s/it]  3%|▎         | 14/500 [15:47<9:27:51, 70.11s/it]Epoch: 14 done with learning rate 1.20E-03, Train loss: 7.76E+07, Train scatter: [0.7316 0.0854 0.531  0.9954]
L1 regularization loss: 7.54E-01, L2 regularization loss: 1.37E-01
Test scatter: [0.7211 0.0855 0.5232 0.985 ], Lowest was [0.6325 0.0855 0.5232 0.985 ]
Median for last 10 epochs: [0.7855 0.0944 0.534  0.9851], Epochs since improvement 0
  3%|▎         | 15/500 [16:40<8:45:57, 65.07s/it]  3%|▎         | 16/500 [18:01<9:23:38, 69.87s/it]Epoch: 16 done with learning rate 1.44E-03, Train loss: 7.69E+07, Train scatter: [0.4947 0.0824 0.5276 0.9954]
L1 regularization loss: 7.57E-01, L2 regularization loss: 1.40E-01
Test scatter: [0.4957 0.0819 0.5196 0.985 ], Lowest was [0.4957 0.0819 0.5196 0.985 ]
Median for last 10 epochs: [0.7211 0.0894 0.5283 0.9851], Epochs since improvement 0
  3%|▎         | 17/500 [18:55<8:42:26, 64.90s/it]  4%|▎         | 18/500 [20:16<9:20:53, 69.82s/it]Epoch: 18 done with learning rate 1.70E-03, Train loss: 7.60E+07, Train scatter: [0.6375 0.0911 0.5133 0.9954]
L1 regularization loss: 7.62E-01, L2 regularization loss: 1.42E-01
Test scatter: [0.6358 0.0904 0.5047 0.985 ], Lowest was [0.4957 0.0819 0.5047 0.985 ]
Median for last 10 epochs: [0.6358 0.0894 0.5232 0.985 ], Epochs since improvement 0
  4%|▍         | 19/500 [21:09<8:40:30, 64.93s/it]Epoch: 20 done with learning rate 1.99E-03, Train loss: 7.61E+07, Train scatter: [0.478  0.09   0.5109 0.9954]
L1 regularization loss: 7.67E-01, L2 regularization loss: 1.44E-01
Test scatter: [0.4806 0.0902 0.5039 0.985 ], Lowest was [0.4806 0.0819 0.5039 0.985 ]
Median for last 10 epochs: [0.6325 0.0894 0.5196 0.985 ], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
  4%|▍         | 20/500 [22:37<9:34:42, 71.84s/it]  4%|▍         | 21/500 [23:31<8:49:40, 66.35s/it]  4%|▍         | 22/500 [24:52<9:23:29, 70.73s/it]Epoch: 22 done with learning rate 2.30E-03, Train loss: 7.45E+07, Train scatter: [0.562  0.0948 0.471  0.9954]
L1 regularization loss: 7.71E-01, L2 regularization loss: 1.46E-01
Test scatter: [0.5584 0.0938 0.4651 0.985 ], Lowest was [0.4806 0.0819 0.4651 0.985 ]
Median for last 10 epochs: [0.5584 0.0902 0.5047 0.985 ], Epochs since improvement 0
  5%|▍         | 23/500 [25:46<8:41:30, 65.60s/it]  5%|▍         | 24/500 [27:06<9:15:19, 70.00s/it]Epoch: 24 done with learning rate 2.63E-03, Train loss: 7.32E+07, Train scatter: [0.5467 0.0906 0.3503 0.9953]
L1 regularization loss: 7.76E-01, L2 regularization loss: 1.48E-01
Test scatter: [0.5482 0.0912 0.3564 0.985 ], Lowest was [0.4806 0.0819 0.3564 0.985 ]
Median for last 10 epochs: [0.5482 0.0904 0.5039 0.985 ], Epochs since improvement 0
  5%|▌         | 25/500 [27:59<8:35:14, 65.08s/it]  5%|▌         | 26/500 [29:20<9:11:16, 69.78s/it]Epoch: 26 done with learning rate 2.98E-03, Train loss: 7.24E+07, Train scatter: [0.6354 0.0887 0.358  0.9954]
L1 regularization loss: 7.83E-01, L2 regularization loss: 1.51E-01
Test scatter: [0.6498 0.0891 0.3613 0.985 ], Lowest was [0.4806 0.0819 0.3564 0.985 ]
Median for last 10 epochs: [0.5584 0.0904 0.4651 0.985 ], Epochs since improvement 2
  5%|▌         | 27/500 [30:14<8:31:30, 64.88s/it]  6%|▌         | 28/500 [31:34<9:08:01, 69.66s/it]Epoch: 28 done with learning rate 3.34E-03, Train loss: 7.23E+07, Train scatter: [0.4625 0.0829 0.3306 0.9954]
L1 regularization loss: 7.88E-01, L2 regularization loss: 1.53E-01
Test scatter: [0.4682 0.0831 0.3363 0.985 ], Lowest was [0.4682 0.0819 0.3363 0.985 ]
Median for last 10 epochs: [0.5482 0.0902 0.3613 0.985 ], Epochs since improvement 0
  6%|▌         | 29/500 [32:28<8:28:24, 64.77s/it]Epoch: 30 done with learning rate 3.72E-03, Train loss: 8.57E+10, Train scatter: [0.935  0.1729 0.5441 0.9954]
L1 regularization loss: 1.49E+00, L2 regularization loss: 2.91E-01
Test scatter: [0.9194 0.169  0.5355 0.985 ], Lowest was [0.4682 0.0819 0.3363 0.985 ]
Median for last 10 epochs: [0.5584 0.0912 0.3613 0.985 ], Epochs since improvement 2
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
  6%|▌         | 30/500 [33:55<9:21:10, 71.64s/it]  6%|▌         | 31/500 [34:49<8:37:11, 66.17s/it]  6%|▋         | 32/500 [36:09<9:09:25, 70.44s/it]Epoch: 32 done with learning rate 4.10E-03, Train loss: 2.18E+08, Train scatter: [0.9346 0.1716 0.544  0.9954]
L1 regularization loss: 1.53E+00, L2 regularization loss: 3.11E-01
Test scatter: [0.9189 0.1678 0.5354 0.985 ], Lowest was [0.4682 0.0819 0.3363 0.985 ]
Median for last 10 epochs: [0.6498 0.0912 0.3613 0.985 ], Epochs since improvement 4
  7%|▋         | 33/500 [37:03<8:28:14, 65.30s/it]  7%|▋         | 34/500 [38:23<9:03:22, 69.96s/it]Epoch: 34 done with learning rate 4.50E-03, Train loss: 1.61E+08, Train scatter: [0.9348 0.1539 0.5439 0.9954]
L1 regularization loss: 1.53E+00, L2 regularization loss: 3.14E-01
Test scatter: [0.9192 0.1509 0.5353 0.985 ], Lowest was [0.4682 0.0819 0.3363 0.985 ]
Median for last 10 epochs: [0.9189 0.1509 0.5353 0.985 ], Epochs since improvement 6
  7%|▋         | 35/500 [39:17<8:23:51, 65.01s/it]  7%|▋         | 36/500 [40:37<8:57:56, 69.56s/it]Epoch: 36 done with learning rate 4.90E-03, Train loss: 1.43E+08, Train scatter: [0.9346 0.1444 0.5438 0.9954]
L1 regularization loss: 1.53E+00, L2 regularization loss: 3.17E-01
Test scatter: [0.919  0.1421 0.5352 0.985 ], Lowest was [0.4682 0.0819 0.3363 0.985 ]
Median for last 10 epochs: [0.919  0.1509 0.5353 0.985 ], Epochs since improvement 8
  7%|▋         | 37/500 [41:31<8:20:07, 64.81s/it]  8%|▊         | 38/500 [42:51<8:54:38, 69.43s/it]Epoch: 38 done with learning rate 5.30E-03, Train loss: 1.24E+08, Train scatter: [0.9345 0.1388 0.5429 0.9949]
L1 regularization loss: 1.53E+00, L2 regularization loss: 3.21E-01
Test scatter: [0.919  0.138  0.5344 0.9846], Lowest was [0.4682 0.0819 0.3363 0.9846]
Median for last 10 epochs: [0.919  0.1509 0.5353 0.985 ], Epochs since improvement 0
  8%|▊         | 39/500 [43:44<8:16:28, 64.62s/it]Epoch: 40 done with learning rate 5.70E-03, Train loss: 7.02E+07, Train scatter: [0.9346 0.1408 0.5412 0.9942]
L1 regularization loss: 1.53E+00, L2 regularization loss: 3.26E-01
Test scatter: [0.919  0.1411 0.5329 0.9838], Lowest was [0.4682 0.0819 0.3363 0.9838]
Median for last 10 epochs: [0.919  0.1421 0.5352 0.985 ], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
  8%|▊         | 40/500 [45:13<9:10:05, 71.75s/it]  8%|▊         | 41/500 [46:06<8:27:12, 66.30s/it]  8%|▊         | 42/500 [47:27<8:59:18, 70.65s/it]Epoch: 42 done with learning rate 6.10E-03, Train loss: 2.66E+07, Train scatter: [0.9346 0.1189 0.529  1.1012]
L1 regularization loss: 1.54E+00, L2 regularization loss: 3.39E-01
Test scatter: [0.9192 0.118  0.5188 1.0821], Lowest was [0.4682 0.0819 0.3363 0.9838]
Median for last 10 epochs: [0.919  0.1411 0.5344 0.985 ], Epochs since improvement 2
  9%|▊         | 43/500 [48:21<8:18:51, 65.49s/it]  9%|▉         | 44/500 [49:41<8:52:16, 70.04s/it]Epoch: 44 done with learning rate 6.49E-03, Train loss: 1.70E+07, Train scatter: [0.9335 0.1102 0.5067 0.6891]
L1 regularization loss: 1.55E+00, L2 regularization loss: 3.45E-01
Test scatter: [0.918  0.109  0.5008 0.6854], Lowest was [0.4682 0.0819 0.3363 0.6854]
Median for last 10 epochs: [0.919  0.138  0.5329 0.9846], Epochs since improvement 0
  9%|▉         | 45/500 [50:35<8:13:33, 65.08s/it]  9%|▉         | 46/500 [51:55<8:46:36, 69.60s/it]Epoch: 46 done with learning rate 6.87E-03, Train loss: 1.56E+07, Train scatter: [0.9332 0.1099 0.5173 0.684 ]
L1 regularization loss: 1.55E+00, L2 regularization loss: 3.49E-01
Test scatter: [0.9178 0.1084 0.5101 0.6767], Lowest was [0.4682 0.0819 0.3363 0.6767]
Median for last 10 epochs: [0.919  0.118  0.5188 0.9838], Epochs since improvement 0
  9%|▉         | 47/500 [52:48<8:09:04, 64.78s/it] 10%|▉         | 48/500 [54:09<8:43:45, 69.53s/it]Epoch: 48 done with learning rate 7.24E-03, Train loss: 1.48E+07, Train scatter: [0.9329 0.1078 0.5164 0.672 ]
L1 regularization loss: 1.55E+00, L2 regularization loss: 3.55E-01
Test scatter: [0.9174 0.1065 0.5091 0.6652], Lowest was [0.4682 0.0819 0.3363 0.6652]
Median for last 10 epochs: [0.918  0.109  0.5101 0.6854], Epochs since improvement 0
 10%|▉         | 49/500 [55:03<8:06:34, 64.73s/it]Epoch: 50 done with learning rate 7.60E-03, Train loss: 1.41E+07, Train scatter: [0.9302 0.1066 0.4994 0.6735]
L1 regularization loss: 1.55E+00, L2 regularization loss: 3.65E-01
Test scatter: [0.9148 0.1053 0.4929 0.6666], Lowest was [0.4682 0.0819 0.3363 0.6652]
Median for last 10 epochs: [0.9178 0.1084 0.5091 0.6767], Epochs since improvement 2
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 10%|█         | 50/500 [56:31<8:59:03, 71.87s/it] 10%|█         | 51/500 [57:25<8:16:46, 66.38s/it] 10%|█         | 52/500 [58:45<8:46:52, 70.56s/it]Epoch: 52 done with learning rate 7.94E-03, Train loss: 1.36E+07, Train scatter: [0.9294 0.1059 0.507  0.6534]
L1 regularization loss: 1.56E+00, L2 regularization loss: 3.71E-01
Test scatter: [0.9141 0.1046 0.5002 0.6466], Lowest was [0.4682 0.0819 0.3363 0.6466]
Median for last 10 epochs: [0.9174 0.1065 0.5008 0.6666], Epochs since improvement 0
 11%|█         | 53/500 [59:39<8:07:45, 65.47s/it] 11%|█         | 54/500 [1:00:59<8:40:28, 70.02s/it]Epoch: 54 done with learning rate 8.26E-03, Train loss: 1.29E+07, Train scatter: [0.9288 0.1032 0.4963 0.6431]
L1 regularization loss: 1.56E+00, L2 regularization loss: 3.77E-01
Test scatter: [0.9135 0.1022 0.4903 0.6376], Lowest was [0.4682 0.0819 0.3363 0.6376]
Median for last 10 epochs: [0.9148 0.1053 0.5002 0.6652], Epochs since improvement 0
 11%|█         | 55/500 [1:01:53<8:02:38, 65.08s/it] 11%|█         | 56/500 [1:03:14<8:36:25, 69.79s/it]Epoch: 56 done with learning rate 8.56E-03, Train loss: 1.23E+07, Train scatter: [0.9262 0.102  0.5026 0.639 ]
L1 regularization loss: 1.56E+00, L2 regularization loss: 3.85E-01
Test scatter: [0.911  0.1011 0.4967 0.6348], Lowest was [0.4682 0.0819 0.3363 0.6348]
Median for last 10 epochs: [0.9141 0.1046 0.4967 0.6466], Epochs since improvement 0
 11%|█▏        | 57/500 [1:04:07<7:59:31, 64.95s/it] 12%|█▏        | 58/500 [1:05:28<8:33:43, 69.74s/it]Epoch: 58 done with learning rate 8.83E-03, Train loss: 1.19E+07, Train scatter: [0.9233 0.1019 0.4596 0.6378]
L1 regularization loss: 1.56E+00, L2 regularization loss: 3.94E-01
Test scatter: [0.9082 0.101  0.4564 0.6335], Lowest was [0.4682 0.0819 0.3363 0.6335]
Median for last 10 epochs: [0.9135 0.1022 0.4929 0.6376], Epochs since improvement 0
 12%|█▏        | 59/500 [1:06:22<7:56:42, 64.86s/it]Epoch: 60 done with learning rate 9.08E-03, Train loss: 1.13E+07, Train scatter: [0.9205 0.1011 0.458  0.6245]
L1 regularization loss: 1.57E+00, L2 regularization loss: 4.02E-01
Test scatter: [0.9056 0.0997 0.4538 0.6177], Lowest was [0.4682 0.0819 0.3363 0.6177]
Median for last 10 epochs: [0.911  0.1011 0.4903 0.6348], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 12%|█▏        | 60/500 [1:07:50<8:48:03, 72.01s/it] 12%|█▏        | 61/500 [1:08:44<8:06:22, 66.48s/it] 12%|█▏        | 62/500 [1:10:05<8:37:31, 70.89s/it]Epoch: 62 done with learning rate 9.31E-03, Train loss: 1.13E+07, Train scatter: [0.9187 0.0998 0.506  0.611 ]
L1 regularization loss: 1.57E+00, L2 regularization loss: 4.12E-01
Test scatter: [0.9039 0.0983 0.4995 0.604 ], Lowest was [0.4682 0.0819 0.3363 0.604 ]
Median for last 10 epochs: [0.9082 0.101  0.4903 0.6335], Epochs since improvement 0
 13%|█▎        | 63/500 [1:10:59<7:58:20, 65.68s/it] 13%|█▎        | 64/500 [1:12:20<8:30:38, 70.27s/it]Epoch: 64 done with learning rate 9.50E-03, Train loss: 1.04E+07, Train scatter: [0.9121 0.1077 0.4988 0.6283]
L1 regularization loss: 1.57E+00, L2 regularization loss: 4.22E-01
Test scatter: [0.8976 0.1042 0.4898 0.6171], Lowest was [0.4682 0.0819 0.3363 0.604 ]
Median for last 10 epochs: [0.9056 0.101  0.4898 0.6177], Epochs since improvement 2
 13%|█▎        | 65/500 [1:13:13<7:52:47, 65.21s/it] 13%|█▎        | 66/500 [1:14:33<8:24:39, 69.77s/it]Epoch: 66 done with learning rate 9.66E-03, Train loss: 9.98E+06, Train scatter: [0.915  0.0976 0.4443 0.5951]
L1 regularization loss: 1.58E+00, L2 regularization loss: 4.30E-01
Test scatter: [0.9008 0.0961 0.4411 0.5884], Lowest was [0.4682 0.0819 0.3363 0.5884]
Median for last 10 epochs: [0.9039 0.0997 0.4564 0.6171], Epochs since improvement 0
 13%|█▎        | 67/500 [1:15:27<7:48:29, 64.92s/it] 14%|█▎        | 68/500 [1:16:48<8:22:20, 69.77s/it]Epoch: 68 done with learning rate 9.80E-03, Train loss: 9.75E+06, Train scatter: [0.9083 0.1004 0.4454 0.6053]
L1 regularization loss: 1.58E+00, L2 regularization loss: 4.37E-01
Test scatter: [0.8945 0.0988 0.4412 0.602 ], Lowest was [0.4682 0.0819 0.3363 0.5884]
Median for last 10 epochs: [0.9008 0.0988 0.4538 0.604 ], Epochs since improvement 2
 14%|█▍        | 69/500 [1:17:42<7:46:22, 64.92s/it]Epoch: 70 done with learning rate 9.90E-03, Train loss: 9.35E+06, Train scatter: [0.9036 0.1001 0.4385 0.59  ]
L1 regularization loss: 1.58E+00, L2 regularization loss: 4.43E-01
Test scatter: [0.8901 0.0984 0.4345 0.582 ], Lowest was [0.4682 0.0819 0.3363 0.582 ]
Median for last 10 epochs: [0.8976 0.0984 0.4412 0.602 ], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 14%|█▍        | 70/500 [1:19:11<8:36:47, 72.11s/it] 14%|█▍        | 71/500 [1:20:04<7:55:45, 66.54s/it] 14%|█▍        | 72/500 [1:21:25<8:24:46, 70.76s/it]Epoch: 72 done with learning rate 9.96E-03, Train loss: 8.83E+06, Train scatter: [0.906  0.0979 0.4319 0.5859]
L1 regularization loss: 1.58E+00, L2 regularization loss: 4.49E-01
Test scatter: [0.8927 0.0974 0.4298 0.5844], Lowest was [0.4682 0.0819 0.3363 0.582 ]
Median for last 10 epochs: [0.8945 0.0984 0.4411 0.5884], Epochs since improvement 2
 15%|█▍        | 73/500 [1:22:18<7:47:02, 65.63s/it] 15%|█▍        | 74/500 [1:23:39<8:18:35, 70.22s/it]Epoch: 74 done with learning rate 1.00E-02, Train loss: 8.63E+06, Train scatter: [0.8986 0.0956 0.4305 0.5786]
L1 regularization loss: 1.58E+00, L2 regularization loss: 4.54E-01
Test scatter: [0.8855 0.0928 0.4274 0.5705], Lowest was [0.4682 0.0819 0.3363 0.5705]
Median for last 10 epochs: [0.8927 0.0974 0.4345 0.5844], Epochs since improvement 0
 15%|█▌        | 75/500 [1:24:33<7:41:52, 65.21s/it] 15%|█▌        | 76/500 [1:25:54<8:14:21, 69.96s/it]Epoch: 76 done with learning rate 1.00E-02, Train loss: 8.22E+06, Train scatter: [0.8979 0.0961 0.4302 0.5833]
L1 regularization loss: 1.59E+00, L2 regularization loss: 4.58E-01
Test scatter: [0.885  0.0955 0.4279 0.5839], Lowest was [0.4682 0.0819 0.3363 0.5705]
Median for last 10 epochs: [0.8901 0.0974 0.4298 0.5839], Epochs since improvement 2
 15%|█▌        | 77/500 [1:26:47<7:38:21, 65.02s/it] 16%|█▌        | 78/500 [1:28:09<8:13:27, 70.16s/it]Epoch: 78 done with learning rate 1.00E-02, Train loss: 8.00E+06, Train scatter: [0.8943 0.0946 0.427  0.5701]
L1 regularization loss: 1.59E+00, L2 regularization loss: 4.64E-01
Test scatter: [0.8818 0.0927 0.4227 0.5644], Lowest was [0.4682 0.0819 0.3363 0.5644]
Median for last 10 epochs: [0.8855 0.0955 0.4279 0.582 ], Epochs since improvement 0
 16%|█▌        | 79/500 [1:29:03<7:37:14, 65.16s/it]Epoch: 80 done with learning rate 1.00E-02, Train loss: 7.71E+06, Train scatter: [0.8845 0.0972 0.4246 0.5717]
L1 regularization loss: 1.59E+00, L2 regularization loss: 4.71E-01
Test scatter: [0.8726 0.097  0.422  0.5706], Lowest was [0.4682 0.0819 0.3363 0.5644]
Median for last 10 epochs: [0.885  0.0955 0.4274 0.5706], Epochs since improvement 2
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 16%|█▌        | 80/500 [1:30:31<8:24:41, 72.10s/it] 16%|█▌        | 81/500 [1:31:25<7:44:28, 66.51s/it] 16%|█▋        | 82/500 [1:32:46<8:13:22, 70.82s/it]Epoch: 82 done with learning rate 9.99E-03, Train loss: 7.50E+06, Train scatter: [0.8757 0.0953 0.4232 0.5647]
L1 regularization loss: 1.60E+00, L2 regularization loss: 4.77E-01
Test scatter: [0.8642 0.0948 0.4198 0.5602], Lowest was [0.4682 0.0819 0.3363 0.5602]
Median for last 10 epochs: [0.8818 0.0948 0.4227 0.5705], Epochs since improvement 0
 17%|█▋        | 83/500 [1:33:39<7:36:07, 65.63s/it] 17%|█▋        | 84/500 [1:35:01<8:09:26, 70.59s/it]Epoch: 84 done with learning rate 9.99E-03, Train loss: 7.32E+06, Train scatter: [0.8622 0.0976 0.4257 0.5867]
L1 regularization loss: 1.60E+00, L2 regularization loss: 4.84E-01
Test scatter: [0.8515 0.0975 0.4246 0.5916], Lowest was [0.4682 0.0819 0.3363 0.5602]
Median for last 10 epochs: [0.8726 0.0955 0.4227 0.5706], Epochs since improvement 2
 17%|█▋        | 85/500 [1:35:55<7:32:49, 65.47s/it] 17%|█▋        | 86/500 [1:37:15<8:02:45, 69.96s/it]Epoch: 86 done with learning rate 9.98E-03, Train loss: 7.04E+06, Train scatter: [0.8517 0.0951 0.4241 0.5776]
L1 regularization loss: 1.61E+00, L2 regularization loss: 4.93E-01
Test scatter: [0.8412 0.0949 0.4223 0.5802], Lowest was [0.4682 0.0819 0.3363 0.5602]
Median for last 10 epochs: [0.8642 0.0949 0.4223 0.5706], Epochs since improvement 4
 17%|█▋        | 87/500 [1:38:09<7:27:48, 65.06s/it] 18%|█▊        | 88/500 [1:39:29<7:58:36, 69.70s/it]Epoch: 88 done with learning rate 9.98E-03, Train loss: 6.68E+06, Train scatter: [0.8294 0.0938 0.4143 0.566 ]
L1 regularization loss: 1.61E+00, L2 regularization loss: 5.02E-01
Test scatter: [0.8196 0.0935 0.412  0.5673], Lowest was [0.4682 0.0819 0.3363 0.5602]
Median for last 10 epochs: [0.8515 0.0949 0.422  0.5706], Epochs since improvement 6
 18%|█▊        | 89/500 [1:40:23<7:24:06, 64.83s/it]Epoch: 90 done with learning rate 9.97E-03, Train loss: 6.32E+06, Train scatter: [0.7877 0.0942 0.4102 0.5633]
L1 regularization loss: 1.61E+00, L2 regularization loss: 5.19E-01
Test scatter: [0.7766 0.0925 0.4068 0.5607], Lowest was [0.4682 0.0819 0.3363 0.5602]
Median for last 10 epochs: [0.8412 0.0948 0.4198 0.5673], Epochs since improvement 8
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 18%|█▊        | 90/500 [1:41:54<8:17:03, 72.74s/it] 18%|█▊        | 91/500 [1:42:48<7:36:32, 66.97s/it] 18%|█▊        | 92/500 [1:44:08<8:03:41, 71.13s/it]Epoch: 92 done with learning rate 9.96E-03, Train loss: 6.03E+06, Train scatter: [0.765  0.0914 0.4092 0.5514]
L1 regularization loss: 1.62E+00, L2 regularization loss: 5.29E-01
Test scatter: [0.7529 0.0895 0.4049 0.5472], Lowest was [0.4682 0.0819 0.3363 0.5472]
Median for last 10 epochs: [0.8196 0.0935 0.412  0.5673], Epochs since improvement 0
 19%|█▊        | 93/500 [1:45:02<7:26:46, 65.86s/it] 19%|█▉        | 94/500 [1:46:23<7:56:10, 70.37s/it]Epoch: 94 done with learning rate 9.95E-03, Train loss: 5.63E+06, Train scatter: [0.7123 0.0954 0.4112 0.5563]
L1 regularization loss: 1.62E+00, L2 regularization loss: 5.41E-01
Test scatter: [0.697  0.0919 0.4041 0.5459], Lowest was [0.4682 0.0819 0.3363 0.5459]
Median for last 10 epochs: [0.7766 0.0925 0.4068 0.5607], Epochs since improvement 0
 19%|█▉        | 95/500 [1:47:16<7:20:57, 65.33s/it] 19%|█▉        | 96/500 [1:48:38<7:52:16, 70.14s/it]Epoch: 96 done with learning rate 9.94E-03, Train loss: 5.25E+06, Train scatter: [0.6584 0.0943 0.4042 0.5557]
L1 regularization loss: 1.62E+00, L2 regularization loss: 5.56E-01
Test scatter: [0.6447 0.0938 0.4026 0.5582], Lowest was [0.4682 0.0819 0.3363 0.5459]
Median for last 10 epochs: [0.7529 0.0925 0.4049 0.5582], Epochs since improvement 2
 19%|█▉        | 97/500 [1:49:32<7:18:04, 65.22s/it] 20%|█▉        | 98/500 [1:50:53<7:49:01, 70.00s/it]Epoch: 98 done with learning rate 9.93E-03, Train loss: 4.95E+06, Train scatter: [0.6644 0.0916 0.4015 0.5396]
L1 regularization loss: 1.63E+00, L2 regularization loss: 5.73E-01
Test scatter: [0.6454 0.0898 0.3976 0.5327], Lowest was [0.4682 0.0819 0.3363 0.5327]
Median for last 10 epochs: [0.697  0.0919 0.4041 0.5472], Epochs since improvement 0
 20%|█▉        | 99/500 [1:51:46<7:14:40, 65.04s/it]Epoch: 100 done with learning rate 9.91E-03, Train loss: 4.80E+06, Train scatter: [0.638  0.0906 0.3979 0.5334]
L1 regularization loss: 1.63E+00, L2 regularization loss: 5.92E-01
Test scatter: [0.6197 0.0885 0.3955 0.5285], Lowest was [0.4682 0.0819 0.3363 0.5285]
Median for last 10 epochs: [0.6454 0.0898 0.4026 0.5459], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 20%|██        | 100/500 [1:53:16<8:02:24, 72.36s/it] 20%|██        | 101/500 [1:54:09<7:23:46, 66.73s/it] 20%|██        | 102/500 [1:55:30<7:50:36, 70.95s/it]Epoch: 102 done with learning rate 9.90E-03, Train loss: 4.53E+06, Train scatter: [0.6394 0.0914 0.4011 0.5407]
L1 regularization loss: 1.63E+00, L2 regularization loss: 6.09E-01
Test scatter: [0.622  0.0895 0.3982 0.5318], Lowest was [0.4682 0.0819 0.3363 0.5285]
Median for last 10 epochs: [0.6447 0.0898 0.3982 0.5327], Epochs since improvement 2
 21%|██        | 103/500 [1:56:24<7:14:59, 65.74s/it] 21%|██        | 104/500 [1:57:45<7:45:17, 70.50s/it]Epoch: 104 done with learning rate 9.89E-03, Train loss: 4.06E+06, Train scatter: [0.6269 0.089  0.3966 0.525 ]
L1 regularization loss: 1.63E+00, L2 regularization loss: 6.24E-01
Test scatter: [0.6065 0.0867 0.394  0.5188], Lowest was [0.4682 0.0819 0.3363 0.5188]
Median for last 10 epochs: [0.622  0.0895 0.3976 0.5318], Epochs since improvement 0
 21%|██        | 105/500 [1:58:39<7:10:59, 65.47s/it] 21%|██        | 106/500 [2:00:00<7:41:25, 70.27s/it]Epoch: 106 done with learning rate 9.87E-03, Train loss: 3.94E+06, Train scatter: [0.6505 0.091  0.3966 0.5252]
L1 regularization loss: 1.63E+00, L2 regularization loss: 6.40E-01
Test scatter: [0.6301 0.088  0.3924 0.5178], Lowest was [0.4682 0.0819 0.3363 0.5178]
Median for last 10 epochs: [0.622  0.0885 0.3955 0.5285], Epochs since improvement 0
 21%|██▏       | 107/500 [2:00:54<7:07:29, 65.27s/it] 22%|██▏       | 108/500 [2:02:16<7:38:55, 70.24s/it]Epoch: 108 done with learning rate 9.85E-03, Train loss: 3.80E+06, Train scatter: [0.5198 0.0931 0.3982 0.5305]
L1 regularization loss: 1.64E+00, L2 regularization loss: 6.56E-01
Test scatter: [0.503  0.0914 0.3964 0.5247], Lowest was [0.4682 0.0819 0.3363 0.5178]
Median for last 10 epochs: [0.6197 0.0885 0.3955 0.5247], Epochs since improvement 2
 22%|██▏       | 109/500 [2:03:09<7:05:10, 65.24s/it]Epoch: 110 done with learning rate 9.83E-03, Train loss: 3.52E+06, Train scatter: [0.6262 0.0928 0.3923 0.52  ]
L1 regularization loss: 1.64E+00, L2 regularization loss: 6.70E-01
Test scatter: [0.6049 0.0895 0.3877 0.5116], Lowest was [0.4682 0.0819 0.3363 0.5116]
Median for last 10 epochs: [0.6065 0.0895 0.394  0.5188], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 22%|██▏       | 110/500 [2:04:39<7:51:02, 72.47s/it] 22%|██▏       | 111/500 [2:05:32<7:13:14, 66.83s/it] 22%|██▏       | 112/500 [2:06:53<7:39:08, 71.00s/it]Epoch: 112 done with learning rate 9.81E-03, Train loss: 3.45E+06, Train scatter: [0.5352 0.0882 0.3948 0.5273]
L1 regularization loss: 1.64E+00, L2 regularization loss: 6.84E-01
Test scatter: [0.5194 0.0871 0.3931 0.524 ], Lowest was [0.4682 0.0819 0.3363 0.5116]
Median for last 10 epochs: [0.6049 0.088  0.3931 0.5188], Epochs since improvement 2
 23%|██▎       | 113/500 [2:07:47<7:04:24, 65.80s/it] 23%|██▎       | 114/500 [2:09:08<7:32:57, 70.41s/it]Epoch: 114 done with learning rate 9.79E-03, Train loss: 3.32E+06, Train scatter: [0.5047 0.0862 0.3902 0.5262]
L1 regularization loss: 1.64E+00, L2 regularization loss: 6.97E-01
Test scatter: [0.4933 0.085  0.3894 0.5256], Lowest was [0.4682 0.0819 0.3363 0.5116]
Median for last 10 epochs: [0.5194 0.088  0.3924 0.524 ], Epochs since improvement 4
 23%|██▎       | 115/500 [2:10:01<6:59:18, 65.35s/it] 23%|██▎       | 116/500 [2:11:22<7:27:46, 69.96s/it]Epoch: 116 done with learning rate 9.77E-03, Train loss: 3.09E+06, Train scatter: [0.5413 0.0799 0.3867 0.5086]
L1 regularization loss: 1.64E+00, L2 regularization loss: 7.11E-01
Test scatter: [0.523  0.0781 0.384  0.5032], Lowest was [0.4682 0.0781 0.3363 0.5032]
Median for last 10 epochs: [0.5194 0.0871 0.3894 0.524 ], Epochs since improvement 0
 23%|██▎       | 117/500 [2:12:16<6:55:19, 65.06s/it] 24%|██▎       | 118/500 [2:13:37<7:24:19, 69.79s/it]Epoch: 118 done with learning rate 9.75E-03, Train loss: 2.95E+06, Train scatter: [0.5375 0.0774 0.3879 0.5124]
L1 regularization loss: 1.65E+00, L2 regularization loss: 7.27E-01
Test scatter: [0.5198 0.076  0.3849 0.508 ], Lowest was [0.4682 0.076  0.3363 0.5032]
Median for last 10 epochs: [0.5198 0.085  0.3877 0.5116], Epochs since improvement 0
 24%|██▍       | 119/500 [2:14:30<6:52:14, 64.92s/it]Epoch: 120 done with learning rate 9.73E-03, Train loss: 2.73E+06, Train scatter: [0.6011 0.0757 0.3839 0.5122]
L1 regularization loss: 1.65E+00, L2 regularization loss: 7.40E-01
Test scatter: [0.5802 0.0743 0.3793 0.5057], Lowest was [0.4682 0.0743 0.3363 0.5032]
Median for last 10 epochs: [0.5198 0.0781 0.3849 0.508 ], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 24%|██▍       | 120/500 [2:15:58<7:35:20, 71.90s/it] 24%|██▍       | 121/500 [2:16:52<6:59:49, 66.46s/it] 24%|██▍       | 122/500 [2:18:14<7:27:13, 70.99s/it]Epoch: 122 done with learning rate 9.70E-03, Train loss: 2.56E+06, Train scatter: [0.5648 0.0749 0.3847 0.5147]
L1 regularization loss: 1.65E+00, L2 regularization loss: 7.53E-01
Test scatter: [0.5471 0.0742 0.3813 0.5112], Lowest was [0.4682 0.0742 0.3363 0.5032]
Median for last 10 epochs: [0.523 0.076 0.384 0.508], Epochs since improvement 0
 25%|██▍       | 123/500 [2:19:07<6:53:28, 65.80s/it] 25%|██▍       | 124/500 [2:20:28<7:20:22, 70.27s/it]Epoch: 124 done with learning rate 9.68E-03, Train loss: 2.56E+06, Train scatter: [0.5096 0.0755 0.383  0.5131]
L1 regularization loss: 1.65E+00, L2 regularization loss: 7.65E-01
Test scatter: [0.4931 0.0746 0.3797 0.507 ], Lowest was [0.4682 0.0742 0.3363 0.5032]
Median for last 10 epochs: [0.523  0.0746 0.3813 0.507 ], Epochs since improvement 2
 25%|██▌       | 125/500 [2:21:22<6:48:19, 65.33s/it] 25%|██▌       | 126/500 [2:22:42<7:14:43, 69.74s/it]Epoch: 126 done with learning rate 9.65E-03, Train loss: 2.33E+06, Train scatter: [0.4537 0.0762 0.3788 0.505 ]
L1 regularization loss: 1.65E+00, L2 regularization loss: 7.73E-01
Test scatter: [0.4429 0.0742 0.3759 0.5005], Lowest was [0.4429 0.0742 0.3363 0.5005]
Median for last 10 epochs: [0.5198 0.0743 0.3797 0.507 ], Epochs since improvement 0
 25%|██▌       | 127/500 [2:23:35<6:42:19, 64.72s/it] 26%|██▌       | 128/500 [2:24:55<7:08:53, 69.18s/it]Epoch: 128 done with learning rate 9.62E-03, Train loss: 2.29E+06, Train scatter: [0.4306 0.0737 0.3791 0.509 ]
L1 regularization loss: 1.66E+00, L2 regularization loss: 7.83E-01
Test scatter: [0.4254 0.0733 0.3775 0.5067], Lowest was [0.4254 0.0733 0.3363 0.5005]
Median for last 10 epochs: [0.4931 0.0742 0.3793 0.5067], Epochs since improvement 0
 26%|██▌       | 129/500 [2:25:48<6:37:43, 64.32s/it]Epoch: 130 done with learning rate 9.59E-03, Train loss: 2.32E+06, Train scatter: [0.4417 0.0774 0.3875 0.5157]
L1 regularization loss: 1.66E+00, L2 regularization loss: 7.92E-01
Test scatter: [0.4351 0.0777 0.3863 0.5111], Lowest was [0.4254 0.0733 0.3363 0.5005]
Median for last 10 epochs: [0.4429 0.0742 0.3797 0.507 ], Epochs since improvement 2
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 26%|██▌       | 130/500 [2:27:14<7:17:47, 70.99s/it] 26%|██▌       | 131/500 [2:28:07<6:43:29, 65.61s/it] 26%|██▋       | 132/500 [2:29:27<7:08:54, 69.93s/it]Epoch: 132 done with learning rate 9.56E-03, Train loss: 2.12E+06, Train scatter: [0.4966 0.0724 0.3825 0.5048]
L1 regularization loss: 1.67E+00, L2 regularization loss: 8.00E-01
Test scatter: [0.4815 0.0707 0.3781 0.4997], Lowest was [0.4254 0.0707 0.3363 0.4997]
Median for last 10 epochs: [0.4429 0.0742 0.3781 0.5067], Epochs since improvement 0
 27%|██▋       | 133/500 [2:30:20<6:36:55, 64.89s/it] 27%|██▋       | 134/500 [2:31:41<7:03:59, 69.51s/it]Epoch: 134 done with learning rate 9.53E-03, Train loss: 2.08E+06, Train scatter: [0.4847 0.0709 0.3651 0.4972]
L1 regularization loss: 1.67E+00, L2 regularization loss: 8.07E-01
Test scatter: [0.4647 0.0708 0.3631 0.4931], Lowest was [0.4254 0.0707 0.3363 0.4931]
Median for last 10 epochs: [0.4429 0.0733 0.3775 0.5005], Epochs since improvement 0
 27%|██▋       | 135/500 [2:32:34<6:32:38, 64.54s/it] 27%|██▋       | 136/500 [2:33:54<7:00:52, 69.37s/it]Epoch: 136 done with learning rate 9.50E-03, Train loss: 2.03E+06, Train scatter: [0.4474 0.0726 0.3706 0.4986]
L1 regularization loss: 1.67E+00, L2 regularization loss: 8.15E-01
Test scatter: [0.4312 0.0728 0.3688 0.494 ], Lowest was [0.4254 0.0707 0.3363 0.4931]
Median for last 10 epochs: [0.4351 0.0728 0.3775 0.4997], Epochs since improvement 2
 27%|██▋       | 137/500 [2:34:47<6:29:54, 64.45s/it] 28%|██▊       | 138/500 [2:36:07<6:56:19, 69.01s/it]Epoch: 138 done with learning rate 9.47E-03, Train loss: 1.99E+06, Train scatter: [0.4486 0.0833 0.3686 0.5013]
L1 regularization loss: 1.67E+00, L2 regularization loss: 8.23E-01
Test scatter: [0.4401 0.0851 0.3712 0.4986], Lowest was [0.4254 0.0707 0.3363 0.4931]
Median for last 10 epochs: [0.4401 0.0728 0.3712 0.4986], Epochs since improvement 4
 28%|██▊       | 139/500 [2:37:00<6:26:35, 64.25s/it]Epoch: 140 done with learning rate 9.43E-03, Train loss: 1.97E+06, Train scatter: [0.7457 0.0772 0.3775 0.5212]
L1 regularization loss: 1.68E+00, L2 regularization loss: 8.32E-01
Test scatter: [0.6935 0.0761 0.3766 0.5184], Lowest was [0.4254 0.0707 0.3363 0.4931]
Median for last 10 epochs: [0.4647 0.0728 0.3712 0.4986], Epochs since improvement 6
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 28%|██▊       | 140/500 [2:38:28<7:07:35, 71.27s/it] 28%|██▊       | 141/500 [2:39:21<6:33:44, 65.81s/it] 28%|██▊       | 142/500 [2:40:41<6:57:56, 70.05s/it]Epoch: 142 done with learning rate 9.40E-03, Train loss: 2.16E+06, Train scatter: [0.4259 0.0789 0.3886 0.5201]
L1 regularization loss: 1.73E+00, L2 regularization loss: 8.64E-01
Test scatter: [0.4225 0.0771 0.3873 0.5156], Lowest was [0.4225 0.0707 0.3363 0.4931]
Median for last 10 epochs: [0.4401 0.0761 0.3712 0.4986], Epochs since improvement 0
 29%|██▊       | 143/500 [2:41:34<6:26:19, 64.93s/it] 29%|██▉       | 144/500 [2:42:54<6:52:44, 69.56s/it]Epoch: 144 done with learning rate 9.36E-03, Train loss: 1.93E+06, Train scatter: [1.107  0.1107 0.4281 0.6202]
L1 regularization loss: 1.74E+00, L2 regularization loss: 8.68E-01
Test scatter: [1.0142 0.1059 0.4231 0.6149], Lowest was [0.4225 0.0707 0.3363 0.4931]
Median for last 10 epochs: [0.4401 0.0771 0.3766 0.5156], Epochs since improvement 2
 29%|██▉       | 145/500 [2:43:47<6:22:25, 64.63s/it] 29%|██▉       | 146/500 [2:45:08<6:49:20, 69.38s/it]Epoch: 146 done with learning rate 9.33E-03, Train loss: 3.81E+06, Train scatter: [0.6201 0.1012 0.4423 0.6512]
L1 regularization loss: 1.74E+00, L2 regularization loss: 8.78E-01
Test scatter: [0.5894 0.0995 0.4321 0.6373], Lowest was [0.4225 0.0707 0.3363 0.4931]
Median for last 10 epochs: [0.5894 0.0851 0.3873 0.5184], Epochs since improvement 4
 29%|██▉       | 147/500 [2:46:01<6:19:15, 64.46s/it] 30%|██▉       | 148/500 [2:47:21<6:46:16, 69.25s/it]Epoch: 148 done with learning rate 9.29E-03, Train loss: 2.98E+06, Train scatter: [0.5103 0.0912 0.4163 0.6   ]
L1 regularization loss: 1.75E+00, L2 regularization loss: 8.87E-01
Test scatter: [0.497  0.0891 0.4213 0.5931], Lowest was [0.4225 0.0707 0.3363 0.4931]
Median for last 10 epochs: [0.5894 0.0891 0.4213 0.5931], Epochs since improvement 6
 30%|██▉       | 149/500 [2:48:14<6:16:24, 64.34s/it]Epoch: 150 done with learning rate 9.25E-03, Train loss: 2.15E+06, Train scatter: [0.4677 0.0842 0.3998 0.5699]
L1 regularization loss: 1.75E+00, L2 regularization loss: 8.95E-01
Test scatter: [0.4634 0.0821 0.3998 0.5685], Lowest was [0.4225 0.0707 0.3363 0.4931]
Median for last 10 epochs: [0.497  0.0891 0.4213 0.5931], Epochs since improvement 8
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 30%|███       | 150/500 [2:49:41<6:54:41, 71.09s/it] 30%|███       | 151/500 [2:50:34<6:22:00, 65.68s/it] 30%|███       | 152/500 [2:51:53<6:45:25, 69.90s/it]Epoch: 152 done with learning rate 9.21E-03, Train loss: 3.00E+06, Train scatter: [0.6039 0.0929 0.4052 0.572 ]
L1 regularization loss: 1.76E+00, L2 regularization loss: 9.06E-01
Test scatter: [0.5921 0.0927 0.4015 0.5649], Lowest was [0.4225 0.0707 0.3363 0.4931]
Median for last 10 epochs: [0.5894 0.0927 0.4213 0.5931], Epochs since improvement 10
 31%|███       | 153/500 [2:52:46<6:14:50, 64.81s/it] 31%|███       | 154/500 [2:54:06<6:39:51, 69.34s/it]Epoch: 154 done with learning rate 9.17E-03, Train loss: 1.77E+06, Train scatter: [0.4915 0.0812 0.4097 0.5507]
L1 regularization loss: 1.76E+00, L2 regularization loss: 9.15E-01
Test scatter: [0.4864 0.0799 0.4009 0.5454], Lowest was [0.4225 0.0707 0.3363 0.4931]
Median for last 10 epochs: [0.497  0.0891 0.4015 0.5685], Epochs since improvement 12
 31%|███       | 155/500 [2:54:59<6:10:39, 64.46s/it] 31%|███       | 156/500 [2:56:19<6:35:47, 69.03s/it]Epoch: 156 done with learning rate 9.13E-03, Train loss: 2.43E+06, Train scatter: [0.6427 0.0845 0.4018 0.5542]
L1 regularization loss: 1.77E+00, L2 regularization loss: 9.26E-01
Test scatter: [0.6317 0.0832 0.3965 0.5494], Lowest was [0.4225 0.0707 0.3363 0.4931]
Median for last 10 epochs: [0.497  0.0832 0.4009 0.5649], Epochs since improvement 14
 31%|███▏      | 157/500 [2:57:12<6:07:13, 64.24s/it] 32%|███▏      | 158/500 [2:58:33<6:34:25, 69.20s/it]Epoch: 158 done with learning rate 9.09E-03, Train loss: 1.21E+06, Train scatter: [0.4626 0.0799 0.3965 0.5566]
L1 regularization loss: 1.77E+00, L2 regularization loss: 9.32E-01
Test scatter: [0.4584 0.0784 0.3904 0.5497], Lowest was [0.4225 0.0707 0.3363 0.4931]
Median for last 10 epochs: [0.4864 0.0821 0.3998 0.5497], Epochs since improvement 16
 32%|███▏      | 159/500 [2:59:26<6:05:36, 64.33s/it]Epoch: 160 done with learning rate 9.04E-03, Train loss: 1.44E+06, Train scatter: [0.564  0.0788 0.3809 0.5287]
L1 regularization loss: 1.78E+00, L2 regularization loss: 9.39E-01
Test scatter: [0.5572 0.0768 0.3765 0.5221], Lowest was [0.4225 0.0707 0.3363 0.4931]
Median for last 10 epochs: [0.5572 0.0799 0.3965 0.5494], Epochs since improvement 18
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 32%|███▏      | 160/500 [3:00:53<6:42:55, 71.11s/it] 32%|███▏      | 161/500 [3:01:46<6:11:08, 65.69s/it] 32%|███▏      | 162/500 [3:03:07<6:35:41, 70.24s/it]Epoch: 162 done with learning rate 9.00E-03, Train loss: 9.01E+05, Train scatter: [0.4784 0.0982 0.4077 0.6023]
L1 regularization loss: 1.78E+00, L2 regularization loss: 9.47E-01
Test scatter: [0.4741 0.0961 0.4057 0.6061], Lowest was [0.4225 0.0707 0.3363 0.4931]
Median for last 10 epochs: [0.4864 0.0799 0.3965 0.5494], Epochs since improvement 20
 33%|███▎      | 163/500 [3:04:00<6:05:40, 65.10s/it] 33%|███▎      | 164/500 [3:05:20<6:29:33, 69.57s/it]Epoch: 164 done with learning rate 8.96E-03, Train loss: 8.60E+05, Train scatter: [0.4527 0.0725 0.3643 0.5102]
L1 regularization loss: 1.78E+00, L2 regularization loss: 9.52E-01
Test scatter: [0.4466 0.0707 0.362  0.505 ], Lowest was [0.4225 0.0707 0.3363 0.4931]
Median for last 10 epochs: [0.4741 0.0784 0.3904 0.5494], Epochs since improvement 0
 33%|███▎      | 165/500 [3:06:14<6:01:55, 64.82s/it] 33%|███▎      | 166/500 [3:07:37<6:31:19, 70.30s/it]Epoch: 166 done with learning rate 8.91E-03, Train loss: 6.50E+05, Train scatter: [0.435  0.076  0.4197 0.517 ]
L1 regularization loss: 1.78E+00, L2 regularization loss: 9.53E-01
Test scatter: [0.4311 0.0741 0.4186 0.5122], Lowest was [0.4225 0.0707 0.3363 0.4931]
Median for last 10 epochs: [0.4584 0.0768 0.3904 0.5221], Epochs since improvement 2
 33%|███▎      | 167/500 [3:08:30<6:02:41, 65.35s/it] 34%|███▎      | 168/500 [3:09:53<6:30:30, 70.57s/it]Epoch: 168 done with learning rate 8.86E-03, Train loss: 5.63E+05, Train scatter: [0.4557 0.0694 0.3636 0.5118]
L1 regularization loss: 1.78E+00, L2 regularization loss: 9.55E-01
Test scatter: [0.4522 0.068  0.3596 0.5024], Lowest was [0.4225 0.068  0.3363 0.4931]
Median for last 10 epochs: [0.4522 0.0741 0.3765 0.5122], Epochs since improvement 0
 34%|███▍      | 169/500 [3:10:47<6:00:52, 65.42s/it]Epoch: 170 done with learning rate 8.82E-03, Train loss: 5.23E+05, Train scatter: [0.4599 0.0811 0.3954 0.5442]
L1 regularization loss: 1.78E+00, L2 regularization loss: 9.57E-01
Test scatter: [0.4558 0.0784 0.3916 0.5369], Lowest was [0.4225 0.068  0.3363 0.4931]
Median for last 10 epochs: [0.4522 0.0741 0.3916 0.5122], Epochs since improvement 2
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 34%|███▍      | 170/500 [3:12:17<6:40:56, 72.90s/it] 34%|███▍      | 171/500 [3:13:10<6:07:08, 66.96s/it] 34%|███▍      | 172/500 [3:14:31<6:29:44, 71.29s/it]Epoch: 172 done with learning rate 8.77E-03, Train loss: 4.04E+05, Train scatter: [0.4544 0.0735 0.3794 0.5313]
L1 regularization loss: 1.78E+00, L2 regularization loss: 9.62E-01
Test scatter: [0.4536 0.0723 0.3757 0.5191], Lowest was [0.4225 0.068  0.3363 0.4931]
Median for last 10 epochs: [0.4522 0.0723 0.3757 0.5122], Epochs since improvement 4
 35%|███▍      | 173/500 [3:15:24<5:58:38, 65.80s/it] 35%|███▍      | 174/500 [3:16:45<6:21:03, 70.13s/it]Epoch: 174 done with learning rate 8.72E-03, Train loss: 3.68E+05, Train scatter: [0.4363 0.0676 0.3604 0.5119]
L1 regularization loss: 1.79E+00, L2 regularization loss: 9.68E-01
Test scatter: [0.4328 0.0659 0.3586 0.5061], Lowest was [0.4225 0.0659 0.3363 0.4931]
Median for last 10 epochs: [0.4522 0.0723 0.3757 0.5122], Epochs since improvement 0
 35%|███▌      | 175/500 [3:17:38<5:52:08, 65.01s/it] 35%|███▌      | 176/500 [3:18:58<6:16:17, 69.68s/it]Epoch: 176 done with learning rate 8.67E-03, Train loss: 3.98E+05, Train scatter: [0.4594 0.0689 0.3727 0.5073]
L1 regularization loss: 1.79E+00, L2 regularization loss: 9.69E-01
Test scatter: [0.4488 0.0686 0.3682 0.4971], Lowest was [0.4225 0.0659 0.3363 0.4931]
Median for last 10 epochs: [0.4522 0.0686 0.3682 0.5061], Epochs since improvement 2
 35%|███▌      | 177/500 [3:19:51<5:48:18, 64.70s/it] 36%|███▌      | 178/500 [3:21:12<6:13:09, 69.53s/it]Epoch: 178 done with learning rate 8.62E-03, Train loss: 5.72E+05, Train scatter: [0.6074 0.0785 0.3968 0.5443]
L1 regularization loss: 1.81E+00, L2 regularization loss: 9.81E-01
Test scatter: [0.6139 0.0766 0.3892 0.5284], Lowest was [0.4225 0.0659 0.3363 0.4931]
Median for last 10 epochs: [0.4536 0.0723 0.3757 0.5191], Epochs since improvement 4
 36%|███▌      | 179/500 [3:22:05<5:45:26, 64.57s/it]Epoch: 180 done with learning rate 8.57E-03, Train loss: 1.21E+06, Train scatter: [0.4591 0.1045 0.5156 0.6301]
L1 regularization loss: 1.82E+00, L2 regularization loss: 9.98E-01
Test scatter: [0.4591 0.1017 0.5077 0.624 ], Lowest was [0.4225 0.0659 0.3363 0.4931]
Median for last 10 epochs: [0.4536 0.0723 0.3757 0.5191], Epochs since improvement 6
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 36%|███▌      | 180/500 [3:23:33<6:20:54, 71.42s/it] 36%|███▌      | 181/500 [3:24:26<5:50:38, 65.95s/it] 36%|███▋      | 182/500 [3:25:47<6:13:33, 70.48s/it]Epoch: 182 done with learning rate 8.52E-03, Train loss: 3.86E+05, Train scatter: [0.4269 0.1006 0.4096 0.5497]
L1 regularization loss: 1.83E+00, L2 regularization loss: 1.02E+00
Test scatter: [0.4193 0.0974 0.4016 0.5392], Lowest was [0.4193 0.0659 0.3363 0.4931]
Median for last 10 epochs: [0.4488 0.0766 0.3892 0.5284], Epochs since improvement 0
 37%|███▋      | 183/500 [3:26:40<5:44:50, 65.27s/it] 37%|███▋      | 184/500 [3:28:00<6:06:35, 69.60s/it]Epoch: 184 done with learning rate 8.46E-03, Train loss: 1.77E+05, Train scatter: [0.4459 0.1083 0.4131 0.637 ]
L1 regularization loss: 1.84E+00, L2 regularization loss: 1.03E+00
Test scatter: [0.4444 0.1071 0.4194 0.6375], Lowest was [0.4193 0.0659 0.3363 0.4931]
Median for last 10 epochs: [0.4488 0.0974 0.4016 0.5392], Epochs since improvement 2
 37%|███▋      | 185/500 [3:28:53<5:39:25, 64.65s/it] 37%|███▋      | 186/500 [3:30:13<6:02:08, 69.20s/it]Epoch: 186 done with learning rate 8.41E-03, Train loss: 2.32E+05, Train scatter: [0.4196 0.0777 0.383  0.5271]
L1 regularization loss: 1.84E+00, L2 regularization loss: 1.03E+00
Test scatter: [0.4137 0.076  0.3788 0.5198], Lowest was [0.4137 0.0659 0.3363 0.4931]
Median for last 10 epochs: [0.4444 0.0974 0.4016 0.5392], Epochs since improvement 0
 37%|███▋      | 187/500 [3:31:06<5:35:48, 64.37s/it] 38%|███▊      | 188/500 [3:32:26<5:59:14, 69.09s/it]Epoch: 188 done with learning rate 8.35E-03, Train loss: 4.56E+05, Train scatter: [0.458  0.0905 0.4251 0.6177]
L1 regularization loss: 1.86E+00, L2 regularization loss: 1.06E+00
Test scatter: [0.4444 0.0891 0.4198 0.6094], Lowest was [0.4137 0.0659 0.3363 0.4931]
Median for last 10 epochs: [0.4444 0.0974 0.4194 0.6094], Epochs since improvement 2
 38%|███▊      | 189/500 [3:33:19<5:33:01, 64.25s/it]Epoch: 190 done with learning rate 8.30E-03, Train loss: 1.27E+05, Train scatter: [0.4009 0.1096 0.416  0.612 ]
L1 regularization loss: 1.87E+00, L2 regularization loss: 1.07E+00
Test scatter: [0.3942 0.1058 0.4099 0.6056], Lowest was [0.3942 0.0659 0.3363 0.4931]
Median for last 10 epochs: [0.4193 0.0974 0.4099 0.6056], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 38%|███▊      | 190/500 [3:34:46<6:07:10, 71.07s/it] 38%|███▊      | 191/500 [3:35:39<5:38:20, 65.70s/it] 38%|███▊      | 192/500 [3:36:59<5:59:04, 69.95s/it]Epoch: 192 done with learning rate 8.24E-03, Train loss: 1.50E+05, Train scatter: [0.336  0.0805 0.3948 0.5487]
L1 regularization loss: 1.87E+00, L2 regularization loss: 1.07E+00
Test scatter: [0.3277 0.0789 0.3901 0.5417], Lowest was [0.3277 0.0659 0.3363 0.4931]
Median for last 10 epochs: [0.4137 0.0891 0.4099 0.6056], Epochs since improvement 0
 39%|███▊      | 193/500 [3:37:52<5:32:14, 64.93s/it] 39%|███▉      | 194/500 [3:39:12<5:54:37, 69.54s/it]Epoch: 194 done with learning rate 8.19E-03, Train loss: 7.48E+04, Train scatter: [0.4412 0.0738 0.3813 0.5355]
L1 regularization loss: 1.87E+00, L2 regularization loss: 1.07E+00
Test scatter: [0.4298 0.0724 0.3766 0.5224], Lowest was [0.3277 0.0659 0.3363 0.4931]
Median for last 10 epochs: [0.4137 0.0789 0.3901 0.5417], Epochs since improvement 2
 39%|███▉      | 195/500 [3:40:05<5:28:19, 64.59s/it] 39%|███▉      | 196/500 [3:41:26<5:51:20, 69.34s/it]Epoch: 196 done with learning rate 8.13E-03, Train loss: -5.97E+04, Train scatter: [0.3874 0.0743 0.3778 0.5323]
L1 regularization loss: 1.87E+00, L2 regularization loss: 1.08E+00
Test scatter: [0.3795 0.0726 0.3737 0.5244], Lowest was [0.3277 0.0659 0.3363 0.4931]
Median for last 10 epochs: [0.3942 0.0789 0.3901 0.5417], Epochs since improvement 4
 39%|███▉      | 197/500 [3:42:19<5:25:31, 64.46s/it] 40%|███▉      | 198/500 [3:43:38<5:46:26, 68.83s/it]Epoch: 198 done with learning rate 8.07E-03, Train loss: -1.14E+05, Train scatter: [0.3443 0.0871 0.3809 0.5845]
L1 regularization loss: 1.87E+00, L2 regularization loss: 1.08E+00
Test scatter: [0.3401 0.0858 0.3763 0.5859], Lowest was [0.3277 0.0659 0.3363 0.4931]
Median for last 10 epochs: [0.3795 0.0789 0.3766 0.5417], Epochs since improvement 6
 40%|███▉      | 199/500 [3:44:31<5:21:33, 64.10s/it]Epoch: 200 done with learning rate 8.01E-03, Train loss: -6.24E+04, Train scatter: [0.2382 0.0681 0.3751 0.5167]
L1 regularization loss: 1.88E+00, L2 regularization loss: 1.08E+00
Test scatter: [0.2389 0.0675 0.3712 0.5051], Lowest was [0.2389 0.0659 0.3363 0.4931]
Median for last 10 epochs: [0.3401 0.0726 0.3763 0.5244], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 40%|████      | 200/500 [3:45:58<5:54:58, 70.99s/it] 40%|████      | 201/500 [3:46:51<5:26:58, 65.61s/it] 40%|████      | 202/500 [3:48:11<5:47:21, 69.94s/it]Epoch: 202 done with learning rate 7.95E-03, Train loss: -1.80E+05, Train scatter: [0.2329 0.068  0.3652 0.5125]
L1 regularization loss: 1.88E+00, L2 regularization loss: 1.08E+00
Test scatter: [0.2333 0.0667 0.3634 0.5046], Lowest was [0.2333 0.0659 0.3363 0.4931]
Median for last 10 epochs: [0.3401 0.0724 0.3737 0.5224], Epochs since improvement 0
 41%|████      | 203/500 [3:49:04<5:21:12, 64.89s/it] 41%|████      | 204/500 [3:50:25<5:43:51, 69.70s/it]Epoch: 204 done with learning rate 7.89E-03, Train loss: -4.99E+04, Train scatter: [0.4038 0.08   0.3827 0.5366]
L1 regularization loss: 1.88E+00, L2 regularization loss: 1.09E+00
Test scatter: [0.3958 0.0774 0.3781 0.5288], Lowest was [0.2333 0.0659 0.3363 0.4931]
Median for last 10 epochs: [0.3401 0.0726 0.3737 0.5244], Epochs since improvement 2
 41%|████      | 205/500 [3:51:18<5:18:17, 64.74s/it] 41%|████      | 206/500 [3:52:38<5:39:13, 69.23s/it]Epoch: 206 done with learning rate 7.83E-03, Train loss: -8.14E+04, Train scatter: [0.2563 0.0683 0.3607 0.5194]
L1 regularization loss: 1.88E+00, L2 regularization loss: 1.09E+00
Test scatter: [0.2553 0.0672 0.3596 0.5154], Lowest was [0.2333 0.0659 0.3363 0.4931]
Median for last 10 epochs: [0.2553 0.0675 0.3712 0.5154], Epochs since improvement 4
 41%|████▏     | 207/500 [3:53:31<5:14:30, 64.40s/it] 42%|████▏     | 208/500 [3:54:53<5:38:24, 69.54s/it]Epoch: 208 done with learning rate 7.77E-03, Train loss: -2.08E+05, Train scatter: [0.2619 0.0689 0.3674 0.5113]
L1 regularization loss: 1.88E+00, L2 regularization loss: 1.09E+00
Test scatter: [0.2601 0.0676 0.362  0.5001], Lowest was [0.2333 0.0659 0.3363 0.4931]
Median for last 10 epochs: [0.2553 0.0675 0.3634 0.5051], Epochs since improvement 6
 42%|████▏     | 209/500 [3:55:46<5:13:19, 64.60s/it]Epoch: 210 done with learning rate 7.71E-03, Train loss: -2.22E+05, Train scatter: [0.2541 0.0741 0.3681 0.5436]
L1 regularization loss: 1.88E+00, L2 regularization loss: 1.09E+00
Test scatter: [0.2537 0.0734 0.3685 0.5424], Lowest was [0.2333 0.0659 0.3363 0.4931]
Median for last 10 epochs: [0.2553 0.0676 0.3634 0.5154], Epochs since improvement 8
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 42%|████▏     | 210/500 [3:57:13<5:44:57, 71.37s/it] 42%|████▏     | 211/500 [3:58:06<5:17:12, 65.86s/it] 42%|████▏     | 212/500 [3:59:27<5:37:43, 70.36s/it]Epoch: 212 done with learning rate 7.65E-03, Train loss: -2.24E+05, Train scatter: [0.2221 0.0651 0.3559 0.5089]
L1 regularization loss: 1.89E+00, L2 regularization loss: 1.10E+00
Test scatter: [0.2238 0.0642 0.3512 0.4977], Lowest was [0.2238 0.0642 0.3363 0.4931]
Median for last 10 epochs: [0.2553 0.0676 0.362  0.5154], Epochs since improvement 0
 43%|████▎     | 213/500 [4:00:20<5:12:00, 65.23s/it] 43%|████▎     | 214/500 [4:01:40<5:32:40, 69.79s/it]Epoch: 214 done with learning rate 7.58E-03, Train loss: -2.36E+05, Train scatter: [0.2107 0.0639 0.3512 0.4995]
L1 regularization loss: 1.89E+00, L2 regularization loss: 1.10E+00
Test scatter: [0.2125 0.0627 0.3485 0.4902], Lowest was [0.2125 0.0627 0.3363 0.4902]
Median for last 10 epochs: [0.2537 0.0672 0.3596 0.5001], Epochs since improvement 0
 43%|████▎     | 215/500 [4:02:33<5:07:35, 64.76s/it] 43%|████▎     | 216/500 [4:03:54<5:28:26, 69.39s/it]Epoch: 216 done with learning rate 7.52E-03, Train loss: -2.06E+05, Train scatter: [0.214  0.0653 0.3563 0.5039]
L1 regularization loss: 1.89E+00, L2 regularization loss: 1.10E+00
Test scatter: [0.2143 0.0638 0.3513 0.4931], Lowest was [0.2125 0.0627 0.3363 0.4902]
Median for last 10 epochs: [0.2238 0.0642 0.3513 0.4977], Epochs since improvement 2
 43%|████▎     | 217/500 [4:04:47<5:04:13, 64.50s/it] 44%|████▎     | 218/500 [4:06:07<5:25:39, 69.29s/it]Epoch: 218 done with learning rate 7.46E-03, Train loss: -2.33E+05, Train scatter: [0.2119 0.0648 0.3512 0.5012]
L1 regularization loss: 1.89E+00, L2 regularization loss: 1.11E+00
Test scatter: [0.2126 0.0635 0.3482 0.4931], Lowest was [0.2125 0.0627 0.3363 0.4902]
Median for last 10 epochs: [0.2143 0.0638 0.3512 0.4931], Epochs since improvement 4
 44%|████▍     | 219/500 [4:07:00<5:01:37, 64.40s/it]Epoch: 220 done with learning rate 7.39E-03, Train loss: -2.31E+05, Train scatter: [0.2162 0.0731 0.3697 0.5093]
L1 regularization loss: 1.90E+00, L2 regularization loss: 1.11E+00
Test scatter: [0.216  0.0726 0.3718 0.5029], Lowest was [0.2125 0.0627 0.3363 0.4902]
Median for last 10 epochs: [0.2143 0.0638 0.3512 0.4931], Epochs since improvement 6
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 44%|████▍     | 220/500 [4:08:28<5:32:51, 71.33s/it] 44%|████▍     | 221/500 [4:09:21<5:06:07, 65.83s/it] 44%|████▍     | 222/500 [4:10:42<5:26:07, 70.39s/it]Epoch: 222 done with learning rate 7.33E-03, Train loss: -2.05E+05, Train scatter: [0.2138 0.0636 0.3491 0.498 ]
L1 regularization loss: 1.90E+00, L2 regularization loss: 1.11E+00
Test scatter: [0.2138 0.0626 0.3478 0.4892], Lowest was [0.2125 0.0626 0.3363 0.4892]
Median for last 10 epochs: [0.2138 0.0635 0.3485 0.4931], Epochs since improvement 0
 45%|████▍     | 223/500 [4:11:35<5:00:49, 65.16s/it] 45%|████▍     | 224/500 [4:12:55<5:20:06, 69.59s/it]Epoch: 224 done with learning rate 7.26E-03, Train loss: -2.30E+05, Train scatter: [0.2573 0.0768 0.3894 0.5372]
L1 regularization loss: 1.90E+00, L2 regularization loss: 1.12E+00
Test scatter: [0.2566 0.0765 0.3928 0.5328], Lowest was [0.2125 0.0626 0.3363 0.4892]
Median for last 10 epochs: [0.2143 0.0638 0.3513 0.4931], Epochs since improvement 2
 45%|████▌     | 225/500 [4:13:48<4:56:16, 64.64s/it] 45%|████▌     | 226/500 [4:15:07<5:15:35, 69.11s/it]Epoch: 226 done with learning rate 7.20E-03, Train loss: -2.45E+05, Train scatter: [0.21   0.0691 0.3508 0.5003]
L1 regularization loss: 1.90E+00, L2 regularization loss: 1.12E+00
Test scatter: [0.2096 0.0675 0.347  0.4912], Lowest was [0.2096 0.0626 0.3363 0.4892]
Median for last 10 epochs: [0.2138 0.0675 0.3482 0.4931], Epochs since improvement 0
 45%|████▌     | 227/500 [4:16:00<4:52:41, 64.33s/it] 46%|████▌     | 228/500 [4:17:22<5:15:12, 69.53s/it]Epoch: 228 done with learning rate 7.13E-03, Train loss: -1.97E+05, Train scatter: [0.2064 0.0639 0.3476 0.4959]
L1 regularization loss: 1.92E+00, L2 regularization loss: 1.13E+00
Test scatter: [0.2056 0.0628 0.3438 0.4867], Lowest was [0.2056 0.0626 0.3363 0.4867]
Median for last 10 epochs: [0.2138 0.0675 0.3478 0.4912], Epochs since improvement 0
 46%|████▌     | 229/500 [4:18:15<4:51:51, 64.62s/it]Epoch: 230 done with learning rate 7.06E-03, Train loss: -2.38E+05, Train scatter: [0.208  0.0639 0.3461 0.5002]
L1 regularization loss: 1.92E+00, L2 regularization loss: 1.13E+00
Test scatter: [0.2071 0.0626 0.3426 0.4908], Lowest was [0.2056 0.0626 0.3363 0.4867]
Median for last 10 epochs: [0.2096 0.0628 0.347  0.4908], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 46%|████▌     | 230/500 [4:19:44<5:23:34, 71.91s/it] 46%|████▌     | 231/500 [4:20:38<4:57:42, 66.41s/it] 46%|████▋     | 232/500 [4:22:00<5:17:40, 71.12s/it]Epoch: 232 done with learning rate 6.99E-03, Train loss: -2.64E+05, Train scatter: [0.2397 0.0749 0.364  0.5335]
L1 regularization loss: 1.92E+00, L2 regularization loss: 1.14E+00
Test scatter: [0.2406 0.0753 0.3658 0.5295], Lowest was [0.2056 0.0626 0.3363 0.4867]
Median for last 10 epochs: [0.2096 0.0675 0.347  0.4912], Epochs since improvement 2
 47%|████▋     | 233/500 [4:22:53<4:53:11, 65.89s/it] 47%|████▋     | 234/500 [4:24:16<5:13:42, 70.76s/it]Epoch: 234 done with learning rate 6.93E-03, Train loss: -2.41E+05, Train scatter: [0.2017 0.0627 0.3435 0.4936]
L1 regularization loss: 1.92E+00, L2 regularization loss: 1.14E+00
Test scatter: [0.2029 0.0618 0.3424 0.4854], Lowest was [0.2029 0.0618 0.3363 0.4854]
Median for last 10 epochs: [0.2071 0.0628 0.3438 0.4908], Epochs since improvement 0
 47%|████▋     | 235/500 [4:25:09<4:49:58, 65.66s/it] 47%|████▋     | 236/500 [4:26:31<5:09:41, 70.38s/it]Epoch: 236 done with learning rate 6.86E-03, Train loss: -2.59E+05, Train scatter: [0.2052 0.0703 0.3536 0.5172]
L1 regularization loss: 1.92E+00, L2 regularization loss: 1.14E+00
Test scatter: [0.2043 0.0683 0.3492 0.5089], Lowest was [0.2029 0.0618 0.3363 0.4854]
Median for last 10 epochs: [0.2056 0.0628 0.3438 0.4908], Epochs since improvement 2
 47%|████▋     | 237/500 [4:27:24<4:46:29, 65.36s/it] 48%|████▊     | 238/500 [4:28:46<5:06:00, 70.08s/it]Epoch: 238 done with learning rate 6.79E-03, Train loss: -6.79E+04, Train scatter: [0.2541 0.0677 0.345  0.5138]
L1 regularization loss: 1.94E+00, L2 regularization loss: 1.15E+00
Test scatter: [0.2503 0.0659 0.3418 0.5041], Lowest was [0.2029 0.0618 0.3363 0.4854]
Median for last 10 epochs: [0.2071 0.0659 0.3426 0.5041], Epochs since improvement 4
 48%|████▊     | 239/500 [4:29:39<4:43:26, 65.16s/it]Epoch: 240 done with learning rate 6.72E-03, Train loss: -2.63E+05, Train scatter: [0.1925 0.0608 0.3338 0.4893]
L1 regularization loss: 1.93E+00, L2 regularization loss: 1.16E+00
Test scatter: [0.1927 0.0598 0.3303 0.4809], Lowest was [0.1927 0.0598 0.3303 0.4809]
Median for last 10 epochs: [0.2043 0.0659 0.3424 0.5041], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 48%|████▊     | 240/500 [4:31:07<5:12:19, 72.08s/it] 48%|████▊     | 241/500 [4:32:01<4:47:15, 66.54s/it] 48%|████▊     | 242/500 [4:33:23<5:05:24, 71.03s/it]Epoch: 242 done with learning rate 6.65E-03, Train loss: -2.70E+05, Train scatter: [0.2209 0.0614 0.3412 0.4902]
L1 regularization loss: 1.93E+00, L2 regularization loss: 1.16E+00
Test scatter: [0.2177 0.0601 0.3366 0.4807], Lowest was [0.1927 0.0598 0.3303 0.4807]
Median for last 10 epochs: [0.2043 0.0618 0.3418 0.4854], Epochs since improvement 0
 49%|████▊     | 243/500 [4:34:16<4:41:51, 65.80s/it] 49%|████▉     | 244/500 [4:35:39<5:02:32, 70.91s/it]Epoch: 244 done with learning rate 6.58E-03, Train loss: -2.28E+05, Train scatter: [0.1936 0.0631 0.3382 0.4929]
L1 regularization loss: 1.94E+00, L2 regularization loss: 1.17E+00
Test scatter: [0.1932 0.0616 0.3346 0.4856], Lowest was [0.1927 0.0598 0.3303 0.4807]
Median for last 10 epochs: [0.2043 0.0616 0.3366 0.4856], Epochs since improvement 2
 49%|████▉     | 245/500 [4:36:33<4:39:19, 65.72s/it] 49%|████▉     | 246/500 [4:37:54<4:58:15, 70.46s/it]Epoch: 246 done with learning rate 6.51E-03, Train loss: -2.62E+05, Train scatter: [0.2037 0.0613 0.3341 0.4901]
L1 regularization loss: 1.94E+00, L2 regularization loss: 1.17E+00
Test scatter: [0.2029 0.0603 0.3305 0.4836], Lowest was [0.1927 0.0598 0.3303 0.4807]
Median for last 10 epochs: [0.2029 0.0603 0.3346 0.4836], Epochs since improvement 4
 49%|████▉     | 247/500 [4:38:48<4:35:54, 65.43s/it] 50%|████▉     | 248/500 [4:40:10<4:55:48, 70.43s/it]Epoch: 248 done with learning rate 6.44E-03, Train loss: -2.47E+05, Train scatter: [0.1881 0.06   0.3308 0.4859]
L1 regularization loss: 1.94E+00, L2 regularization loss: 1.17E+00
Test scatter: [0.1882 0.0591 0.3289 0.4782], Lowest was [0.1882 0.0591 0.3289 0.4782]
Median for last 10 epochs: [0.1932 0.0601 0.3305 0.4809], Epochs since improvement 0
 50%|████▉     | 249/500 [4:41:04<4:33:34, 65.40s/it]Epoch: 250 done with learning rate 6.37E-03, Train loss: 6.98E+05, Train scatter: [0.4937 0.1055 0.5364 0.7449]
L1 regularization loss: 2.14E+00, L2 regularization loss: 1.30E+00
Test scatter: [0.4911 0.1035 0.5281 0.7406], Lowest was [0.1882 0.0591 0.3289 0.4782]
Median for last 10 epochs: [0.2029 0.0603 0.3346 0.4836], Epochs since improvement 2
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 50%|█████     | 250/500 [4:42:34<5:04:23, 73.06s/it] 50%|█████     | 251/500 [4:43:28<4:38:51, 67.19s/it] 50%|█████     | 252/500 [4:44:49<4:55:08, 71.41s/it]Epoch: 252 done with learning rate 6.30E-03, Train loss: 1.54E+05, Train scatter: [0.4473 0.0915 0.499  0.6225]
L1 regularization loss: 2.14E+00, L2 regularization loss: 1.31E+00
Test scatter: [0.4401 0.0896 0.4912 0.6117], Lowest was [0.1882 0.0591 0.3289 0.4782]
Median for last 10 epochs: [0.2029 0.0616 0.3346 0.4856], Epochs since improvement 4
 51%|█████     | 253/500 [4:45:43<4:31:51, 66.04s/it] 51%|█████     | 254/500 [4:47:04<4:49:12, 70.54s/it]Epoch: 254 done with learning rate 6.23E-03, Train loss: 6.02E+04, Train scatter: [0.4284 0.1058 0.5521 0.5782]
L1 regularization loss: 2.14E+00, L2 regularization loss: 1.32E+00
Test scatter: [0.421  0.1042 0.5438 0.5664], Lowest was [0.1882 0.0591 0.3289 0.4782]
Median for last 10 epochs: [0.421  0.0896 0.4912 0.5664], Epochs since improvement 6
 51%|█████     | 255/500 [4:47:57<4:27:11, 65.44s/it] 51%|█████     | 256/500 [4:49:18<4:45:16, 70.15s/it]Epoch: 256 done with learning rate 6.15E-03, Train loss: 3.68E+03, Train scatter: [0.4098 0.0797 0.5467 0.5512]
L1 regularization loss: 2.14E+00, L2 regularization loss: 1.33E+00
Test scatter: [0.4018 0.0778 0.5384 0.5386], Lowest was [0.1882 0.0591 0.3289 0.4782]
Median for last 10 epochs: [0.421  0.0896 0.5281 0.5664], Epochs since improvement 8
 51%|█████▏    | 257/500 [4:50:12<4:23:59, 65.18s/it] 52%|█████▏    | 258/500 [4:51:33<4:42:04, 69.94s/it]Epoch: 258 done with learning rate 6.08E-03, Train loss: -1.72E+04, Train scatter: [0.3483 0.0897 0.4743 0.5544]
L1 regularization loss: 2.14E+00, L2 regularization loss: 1.34E+00
Test scatter: [0.3407 0.0883 0.4684 0.5413], Lowest was [0.1882 0.0591 0.3289 0.4782]
Median for last 10 epochs: [0.421  0.0896 0.5281 0.5664], Epochs since improvement 10
 52%|█████▏    | 259/500 [4:52:27<4:21:09, 65.02s/it]Epoch: 260 done with learning rate 6.01E-03, Train loss: -8.72E+04, Train scatter: [0.4343 0.0724 0.4349 0.5369]
L1 regularization loss: 2.19E+00, L2 regularization loss: 1.37E+00
Test scatter: [0.4256 0.0709 0.4247 0.5282], Lowest was [0.1882 0.0591 0.3289 0.4782]
Median for last 10 epochs: [0.421  0.0883 0.4912 0.5413], Epochs since improvement 12
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 52%|█████▏    | 260/500 [4:53:55<4:48:04, 72.02s/it] 52%|█████▏    | 261/500 [4:54:49<4:24:51, 66.49s/it] 52%|█████▏    | 262/500 [4:56:10<4:41:29, 70.96s/it]Epoch: 262 done with learning rate 5.94E-03, Train loss: -8.40E+04, Train scatter: [0.3053 0.0685 0.3907 0.5251]
L1 regularization loss: 2.19E+00, L2 regularization loss: 1.38E+00
Test scatter: [0.2994 0.067  0.3837 0.5159], Lowest was [0.1882 0.0591 0.3289 0.4782]
Median for last 10 epochs: [0.4018 0.0778 0.4684 0.5386], Epochs since improvement 14
 53%|█████▎    | 263/500 [4:57:03<4:19:39, 65.73s/it] 53%|█████▎    | 264/500 [4:58:25<4:37:24, 70.53s/it]Epoch: 264 done with learning rate 5.86E-03, Train loss: -2.23E+05, Train scatter: [0.246  0.0652 0.3653 0.509 ]
L1 regularization loss: 2.18E+00, L2 regularization loss: 1.38E+00
Test scatter: [0.2434 0.0638 0.3601 0.5001], Lowest was [0.1882 0.0591 0.3289 0.4782]
Median for last 10 epochs: [0.3407 0.0709 0.4247 0.5282], Epochs since improvement 16
 53%|█████▎    | 265/500 [4:59:19<4:16:21, 65.45s/it] 53%|█████▎    | 266/500 [5:00:41<4:34:55, 70.49s/it]Epoch: 266 done with learning rate 5.79E-03, Train loss: -2.38E+05, Train scatter: [0.3065 0.0661 0.3767 0.5188]
L1 regularization loss: 2.18E+00, L2 regularization loss: 1.38E+00
Test scatter: [0.2941 0.0652 0.3727 0.5103], Lowest was [0.1882 0.0591 0.3289 0.4782]
Median for last 10 epochs: [0.2994 0.067  0.3837 0.5159], Epochs since improvement 18
 53%|█████▎    | 267/500 [5:01:35<4:14:00, 65.41s/it] 54%|█████▎    | 268/500 [5:02:56<4:31:18, 70.17s/it]Epoch: 268 done with learning rate 5.72E-03, Train loss: -1.84E+05, Train scatter: [0.2759 0.0658 0.3585 0.5065]
L1 regularization loss: 2.17E+00, L2 regularization loss: 1.38E+00
Test scatter: [0.2716 0.0646 0.3551 0.4969], Lowest was [0.1882 0.0591 0.3289 0.4782]
Median for last 10 epochs: [0.2941 0.0652 0.3727 0.5103], Epochs since improvement 20
 54%|█████▍    | 269/500 [5:03:49<4:11:00, 65.20s/it]Epoch: 270 done with learning rate 5.64E-03, Train loss: -2.73E+05, Train scatter: [0.3215 0.0652 0.3532 0.4979]
L1 regularization loss: 2.17E+00, L2 regularization loss: 1.38E+00
Test scatter: [0.3172 0.0644 0.3494 0.4898], Lowest was [0.1882 0.0591 0.3289 0.4782]
Median for last 10 epochs: [0.2941 0.0646 0.3601 0.5001], Epochs since improvement 22
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 54%|█████▍    | 269/500 [5:05:18<4:22:10, 68.10s/it]
Exited after 270 epochs due to early stopping
18318.58 seconds spent training, 36.637 seconds per epoch. Processed 1901 trees per second
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
[0.31724003 0.06436699 0.34942028 0.48981848]
{'epoch_exit': 269, 'scatter_m_star': 0.31724003, 'lowest_m_star': 0.18817055, 'last20_m_star': 0.3289968, 'last10_m_star': 0.29410046, 'scatter_v_disk': 0.06436699, 'lowest_v_disk': 0.05909855, 'last20_v_disk': 0.06895284, 'last10_v_disk': 0.06458711, 'scatter_m_cold': 0.34942028, 'lowest_m_cold': 0.32888132, 'last20_m_cold': 0.4042043, 'last10_m_cold': 0.36014712, 'scatter_sfr_100': 0.48981848, 'lowest_sfr_100': 0.47824988, 'last20_sfr_100': 0.5220473, 'last10_sfr_100': 0.50008816}
Finished 1/1
Exited training after None epochs
Experiment 6 done: 6 / 6
