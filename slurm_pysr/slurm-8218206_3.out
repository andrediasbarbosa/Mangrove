Starting process with 6 experiments
{'conv_layers': [1, 2, 3], 'batch_size': [256, 512]}
Exploring ['conv_layers', 'batch_size']
Currently doing [  1 256]
{'experiment': 'GraphMerge', 'group': 'meta_pysr', 'move': False, 'model': 'MetaEdge', 'log': True, 'run_params': {'n_epochs': 500, 'n_trials': 1, 'batch_size': 256, 'val_epoch': 2, 'early_stopping': True, 'patience': 20, 'l1_lambda': 0.0001, 'l2_lambda': 0.0001, 'loss_func': 'GaussNd', 'metrics': 'test_multi_varrho', 'performance_plot': 'multi_base', 'save': True, 'seed': True, 'num_workers': 4}, 'learn_params': {'learning_rate': 0.01, 'schedule': 'onecycle', 'g_up': 1, 'g_down': 0.95, 'warmup': 4, 'period': 5, 'eta_min': 1e-05}, 'hyper_params': {'hidden_channels': 128, 'conv_layers': 1, 'conv_activation': 'relu', 'decode_activation': 'leakyrelu', 'decode_layers': 2, 'layernorm': True, 'agg': 'sum', 'variance': True, 'rho': 0}, 'data_params': {'case': 'vlarge_all_4t_z0.0_quantile_raw', 'targets': [0, 1, 2, 3], 'del_feats': [], 'split': 0.8, 'test': 0, 'scale': 0}, 'experiment_name': 'con1bat256'}
/home/cj1223/../../scratch/gpfs/cj1223/GraphResults/results_meta_pysr_120322/MetaEdge_vlarge_all_4t_z0.0_quantile_raw_dmwmlx
RelU conv activation
LeakyRelU decode activation
N_params 298056
Made folder for saving model
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:31<4:20:48, 31.36s/it]  0%|          | 2/500 [01:19<5:42:46, 41.30s/it]Epoch: 2 done with learning rate 4.17E-04, Train loss: 1.27E+07, Train scatter: [0.9352 0.1672 0.5441 0.9954]
L1 regularization loss: 1.52E+00, L2 regularization loss: 3.31E-01
Test scatter: [0.9196 0.1665 0.5355 0.9851], Lowest was [0.9196 0.1665 0.5355 0.9851]
Median for last 10 epochs: [0.9196 0.1665 0.5355 0.9851], Epochs since improvement 2
  1%|          | 3/500 [01:50<5:04:03, 36.71s/it]  1%|          | 4/500 [02:39<5:42:32, 41.44s/it]Epoch: 4 done with learning rate 4.67E-04, Train loss: 6.86E+06, Train scatter: [0.9351 0.1479 0.5439 0.9954]
L1 regularization loss: 1.53E+00, L2 regularization loss: 3.54E-01
Test scatter: [0.9195 0.1467 0.5353 0.985 ], Lowest was [0.9195 0.1467 0.5353 0.985 ]
Median for last 10 epochs: [0.9195 0.1467 0.5353 0.985 ], Epochs since improvement 0
  1%|          | 5/500 [03:10<5:11:23, 37.75s/it]  1%|          | 6/500 [03:59<5:42:33, 41.61s/it]Epoch: 6 done with learning rate 5.51E-04, Train loss: 5.24E+06, Train scatter: [0.9346 0.1261 0.5421 0.722 ]
L1 regularization loss: 1.57E+00, L2 regularization loss: 3.90E-01
Test scatter: [0.9189 0.1255 0.5334 0.7119], Lowest was [0.9189 0.1255 0.5334 0.7119]
Median for last 10 epochs: [0.9189 0.1255 0.5334 0.7119], Epochs since improvement 0
  1%|▏         | 7/500 [04:31<5:14:20, 38.26s/it]  2%|▏         | 8/500 [05:19<5:40:43, 41.55s/it]Epoch: 8 done with learning rate 6.67E-04, Train loss: 4.56E+06, Train scatter: [0.9174 0.1105 0.5335 0.621 ]
L1 regularization loss: 1.59E+00, L2 regularization loss: 4.03E-01
Test scatter: [0.9013 0.1116 0.5251 0.6128], Lowest was [0.9013 0.1116 0.5251 0.6128]
Median for last 10 epochs: [0.9101 0.1186 0.5293 0.6623], Epochs since improvement 0
  2%|▏         | 9/500 [05:51<5:13:52, 38.35s/it]Epoch: 10 done with learning rate 8.15E-04, Train loss: 3.59E+06, Train scatter: [0.7728 0.1009 0.4512 0.6361]
L1 regularization loss: 1.61E+00, L2 regularization loss: 4.15E-01
Test scatter: [0.7548 0.1002 0.4377 0.6205], Lowest was [0.7548 0.1002 0.4377 0.6128]
Median for last 10 epochs: [0.9013 0.1116 0.5251 0.6205], Epochs since improvement 0
  2%|▏         | 10/500 [06:45<5:53:58, 43.34s/it]  2%|▏         | 11/500 [07:16<5:22:55, 39.62s/it]  2%|▏         | 12/500 [08:05<5:45:39, 42.50s/it]Epoch: 12 done with learning rate 9.94E-04, Train loss: 2.30E+06, Train scatter: [0.579  0.0945 0.4102 0.5874]
L1 regularization loss: 1.63E+00, L2 regularization loss: 4.26E-01
Test scatter: [0.5763 0.0947 0.4118 0.586 ], Lowest was [0.5763 0.0947 0.4118 0.586 ]
Median for last 10 epochs: [0.9013 0.1116 0.5251 0.6205], Epochs since improvement 0
  3%|▎         | 13/500 [08:37<5:17:18, 39.09s/it]  3%|▎         | 14/500 [09:26<5:41:34, 42.17s/it]Epoch: 14 done with learning rate 1.20E-03, Train loss: 1.50E+06, Train scatter: [0.5825 0.0919 0.3665 0.5899]
L1 regularization loss: 1.64E+00, L2 regularization loss: 4.33E-01
Test scatter: [0.559  0.0942 0.3768 0.5941], Lowest was [0.559  0.0942 0.3768 0.586 ]
Median for last 10 epochs: [0.7548 0.1002 0.4377 0.6128], Epochs since improvement 0
  3%|▎         | 15/500 [09:57<5:14:12, 38.87s/it]  3%|▎         | 16/500 [10:46<5:37:53, 41.89s/it]Epoch: 16 done with learning rate 1.44E-03, Train loss: 1.17E+06, Train scatter: [0.5288 0.0881 0.3704 0.5962]
L1 regularization loss: 1.65E+00, L2 regularization loss: 4.41E-01
Test scatter: [0.5268 0.0887 0.3796 0.5958], Lowest was [0.5268 0.0887 0.3768 0.586 ]
Median for last 10 epochs: [0.5763 0.0947 0.4118 0.5958], Epochs since improvement 0
  3%|▎         | 17/500 [11:17<5:11:29, 38.69s/it]  4%|▎         | 18/500 [12:06<5:35:34, 41.77s/it]Epoch: 18 done with learning rate 1.70E-03, Train loss: 1.16E+06, Train scatter: [0.4954 0.0862 0.3625 0.5721]
L1 regularization loss: 1.68E+00, L2 regularization loss: 4.50E-01
Test scatter: [0.4922 0.0881 0.3659 0.574 ], Lowest was [0.4922 0.0881 0.3659 0.574 ]
Median for last 10 epochs: [0.559  0.0942 0.3796 0.5941], Epochs since improvement 0
  4%|▍         | 19/500 [12:38<5:09:50, 38.65s/it]Epoch: 20 done with learning rate 1.99E-03, Train loss: 9.08E+05, Train scatter: [0.4683 0.0825 0.3224 0.5499]
L1 regularization loss: 1.70E+00, L2 regularization loss: 4.61E-01
Test scatter: [0.459  0.0837 0.3303 0.5513], Lowest was [0.459  0.0837 0.3303 0.5513]
Median for last 10 epochs: [0.5268 0.0887 0.3768 0.586 ], Epochs since improvement 0
  4%|▍         | 20/500 [13:32<5:47:16, 43.41s/it]  4%|▍         | 21/500 [14:03<5:17:23, 39.76s/it]  4%|▍         | 22/500 [14:52<5:38:38, 42.51s/it]Epoch: 22 done with learning rate 2.30E-03, Train loss: 7.69E+05, Train scatter: [0.5794 0.0802 0.3218 0.5657]
L1 regularization loss: 1.73E+00, L2 regularization loss: 4.75E-01
Test scatter: [0.5552 0.0815 0.3257 0.5641], Lowest was [0.459  0.0815 0.3257 0.5513]
Median for last 10 epochs: [0.5268 0.0881 0.3659 0.574 ], Epochs since improvement 0
  5%|▍         | 23/500 [15:23<5:10:53, 39.11s/it]  5%|▍         | 24/500 [16:12<5:33:48, 42.08s/it]Epoch: 24 done with learning rate 2.63E-03, Train loss: 6.98E+05, Train scatter: [0.4501 0.0786 0.3383 0.5298]
L1 regularization loss: 1.77E+00, L2 regularization loss: 4.91E-01
Test scatter: [0.4519 0.0806 0.3469 0.5303], Lowest was [0.4519 0.0806 0.3257 0.5303]
Median for last 10 epochs: [0.4922 0.0837 0.3469 0.5641], Epochs since improvement 0
  5%|▌         | 25/500 [16:44<5:07:30, 38.84s/it]  5%|▌         | 26/500 [17:32<5:29:50, 41.75s/it]Epoch: 26 done with learning rate 2.98E-03, Train loss: 6.12E+05, Train scatter: [0.4503 0.0775 0.312  0.5318]
L1 regularization loss: 1.80E+00, L2 regularization loss: 5.09E-01
Test scatter: [0.4495 0.0796 0.3216 0.5284], Lowest was [0.4495 0.0796 0.3216 0.5284]
Median for last 10 epochs: [0.459  0.0815 0.3303 0.5513], Epochs since improvement 0
  5%|▌         | 27/500 [18:04<5:04:20, 38.60s/it]  6%|▌         | 28/500 [18:52<5:27:45, 41.66s/it]Epoch: 28 done with learning rate 3.34E-03, Train loss: 5.32E+05, Train scatter: [0.442  0.0759 0.3265 0.5295]
L1 regularization loss: 1.84E+00, L2 regularization loss: 5.31E-01
Test scatter: [0.4396 0.0777 0.333  0.531 ], Lowest was [0.4396 0.0777 0.3216 0.5284]
Median for last 10 epochs: [0.4519 0.0806 0.3303 0.531 ], Epochs since improvement 0
  6%|▌         | 29/500 [19:24<5:02:21, 38.52s/it]Epoch: 30 done with learning rate 3.72E-03, Train loss: 5.32E+05, Train scatter: [0.6283 0.08   0.3063 0.5135]
L1 regularization loss: 1.89E+00, L2 regularization loss: 5.55E-01
Test scatter: [0.72   0.084  0.3169 0.5165], Lowest was [0.4396 0.0777 0.3169 0.5165]
Median for last 10 epochs: [0.4519 0.0806 0.3257 0.5303], Epochs since improvement 0
  6%|▌         | 30/500 [20:17<5:37:12, 43.05s/it]  6%|▌         | 31/500 [20:49<5:08:59, 39.53s/it]  6%|▋         | 32/500 [21:38<5:30:43, 42.40s/it]Epoch: 32 done with learning rate 4.10E-03, Train loss: 4.20E+05, Train scatter: [0.4743 0.0762 0.2957 0.522 ]
L1 regularization loss: 1.92E+00, L2 regularization loss: 5.81E-01
Test scatter: [0.4678 0.0775 0.3022 0.5176], Lowest was [0.4396 0.0775 0.3022 0.5165]
Median for last 10 epochs: [0.4519 0.0796 0.3216 0.5284], Epochs since improvement 0
  7%|▋         | 33/500 [22:09<5:03:55, 39.05s/it]  7%|▋         | 34/500 [22:58<5:27:10, 42.12s/it]Epoch: 34 done with learning rate 4.50E-03, Train loss: 3.69E+05, Train scatter: [0.4684 0.0769 0.3034 0.5129]
L1 regularization loss: 1.96E+00, L2 regularization loss: 6.10E-01
Test scatter: [0.5327 0.0797 0.3145 0.5098], Lowest was [0.4396 0.0775 0.3022 0.5098]
Median for last 10 epochs: [0.4678 0.0796 0.3169 0.5176], Epochs since improvement 0
  7%|▋         | 35/500 [23:29<5:01:18, 38.88s/it]  7%|▋         | 36/500 [24:18<5:24:01, 41.90s/it]Epoch: 36 done with learning rate 4.90E-03, Train loss: 3.07E+05, Train scatter: [0.3922 0.0701 0.2753 0.5126]
L1 regularization loss: 2.00E+00, L2 regularization loss: 6.38E-01
Test scatter: [0.4105 0.0717 0.2835 0.5027], Lowest was [0.4105 0.0717 0.2835 0.5027]
Median for last 10 epochs: [0.4678 0.0777 0.3145 0.5165], Epochs since improvement 0
  7%|▋         | 37/500 [24:50<4:58:40, 38.71s/it]  8%|▊         | 38/500 [25:39<5:22:21, 41.87s/it]Epoch: 38 done with learning rate 5.30E-03, Train loss: 2.87E+05, Train scatter: [0.4044 0.0733 0.288  0.5042]
L1 regularization loss: 2.04E+00, L2 regularization loss: 6.69E-01
Test scatter: [0.4072 0.075  0.3009 0.502 ], Lowest was [0.4072 0.0717 0.2835 0.502 ]
Median for last 10 epochs: [0.4678 0.0775 0.3022 0.5098], Epochs since improvement 0
  8%|▊         | 39/500 [26:10<4:57:04, 38.66s/it]Epoch: 40 done with learning rate 5.70E-03, Train loss: 1.05E+05, Train scatter: [0.3558 0.0699 0.2823 0.4935]
L1 regularization loss: 2.07E+00, L2 regularization loss: 7.00E-01
Test scatter: [0.3607 0.0715 0.2891 0.4951], Lowest was [0.3607 0.0715 0.2835 0.4951]
Median for last 10 epochs: [0.4105 0.075  0.3009 0.5027], Epochs since improvement 0
  8%|▊         | 40/500 [27:04<5:32:30, 43.37s/it]  8%|▊         | 41/500 [27:36<5:03:44, 39.70s/it]  8%|▊         | 42/500 [28:25<5:24:42, 42.54s/it]Epoch: 42 done with learning rate 6.10E-03, Train loss: -2.60E+05, Train scatter: [0.4131 0.0636 0.2636 0.4763]
L1 regularization loss: 2.09E+00, L2 regularization loss: 7.22E-01
Test scatter: [0.4087 0.0648 0.2763 0.4774], Lowest was [0.3607 0.0648 0.2763 0.4774]
Median for last 10 epochs: [0.4087 0.0717 0.2891 0.502 ], Epochs since improvement 0
  9%|▊         | 43/500 [28:56<4:58:28, 39.19s/it]  9%|▉         | 44/500 [29:45<5:18:57, 41.97s/it]Epoch: 44 done with learning rate 6.49E-03, Train loss: -2.77E+05, Train scatter: [0.4878 0.0625 0.2658 0.4799]
L1 regularization loss: 2.16E+00, L2 regularization loss: 7.53E-01
Test scatter: [0.4889 0.0639 0.278  0.4849], Lowest was [0.3607 0.0639 0.2763 0.4774]
Median for last 10 epochs: [0.4087 0.0715 0.2835 0.4951], Epochs since improvement 0
  9%|▉         | 45/500 [30:16<4:53:55, 38.76s/it]  9%|▉         | 46/500 [31:05<5:17:09, 41.92s/it]Epoch: 46 done with learning rate 6.87E-03, Train loss: -2.81E+05, Train scatter: [0.3527 0.061  0.2736 0.4855]
L1 regularization loss: 2.18E+00, L2 regularization loss: 7.79E-01
Test scatter: [0.3508 0.0621 0.2831 0.4828], Lowest was [0.3508 0.0621 0.2763 0.4774]
Median for last 10 epochs: [0.4072 0.0648 0.2831 0.4849], Epochs since improvement 0
  9%|▉         | 47/500 [31:37<4:52:36, 38.76s/it] 10%|▉         | 48/500 [32:26<5:16:27, 42.01s/it]Epoch: 48 done with learning rate 7.24E-03, Train loss: -2.90E+05, Train scatter: [0.3757 0.0641 0.2876 0.4863]
L1 regularization loss: 2.21E+00, L2 regularization loss: 8.06E-01
Test scatter: [0.3742 0.0642 0.294  0.4853], Lowest was [0.3508 0.0621 0.2763 0.4774]
Median for last 10 epochs: [0.3742 0.0642 0.2831 0.4849], Epochs since improvement 2
 10%|▉         | 49/500 [32:58<4:52:04, 38.86s/it]Epoch: 50 done with learning rate 7.60E-03, Train loss: -2.84E+05, Train scatter: [0.3566 0.0613 0.2721 0.4659]
L1 regularization loss: 2.25E+00, L2 regularization loss: 8.38E-01
Test scatter: [0.3532 0.0623 0.2794 0.4664], Lowest was [0.3508 0.0621 0.2763 0.4664]
Median for last 10 epochs: [0.3742 0.0639 0.2794 0.4828], Epochs since improvement 0
 10%|█         | 50/500 [33:52<5:26:07, 43.48s/it] 10%|█         | 51/500 [34:23<4:58:31, 39.89s/it] 10%|█         | 52/500 [35:13<5:18:44, 42.69s/it]Epoch: 52 done with learning rate 7.94E-03, Train loss: 8.79E+07, Train scatter: [0.9354 0.1728 0.5441 0.9954]
L1 regularization loss: 4.29E+00, L2 regularization loss: 1.62E+00
Test scatter: [0.9198 0.169  0.5355 0.985 ], Lowest was [0.3508 0.0621 0.2763 0.4664]
Median for last 10 epochs: [0.3742 0.0639 0.2831 0.4849], Epochs since improvement 2
 11%|█         | 53/500 [35:44<4:52:49, 39.30s/it] 11%|█         | 54/500 [36:33<5:14:40, 42.33s/it]Epoch: 54 done with learning rate 8.26E-03, Train loss: 1.53E+06, Train scatter: [0.9359 0.1728 0.544  0.9954]
L1 regularization loss: 4.29E+00, L2 regularization loss: 1.70E+00
Test scatter: [0.9203 0.1689 0.5355 0.9851], Lowest was [0.3508 0.0621 0.2763 0.4664]
Median for last 10 epochs: [0.3742 0.0642 0.294  0.4853], Epochs since improvement 4
 11%|█         | 55/500 [37:05<4:49:41, 39.06s/it] 11%|█         | 56/500 [37:54<5:11:22, 42.08s/it]Epoch: 56 done with learning rate 8.56E-03, Train loss: 1.11E+06, Train scatter: [0.9356 0.1727 0.544  0.9954]
L1 regularization loss: 4.32E+00, L2 regularization loss: 1.80E+00
Test scatter: [0.92   0.1689 0.5354 0.9851], Lowest was [0.3508 0.0621 0.2763 0.4664]
Median for last 10 epochs: [0.9198 0.1689 0.5354 0.985 ], Epochs since improvement 6
 11%|█▏        | 57/500 [38:25<4:47:01, 38.87s/it] 12%|█▏        | 58/500 [39:14<5:08:22, 41.86s/it]Epoch: 58 done with learning rate 8.83E-03, Train loss: 1.01E+06, Train scatter: [0.9345 0.1726 0.544  0.9952]
L1 regularization loss: 4.29E+00, L2 regularization loss: 1.76E+00
Test scatter: [0.9192 0.1688 0.5354 0.9849], Lowest was [0.3508 0.0621 0.2763 0.4664]
Median for last 10 epochs: [0.9198 0.1689 0.5354 0.985 ], Epochs since improvement 8
 12%|█▏        | 59/500 [39:46<4:44:32, 38.71s/it]Epoch: 60 done with learning rate 9.08E-03, Train loss: 8.72E+05, Train scatter: [0.9262 0.1722 0.5436 0.9923]
L1 regularization loss: 4.27E+00, L2 regularization loss: 1.75E+00
Test scatter: [0.9107 0.1684 0.535  0.982 ], Lowest was [0.3508 0.0621 0.2763 0.4664]
Median for last 10 epochs: [0.9198 0.1689 0.5354 0.985 ], Epochs since improvement 10
 12%|█▏        | 60/500 [40:40<5:19:27, 43.56s/it] 12%|█▏        | 61/500 [41:12<4:52:04, 39.92s/it] 12%|█▏        | 62/500 [42:01<5:12:34, 42.82s/it]Epoch: 62 done with learning rate 9.31E-03, Train loss: 1.01E+08, Train scatter: [0.9351 0.1728 0.5441 0.9952]
L1 regularization loss: 5.26E+00, L2 regularization loss: 2.49E+00
Test scatter: [0.9195 0.169  0.5355 0.9849], Lowest was [0.3508 0.0621 0.2763 0.4664]
Median for last 10 epochs: [0.9195 0.1689 0.5354 0.9849], Epochs since improvement 12
 13%|█▎        | 63/500 [42:33<4:46:59, 39.40s/it] 13%|█▎        | 64/500 [43:22<5:07:07, 42.27s/it]Epoch: 64 done with learning rate 9.50E-03, Train loss: 2.39E+06, Train scatter: [0.9351 0.1728 0.5441 0.9952]
L1 regularization loss: 5.27E+00, L2 regularization loss: 2.52E+00
Test scatter: [0.9195 0.169  0.5355 0.9849], Lowest was [0.3508 0.0621 0.2763 0.4664]
Median for last 10 epochs: [0.9195 0.1689 0.5354 0.9849], Epochs since improvement 14
 13%|█▎        | 65/500 [43:53<4:42:34, 38.98s/it] 13%|█▎        | 66/500 [44:43<5:05:14, 42.20s/it]Epoch: 66 done with learning rate 9.66E-03, Train loss: 2.02E+06, Train scatter: [0.9351 0.1728 0.5441 0.9952]
L1 regularization loss: 5.27E+00, L2 regularization loss: 2.54E+00
Test scatter: [0.9195 0.169  0.5355 0.9849], Lowest was [0.3508 0.0621 0.2763 0.4664]
Median for last 10 epochs: [0.9195 0.169  0.5355 0.9849], Epochs since improvement 16
 13%|█▎        | 67/500 [45:14<4:41:01, 38.94s/it] 14%|█▎        | 68/500 [46:04<5:03:09, 42.11s/it]Epoch: 68 done with learning rate 9.80E-03, Train loss: 1.78E+06, Train scatter: [0.9351 0.1728 0.5441 0.9952]
L1 regularization loss: 5.26E+00, L2 regularization loss: 2.56E+00
Test scatter: [0.9195 0.169  0.5355 0.9849], Lowest was [0.3508 0.0621 0.2763 0.4664]
Median for last 10 epochs: [0.9195 0.169  0.5355 0.9849], Epochs since improvement 18
 14%|█▍        | 69/500 [46:35<4:39:26, 38.90s/it]Epoch: 70 done with learning rate 9.90E-03, Train loss: 1.61E+06, Train scatter: [0.9351 0.1728 0.5441 0.9952]
L1 regularization loss: 5.26E+00, L2 regularization loss: 2.59E+00
Test scatter: [0.9195 0.1689 0.5355 0.9849], Lowest was [0.3508 0.0621 0.2763 0.4664]
Median for last 10 epochs: [0.9195 0.169  0.5355 0.9849], Epochs since improvement 20
 14%|█▍        | 70/500 [47:31<5:15:40, 44.05s/it] 14%|█▍        | 71/500 [48:03<4:48:00, 40.28s/it] 14%|█▍        | 71/500 [48:52<4:55:15, 41.30s/it]
Epoch: 72 done with learning rate 9.96E-03, Train loss: 1.46E+06, Train scatter: [0.9351 0.1727 0.5441 0.9953]
L1 regularization loss: 5.25E+00, L2 regularization loss: 2.67E+00
Test scatter: [0.9195 0.1689 0.5355 0.9849], Lowest was [0.3508 0.0621 0.2763 0.4664]
Median for last 10 epochs: [0.9195 0.169  0.5355 0.9849], Epochs since improvement 22
Exited after 72 epochs due to early stopping
2932.03 seconds spent training, 5.864 seconds per epoch. Processed 11875 trees per second
[0.9194764  0.16885738 0.53547406 0.984887  ]
{'epoch_exit': 71, 'scatter_m_star': 0.9194764, 'lowest_m_star': 0.35075948, 'last20_m_star': 0.91951483, 'last10_m_star': 0.91951466, 'scatter_v_disk': 0.16885738, 'lowest_v_disk': 0.062116943, 'last20_v_disk': 0.16892108, 'last10_v_disk': 0.16895232, 'scatter_m_cold': 0.53547406, 'lowest_m_cold': 0.27633527, 'last20_m_cold': 0.535489, 'last10_m_cold': 0.5354891, 'scatter_sfr_100': 0.984887, 'lowest_sfr_100': 0.4664282, 'last20_sfr_100': 0.98486865, 'last10_sfr_100': 0.98487246}
Finished 1/1
Exited training after None epochs
Experiment 1 done: 1 / 6
Exploring ['conv_layers', 'batch_size']
Currently doing [  1 512]
{'experiment': 'GraphMerge', 'group': 'meta_pysr', 'move': False, 'model': 'MetaEdge', 'log': True, 'run_params': {'n_epochs': 500, 'n_trials': 1, 'batch_size': 512, 'val_epoch': 2, 'early_stopping': True, 'patience': 20, 'l1_lambda': 0.0001, 'l2_lambda': 0.0001, 'loss_func': 'GaussNd', 'metrics': 'test_multi_varrho', 'performance_plot': 'multi_base', 'save': True, 'seed': True, 'num_workers': 4}, 'learn_params': {'learning_rate': 0.01, 'schedule': 'onecycle', 'g_up': 1, 'g_down': 0.95, 'warmup': 4, 'period': 5, 'eta_min': 1e-05}, 'hyper_params': {'hidden_channels': 128, 'conv_layers': 1, 'conv_activation': 'relu', 'decode_activation': 'leakyrelu', 'decode_layers': 2, 'layernorm': True, 'agg': 'sum', 'variance': True, 'rho': 0, 'in_channels': 43, 'out_channels': 4}, 'data_params': {'case': 'vlarge_all_4t_z0.0_quantile_raw', 'targets': [0, 1, 2, 3], 'del_feats': [], 'split': 0.8, 'test': 0, 'scale': 0}, 'experiment_name': 'con1bat512'}
/home/cj1223/../../scratch/gpfs/cj1223/GraphResults/results_meta_pysr_120322/MetaEdge_vlarge_all_4t_z0.0_quantile_raw_hlgove
RelU conv activation
LeakyRelU decode activation
N_params 298056
Made folder for saving model
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:28<3:55:24, 28.31s/it]  0%|          | 2/500 [01:13<5:17:44, 38.28s/it]Epoch: 2 done with learning rate 4.17E-04, Train loss: 2.36E+07, Train scatter: [0.9353 0.1639 0.5442 0.9954]
L1 regularization loss: 1.51E+00, L2 regularization loss: 3.26E-01
Test scatter: [0.9197 0.1661 0.5356 0.9851], Lowest was [0.9197 0.1661 0.5356 0.9851]
Median for last 10 epochs: [0.9197 0.1661 0.5356 0.9851], Epochs since improvement 2
  1%|          | 3/500 [01:40<4:36:00, 33.32s/it]  1%|          | 4/500 [02:27<5:17:57, 38.46s/it]Epoch: 4 done with learning rate 4.67E-04, Train loss: 1.19E+07, Train scatter: [0.9353 0.1768 0.5441 0.9954]
L1 regularization loss: 1.52E+00, L2 regularization loss: 3.32E-01
Test scatter: [0.9197 0.1781 0.5355 0.9851], Lowest was [0.9197 0.1661 0.5355 0.9851]
Median for last 10 epochs: [0.9197 0.1721 0.5355 0.9851], Epochs since improvement 0
  1%|          | 5/500 [02:54<4:44:23, 34.47s/it]  1%|          | 6/500 [03:40<5:15:21, 38.30s/it]Epoch: 6 done with learning rate 5.51E-04, Train loss: 9.49E+06, Train scatter: [0.9352 0.1674 0.5441 0.9954]
L1 regularization loss: 1.54E+00, L2 regularization loss: 3.44E-01
Test scatter: [0.9196 0.1631 0.5356 0.9851], Lowest was [0.9196 0.1631 0.5355 0.9851]
Median for last 10 epochs: [0.9196 0.1631 0.5355 0.9851], Epochs since improvement 0
  1%|▏         | 7/500 [04:07<4:45:19, 34.73s/it]  2%|▏         | 8/500 [04:53<5:14:34, 38.36s/it]Epoch: 8 done with learning rate 6.67E-04, Train loss: 8.42E+06, Train scatter: [0.9352 0.1465 0.5441 0.995 ]
L1 regularization loss: 1.55E+00, L2 regularization loss: 3.66E-01
Test scatter: [0.9196 0.1414 0.5355 0.9846], Lowest was [0.9196 0.1414 0.5355 0.9846]
Median for last 10 epochs: [0.9196 0.1522 0.5355 0.9848], Epochs since improvement 0
  2%|▏         | 9/500 [05:21<4:45:54, 34.94s/it]Epoch: 10 done with learning rate 8.15E-04, Train loss: 7.80E+06, Train scatter: [0.935  0.1316 0.5441 0.6871]
L1 regularization loss: 1.59E+00, L2 regularization loss: 3.96E-01
Test scatter: [0.9194 0.1276 0.5355 0.6887], Lowest was [0.9194 0.1276 0.5355 0.6887]
Median for last 10 epochs: [0.9196 0.1414 0.5355 0.9846], Epochs since improvement 0
  2%|▏         | 10/500 [06:13<5:27:41, 40.13s/it]  2%|▏         | 11/500 [06:40<4:55:32, 36.26s/it]  2%|▏         | 12/500 [07:26<5:17:47, 39.07s/it]Epoch: 12 done with learning rate 9.94E-04, Train loss: 7.26E+06, Train scatter: [0.9291 0.121  0.5439 0.6124]
L1 regularization loss: 1.60E+00, L2 regularization loss: 4.06E-01
Test scatter: [0.9134 0.1174 0.5353 0.605 ], Lowest was [0.9134 0.1174 0.5353 0.605 ]
Median for last 10 epochs: [0.9196 0.1414 0.5355 0.9846], Epochs since improvement 0
  3%|▎         | 13/500 [07:53<4:48:15, 35.51s/it]  3%|▎         | 14/500 [08:39<5:12:12, 38.55s/it]Epoch: 14 done with learning rate 1.20E-03, Train loss: 6.64E+06, Train scatter: [0.8884 0.1141 0.5421 0.602 ]
L1 regularization loss: 1.61E+00, L2 regularization loss: 4.14E-01
Test scatter: [0.8748 0.1114 0.5335 0.5973], Lowest was [0.8748 0.1114 0.5335 0.5973]
Median for last 10 epochs: [0.9194 0.1276 0.5355 0.6887], Epochs since improvement 0
  3%|▎         | 15/500 [09:06<4:44:30, 35.20s/it]  3%|▎         | 16/500 [09:51<5:08:46, 38.28s/it]Epoch: 16 done with learning rate 1.44E-03, Train loss: 1.40E+07, Train scatter: [0.9336 0.1744 0.5441 0.9034]
L1 regularization loss: 1.69E+00, L2 regularization loss: 4.47E-01
Test scatter: [0.9181 0.1712 0.5355 0.9004], Lowest was [0.8748 0.1114 0.5335 0.5973]
Median for last 10 epochs: [0.9181 0.1276 0.5355 0.6887], Epochs since improvement 2
  3%|▎         | 17/500 [10:19<4:41:39, 34.99s/it]  4%|▎         | 18/500 [11:05<5:07:44, 38.31s/it]Epoch: 18 done with learning rate 1.70E-03, Train loss: 8.59E+06, Train scatter: [0.9149 0.1569 0.5439 0.7776]
L1 regularization loss: 1.70E+00, L2 regularization loss: 4.58E-01
Test scatter: [0.9008 0.1524 0.5353 0.7851], Lowest was [0.8748 0.1114 0.5335 0.5973]
Median for last 10 epochs: [0.9134 0.1276 0.5353 0.6887], Epochs since improvement 4
  4%|▍         | 19/500 [11:32<4:40:50, 35.03s/it]Epoch: 20 done with learning rate 1.99E-03, Train loss: 7.16E+06, Train scatter: [0.7657 0.1414 0.5426 0.6958]
L1 regularization loss: 1.72E+00, L2 regularization loss: 4.81E-01
Test scatter: [0.7668 0.1389 0.534  0.6932], Lowest was [0.7668 0.1114 0.5335 0.5973]
Median for last 10 epochs: [0.9008 0.1389 0.5353 0.6932], Epochs since improvement 0
  4%|▍         | 20/500 [12:24<5:19:54, 39.99s/it]  4%|▍         | 21/500 [12:51<4:49:16, 36.23s/it]  4%|▍         | 22/500 [13:37<5:11:14, 39.07s/it]Epoch: 22 done with learning rate 2.30E-03, Train loss: 6.13E+06, Train scatter: [0.6434 0.1328 0.5399 0.6995]
L1 regularization loss: 1.73E+00, L2 regularization loss: 5.02E-01
Test scatter: [0.6577 0.1309 0.5312 0.6937], Lowest was [0.6577 0.1114 0.5312 0.5973]
Median for last 10 epochs: [0.8748 0.1389 0.534  0.6937], Epochs since improvement 0
  5%|▍         | 23/500 [14:05<4:43:20, 35.64s/it]  5%|▍         | 24/500 [14:50<5:06:48, 38.67s/it]Epoch: 24 done with learning rate 2.63E-03, Train loss: 5.52E+06, Train scatter: [0.5655 0.1224 0.5322 0.6274]
L1 regularization loss: 1.74E+00, L2 regularization loss: 5.18E-01
Test scatter: [0.5851 0.1215 0.525  0.6346], Lowest was [0.5851 0.1114 0.525  0.5973]
Median for last 10 epochs: [0.7668 0.1389 0.534  0.6937], Epochs since improvement 0
  5%|▌         | 25/500 [15:18<4:39:42, 35.33s/it]  5%|▌         | 26/500 [16:03<5:02:36, 38.30s/it]Epoch: 26 done with learning rate 2.98E-03, Train loss: 4.87E+06, Train scatter: [0.5037 0.117  0.5106 0.5929]
L1 regularization loss: 1.76E+00, L2 regularization loss: 5.32E-01
Test scatter: [0.5148 0.1167 0.5041 0.59  ], Lowest was [0.5148 0.1114 0.5041 0.59  ]
Median for last 10 epochs: [0.6577 0.1309 0.5312 0.6932], Epochs since improvement 0
  5%|▌         | 27/500 [16:31<4:36:25, 35.06s/it]  6%|▌         | 28/500 [17:16<5:01:10, 38.29s/it]Epoch: 28 done with learning rate 3.34E-03, Train loss: 4.10E+06, Train scatter: [0.5228 0.1177 0.4668 0.6069]
L1 regularization loss: 1.77E+00, L2 regularization loss: 5.44E-01
Test scatter: [0.5478 0.1192 0.4712 0.623 ], Lowest was [0.5148 0.1114 0.4712 0.59  ]
Median for last 10 epochs: [0.5851 0.1215 0.525  0.6346], Epochs since improvement 0
  6%|▌         | 29/500 [17:44<4:34:46, 35.00s/it]Epoch: 30 done with learning rate 3.72E-03, Train loss: 3.76E+06, Train scatter: [0.4993 0.1127 0.4574 0.6011]
L1 regularization loss: 1.79E+00, L2 regularization loss: 5.55E-01
Test scatter: [0.5145 0.114  0.4582 0.6077], Lowest was [0.5145 0.1114 0.4582 0.59  ]
Median for last 10 epochs: [0.5478 0.1192 0.5041 0.623 ], Epochs since improvement 0
  6%|▌         | 30/500 [18:35<5:13:09, 39.98s/it]  6%|▌         | 31/500 [19:03<4:43:12, 36.23s/it]  6%|▋         | 32/500 [19:49<5:05:06, 39.12s/it]Epoch: 32 done with learning rate 4.10E-03, Train loss: 3.36E+06, Train scatter: [0.5137 0.103  0.4075 0.5968]
L1 regularization loss: 1.80E+00, L2 regularization loss: 5.66E-01
Test scatter: [0.5249 0.1025 0.4061 0.5946], Lowest was [0.5145 0.1025 0.4061 0.59  ]
Median for last 10 epochs: [0.5249 0.1167 0.4712 0.6077], Epochs since improvement 0
  7%|▋         | 33/500 [20:16<4:37:21, 35.64s/it]  7%|▋         | 34/500 [21:02<5:00:47, 38.73s/it]Epoch: 34 done with learning rate 4.50E-03, Train loss: 2.44E+06, Train scatter: [0.514  0.0989 0.3796 0.6019]
L1 regularization loss: 1.82E+00, L2 regularization loss: 5.78E-01
Test scatter: [0.516  0.1005 0.3849 0.6177], Lowest was [0.5145 0.1005 0.3849 0.59  ]
Median for last 10 epochs: [0.516  0.114  0.4582 0.6077], Epochs since improvement 0
  7%|▋         | 35/500 [21:30<4:33:57, 35.35s/it]  7%|▋         | 36/500 [22:16<4:59:04, 38.67s/it]Epoch: 36 done with learning rate 4.90E-03, Train loss: 2.12E+06, Train scatter: [0.5879 0.109  0.4154 0.6375]
L1 regularization loss: 1.83E+00, L2 regularization loss: 5.89E-01
Test scatter: [0.6187 0.1156 0.4271 0.666 ], Lowest was [0.5145 0.1005 0.3849 0.59  ]
Median for last 10 epochs: [0.5249 0.114  0.4271 0.6177], Epochs since improvement 2
  7%|▋         | 37/500 [22:43<4:32:16, 35.28s/it]  8%|▊         | 38/500 [23:30<4:57:18, 38.61s/it]Epoch: 38 done with learning rate 5.30E-03, Train loss: 2.28E+06, Train scatter: [0.4376 0.0945 0.3868 0.5755]
L1 regularization loss: 1.86E+00, L2 regularization loss: 6.10E-01
Test scatter: [0.4622 0.0956 0.3863 0.5854], Lowest was [0.4622 0.0956 0.3849 0.5854]
Median for last 10 epochs: [0.516  0.1025 0.4061 0.6077], Epochs since improvement 0
  8%|▊         | 39/500 [23:57<4:30:54, 35.26s/it]Epoch: 40 done with learning rate 5.70E-03, Train loss: 1.96E+06, Train scatter: [0.4988 0.0903 0.3612 0.5615]
L1 regularization loss: 1.88E+00, L2 regularization loss: 6.23E-01
Test scatter: [0.5148 0.0925 0.3621 0.5697], Lowest was [0.4622 0.0925 0.3621 0.5697]
Median for last 10 epochs: [0.516  0.1005 0.3863 0.5946], Epochs since improvement 0
  8%|▊         | 40/500 [24:49<5:09:23, 40.36s/it]  8%|▊         | 41/500 [25:17<4:39:20, 36.51s/it]  8%|▊         | 42/500 [26:03<5:00:48, 39.41s/it]Epoch: 42 done with learning rate 6.10E-03, Train loss: 1.77E+06, Train scatter: [0.4156 0.0886 0.3655 0.5674]
L1 regularization loss: 1.90E+00, L2 regularization loss: 6.36E-01
Test scatter: [0.4326 0.089  0.3663 0.569 ], Lowest was [0.4326 0.089  0.3621 0.569 ]
Median for last 10 epochs: [0.5148 0.0956 0.3849 0.5854], Epochs since improvement 0
  9%|▊         | 43/500 [26:30<4:32:40, 35.80s/it]  9%|▉         | 44/500 [27:17<4:55:46, 38.92s/it]Epoch: 44 done with learning rate 6.49E-03, Train loss: 1.61E+06, Train scatter: [0.409  0.0858 0.3348 0.5496]
L1 regularization loss: 1.91E+00, L2 regularization loss: 6.50E-01
Test scatter: [0.4192 0.0864 0.338  0.5533], Lowest was [0.4192 0.0864 0.338  0.5533]
Median for last 10 epochs: [0.4622 0.0925 0.3663 0.5697], Epochs since improvement 0
  9%|▉         | 45/500 [27:44<4:28:46, 35.44s/it]  9%|▉         | 46/500 [28:30<4:51:58, 38.59s/it]Epoch: 46 done with learning rate 6.87E-03, Train loss: 1.66E+06, Train scatter: [0.5092 0.0857 0.335  0.5442]
L1 regularization loss: 1.93E+00, L2 regularization loss: 6.67E-01
Test scatter: [0.4971 0.0885 0.336  0.5502], Lowest was [0.4192 0.0864 0.336  0.5502]
Median for last 10 epochs: [0.4622 0.089  0.3621 0.569 ], Epochs since improvement 0
  9%|▉         | 47/500 [28:57<4:25:55, 35.22s/it] 10%|▉         | 48/500 [29:43<4:49:33, 38.44s/it]Epoch: 48 done with learning rate 7.24E-03, Train loss: 1.52E+06, Train scatter: [0.3849 0.0834 0.3515 0.5616]
L1 regularization loss: 1.95E+00, L2 regularization loss: 6.86E-01
Test scatter: [0.3966 0.0844 0.3548 0.5665], Lowest was [0.3966 0.0844 0.336  0.5502]
Median for last 10 epochs: [0.4326 0.0885 0.3548 0.5665], Epochs since improvement 0
 10%|▉         | 49/500 [30:11<4:24:34, 35.20s/it]Epoch: 50 done with learning rate 7.60E-03, Train loss: 1.45E+06, Train scatter: [0.4505 0.0821 0.3484 0.5551]
L1 regularization loss: 1.98E+00, L2 regularization loss: 7.10E-01
Test scatter: [0.4483 0.0836 0.3525 0.5596], Lowest was [0.3966 0.0836 0.336  0.5502]
Median for last 10 epochs: [0.4326 0.0864 0.3525 0.5596], Epochs since improvement 0
 10%|█         | 50/500 [31:05<5:05:37, 40.75s/it] 10%|█         | 51/500 [31:32<4:35:21, 36.80s/it] 10%|█         | 52/500 [32:18<4:55:40, 39.60s/it]Epoch: 52 done with learning rate 7.94E-03, Train loss: 1.31E+06, Train scatter: [0.3737 0.0798 0.3121 0.5565]
L1 regularization loss: 2.01E+00, L2 regularization loss: 7.32E-01
Test scatter: [0.3924 0.0809 0.3139 0.5626], Lowest was [0.3924 0.0809 0.3139 0.5502]
Median for last 10 epochs: [0.4192 0.0844 0.338  0.5596], Epochs since improvement 0
 11%|█         | 53/500 [32:46<4:28:34, 36.05s/it] 11%|█         | 54/500 [33:32<4:50:23, 39.07s/it]Epoch: 54 done with learning rate 8.26E-03, Train loss: 1.26E+06, Train scatter: [0.3736 0.0839 0.3133 0.5395]
L1 regularization loss: 2.03E+00, L2 regularization loss: 7.56E-01
Test scatter: [0.3939 0.0878 0.3213 0.5488], Lowest was [0.3924 0.0809 0.3139 0.5488]
Median for last 10 epochs: [0.3966 0.0844 0.336  0.5596], Epochs since improvement 0
 11%|█         | 55/500 [34:00<4:25:16, 35.77s/it] 11%|█         | 56/500 [34:47<4:49:11, 39.08s/it]Epoch: 56 done with learning rate 8.56E-03, Train loss: 1.47E+06, Train scatter: [0.5113 0.0837 0.3415 0.5637]
L1 regularization loss: 2.05E+00, L2 regularization loss: 7.80E-01
Test scatter: [0.4972 0.0876 0.353  0.5628], Lowest was [0.3924 0.0809 0.3139 0.5488]
Median for last 10 epochs: [0.3966 0.0844 0.3525 0.5626], Epochs since improvement 2
 11%|█▏        | 57/500 [35:15<4:22:54, 35.61s/it] 12%|█▏        | 58/500 [36:01<4:45:14, 38.72s/it]Epoch: 58 done with learning rate 8.83E-03, Train loss: 1.19E+06, Train scatter: [0.3631 0.0776 0.3053 0.52  ]
L1 regularization loss: 2.08E+00, L2 regularization loss: 8.08E-01
Test scatter: [0.3798 0.0795 0.3068 0.5265], Lowest was [0.3798 0.0795 0.3068 0.5265]
Median for last 10 epochs: [0.3939 0.0836 0.3213 0.5596], Epochs since improvement 0
 12%|█▏        | 59/500 [36:28<4:19:51, 35.35s/it]Epoch: 60 done with learning rate 9.08E-03, Train loss: 1.07E+06, Train scatter: [0.4171 0.0771 0.3173 0.5337]
L1 regularization loss: 2.10E+00, L2 regularization loss: 8.36E-01
Test scatter: [0.4234 0.0778 0.3217 0.5392], Lowest was [0.3798 0.0778 0.3068 0.5265]
Median for last 10 epochs: [0.3939 0.0809 0.3213 0.5488], Epochs since improvement 0
 12%|█▏        | 60/500 [37:20<4:55:50, 40.34s/it] 12%|█▏        | 61/500 [37:48<4:27:14, 36.52s/it] 12%|█▏        | 62/500 [38:33<4:46:44, 39.28s/it]Epoch: 62 done with learning rate 9.31E-03, Train loss: 1.15E+06, Train scatter: [0.3487 0.0753 0.3001 0.5344]
L1 regularization loss: 2.14E+00, L2 regularization loss: 8.74E-01
Test scatter: [0.3593 0.0767 0.3051 0.5316], Lowest was [0.3593 0.0767 0.3051 0.5265]
Median for last 10 epochs: [0.3939 0.0795 0.3213 0.5392], Epochs since improvement 0
 13%|█▎        | 63/500 [39:01<4:20:17, 35.74s/it] 13%|█▎        | 64/500 [39:48<4:44:18, 39.12s/it]Epoch: 64 done with learning rate 9.50E-03, Train loss: 1.01E+06, Train scatter: [0.3816 0.0752 0.3116 0.5351]
L1 regularization loss: 2.16E+00, L2 regularization loss: 9.07E-01
Test scatter: [0.3983 0.0769 0.3188 0.5432], Lowest was [0.3593 0.0767 0.3051 0.5265]
Median for last 10 epochs: [0.3983 0.0778 0.3188 0.5392], Epochs since improvement 2
 13%|█▎        | 65/500 [40:15<4:18:15, 35.62s/it] 13%|█▎        | 66/500 [41:01<4:40:27, 38.77s/it]Epoch: 66 done with learning rate 9.66E-03, Train loss: 8.85E+05, Train scatter: [0.3382 0.0759 0.3008 0.5263]
L1 regularization loss: 2.20E+00, L2 regularization loss: 9.41E-01
Test scatter: [0.3569 0.0782 0.3077 0.5285], Lowest was [0.3569 0.0767 0.3051 0.5265]
Median for last 10 epochs: [0.3798 0.0778 0.3077 0.5316], Epochs since improvement 0
 13%|█▎        | 67/500 [41:29<4:15:17, 35.38s/it] 14%|█▎        | 68/500 [42:15<4:37:03, 38.48s/it]Epoch: 68 done with learning rate 9.80E-03, Train loss: 8.84E+05, Train scatter: [0.4117 0.0745 0.2903 0.5113]
L1 regularization loss: 2.22E+00, L2 regularization loss: 9.73E-01
Test scatter: [0.4104 0.0763 0.2992 0.5169], Lowest was [0.3569 0.0763 0.2992 0.5169]
Median for last 10 epochs: [0.3983 0.0769 0.3077 0.5316], Epochs since improvement 0
 14%|█▍        | 69/500 [42:42<4:12:47, 35.19s/it]Epoch: 70 done with learning rate 9.90E-03, Train loss: 6.92E+05, Train scatter: [0.3425 0.0734 0.3233 0.5307]
L1 regularization loss: 2.23E+00, L2 regularization loss: 1.00E+00
Test scatter: [0.3523 0.0741 0.3318 0.5363], Lowest was [0.3523 0.0741 0.2992 0.5169]
Median for last 10 epochs: [0.3593 0.0767 0.3077 0.5316], Epochs since improvement 0
 14%|█▍        | 70/500 [43:34<4:47:43, 40.15s/it] 14%|█▍        | 71/500 [44:01<4:20:09, 36.39s/it] 14%|█▍        | 72/500 [44:48<4:41:37, 39.48s/it]Epoch: 72 done with learning rate 9.96E-03, Train loss: 8.80E+05, Train scatter: [0.6355 0.0821 0.4514 0.5586]
L1 regularization loss: 2.27E+00, L2 regularization loss: 1.04E+00
Test scatter: [0.6243 0.0818 0.442  0.5582], Lowest was [0.3523 0.0741 0.2992 0.5169]
Median for last 10 epochs: [0.3983 0.0769 0.3188 0.5363], Epochs since improvement 2
 15%|█▍        | 73/500 [45:16<4:15:48, 35.95s/it] 15%|█▍        | 74/500 [46:02<4:37:42, 39.11s/it]Epoch: 74 done with learning rate 1.00E-02, Train loss: 5.53E+05, Train scatter: [0.4221 0.0727 0.2773 0.4957]
L1 regularization loss: 2.28E+00, L2 regularization loss: 1.07E+00
Test scatter: [0.4256 0.0737 0.2846 0.499 ], Lowest was [0.3523 0.0737 0.2846 0.499 ]
Median for last 10 epochs: [0.4104 0.0763 0.3077 0.5285], Epochs since improvement 0
 15%|█▌        | 75/500 [46:30<4:12:18, 35.62s/it] 15%|█▌        | 76/500 [47:16<4:34:10, 38.80s/it]Epoch: 76 done with learning rate 1.00E-02, Train loss: 5.50E+05, Train scatter: [0.2986 0.0715 0.2787 0.4939]
L1 regularization loss: 2.30E+00, L2 regularization loss: 1.09E+00
Test scatter: [0.319  0.0731 0.2872 0.5017], Lowest was [0.319  0.0731 0.2846 0.499 ]
Median for last 10 epochs: [0.4104 0.0741 0.2992 0.5169], Epochs since improvement 0
 15%|█▌        | 77/500 [47:43<4:09:25, 35.38s/it] 16%|█▌        | 78/500 [48:30<4:32:29, 38.74s/it]Epoch: 78 done with learning rate 1.00E-02, Train loss: 5.09E+05, Train scatter: [0.3119 0.0734 0.279  0.5001]
L1 regularization loss: 2.33E+00, L2 regularization loss: 1.12E+00
Test scatter: [0.3302 0.0744 0.2854 0.5037], Lowest was [0.319  0.0731 0.2846 0.499 ]
Median for last 10 epochs: [0.3523 0.0741 0.2872 0.5037], Epochs since improvement 2
 16%|█▌        | 79/500 [48:58<4:08:13, 35.38s/it]Epoch: 80 done with learning rate 1.00E-02, Train loss: 5.35E+05, Train scatter: [0.3047 0.071  0.2749 0.4883]
L1 regularization loss: 2.34E+00, L2 regularization loss: 1.15E+00
Test scatter: [0.3197 0.0716 0.283  0.4941], Lowest was [0.319  0.0716 0.283  0.4941]
Median for last 10 epochs: [0.3302 0.0737 0.2854 0.5017], Epochs since improvement 0
 16%|█▌        | 80/500 [49:50<4:42:41, 40.38s/it] 16%|█▌        | 81/500 [50:17<4:14:58, 36.51s/it] 16%|█▋        | 82/500 [51:03<4:34:29, 39.40s/it]Epoch: 82 done with learning rate 9.99E-03, Train loss: 4.21E+05, Train scatter: [0.3215 0.0698 0.276  0.4876]
L1 regularization loss: 2.37E+00, L2 regularization loss: 1.18E+00
Test scatter: [0.3236 0.0711 0.283  0.4921], Lowest was [0.319  0.0711 0.283  0.4921]
Median for last 10 epochs: [0.3236 0.0731 0.2846 0.499 ], Epochs since improvement 0
 17%|█▋        | 83/500 [51:31<4:09:09, 35.85s/it] 17%|█▋        | 84/500 [52:17<4:30:46, 39.05s/it]Epoch: 84 done with learning rate 9.99E-03, Train loss: 4.59E+05, Train scatter: [0.4552 0.0723 0.2946 0.5243]
L1 regularization loss: 2.38E+00, L2 regularization loss: 1.21E+00
Test scatter: [0.4648 0.074  0.3    0.5262], Lowest was [0.319  0.0711 0.283  0.4921]
Median for last 10 epochs: [0.3236 0.0731 0.2854 0.5017], Epochs since improvement 2
 17%|█▋        | 85/500 [52:45<4:06:17, 35.61s/it] 17%|█▋        | 86/500 [53:31<4:28:10, 38.87s/it]Epoch: 86 done with learning rate 9.98E-03, Train loss: 4.20E+05, Train scatter: [0.2982 0.0694 0.2778 0.4827]
L1 regularization loss: 2.40E+00, L2 regularization loss: 1.23E+00
Test scatter: [0.3049 0.0707 0.2833 0.4858], Lowest was [0.3049 0.0707 0.283  0.4858]
Median for last 10 epochs: [0.3236 0.0716 0.2833 0.4941], Epochs since improvement 0
 17%|█▋        | 87/500 [53:59<4:03:38, 35.40s/it] 18%|█▊        | 88/500 [54:46<4:27:02, 38.89s/it]Epoch: 88 done with learning rate 9.98E-03, Train loss: 3.30E+05, Train scatter: [0.2729 0.0674 0.269  0.4817]
L1 regularization loss: 2.41E+00, L2 regularization loss: 1.26E+00
Test scatter: [0.2872 0.0686 0.2765 0.483 ], Lowest was [0.2872 0.0686 0.2765 0.483 ]
Median for last 10 epochs: [0.3197 0.0711 0.283  0.4921], Epochs since improvement 0
 18%|█▊        | 89/500 [55:13<4:02:47, 35.44s/it]Epoch: 90 done with learning rate 9.97E-03, Train loss: 2.57E+05, Train scatter: [0.304  0.0671 0.2659 0.4805]
L1 regularization loss: 2.42E+00, L2 regularization loss: 1.29E+00
Test scatter: [0.3097 0.0676 0.2716 0.4845], Lowest was [0.2872 0.0676 0.2716 0.483 ]
Median for last 10 epochs: [0.3097 0.0707 0.283  0.4858], Epochs since improvement 0
 18%|█▊        | 90/500 [56:05<4:35:18, 40.29s/it] 18%|█▊        | 91/500 [56:32<4:08:17, 36.42s/it] 18%|█▊        | 92/500 [57:19<4:28:16, 39.45s/it]Epoch: 92 done with learning rate 9.96E-03, Train loss: 2.53E+05, Train scatter: [0.3708 0.0826 0.2758 0.5486]
L1 regularization loss: 2.43E+00, L2 regularization loss: 1.31E+00
Test scatter: [0.3768 0.0841 0.2834 0.5421], Lowest was [0.2872 0.0676 0.2716 0.483 ]
Median for last 10 epochs: [0.3097 0.0707 0.2833 0.4858], Epochs since improvement 2
 19%|█▊        | 93/500 [57:46<4:03:19, 35.87s/it] 19%|█▉        | 94/500 [58:33<4:24:35, 39.10s/it]Epoch: 94 done with learning rate 9.95E-03, Train loss: 1.80E+05, Train scatter: [0.3258 0.0652 0.2611 0.4885]
L1 regularization loss: 2.45E+00, L2 regularization loss: 1.34E+00
Test scatter: [0.3294 0.0658 0.2706 0.4921], Lowest was [0.2872 0.0658 0.2706 0.483 ]
Median for last 10 epochs: [0.3097 0.0686 0.2765 0.4858], Epochs since improvement 0
 19%|█▉        | 95/500 [59:00<4:00:20, 35.61s/it] 19%|█▉        | 96/500 [59:47<4:22:49, 39.03s/it]Epoch: 96 done with learning rate 9.94E-03, Train loss: 8.72E+04, Train scatter: [0.273  0.0651 0.2591 0.4808]
L1 regularization loss: 2.46E+00, L2 regularization loss: 1.37E+00
Test scatter: [0.28   0.0653 0.2677 0.4828], Lowest was [0.28   0.0653 0.2677 0.4828]
Median for last 10 epochs: [0.3097 0.0676 0.2716 0.4845], Epochs since improvement 0
 19%|█▉        | 97/500 [1:00:15<3:58:38, 35.53s/it] 20%|█▉        | 98/500 [1:01:01<4:19:13, 38.69s/it]Epoch: 98 done with learning rate 9.93E-03, Train loss: 3.12E+04, Train scatter: [0.284  0.0671 0.2631 0.4811]
L1 regularization loss: 2.48E+00, L2 regularization loss: 1.39E+00
Test scatter: [0.288  0.0677 0.2727 0.4878], Lowest was [0.28   0.0653 0.2677 0.4828]
Median for last 10 epochs: [0.3097 0.0676 0.2716 0.4878], Epochs since improvement 2
 20%|█▉        | 99/500 [1:01:28<3:56:23, 35.37s/it]Epoch: 100 done with learning rate 9.91E-03, Train loss: -3.96E+04, Train scatter: [0.2525 0.064  0.2896 0.4877]
L1 regularization loss: 2.50E+00, L2 regularization loss: 1.42E+00
Test scatter: [0.2627 0.0653 0.2971 0.4981], Lowest was [0.2627 0.0653 0.2677 0.4828]
Median for last 10 epochs: [0.288  0.0658 0.2727 0.4921], Epochs since improvement 0
 20%|██        | 100/500 [1:02:20<4:29:12, 40.38s/it] 20%|██        | 101/500 [1:02:48<4:02:36, 36.48s/it] 20%|██        | 102/500 [1:03:34<4:21:35, 39.43s/it]Epoch: 102 done with learning rate 9.90E-03, Train loss: -9.46E+04, Train scatter: [0.2785 0.0661 0.2795 0.5225]
L1 regularization loss: 2.50E+00, L2 regularization loss: 1.44E+00
Test scatter: [0.2862 0.0678 0.2911 0.5236], Lowest was [0.2627 0.0653 0.2677 0.4828]
Median for last 10 epochs: [0.2862 0.0658 0.2727 0.4921], Epochs since improvement 2
 21%|██        | 103/500 [1:04:01<3:57:03, 35.83s/it] 21%|██        | 104/500 [1:04:48<4:17:54, 39.08s/it]Epoch: 104 done with learning rate 9.89E-03, Train loss: -1.53E+05, Train scatter: [0.2407 0.0606 0.2687 0.4752]
L1 regularization loss: 2.51E+00, L2 regularization loss: 1.47E+00
Test scatter: [0.2497 0.0611 0.2781 0.4791], Lowest was [0.2497 0.0611 0.2677 0.4791]
Median for last 10 epochs: [0.28   0.0653 0.2781 0.4878], Epochs since improvement 0
 21%|██        | 105/500 [1:05:16<3:54:27, 35.61s/it] 21%|██        | 106/500 [1:06:03<4:16:07, 39.00s/it]Epoch: 106 done with learning rate 9.87E-03, Train loss: -1.95E+05, Train scatter: [0.2769 0.0615 0.2618 0.4832]
L1 regularization loss: 2.54E+00, L2 regularization loss: 1.50E+00
Test scatter: [0.2821 0.0617 0.2708 0.4848], Lowest was [0.2497 0.0611 0.2677 0.4791]
Median for last 10 epochs: [0.2821 0.0653 0.2781 0.4878], Epochs since improvement 2
 21%|██▏       | 107/500 [1:06:30<3:52:51, 35.55s/it] 22%|██▏       | 108/500 [1:07:17<4:14:23, 38.94s/it]Epoch: 108 done with learning rate 9.85E-03, Train loss: -2.27E+05, Train scatter: [0.2382 0.06   0.2681 0.4678]
L1 regularization loss: 2.54E+00, L2 regularization loss: 1.52E+00
Test scatter: [0.2545 0.0607 0.2775 0.47  ], Lowest was [0.2497 0.0607 0.2677 0.47  ]
Median for last 10 epochs: [0.2627 0.0617 0.2781 0.4848], Epochs since improvement 0
 22%|██▏       | 109/500 [1:07:44<3:51:17, 35.49s/it]Epoch: 110 done with learning rate 9.83E-03, Train loss: -2.43E+05, Train scatter: [0.231  0.0596 0.2562 0.4685]
L1 regularization loss: 2.57E+00, L2 regularization loss: 1.55E+00
Test scatter: [0.2437 0.0599 0.268  0.4725], Lowest was [0.2437 0.0599 0.2677 0.47  ]
Median for last 10 epochs: [0.2545 0.0611 0.2775 0.4791], Epochs since improvement 0
 22%|██▏       | 110/500 [1:08:36<4:21:31, 40.24s/it] 22%|██▏       | 111/500 [1:09:03<3:56:09, 36.43s/it] 22%|██▏       | 112/500 [1:09:49<4:12:59, 39.12s/it]Epoch: 112 done with learning rate 9.81E-03, Train loss: -2.48E+05, Train scatter: [0.2436 0.0584 0.253  0.4636]
L1 regularization loss: 2.59E+00, L2 regularization loss: 1.58E+00
Test scatter: [0.2486 0.0595 0.2624 0.4687], Lowest was [0.2437 0.0595 0.2624 0.4687]
Median for last 10 epochs: [0.2497 0.0607 0.2708 0.4725], Epochs since improvement 0
 23%|██▎       | 113/500 [1:10:16<3:49:45, 35.62s/it] 23%|██▎       | 114/500 [1:11:02<4:08:18, 38.60s/it]Epoch: 114 done with learning rate 9.79E-03, Train loss: -2.60E+05, Train scatter: [0.2207 0.0588 0.2584 0.4596]
L1 regularization loss: 2.61E+00, L2 regularization loss: 1.61E+00
Test scatter: [0.2352 0.0597 0.2681 0.4619], Lowest was [0.2352 0.0595 0.2624 0.4619]
Median for last 10 epochs: [0.2486 0.0599 0.2681 0.47  ], Epochs since improvement 0
 23%|██▎       | 115/500 [1:11:29<3:46:20, 35.28s/it] 23%|██▎       | 116/500 [1:12:15<4:06:26, 38.51s/it]Epoch: 116 done with learning rate 9.77E-03, Train loss: -2.81E+05, Train scatter: [0.2125 0.0585 0.2581 0.4668]
L1 regularization loss: 2.63E+00, L2 regularization loss: 1.64E+00
Test scatter: [0.2234 0.0596 0.2683 0.4692], Lowest was [0.2234 0.0595 0.2624 0.4619]
Median for last 10 epochs: [0.2437 0.0597 0.2681 0.4692], Epochs since improvement 0
 23%|██▎       | 117/500 [1:12:42<3:44:07, 35.11s/it] 24%|██▎       | 118/500 [1:13:29<4:05:55, 38.63s/it]Epoch: 118 done with learning rate 9.75E-03, Train loss: -2.81E+05, Train scatter: [0.2474 0.0576 0.2646 0.4649]
L1 regularization loss: 2.65E+00, L2 regularization loss: 1.66E+00
Test scatter: [0.2543 0.0585 0.2744 0.4713], Lowest was [0.2234 0.0585 0.2624 0.4619]
Median for last 10 epochs: [0.2437 0.0596 0.2681 0.4692], Epochs since improvement 0
 24%|██▍       | 119/500 [1:13:57<3:43:50, 35.25s/it]Epoch: 120 done with learning rate 9.73E-03, Train loss: -2.88E+05, Train scatter: [0.2304 0.0588 0.2559 0.4711]
L1 regularization loss: 2.65E+00, L2 regularization loss: 1.68E+00
Test scatter: [0.2429 0.0602 0.2655 0.4762], Lowest was [0.2234 0.0585 0.2624 0.4619]
Median for last 10 epochs: [0.2429 0.0596 0.2681 0.4692], Epochs since improvement 2
 24%|██▍       | 120/500 [1:14:49<4:15:34, 40.35s/it] 24%|██▍       | 121/500 [1:15:16<3:50:47, 36.54s/it] 24%|██▍       | 122/500 [1:16:02<4:06:38, 39.15s/it]Epoch: 122 done with learning rate 9.70E-03, Train loss: 7.82E+05, Train scatter: [0.6116 0.1275 0.5059 0.9499]
L1 regularization loss: 3.63E+00, L2 regularization loss: 2.41E+00
Test scatter: [0.6201 0.1272 0.5039 0.9426], Lowest was [0.2234 0.0585 0.2624 0.4619]
Median for last 10 epochs: [0.2429 0.0597 0.2683 0.4713], Epochs since improvement 4
 25%|██▍       | 123/500 [1:16:29<3:43:38, 35.59s/it] 25%|██▍       | 124/500 [1:17:14<4:01:29, 38.53s/it]Epoch: 124 done with learning rate 9.68E-03, Train loss: 8.13E+04, Train scatter: [0.4363 0.1061 0.5018 0.832 ]
L1 regularization loss: 3.68E+00, L2 regularization loss: 2.54E+00
Test scatter: [0.4552 0.1074 0.5031 0.8276], Lowest was [0.2234 0.0585 0.2624 0.4619]
Median for last 10 epochs: [0.2543 0.0602 0.2744 0.4762], Epochs since improvement 6
 25%|██▌       | 125/500 [1:17:42<3:40:17, 35.25s/it] 25%|██▌       | 126/500 [1:18:28<3:59:52, 38.48s/it]Epoch: 126 done with learning rate 9.65E-03, Train loss: -3.92E+04, Train scatter: [0.4374 0.0972 0.4212 0.7786]
L1 regularization loss: 3.70E+00, L2 regularization loss: 2.65E+00
Test scatter: [0.449  0.0996 0.4262 0.7723], Lowest was [0.2234 0.0585 0.2624 0.4619]
Median for last 10 epochs: [0.449  0.0996 0.4262 0.7723], Epochs since improvement 8
 25%|██▌       | 127/500 [1:18:56<3:39:01, 35.23s/it] 26%|██▌       | 128/500 [1:19:42<3:58:59, 38.55s/it]Epoch: 128 done with learning rate 9.62E-03, Train loss: -9.06E+04, Train scatter: [0.3769 0.0926 0.4077 0.737 ]
L1 regularization loss: 3.71E+00, L2 regularization loss: 2.73E+00
Test scatter: [0.3989 0.0956 0.4163 0.7335], Lowest was [0.2234 0.0585 0.2624 0.4619]
Median for last 10 epochs: [0.449  0.0996 0.4262 0.7723], Epochs since improvement 10
 26%|██▌       | 129/500 [1:20:09<3:37:28, 35.17s/it]Epoch: 130 done with learning rate 9.59E-03, Train loss: -1.15E+05, Train scatter: [0.3698 0.09   0.4022 0.6822]
L1 regularization loss: 3.73E+00, L2 regularization loss: 2.86E+00
Test scatter: [0.3978 0.0933 0.4102 0.6823], Lowest was [0.2234 0.0585 0.2624 0.4619]
Median for last 10 epochs: [0.449  0.0996 0.4262 0.7723], Epochs since improvement 12
 26%|██▌       | 130/500 [1:21:01<4:07:28, 40.13s/it] 26%|██▌       | 131/500 [1:21:29<3:43:36, 36.36s/it] 26%|██▋       | 132/500 [1:22:15<4:00:47, 39.26s/it]Epoch: 132 done with learning rate 9.56E-03, Train loss: -1.34E+05, Train scatter: [0.35   0.0882 0.4464 0.6302]
L1 regularization loss: 3.76E+00, L2 regularization loss: 3.05E+00
Test scatter: [0.3852 0.0917 0.4483 0.6352], Lowest was [0.2234 0.0585 0.2624 0.4619]
Median for last 10 epochs: [0.3989 0.0956 0.4262 0.7335], Epochs since improvement 14
 27%|██▋       | 133/500 [1:22:42<3:38:24, 35.71s/it] 27%|██▋       | 134/500 [1:23:29<3:58:56, 39.17s/it]Epoch: 134 done with learning rate 9.53E-03, Train loss: -1.52E+05, Train scatter: [0.3447 0.0863 0.3834 0.6073]
L1 regularization loss: 3.78E+00, L2 regularization loss: 3.23E+00
Test scatter: [0.3776 0.0893 0.3945 0.6144], Lowest was [0.2234 0.0585 0.2624 0.4619]
Median for last 10 epochs: [0.3978 0.0933 0.4163 0.6823], Epochs since improvement 16
 27%|██▋       | 135/500 [1:23:57<3:36:38, 35.61s/it] 27%|██▋       | 136/500 [1:24:42<3:54:11, 38.60s/it]Epoch: 136 done with learning rate 9.50E-03, Train loss: -1.64E+05, Train scatter: [0.3372 0.0846 0.3875 0.5946]
L1 regularization loss: 3.79E+00, L2 regularization loss: 3.35E+00
Test scatter: [0.3688 0.0877 0.3968 0.603 ], Lowest was [0.2234 0.0585 0.2624 0.4619]
Median for last 10 epochs: [0.3852 0.0917 0.4102 0.6352], Epochs since improvement 18
 27%|██▋       | 137/500 [1:25:10<3:33:27, 35.28s/it] 28%|██▊       | 138/500 [1:25:56<3:52:47, 38.58s/it]Epoch: 138 done with learning rate 9.47E-03, Train loss: -1.76E+05, Train scatter: [0.3223 0.0833 0.3825 0.5854]
L1 regularization loss: 3.80E+00, L2 regularization loss: 3.46E+00
Test scatter: [0.3565 0.0861 0.3936 0.5939], Lowest was [0.2234 0.0585 0.2624 0.4619]
Median for last 10 epochs: [0.3776 0.0893 0.3968 0.6144], Epochs since improvement 20
 28%|██▊       | 139/500 [1:26:23<3:32:12, 35.27s/it]Epoch: 140 done with learning rate 9.43E-03, Train loss: -1.80E+05, Train scatter: [0.3372 0.0854 0.3719 0.5851]
L1 regularization loss: 3.79E+00, L2 regularization loss: 3.56E+00
Test scatter: [0.3669 0.0897 0.3802 0.5918], Lowest was [0.2234 0.0585 0.2624 0.4619]
Median for last 10 epochs: [0.3688 0.0893 0.3945 0.603 ], Epochs since improvement 22
 28%|██▊       | 139/500 [1:27:16<3:46:38, 37.67s/it]
Exited after 140 epochs due to early stopping
5236.12 seconds spent training, 10.472 seconds per epoch. Processed 6650 trees per second
[0.36687294 0.08972158 0.3801401  0.59175676]
{'epoch_exit': 139, 'scatter_m_star': 0.36687294, 'lowest_m_star': 0.22335315, 'last20_m_star': 0.39149526, 'last10_m_star': 0.36884856, 'scatter_v_disk': 0.08972158, 'lowest_v_disk': 0.05849165, 'last20_v_disk': 0.09248985, 'last10_v_disk': 0.089301914, 'scatter_m_cold': 0.3801401, 'lowest_m_cold': 0.26243374, 'last20_m_cold': 0.4132573, 'last10_m_cold': 0.39447564, 'scatter_sfr_100': 0.59175676, 'lowest_sfr_100': 0.4618861, 'last20_sfr_100': 0.65877175, 'last10_sfr_100': 0.603024}
Finished 1/1
Exited training after None epochs
Experiment 2 done: 2 / 6
Exploring ['conv_layers', 'batch_size']
Currently doing [  2 256]
{'experiment': 'GraphMerge', 'group': 'meta_pysr', 'move': False, 'model': 'MetaEdge', 'log': True, 'run_params': {'n_epochs': 500, 'n_trials': 1, 'batch_size': 256, 'val_epoch': 2, 'early_stopping': True, 'patience': 20, 'l1_lambda': 0.0001, 'l2_lambda': 0.0001, 'loss_func': 'GaussNd', 'metrics': 'test_multi_varrho', 'performance_plot': 'multi_base', 'save': True, 'seed': True, 'num_workers': 4}, 'learn_params': {'learning_rate': 0.01, 'schedule': 'onecycle', 'g_up': 1, 'g_down': 0.95, 'warmup': 4, 'period': 5, 'eta_min': 1e-05}, 'hyper_params': {'hidden_channels': 128, 'conv_layers': 2, 'conv_activation': 'relu', 'decode_activation': 'leakyrelu', 'decode_layers': 2, 'layernorm': True, 'agg': 'sum', 'variance': True, 'rho': 0, 'in_channels': 43, 'out_channels': 4}, 'data_params': {'case': 'vlarge_all_4t_z0.0_quantile_raw', 'targets': [0, 1, 2, 3], 'del_feats': [], 'split': 0.8, 'test': 0, 'scale': 0}, 'experiment_name': 'con2bat256'}
/home/cj1223/../../scratch/gpfs/cj1223/GraphResults/results_meta_pysr_120322/MetaEdge_vlarge_all_4t_z0.0_quantile_raw_ignecz
RelU conv activation
LeakyRelU decode activation
N_params 422280
Made folder for saving model
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:47<6:37:55, 47.85s/it]  0%|          | 2/500 [01:59<8:31:52, 61.67s/it]Epoch: 2 done with learning rate 4.17E-04, Train loss: 2.28E+07, Train scatter: [0.9351 0.1389 0.5441 0.9954]
L1 regularization loss: 1.99E+00, L2 regularization loss: 3.83E-01
Test scatter: [0.9195 0.1361 0.5355 0.9851], Lowest was [0.9195 0.1361 0.5355 0.9851]
Median for last 10 epochs: [0.9195 0.1361 0.5355 0.9851], Epochs since improvement 2
  1%|          | 3/500 [02:45<7:34:35, 54.88s/it]  1%|          | 4/500 [03:57<8:28:14, 61.48s/it]Epoch: 4 done with learning rate 4.67E-04, Train loss: 1.47E+07, Train scatter: [0.9317 0.0967 0.544  0.9951]
L1 regularization loss: 2.01E+00, L2 regularization loss: 4.12E-01
Test scatter: [0.9158 0.0958 0.5354 0.9847], Lowest was [0.9158 0.0958 0.5354 0.9847]
Median for last 10 epochs: [0.9158 0.0958 0.5354 0.9847], Epochs since improvement 0
  1%|          | 5/500 [04:44<7:43:30, 56.18s/it]  1%|          | 6/500 [05:56<8:27:27, 61.63s/it]Epoch: 6 done with learning rate 5.51E-04, Train loss: 9.21E+06, Train scatter: [0.7653 0.087  0.5439 0.6117]
L1 regularization loss: 2.06E+00, L2 regularization loss: 4.55E-01
Test scatter: [0.7532 0.0866 0.5354 0.6124], Lowest was [0.7532 0.0866 0.5354 0.6124]
Median for last 10 epochs: [0.7532 0.0866 0.5354 0.6124], Epochs since improvement 0
  1%|▏         | 7/500 [06:43<7:46:51, 56.82s/it]  2%|▏         | 8/500 [07:55<8:25:59, 61.71s/it]Epoch: 8 done with learning rate 6.67E-04, Train loss: 5.39E+06, Train scatter: [0.4769 0.0768 0.5439 0.5521]
L1 regularization loss: 2.07E+00, L2 regularization loss: 4.68E-01
Test scatter: [0.4733 0.0764 0.5353 0.5491], Lowest was [0.4733 0.0764 0.5353 0.5491]
Median for last 10 epochs: [0.6132 0.0815 0.5353 0.5807], Epochs since improvement 0
  2%|▏         | 9/500 [08:42<7:47:05, 57.08s/it]Epoch: 10 done with learning rate 8.15E-04, Train loss: 4.11E+06, Train scatter: [0.4855 0.0771 0.5438 0.5595]
L1 regularization loss: 2.07E+00, L2 regularization loss: 4.76E-01
Test scatter: [0.4904 0.0766 0.5353 0.5611], Lowest was [0.4733 0.0764 0.5353 0.5491]
Median for last 10 epochs: [0.4904 0.0766 0.5353 0.5611], Epochs since improvement 0
  2%|▏         | 10/500 [10:01<8:41:12, 63.82s/it]  2%|▏         | 11/500 [10:48<7:58:44, 58.74s/it]  2%|▏         | 12/500 [12:00<8:29:30, 62.64s/it]Epoch: 12 done with learning rate 9.94E-04, Train loss: 3.63E+06, Train scatter: [0.3501 0.0749 0.5439 0.5251]
L1 regularization loss: 2.08E+00, L2 regularization loss: 4.82E-01
Test scatter: [0.3581 0.0746 0.5353 0.5214], Lowest was [0.3581 0.0746 0.5353 0.5214]
Median for last 10 epochs: [0.4904 0.0766 0.5353 0.5611], Epochs since improvement 0
  3%|▎         | 13/500 [12:47<7:50:36, 57.98s/it]  3%|▎         | 14/500 [13:59<8:24:14, 62.25s/it]Epoch: 14 done with learning rate 1.20E-03, Train loss: 3.44E+06, Train scatter: [0.2387 0.0702 0.5439 0.5118]
L1 regularization loss: 2.09E+00, L2 regularization loss: 4.86E-01
Test scatter: [0.2404 0.0699 0.5353 0.5038], Lowest was [0.2404 0.0699 0.5353 0.5038]
Median for last 10 epochs: [0.4733 0.0764 0.5353 0.5491], Epochs since improvement 0
  3%|▎         | 15/500 [14:46<7:46:16, 57.68s/it]  3%|▎         | 16/500 [15:58<8:19:05, 61.87s/it]Epoch: 16 done with learning rate 1.44E-03, Train loss: 3.39E+06, Train scatter: [0.2473 0.0689 0.5438 0.5077]
L1 regularization loss: 2.10E+00, L2 regularization loss: 4.90E-01
Test scatter: [0.2494 0.0684 0.5353 0.4995], Lowest was [0.2404 0.0684 0.5353 0.4995]
Median for last 10 epochs: [0.3581 0.0746 0.5353 0.5214], Epochs since improvement 0
  3%|▎         | 17/500 [16:45<7:41:46, 57.36s/it]  4%|▎         | 18/500 [17:57<8:16:27, 61.80s/it]Epoch: 18 done with learning rate 1.70E-03, Train loss: 3.20E+06, Train scatter: [0.2111 0.0668 0.5437 0.512 ]
L1 regularization loss: 2.10E+00, L2 regularization loss: 4.93E-01
Test scatter: [0.2214 0.0669 0.5352 0.4991], Lowest was [0.2214 0.0669 0.5352 0.4991]
Median for last 10 epochs: [0.2494 0.0699 0.5353 0.5038], Epochs since improvement 0
  4%|▍         | 19/500 [18:44<7:39:33, 57.33s/it]Epoch: 20 done with learning rate 1.99E-03, Train loss: 3.09E+06, Train scatter: [0.2523 0.0656 0.5437 0.5408]
L1 regularization loss: 2.11E+00, L2 regularization loss: 4.98E-01
Test scatter: [0.2562 0.0657 0.5351 0.5369], Lowest was [0.2214 0.0657 0.5351 0.4991]
Median for last 10 epochs: [0.2494 0.0684 0.5353 0.5038], Epochs since improvement 0
  4%|▍         | 20/500 [20:03<8:30:58, 63.87s/it]  4%|▍         | 21/500 [20:50<7:50:30, 58.94s/it]  4%|▍         | 22/500 [22:02<8:19:54, 62.75s/it]Epoch: 22 done with learning rate 2.30E-03, Train loss: 3.04E+06, Train scatter: [0.1997 0.066  0.5437 0.5041]
L1 regularization loss: 2.13E+00, L2 regularization loss: 5.02E-01
Test scatter: [0.2069 0.0656 0.5351 0.4976], Lowest was [0.2069 0.0656 0.5351 0.4976]
Median for last 10 epochs: [0.2404 0.0669 0.5352 0.4995], Epochs since improvement 0
  5%|▍         | 23/500 [22:49<7:41:31, 58.05s/it]  5%|▍         | 24/500 [24:01<8:13:40, 62.23s/it]Epoch: 24 done with learning rate 2.63E-03, Train loss: 2.97E+06, Train scatter: [0.1946 0.0638 0.5437 0.5143]
L1 regularization loss: 2.14E+00, L2 regularization loss: 5.07E-01
Test scatter: [0.2012 0.0638 0.5351 0.5083], Lowest was [0.2012 0.0638 0.5351 0.4976]
Median for last 10 epochs: [0.2214 0.0657 0.5351 0.4995], Epochs since improvement 0
  5%|▌         | 25/500 [24:48<7:37:08, 57.74s/it]  5%|▌         | 26/500 [26:01<8:10:52, 62.14s/it]Epoch: 26 done with learning rate 2.98E-03, Train loss: 2.94E+06, Train scatter: [0.2916 0.0753 0.5436 0.5269]
L1 regularization loss: 2.15E+00, L2 regularization loss: 5.12E-01
Test scatter: [0.2919 0.0772 0.535  0.525 ], Lowest was [0.2012 0.0638 0.535  0.4976]
Median for last 10 epochs: [0.2214 0.0657 0.5351 0.5083], Epochs since improvement 0
  5%|▌         | 27/500 [26:48<7:34:27, 57.65s/it]  6%|▌         | 28/500 [28:00<8:07:11, 61.93s/it]Epoch: 28 done with learning rate 3.34E-03, Train loss: 2.90E+06, Train scatter: [0.2702 0.0701 0.5434 0.5218]
L1 regularization loss: 2.16E+00, L2 regularization loss: 5.18E-01
Test scatter: [0.274  0.0716 0.5349 0.5198], Lowest was [0.2012 0.0638 0.5349 0.4976]
Median for last 10 epochs: [0.2562 0.0657 0.5351 0.5198], Epochs since improvement 0
  6%|▌         | 29/500 [28:47<7:31:25, 57.51s/it]Epoch: 30 done with learning rate 3.72E-03, Train loss: 2.88E+06, Train scatter: [0.1955 0.0621 0.5434 0.4912]
L1 regularization loss: 2.18E+00, L2 regularization loss: 5.26E-01
Test scatter: [0.2062 0.0627 0.5348 0.4846], Lowest was [0.2012 0.0627 0.5348 0.4846]
Median for last 10 epochs: [0.2069 0.0656 0.535  0.5083], Epochs since improvement 0
  6%|▌         | 30/500 [30:06<8:20:15, 63.86s/it]  6%|▌         | 31/500 [30:53<7:40:21, 58.89s/it]  6%|▋         | 32/500 [32:05<8:10:14, 62.85s/it]Epoch: 32 done with learning rate 4.10E-03, Train loss: 2.84E+06, Train scatter: [0.2109 0.0656 0.5434 0.4962]
L1 regularization loss: 2.20E+00, L2 regularization loss: 5.33E-01
Test scatter: [0.216  0.0663 0.5349 0.4928], Lowest was [0.2012 0.0627 0.5348 0.4846]
Median for last 10 epochs: [0.216  0.0663 0.5349 0.5083], Epochs since improvement 2
  7%|▋         | 33/500 [32:52<7:32:53, 58.19s/it]  7%|▋         | 34/500 [34:04<8:02:13, 62.09s/it]Epoch: 34 done with learning rate 4.50E-03, Train loss: 2.83E+06, Train scatter: [0.2395 0.0608 0.5433 0.4807]
L1 regularization loss: 2.22E+00, L2 regularization loss: 5.43E-01
Test scatter: [0.2384 0.0601 0.5348 0.4746], Lowest was [0.2012 0.0601 0.5348 0.4746]
Median for last 10 epochs: [0.2384 0.0663 0.5349 0.4928], Epochs since improvement 0
  7%|▋         | 35/500 [34:51<7:27:03, 57.68s/it]  7%|▋         | 36/500 [36:02<7:56:22, 61.60s/it]Epoch: 36 done with learning rate 4.90E-03, Train loss: 2.86E+06, Train scatter: [0.4361 0.0936 0.5431 0.5198]
L1 regularization loss: 2.26E+00, L2 regularization loss: 5.55E-01
Test scatter: [0.4289 0.0924 0.5346 0.511 ], Lowest was [0.2012 0.0601 0.5346 0.4746]
Median for last 10 epochs: [0.2384 0.0663 0.5348 0.4928], Epochs since improvement 0
  7%|▋         | 37/500 [36:49<7:21:50, 57.26s/it]  8%|▊         | 38/500 [38:01<7:55:53, 61.80s/it]Epoch: 38 done with learning rate 5.30E-03, Train loss: 5.48E+06, Train scatter: [0.9161 0.172  0.544  0.9724]
L1 regularization loss: 2.94E+00, L2 regularization loss: 7.70E-01
Test scatter: [0.9008 0.1682 0.5354 0.9628], Lowest was [0.2012 0.0601 0.5346 0.4746]
Median for last 10 epochs: [0.2384 0.0663 0.5348 0.4928], Epochs since improvement 2
  8%|▊         | 39/500 [38:49<7:21:26, 57.46s/it]Epoch: 40 done with learning rate 5.70E-03, Train loss: 3.53E+06, Train scatter: [0.5508 0.1191 0.544  0.7316]
L1 regularization loss: 3.00E+00, L2 regularization loss: 8.47E-01
Test scatter: [0.5471 0.1188 0.5354 0.7285], Lowest was [0.2012 0.0601 0.5346 0.4746]
Median for last 10 epochs: [0.4289 0.0924 0.5349 0.511 ], Epochs since improvement 4
  8%|▊         | 40/500 [40:07<8:08:22, 63.70s/it]  8%|▊         | 41/500 [40:54<7:29:49, 58.80s/it]  8%|▊         | 42/500 [42:06<7:58:01, 62.62s/it]Epoch: 42 done with learning rate 6.10E-03, Train loss: 3.36E+06, Train scatter: [0.8977 0.1176 0.544  0.7448]
L1 regularization loss: 3.05E+00, L2 regularization loss: 9.01E-01
Test scatter: [0.8835 0.1176 0.5354 0.7527], Lowest was [0.2012 0.0601 0.5346 0.4746]
Median for last 10 epochs: [0.5471 0.1176 0.5354 0.7285], Epochs since improvement 6
  9%|▊         | 43/500 [42:53<7:22:06, 58.04s/it]  9%|▉         | 44/500 [44:05<7:53:04, 62.25s/it]Epoch: 44 done with learning rate 6.49E-03, Train loss: 3.27E+06, Train scatter: [0.518  0.0958 0.544  0.5894]
L1 regularization loss: 3.06E+00, L2 regularization loss: 9.23E-01
Test scatter: [0.5069 0.0952 0.5354 0.5826], Lowest was [0.2012 0.0601 0.5346 0.4746]
Median for last 10 epochs: [0.5471 0.1176 0.5354 0.7285], Epochs since improvement 8
  9%|▉         | 45/500 [44:52<7:17:58, 57.76s/it]  9%|▉         | 46/500 [46:04<7:49:26, 62.04s/it]Epoch: 46 done with learning rate 6.87E-03, Train loss: 3.18E+06, Train scatter: [0.3845 0.0927 0.5439 0.6104]
L1 regularization loss: 3.08E+00, L2 regularization loss: 9.44E-01
Test scatter: [0.3788 0.0921 0.5353 0.6066], Lowest was [0.2012 0.0601 0.5346 0.4746]
Median for last 10 epochs: [0.5471 0.1176 0.5354 0.7285], Epochs since improvement 10
  9%|▉         | 47/500 [46:52<7:14:55, 57.61s/it] 10%|▉         | 48/500 [48:03<7:43:50, 61.57s/it]Epoch: 48 done with learning rate 7.24E-03, Train loss: 3.09E+06, Train scatter: [0.5059 0.0889 0.5438 0.5761]
L1 regularization loss: 3.09E+00, L2 regularization loss: 9.61E-01
Test scatter: [0.5025 0.0886 0.5353 0.5728], Lowest was [0.2012 0.0601 0.5346 0.4746]
Median for last 10 epochs: [0.5069 0.0952 0.5354 0.6066], Epochs since improvement 12
 10%|▉         | 49/500 [48:50<7:10:04, 57.22s/it]Epoch: 50 done with learning rate 7.60E-03, Train loss: 3.04E+06, Train scatter: [0.4707 0.0831 0.5438 0.5599]
L1 regularization loss: 3.10E+00, L2 regularization loss: 9.73E-01
Test scatter: [0.4612 0.0842 0.5353 0.5597], Lowest was [0.2012 0.0601 0.5346 0.4746]
Median for last 10 epochs: [0.5025 0.0921 0.5353 0.5826], Epochs since improvement 14
 10%|█         | 50/500 [50:07<7:54:48, 63.31s/it] 10%|█         | 51/500 [50:54<7:17:37, 58.48s/it] 10%|█         | 52/500 [52:07<7:47:48, 62.65s/it]Epoch: 52 done with learning rate 7.94E-03, Train loss: 3.01E+06, Train scatter: [0.4814 0.079  0.5438 0.5441]
L1 regularization loss: 3.10E+00, L2 regularization loss: 9.86E-01
Test scatter: [0.4807 0.0794 0.5353 0.5436], Lowest was [0.2012 0.0601 0.5346 0.4746]
Median for last 10 epochs: [0.4807 0.0886 0.5353 0.5728], Epochs since improvement 16
 11%|█         | 53/500 [52:54<7:12:21, 58.04s/it] 11%|█         | 54/500 [54:04<7:38:51, 61.73s/it]Epoch: 54 done with learning rate 8.26E-03, Train loss: 3.02E+06, Train scatter: [0.3053 0.0832 0.5438 0.5417]
L1 regularization loss: 3.13E+00, L2 regularization loss: 1.01E+00
Test scatter: [0.3227 0.0836 0.5353 0.5435], Lowest was [0.2012 0.0601 0.5346 0.4746]
Median for last 10 epochs: [0.4612 0.0842 0.5353 0.5597], Epochs since improvement 18
 11%|█         | 55/500 [54:52<7:05:40, 57.39s/it] 11%|█         | 56/500 [56:03<7:36:23, 61.67s/it]Epoch: 56 done with learning rate 8.56E-03, Train loss: 2.99E+06, Train scatter: [0.2586 0.0773 0.5438 0.5311]
L1 regularization loss: 3.16E+00, L2 regularization loss: 1.04E+00
Test scatter: [0.2812 0.0776 0.5352 0.5303], Lowest was [0.2012 0.0601 0.5346 0.4746]
Median for last 10 epochs: [0.4612 0.0836 0.5353 0.5436], Epochs since improvement 20
 11%|█▏        | 57/500 [56:51<7:03:26, 57.35s/it] 11%|█▏        | 57/500 [58:02<7:31:05, 61.10s/it]
Epoch: 58 done with learning rate 8.83E-03, Train loss: 2.99E+06, Train scatter: [0.416  0.0726 0.5438 0.5271]
L1 regularization loss: 3.19E+00, L2 regularization loss: 1.07E+00
Test scatter: [0.4005 0.073  0.5352 0.5238], Lowest was [0.2012 0.0601 0.5346 0.4746]
Median for last 10 epochs: [0.4005 0.0794 0.5353 0.5435], Epochs since improvement 22
Exited after 58 epochs due to early stopping
3482.47 seconds spent training, 6.965 seconds per epoch. Processed 9998 trees per second
[0.40050247 0.07297312 0.53521633 0.52376574]
{'epoch_exit': 57, 'scatter_m_star': 0.40050247, 'lowest_m_star': 0.2011799, 'last20_m_star': 0.47092682, 'last10_m_star': 0.4005152, 'scatter_v_disk': 0.072973125, 'lowest_v_disk': 0.06012533, 'last20_v_disk': 0.08639759, 'last10_v_disk': 0.0793969, 'scatter_m_cold': 0.53521633, 'lowest_m_cold': 0.5345693, 'last20_m_cold': 0.53527015, 'last10_m_cold': 0.5352633, 'scatter_sfr_100': 0.52376574, 'lowest_sfr_100': 0.47461, 'last20_sfr_100': 0.56622046, 'last10_sfr_100': 0.5435325}
Finished 1/1
Exited training after None epochs
Experiment 3 done: 3 / 6
Exploring ['conv_layers', 'batch_size']
Currently doing [  2 512]
{'experiment': 'GraphMerge', 'group': 'meta_pysr', 'move': False, 'model': 'MetaEdge', 'log': True, 'run_params': {'n_epochs': 500, 'n_trials': 1, 'batch_size': 512, 'val_epoch': 2, 'early_stopping': True, 'patience': 20, 'l1_lambda': 0.0001, 'l2_lambda': 0.0001, 'loss_func': 'GaussNd', 'metrics': 'test_multi_varrho', 'performance_plot': 'multi_base', 'save': True, 'seed': True, 'num_workers': 4}, 'learn_params': {'learning_rate': 0.01, 'schedule': 'onecycle', 'g_up': 1, 'g_down': 0.95, 'warmup': 4, 'period': 5, 'eta_min': 1e-05}, 'hyper_params': {'hidden_channels': 128, 'conv_layers': 2, 'conv_activation': 'relu', 'decode_activation': 'leakyrelu', 'decode_layers': 2, 'layernorm': True, 'agg': 'sum', 'variance': True, 'rho': 0, 'in_channels': 43, 'out_channels': 4}, 'data_params': {'case': 'vlarge_all_4t_z0.0_quantile_raw', 'targets': [0, 1, 2, 3], 'del_feats': [], 'split': 0.8, 'test': 0, 'scale': 0}, 'experiment_name': 'con2bat512'}
/home/cj1223/../../scratch/gpfs/cj1223/GraphResults/results_meta_pysr_120322/MetaEdge_vlarge_all_4t_z0.0_quantile_raw_ngxozq
RelU conv activation
LeakyRelU decode activation
N_params 422280
Made folder for saving model
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:41<5:42:15, 41.15s/it]  0%|          | 2/500 [01:44<7:31:32, 54.40s/it]Epoch: 2 done with learning rate 4.17E-04, Train loss: 3.01E+07, Train scatter: [0.9352 0.1711 0.5441 0.9954]
L1 regularization loss: 1.98E+00, L2 regularization loss: 3.76E-01
Test scatter: [0.9196 0.1673 0.5356 0.9851], Lowest was [0.9196 0.1673 0.5356 0.9851]
Median for last 10 epochs: [0.9196 0.1673 0.5356 0.9851], Epochs since improvement 2
  1%|          | 3/500 [02:25<6:37:35, 48.00s/it]  1%|          | 4/500 [03:29<7:28:22, 54.24s/it]Epoch: 4 done with learning rate 4.67E-04, Train loss: 2.21E+07, Train scatter: [0.9352 0.1561 0.5441 0.9954]
L1 regularization loss: 1.99E+00, L2 regularization loss: 3.85E-01
Test scatter: [0.9196 0.1522 0.5355 0.9851], Lowest was [0.9196 0.1522 0.5355 0.9851]
Median for last 10 epochs: [0.9196 0.1522 0.5355 0.9851], Epochs since improvement 0
  1%|          | 5/500 [04:09<6:46:18, 49.25s/it]  1%|          | 6/500 [05:12<7:23:40, 53.89s/it]Epoch: 6 done with learning rate 5.51E-04, Train loss: 1.71E+07, Train scatter: [0.9348 0.1084 0.544  0.9954]
L1 regularization loss: 2.01E+00, L2 regularization loss: 4.01E-01
Test scatter: [0.9192 0.1066 0.5355 0.985 ], Lowest was [0.9192 0.1066 0.5355 0.985 ]
Median for last 10 epochs: [0.9192 0.1066 0.5355 0.985 ], Epochs since improvement 0
  1%|▏         | 7/500 [05:53<6:47:27, 49.59s/it]  2%|▏         | 8/500 [06:56<7:22:56, 54.02s/it]Epoch: 8 done with learning rate 6.67E-04, Train loss: 1.31E+07, Train scatter: [0.8978 0.091  0.5438 0.7935]
L1 regularization loss: 2.04E+00, L2 regularization loss: 4.28E-01
Test scatter: [0.8839 0.091  0.5353 0.8048], Lowest was [0.8839 0.091  0.5353 0.8048]
Median for last 10 epochs: [0.9016 0.0988 0.5354 0.8949], Epochs since improvement 0
  2%|▏         | 9/500 [07:37<6:47:58, 49.85s/it]Epoch: 10 done with learning rate 8.15E-04, Train loss: 9.54E+06, Train scatter: [0.7296 0.0971 0.5437 0.6079]
L1 regularization loss: 2.06E+00, L2 regularization loss: 4.56E-01
Test scatter: [0.7124 0.0955 0.5351 0.6011], Lowest was [0.7124 0.091  0.5351 0.6011]
Median for last 10 epochs: [0.8839 0.0955 0.5353 0.8048], Epochs since improvement 0
  2%|▏         | 10/500 [08:46<7:36:24, 55.89s/it]  2%|▏         | 11/500 [09:27<6:57:47, 51.26s/it]  2%|▏         | 12/500 [10:31<7:28:35, 55.15s/it]Epoch: 12 done with learning rate 9.94E-04, Train loss: 6.76E+06, Train scatter: [0.6361 0.0868 0.5438 0.5951]
L1 regularization loss: 2.08E+00, L2 regularization loss: 4.71E-01
Test scatter: [0.6292 0.0873 0.5352 0.5964], Lowest was [0.6292 0.0873 0.5351 0.5964]
Median for last 10 epochs: [0.8839 0.0955 0.5353 0.8048], Epochs since improvement 0
  3%|▎         | 13/500 [11:12<6:51:44, 50.73s/it]  3%|▎         | 14/500 [12:16<7:25:01, 54.94s/it]Epoch: 14 done with learning rate 1.20E-03, Train loss: 5.26E+06, Train scatter: [0.5781 0.0842 0.5438 0.5547]
L1 regularization loss: 2.09E+00, L2 regularization loss: 4.81E-01
Test scatter: [0.5762 0.0841 0.5352 0.5559], Lowest was [0.5762 0.0841 0.5351 0.5559]
Median for last 10 epochs: [0.7124 0.091  0.5352 0.6011], Epochs since improvement 0
  3%|▎         | 15/500 [12:57<6:48:31, 50.54s/it]  3%|▎         | 16/500 [14:00<7:18:44, 54.39s/it]Epoch: 16 done with learning rate 1.44E-03, Train loss: 4.68E+06, Train scatter: [0.6117 0.0931 0.5438 0.6475]
L1 regularization loss: 2.10E+00, L2 regularization loss: 4.90E-01
Test scatter: [0.61   0.0935 0.5352 0.6508], Lowest was [0.5762 0.0841 0.5351 0.5559]
Median for last 10 epochs: [0.6292 0.091  0.5352 0.6011], Epochs since improvement 2
  3%|▎         | 17/500 [14:40<6:44:20, 50.23s/it]  4%|▎         | 18/500 [15:45<7:17:50, 54.50s/it]Epoch: 18 done with learning rate 1.70E-03, Train loss: 4.30E+06, Train scatter: [0.3176 0.0814 0.5437 0.5464]
L1 regularization loss: 2.11E+00, L2 regularization loss: 4.95E-01
Test scatter: [0.3213 0.0808 0.5351 0.5431], Lowest was [0.3213 0.0808 0.5351 0.5431]
Median for last 10 epochs: [0.61   0.0873 0.5352 0.5964], Epochs since improvement 0
  4%|▍         | 19/500 [16:25<6:43:19, 50.31s/it]Epoch: 20 done with learning rate 1.99E-03, Train loss: 3.92E+06, Train scatter: [0.2642 0.0781 0.5437 0.5336]
L1 regularization loss: 2.11E+00, L2 regularization loss: 5.02E-01
Test scatter: [0.2691 0.0785 0.5351 0.5258], Lowest was [0.2691 0.0785 0.5351 0.5258]
Median for last 10 epochs: [0.5762 0.0841 0.5352 0.5559], Epochs since improvement 0
  4%|▍         | 20/500 [17:36<7:30:34, 56.32s/it]  4%|▍         | 21/500 [18:16<6:51:35, 51.56s/it]  4%|▍         | 22/500 [19:21<7:21:29, 55.42s/it]Epoch: 22 done with learning rate 2.30E-03, Train loss: 3.87E+06, Train scatter: [0.2346 0.0769 0.5436 0.5201]
L1 regularization loss: 2.13E+00, L2 regularization loss: 5.07E-01
Test scatter: [0.242  0.0773 0.535  0.5157], Lowest was [0.242  0.0773 0.535  0.5157]
Median for last 10 epochs: [0.3213 0.0808 0.5351 0.5431], Epochs since improvement 0
  5%|▍         | 23/500 [20:01<6:45:44, 51.04s/it]  5%|▍         | 24/500 [21:05<7:15:30, 54.90s/it]Epoch: 24 done with learning rate 2.63E-03, Train loss: 3.73E+06, Train scatter: [0.236  0.075  0.5436 0.5201]
L1 regularization loss: 2.13E+00, L2 regularization loss: 5.10E-01
Test scatter: [0.2401 0.0753 0.535  0.5167], Lowest was [0.2401 0.0753 0.535  0.5157]
Median for last 10 epochs: [0.2691 0.0785 0.5351 0.5258], Epochs since improvement 0
  5%|▌         | 25/500 [21:46<6:40:32, 50.60s/it]  5%|▌         | 26/500 [22:49<7:09:48, 54.41s/it]Epoch: 26 done with learning rate 2.98E-03, Train loss: 3.73E+06, Train scatter: [0.2198 0.0764 0.5435 0.5177]
L1 regularization loss: 2.14E+00, L2 regularization loss: 5.14E-01
Test scatter: [0.2274 0.0772 0.535  0.5136], Lowest was [0.2274 0.0753 0.535  0.5136]
Median for last 10 epochs: [0.242  0.0773 0.535  0.5167], Epochs since improvement 0
  5%|▌         | 27/500 [23:30<6:36:03, 50.24s/it]  6%|▌         | 28/500 [24:34<7:09:02, 54.54s/it]Epoch: 28 done with learning rate 3.34E-03, Train loss: 3.61E+06, Train scatter: [0.2538 0.0745 0.5436 0.5199]
L1 regularization loss: 2.15E+00, L2 regularization loss: 5.17E-01
Test scatter: [0.2591 0.0747 0.535  0.5177], Lowest was [0.2274 0.0747 0.535  0.5136]
Median for last 10 epochs: [0.242  0.0772 0.535  0.5167], Epochs since improvement 0
  6%|▌         | 29/500 [25:15<6:34:56, 50.31s/it]Epoch: 30 done with learning rate 3.72E-03, Train loss: 3.64E+06, Train scatter: [0.2333 0.0745 0.5436 0.5265]
L1 regularization loss: 2.16E+00, L2 regularization loss: 5.25E-01
Test scatter: [0.2393 0.0749 0.535  0.5241], Lowest was [0.2274 0.0747 0.535  0.5136]
Median for last 10 epochs: [0.2401 0.0753 0.535  0.5167], Epochs since improvement 2
  6%|▌         | 30/500 [26:26<7:23:25, 56.61s/it]  6%|▌         | 31/500 [27:07<6:44:40, 51.77s/it]  6%|▋         | 32/500 [28:10<7:11:00, 55.26s/it]Epoch: 32 done with learning rate 4.10E-03, Train loss: 3.41E+06, Train scatter: [0.3395 0.0726 0.5436 0.5481]
L1 regularization loss: 2.17E+00, L2 regularization loss: 5.28E-01
Test scatter: [0.3406 0.0729 0.535  0.5493], Lowest was [0.2274 0.0729 0.535  0.5136]
Median for last 10 epochs: [0.2401 0.0749 0.535  0.5177], Epochs since improvement 0
  7%|▋         | 33/500 [28:51<6:36:18, 50.92s/it]  7%|▋         | 34/500 [29:55<7:05:59, 54.85s/it]Epoch: 34 done with learning rate 4.50E-03, Train loss: 3.33E+06, Train scatter: [0.2068 0.0699 0.5436 0.5023]
L1 regularization loss: 2.19E+00, L2 regularization loss: 5.34E-01
Test scatter: [0.2137 0.0704 0.535  0.4973], Lowest was [0.2137 0.0704 0.535  0.4973]
Median for last 10 epochs: [0.2393 0.0747 0.535  0.5177], Epochs since improvement 0
  7%|▋         | 35/500 [30:35<6:31:37, 50.53s/it]  7%|▋         | 36/500 [31:39<7:01:32, 54.51s/it]Epoch: 36 done with learning rate 4.90E-03, Train loss: 3.30E+06, Train scatter: [0.308  0.0709 0.5436 0.5106]
L1 regularization loss: 2.20E+00, L2 regularization loss: 5.38E-01
Test scatter: [0.3069 0.0715 0.535  0.5095], Lowest was [0.2137 0.0704 0.535  0.4973]
Median for last 10 epochs: [0.2591 0.0729 0.535  0.5177], Epochs since improvement 2
  7%|▋         | 37/500 [32:19<6:28:10, 50.30s/it]  8%|▊         | 38/500 [33:24<6:59:36, 54.50s/it]Epoch: 38 done with learning rate 5.30E-03, Train loss: 3.31E+06, Train scatter: [0.2257 0.0734 0.5435 0.5251]
L1 regularization loss: 2.21E+00, L2 regularization loss: 5.45E-01
Test scatter: [0.2748 0.0735 0.535  0.5166], Lowest was [0.2137 0.0704 0.535  0.4973]
Median for last 10 epochs: [0.2748 0.0729 0.535  0.5166], Epochs since improvement 0
  8%|▊         | 39/500 [34:04<6:26:04, 50.25s/it]Epoch: 40 done with learning rate 5.70E-03, Train loss: 3.26E+06, Train scatter: [0.2189 0.0705 0.5435 0.5035]
L1 regularization loss: 2.23E+00, L2 regularization loss: 5.52E-01
Test scatter: [0.2253 0.0713 0.535  0.4995], Lowest was [0.2137 0.0704 0.535  0.4973]
Median for last 10 epochs: [0.2748 0.0715 0.535  0.5095], Epochs since improvement 2
  8%|▊         | 40/500 [35:14<7:11:24, 56.27s/it]  8%|▊         | 41/500 [35:55<6:33:54, 51.49s/it]  8%|▊         | 42/500 [36:59<7:02:26, 55.34s/it]Epoch: 42 done with learning rate 6.10E-03, Train loss: 3.29E+06, Train scatter: [0.2235 0.0702 0.5435 0.5254]
L1 regularization loss: 2.24E+00, L2 regularization loss: 5.59E-01
Test scatter: [0.2326 0.0709 0.5349 0.5244], Lowest was [0.2137 0.0704 0.5349 0.4973]
Median for last 10 epochs: [0.2326 0.0713 0.535  0.5095], Epochs since improvement 0
  9%|▊         | 43/500 [37:39<6:27:22, 50.86s/it]  9%|▉         | 44/500 [38:44<6:58:00, 55.00s/it]Epoch: 44 done with learning rate 6.49E-03, Train loss: 3.24E+06, Train scatter: [0.2081 0.0691 0.5434 0.4979]
L1 regularization loss: 2.26E+00, L2 regularization loss: 5.69E-01
Test scatter: [0.2969 0.0697 0.5349 0.4933], Lowest was [0.2137 0.0697 0.5349 0.4933]
Median for last 10 epochs: [0.2748 0.0713 0.535  0.5095], Epochs since improvement 0
  9%|▉         | 45/500 [39:25<6:23:57, 50.63s/it]  9%|▉         | 46/500 [40:29<6:53:20, 54.63s/it]Epoch: 46 done with learning rate 6.87E-03, Train loss: 3.21E+06, Train scatter: [0.2256 0.0682 0.5435 0.5137]
L1 regularization loss: 2.27E+00, L2 regularization loss: 5.76E-01
Test scatter: [0.273  0.0692 0.5349 0.5134], Lowest was [0.2137 0.0692 0.5349 0.4933]
Median for last 10 epochs: [0.273  0.0709 0.5349 0.5134], Epochs since improvement 0
  9%|▉         | 47/500 [41:09<6:20:43, 50.43s/it] 10%|▉         | 48/500 [42:15<6:54:33, 55.03s/it]Epoch: 48 done with learning rate 7.24E-03, Train loss: 3.20E+06, Train scatter: [0.2471 0.0698 0.5434 0.5278]
L1 regularization loss: 2.32E+00, L2 regularization loss: 5.91E-01
Test scatter: [0.2477 0.0696 0.5348 0.5216], Lowest was [0.2137 0.0692 0.5348 0.4933]
Median for last 10 epochs: [0.2477 0.0697 0.5349 0.5134], Epochs since improvement 0
 10%|▉         | 49/500 [42:55<6:20:40, 50.64s/it]Epoch: 50 done with learning rate 7.60E-03, Train loss: 3.13E+06, Train scatter: [0.3021 0.0717 0.5433 0.5471]
L1 regularization loss: 2.33E+00, L2 regularization loss: 5.97E-01
Test scatter: [0.3076 0.0716 0.5348 0.5456], Lowest was [0.2137 0.0692 0.5348 0.4933]
Median for last 10 epochs: [0.273  0.0697 0.5349 0.5216], Epochs since improvement 0
 10%|█         | 50/500 [44:06<7:03:58, 56.53s/it] 10%|█         | 51/500 [44:46<6:27:06, 51.73s/it] 10%|█         | 52/500 [45:51<6:55:45, 55.68s/it]Epoch: 52 done with learning rate 7.94E-03, Train loss: 3.10E+06, Train scatter: [0.2662 0.0705 0.5432 0.527 ]
L1 regularization loss: 2.34E+00, L2 regularization loss: 6.01E-01
Test scatter: [0.2702 0.0703 0.5346 0.5271], Lowest was [0.2137 0.0692 0.5346 0.4933]
Median for last 10 epochs: [0.273  0.0697 0.5348 0.5216], Epochs since improvement 0
 11%|█         | 53/500 [46:31<6:20:40, 51.10s/it] 11%|█         | 54/500 [47:36<6:50:55, 55.28s/it]Epoch: 54 done with learning rate 8.26E-03, Train loss: 3.10E+06, Train scatter: [0.259  0.0847 0.5431 0.5304]
L1 regularization loss: 2.35E+00, L2 regularization loss: 6.08E-01
Test scatter: [0.2581 0.0829 0.5345 0.5204], Lowest was [0.2137 0.0692 0.5345 0.4933]
Median for last 10 epochs: [0.2702 0.0703 0.5348 0.5216], Epochs since improvement 0
 11%|█         | 55/500 [48:17<6:17:04, 50.84s/it] 11%|█         | 56/500 [49:21<6:44:57, 54.72s/it]Epoch: 56 done with learning rate 8.56E-03, Train loss: 3.13E+06, Train scatter: [0.3922 0.0687 0.5431 0.4926]
L1 regularization loss: 2.36E+00, L2 regularization loss: 6.16E-01
Test scatter: [0.3888 0.0679 0.5345 0.4867], Lowest was [0.2137 0.0679 0.5345 0.4867]
Median for last 10 epochs: [0.2702 0.0703 0.5346 0.5216], Epochs since improvement 0
 11%|█▏        | 57/500 [50:01<6:12:33, 50.46s/it] 12%|█▏        | 58/500 [51:06<6:43:48, 54.82s/it]Epoch: 58 done with learning rate 8.83E-03, Train loss: 3.09E+06, Train scatter: [0.7233 0.0881 0.543  0.8262]
L1 regularization loss: 2.39E+00, L2 regularization loss: 6.29E-01
Test scatter: [0.7053 0.0867 0.5345 0.8194], Lowest was [0.2137 0.0679 0.5345 0.4867]
Median for last 10 epochs: [0.3076 0.0716 0.5345 0.5271], Epochs since improvement 2
 12%|█▏        | 59/500 [51:47<6:11:10, 50.50s/it]Epoch: 60 done with learning rate 9.08E-03, Train loss: 3.22E+06, Train scatter: [0.4842 0.0918 0.5431 0.5368]
L1 regularization loss: 2.46E+00, L2 regularization loss: 6.64E-01
Test scatter: [0.4868 0.0905 0.5346 0.5321], Lowest was [0.2137 0.0679 0.5345 0.4867]
Median for last 10 epochs: [0.3888 0.0829 0.5345 0.5271], Epochs since improvement 4
 12%|█▏        | 60/500 [52:57<6:53:08, 56.34s/it] 12%|█▏        | 61/500 [53:37<6:17:21, 51.58s/it] 12%|█▏        | 62/500 [54:42<6:45:58, 55.61s/it]Epoch: 62 done with learning rate 9.31E-03, Train loss: 3.18E+06, Train scatter: [0.3967 0.0913 0.5432 0.6409]
L1 regularization loss: 2.50E+00, L2 regularization loss: 6.91E-01
Test scatter: [0.3928 0.0902 0.5347 0.6397], Lowest was [0.2137 0.0679 0.5345 0.4867]
Median for last 10 epochs: [0.3928 0.0867 0.5345 0.5321], Epochs since improvement 6
 13%|█▎        | 63/500 [55:23<6:11:54, 51.06s/it] 13%|█▎        | 64/500 [56:26<6:38:34, 54.85s/it]Epoch: 64 done with learning rate 9.50E-03, Train loss: 3.10E+06, Train scatter: [0.4191 0.0821 0.5429 0.5362]
L1 regularization loss: 2.51E+00, L2 regularization loss: 7.08E-01
Test scatter: [0.4144 0.0816 0.5344 0.5348], Lowest was [0.2137 0.0679 0.5344 0.4867]
Median for last 10 epochs: [0.4144 0.0867 0.5345 0.5348], Epochs since improvement 0
 13%|█▎        | 65/500 [57:07<6:06:37, 50.57s/it] 13%|█▎        | 66/500 [58:10<6:34:07, 54.49s/it]Epoch: 66 done with learning rate 9.66E-03, Train loss: 3.09E+06, Train scatter: [0.2441 0.0855 0.5427 0.524 ]
L1 regularization loss: 2.53E+00, L2 regularization loss: 7.28E-01
Test scatter: [0.2967 0.084  0.5342 0.5174], Lowest was [0.2137 0.0679 0.5342 0.4867]
Median for last 10 epochs: [0.4144 0.0867 0.5345 0.5348], Epochs since improvement 0
 13%|█▎        | 67/500 [58:51<6:02:48, 50.27s/it] 14%|█▎        | 68/500 [59:54<6:29:35, 54.11s/it]Epoch: 68 done with learning rate 9.80E-03, Train loss: 3.06E+06, Train scatter: [0.2355 0.0732 0.5426 0.5157]
L1 regularization loss: 2.56E+00, L2 regularization loss: 7.47E-01
Test scatter: [0.2439 0.0723 0.5341 0.5109], Lowest was [0.2137 0.0679 0.5341 0.4867]
Median for last 10 epochs: [0.3928 0.084  0.5344 0.5321], Epochs since improvement 0
 14%|█▍        | 69/500 [1:00:35<5:59:38, 50.07s/it]Epoch: 70 done with learning rate 9.90E-03, Train loss: 3.07E+06, Train scatter: [0.2247 0.0783 0.5426 0.5075]
L1 regularization loss: 2.57E+00, L2 regularization loss: 7.57E-01
Test scatter: [0.2311 0.0772 0.5341 0.5035], Lowest was [0.2137 0.0679 0.5341 0.4867]
Median for last 10 epochs: [0.2967 0.0816 0.5342 0.5174], Epochs since improvement 0
 14%|█▍        | 70/500 [1:01:44<6:40:15, 55.85s/it] 14%|█▍        | 71/500 [1:02:24<6:06:05, 51.20s/it] 14%|█▍        | 72/500 [1:03:27<6:30:10, 54.70s/it]Epoch: 72 done with learning rate 9.96E-03, Train loss: 3.04E+06, Train scatter: [0.2678 0.0718 0.5427 0.5213]
L1 regularization loss: 2.56E+00, L2 regularization loss: 7.62E-01
Test scatter: [0.2742 0.0718 0.5342 0.5235], Lowest was [0.2137 0.0679 0.5341 0.4867]
Median for last 10 epochs: [0.2742 0.0772 0.5342 0.5174], Epochs since improvement 2
 15%|█▍        | 73/500 [1:04:08<5:58:54, 50.43s/it] 15%|█▍        | 74/500 [1:05:12<6:27:51, 54.63s/it]Epoch: 74 done with learning rate 1.00E-02, Train loss: 3.03E+06, Train scatter: [0.3679 0.0675 0.5424 0.4989]
L1 regularization loss: 2.58E+00, L2 regularization loss: 7.77E-01
Test scatter: [0.3668 0.0674 0.5339 0.4935], Lowest was [0.2137 0.0674 0.5339 0.4867]
Median for last 10 epochs: [0.2742 0.0723 0.5341 0.5109], Epochs since improvement 0
 15%|█▌        | 75/500 [1:05:52<5:56:47, 50.37s/it] 15%|█▌        | 76/500 [1:06:56<6:24:07, 54.36s/it]Epoch: 76 done with learning rate 1.00E-02, Train loss: 3.06E+06, Train scatter: [0.4152 0.0655 0.5423 0.4975]
L1 regularization loss: 2.61E+00, L2 regularization loss: 8.03E-01
Test scatter: [0.4111 0.0652 0.5338 0.4903], Lowest was [0.2137 0.0652 0.5338 0.4867]
Median for last 10 epochs: [0.2742 0.0718 0.5341 0.5035], Epochs since improvement 0
 15%|█▌        | 77/500 [1:07:37<5:53:48, 50.19s/it] 16%|█▌        | 78/500 [1:08:39<6:19:49, 54.00s/it]Epoch: 78 done with learning rate 1.00E-02, Train loss: 3.01E+06, Train scatter: [0.2099 0.066  0.542  0.4942]
L1 regularization loss: 2.62E+00, L2 regularization loss: 8.17E-01
Test scatter: [0.2172 0.0654 0.5335 0.4889], Lowest was [0.2137 0.0652 0.5335 0.4867]
Median for last 10 epochs: [0.2742 0.0674 0.5339 0.4935], Epochs since improvement 0
 16%|█▌        | 79/500 [1:09:20<5:50:33, 49.96s/it]Epoch: 80 done with learning rate 1.00E-02, Train loss: 3.00E+06, Train scatter: [0.2297 0.0727 0.5417 0.5203]
L1 regularization loss: 2.62E+00, L2 regularization loss: 8.30E-01
Test scatter: [0.2331 0.0722 0.5332 0.5112], Lowest was [0.2137 0.0652 0.5332 0.4867]
Median for last 10 epochs: [0.2742 0.0674 0.5338 0.4935], Epochs since improvement 0
 16%|█▌        | 80/500 [1:10:31<6:33:24, 56.20s/it] 16%|█▌        | 81/500 [1:11:12<6:00:10, 51.58s/it] 16%|█▋        | 82/500 [1:12:16<6:26:28, 55.48s/it]Epoch: 82 done with learning rate 9.99E-03, Train loss: 2.98E+06, Train scatter: [0.2831 0.0643 0.5413 0.4846]
L1 regularization loss: 2.63E+00, L2 regularization loss: 8.44E-01
Test scatter: [0.3535 0.0646 0.5328 0.4802], Lowest was [0.2137 0.0646 0.5328 0.4802]
Median for last 10 epochs: [0.3535 0.0654 0.5335 0.4903], Epochs since improvement 0
 17%|█▋        | 83/500 [1:12:57<5:54:56, 51.07s/it] 17%|█▋        | 84/500 [1:14:01<6:21:20, 55.00s/it]Epoch: 84 done with learning rate 9.99E-03, Train loss: 2.97E+06, Train scatter: [0.2013 0.0637 0.5412 0.4919]
L1 regularization loss: 2.65E+00, L2 regularization loss: 8.60E-01
Test scatter: [0.2087 0.0634 0.5326 0.4864], Lowest was [0.2087 0.0634 0.5326 0.4802]
Median for last 10 epochs: [0.2331 0.0652 0.5332 0.4889], Epochs since improvement 0
 17%|█▋        | 85/500 [1:14:42<5:50:40, 50.70s/it] 17%|█▋        | 86/500 [1:15:45<6:16:10, 54.52s/it]Epoch: 86 done with learning rate 9.98E-03, Train loss: 3.13E+06, Train scatter: [0.9094 0.1717 0.5438 0.8211]
L1 regularization loss: 2.78E+00, L2 regularization loss: 9.29E-01
Test scatter: [0.8927 0.1681 0.5352 0.8172], Lowest was [0.2087 0.0634 0.5326 0.4802]
Median for last 10 epochs: [0.2331 0.0654 0.5332 0.4889], Epochs since improvement 2
 17%|█▋        | 87/500 [1:16:26<5:46:21, 50.32s/it] 18%|█▊        | 88/500 [1:17:30<6:14:59, 54.61s/it]Epoch: 88 done with learning rate 9.98E-03, Train loss: 3.12E+06, Train scatter: [0.599  0.0956 0.5434 0.5987]
L1 regularization loss: 2.78E+00, L2 regularization loss: 9.50E-01
Test scatter: [0.597  0.0935 0.5348 0.5895], Lowest was [0.2087 0.0634 0.5326 0.4802]
Median for last 10 epochs: [0.3535 0.0722 0.5332 0.5112], Epochs since improvement 4
 18%|█▊        | 89/500 [1:18:11<5:45:13, 50.40s/it]Epoch: 90 done with learning rate 9.97E-03, Train loss: 3.07E+06, Train scatter: [0.3938 0.0927 0.5433 0.618 ]
L1 regularization loss: 2.77E+00, L2 regularization loss: 9.69E-01
Test scatter: [0.3961 0.091  0.5347 0.6113], Lowest was [0.2087 0.0634 0.5326 0.4802]
Median for last 10 epochs: [0.3961 0.091  0.5347 0.5895], Epochs since improvement 6
 18%|█▊        | 90/500 [1:19:21<6:25:42, 56.45s/it] 18%|█▊        | 91/500 [1:20:02<5:52:50, 51.76s/it] 18%|█▊        | 92/500 [1:21:06<6:16:11, 55.32s/it]Epoch: 92 done with learning rate 9.96E-03, Train loss: 3.02E+06, Train scatter: [0.3876 0.0991 0.5431 0.5693]
L1 regularization loss: 2.80E+00, L2 regularization loss: 9.88E-01
Test scatter: [0.3956 0.096  0.5345 0.5566], Lowest was [0.2087 0.0634 0.5326 0.4802]
Median for last 10 epochs: [0.3961 0.0935 0.5347 0.5895], Epochs since improvement 8
 19%|█▊        | 93/500 [1:21:47<5:45:26, 50.93s/it] 19%|█▉        | 94/500 [1:22:50<6:10:50, 54.81s/it]Epoch: 94 done with learning rate 9.95E-03, Train loss: 4.89E+06, Train scatter: [0.9334 0.1731 0.5441 0.9959]
L1 regularization loss: 3.72E+00, L2 regularization loss: 1.55E+00
Test scatter: [0.9178 0.1692 0.5355 0.9857], Lowest was [0.2087 0.0634 0.5326 0.4802]
Median for last 10 epochs: [0.597  0.096  0.5348 0.6113], Epochs since improvement 10
 19%|█▉        | 95/500 [1:23:31<5:41:28, 50.59s/it] 19%|█▉        | 96/500 [1:24:36<6:08:38, 54.75s/it]Epoch: 96 done with learning rate 9.94E-03, Train loss: 4.16E+06, Train scatter: [0.9328 0.1728 0.5441 0.9953]
L1 regularization loss: 3.73E+00, L2 regularization loss: 1.57E+00
Test scatter: [0.9172 0.1689 0.5355 0.9849], Lowest was [0.2087 0.0634 0.5326 0.4802]
Median for last 10 epochs: [0.597  0.096  0.5348 0.6113], Epochs since improvement 12
 19%|█▉        | 97/500 [1:25:16<5:39:19, 50.52s/it] 20%|█▉        | 98/500 [1:26:21<6:06:24, 54.69s/it]Epoch: 98 done with learning rate 9.93E-03, Train loss: 3.89E+06, Train scatter: [0.9308 0.1725 0.5441 0.9952]
L1 regularization loss: 3.72E+00, L2 regularization loss: 1.58E+00
Test scatter: [0.9151 0.1687 0.5355 0.9849], Lowest was [0.2087 0.0634 0.5326 0.4802]
Median for last 10 epochs: [0.9151 0.1687 0.5355 0.9849], Epochs since improvement 14
 20%|█▉        | 99/500 [1:27:01<5:37:17, 50.47s/it]Epoch: 100 done with learning rate 9.91E-03, Train loss: 3.63E+06, Train scatter: [0.9302 0.1721 0.5441 0.9951]
L1 regularization loss: 3.71E+00, L2 regularization loss: 1.58E+00
Test scatter: [0.9153 0.1684 0.5355 0.9848], Lowest was [0.2087 0.0634 0.5326 0.4802]
Median for last 10 epochs: [0.9153 0.1687 0.5355 0.9849], Epochs since improvement 16
 20%|██        | 100/500 [1:28:12<6:16:15, 56.44s/it] 20%|██        | 101/500 [1:28:52<5:43:44, 51.69s/it] 20%|██        | 102/500 [1:29:57<6:09:05, 55.64s/it]Epoch: 102 done with learning rate 9.90E-03, Train loss: 3.52E+06, Train scatter: [0.9288 0.1707 0.5441 0.9949]
L1 regularization loss: 3.70E+00, L2 regularization loss: 1.57E+00
Test scatter: [0.9145 0.1673 0.5355 0.9846], Lowest was [0.2087 0.0634 0.5326 0.4802]
Median for last 10 epochs: [0.9153 0.1687 0.5355 0.9849], Epochs since improvement 18
 21%|██        | 103/500 [1:30:38<5:38:06, 51.10s/it] 21%|██        | 104/500 [1:31:42<6:02:26, 54.92s/it]Epoch: 104 done with learning rate 9.89E-03, Train loss: 3.48E+06, Train scatter: [0.9248 0.16   0.5441 0.9949]
L1 regularization loss: 3.70E+00, L2 regularization loss: 1.57E+00
Test scatter: [0.9109 0.1582 0.5355 0.9846], Lowest was [0.2087 0.0634 0.5326 0.4802]
Median for last 10 epochs: [0.9151 0.1684 0.5355 0.9848], Epochs since improvement 20
 21%|██        | 105/500 [1:32:22<5:33:00, 50.58s/it] 21%|██        | 105/500 [1:33:26<5:51:29, 53.39s/it]
Epoch: 106 done with learning rate 9.87E-03, Train loss: 3.33E+06, Train scatter: [0.9002 0.1686 0.5441 0.9945]
L1 regularization loss: 3.70E+00, L2 regularization loss: 1.58E+00
Test scatter: [0.8882 0.1652 0.5355 0.9843], Lowest was [0.2087 0.0634 0.5326 0.4802]
Median for last 10 epochs: [0.9145 0.1673 0.5355 0.9846], Epochs since improvement 22
Exited after 106 epochs due to early stopping
5606.02 seconds spent training, 11.212 seconds per epoch. Processed 6211 trees per second
[0.8881836  0.16518298 0.5354563  0.9842396 ]
{'epoch_exit': 105, 'scatter_m_star': 0.8881836, 'lowest_m_star': 0.20870984, 'last20_m_star': 0.9127024, 'last10_m_star': 0.9144616, 'scatter_v_disk': 0.16518298, 'lowest_v_disk': 0.06342661, 'last20_v_disk': 0.16623132, 'last10_v_disk': 0.16727489, 'scatter_m_cold': 0.5354563, 'lowest_m_cold': 0.5326031, 'last20_m_cold': 0.53547376, 'last10_m_cold': 0.53547424, 'scatter_sfr_100': 0.9842396, 'lowest_sfr_100': 0.48020843, 'last20_sfr_100': 0.9846319, 'last10_sfr_100': 0.9846475}
Finished 1/1
Exited training after None epochs
Experiment 4 done: 4 / 6
Exploring ['conv_layers', 'batch_size']
Currently doing [  3 256]
{'experiment': 'GraphMerge', 'group': 'meta_pysr', 'move': False, 'model': 'MetaEdge', 'log': True, 'run_params': {'n_epochs': 500, 'n_trials': 1, 'batch_size': 256, 'val_epoch': 2, 'early_stopping': True, 'patience': 20, 'l1_lambda': 0.0001, 'l2_lambda': 0.0001, 'loss_func': 'GaussNd', 'metrics': 'test_multi_varrho', 'performance_plot': 'multi_base', 'save': True, 'seed': True, 'num_workers': 4}, 'learn_params': {'learning_rate': 0.01, 'schedule': 'onecycle', 'g_up': 1, 'g_down': 0.95, 'warmup': 4, 'period': 5, 'eta_min': 1e-05}, 'hyper_params': {'hidden_channels': 128, 'conv_layers': 3, 'conv_activation': 'relu', 'decode_activation': 'leakyrelu', 'decode_layers': 2, 'layernorm': True, 'agg': 'sum', 'variance': True, 'rho': 0, 'in_channels': 43, 'out_channels': 4}, 'data_params': {'case': 'vlarge_all_4t_z0.0_quantile_raw', 'targets': [0, 1, 2, 3], 'del_feats': [], 'split': 0.8, 'test': 0, 'scale': 0}, 'experiment_name': 'con3bat256'}
/home/cj1223/../../scratch/gpfs/cj1223/GraphResults/results_meta_pysr_120322/MetaEdge_vlarge_all_4t_z0.0_quantile_raw_dbuvqd
RelU conv activation
LeakyRelU decode activation
N_params 546504
Made folder for saving model
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [01:00<8:26:59, 60.96s/it]  0%|          | 2/500 [02:30<10:46:44, 77.92s/it]Epoch: 2 done with learning rate 4.17E-04, Train loss: 9.22E+07, Train scatter: [0.9351 0.1327 0.5441 0.9954]
L1 regularization loss: 2.46E+00, L2 regularization loss: 4.25E-01
Test scatter: [0.9195 0.1278 0.5355 0.9851], Lowest was [0.9195 0.1278 0.5355 0.9851]
Median for last 10 epochs: [0.9195 0.1278 0.5355 0.9851], Epochs since improvement 2
  1%|          | 3/500 [03:32<9:43:13, 70.41s/it]   1%|          | 4/500 [05:04<10:52:03, 78.88s/it]Epoch: 4 done with learning rate 4.67E-04, Train loss: 8.01E+07, Train scatter: [0.9318 0.1003 0.5437 0.9954]
L1 regularization loss: 2.47E+00, L2 regularization loss: 4.29E-01
Test scatter: [0.9161 0.0987 0.5351 0.9851], Lowest was [0.9161 0.0987 0.5351 0.9851]
Median for last 10 epochs: [0.9161 0.0987 0.5351 0.9851], Epochs since improvement 0
  1%|          | 5/500 [06:05<9:58:47, 72.58s/it]   1%|          | 6/500 [07:37<10:52:41, 79.27s/it]Epoch: 6 done with learning rate 5.51E-04, Train loss: 7.72E+07, Train scatter: [0.8418 0.0932 0.5395 0.9954]
L1 regularization loss: 2.49E+00, L2 regularization loss: 4.37E-01
Test scatter: [0.8283 0.092  0.5311 0.9851], Lowest was [0.8283 0.092  0.5311 0.9851]
Median for last 10 epochs: [0.8283 0.092  0.5311 0.9851], Epochs since improvement 0
  1%|▏         | 7/500 [08:39<10:03:07, 73.40s/it]  2%|▏         | 8/500 [10:10<10:48:29, 79.08s/it]Epoch: 8 done with learning rate 6.67E-04, Train loss: 7.47E+07, Train scatter: [0.6548 0.0921 0.3987 0.9954]
L1 regularization loss: 2.51E+00, L2 regularization loss: 4.50E-01
Test scatter: [0.6602 0.0953 0.3901 0.985 ], Lowest was [0.6602 0.092  0.3901 0.985 ]
Median for last 10 epochs: [0.7443 0.0936 0.4606 0.985 ], Epochs since improvement 0
  2%|▏         | 9/500 [11:11<10:01:29, 73.50s/it]Epoch: 10 done with learning rate 8.15E-04, Train loss: 7.33E+07, Train scatter: [0.5882 0.0917 0.3844 0.9954]
L1 regularization loss: 2.52E+00, L2 regularization loss: 4.61E-01
Test scatter: [0.5959 0.0921 0.3814 0.985 ], Lowest was [0.5959 0.092  0.3814 0.985 ]
Median for last 10 epochs: [0.6602 0.0921 0.3901 0.985 ], Epochs since improvement 0
  2%|▏         | 10/500 [12:50<11:05:32, 81.50s/it]  2%|▏         | 11/500 [13:52<10:14:13, 75.36s/it]  2%|▏         | 12/500 [15:24<10:54:09, 80.43s/it]Epoch: 12 done with learning rate 9.94E-04, Train loss: 7.24E+07, Train scatter: [0.5556 0.0836 0.3431 0.9954]
L1 regularization loss: 2.53E+00, L2 regularization loss: 4.68E-01
Test scatter: [0.5623 0.0844 0.3431 0.9851], Lowest was [0.5623 0.0844 0.3431 0.985 ]
Median for last 10 epochs: [0.6602 0.0921 0.3901 0.9851], Epochs since improvement 0
  3%|▎         | 13/500 [16:25<10:05:29, 74.60s/it]  3%|▎         | 14/500 [17:56<10:44:34, 79.58s/it]Epoch: 14 done with learning rate 1.20E-03, Train loss: 7.16E+07, Train scatter: [0.5142 0.0817 0.3244 0.9954]
L1 regularization loss: 2.56E+00, L2 regularization loss: 4.77E-01
Test scatter: [0.521  0.0819 0.3256 0.9851], Lowest was [0.521  0.0819 0.3256 0.985 ]
Median for last 10 epochs: [0.5959 0.092  0.3814 0.9851], Epochs since improvement 0
  3%|▎         | 15/500 [18:57<9:58:33, 74.05s/it]   3%|▎         | 16/500 [20:28<10:37:56, 79.08s/it]Epoch: 16 done with learning rate 1.44E-03, Train loss: 6.92E+07, Train scatter: [0.5277 0.0806 0.3646 0.9955]
L1 regularization loss: 2.59E+00, L2 regularization loss: 4.90E-01
Test scatter: [0.5228 0.0798 0.3697 0.9852], Lowest was [0.521  0.0798 0.3256 0.985 ]
Median for last 10 epochs: [0.5623 0.0844 0.3697 0.9851], Epochs since improvement 0
  3%|▎         | 17/500 [21:30<9:53:44, 73.76s/it]   4%|▎         | 18/500 [23:01<10:35:20, 79.09s/it]Epoch: 18 done with learning rate 1.70E-03, Train loss: 3.10E+07, Train scatter: [0.4942 0.0859 0.3471 0.9134]
L1 regularization loss: 2.61E+00, L2 regularization loss: 5.08E-01
Test scatter: [0.4901 0.0849 0.3492 0.9063], Lowest was [0.4901 0.0798 0.3256 0.9063]
Median for last 10 epochs: [0.5228 0.0844 0.3492 0.9851], Epochs since improvement 0
  4%|▍         | 19/500 [24:02<9:51:00, 73.72s/it] Epoch: 20 done with learning rate 1.99E-03, Train loss: 4.88E+06, Train scatter: [0.4906 0.0809 0.3435 0.5806]
L1 regularization loss: 2.64E+00, L2 regularization loss: 5.24E-01
Test scatter: [0.4838 0.0811 0.3501 0.5839], Lowest was [0.4838 0.0798 0.3256 0.5839]
Median for last 10 epochs: [0.521  0.0819 0.3492 0.9851], Epochs since improvement 0
  4%|▍         | 20/500 [25:41<10:50:13, 81.28s/it]  4%|▍         | 21/500 [26:43<10:01:15, 75.31s/it]  4%|▍         | 22/500 [28:14<10:37:31, 80.02s/it]Epoch: 22 done with learning rate 2.30E-03, Train loss: 3.57E+06, Train scatter: [0.4458 0.0721 0.3088 0.5162]
L1 regularization loss: 2.67E+00, L2 regularization loss: 5.36E-01
Test scatter: [0.4329 0.0714 0.3098 0.5137], Lowest was [0.4329 0.0714 0.3098 0.5137]
Median for last 10 epochs: [0.4901 0.0811 0.3492 0.9063], Epochs since improvement 0
  5%|▍         | 23/500 [29:15<9:51:19, 74.38s/it]   5%|▍         | 24/500 [30:46<10:29:48, 79.39s/it]Epoch: 24 done with learning rate 2.63E-03, Train loss: 3.24E+06, Train scatter: [0.4488 0.0715 0.3258 0.503 ]
L1 regularization loss: 2.71E+00, L2 regularization loss: 5.51E-01
Test scatter: [0.4445 0.0714 0.3294 0.5071], Lowest was [0.4329 0.0714 0.3098 0.5071]
Median for last 10 epochs: [0.4838 0.0798 0.3492 0.5839], Epochs since improvement 0
  5%|▌         | 25/500 [31:47<9:45:46, 73.99s/it]   5%|▌         | 26/500 [33:19<10:27:09, 79.39s/it]Epoch: 26 done with learning rate 2.98E-03, Train loss: 3.04E+06, Train scatter: [0.4392 0.0681 0.2977 0.5065]
L1 regularization loss: 2.74E+00, L2 regularization loss: 5.67E-01
Test scatter: [0.4285 0.0671 0.298  0.5049], Lowest was [0.4285 0.0671 0.298  0.5049]
Median for last 10 epochs: [0.4445 0.0714 0.3294 0.5137], Epochs since improvement 0
  5%|▌         | 27/500 [34:21<9:43:17, 73.99s/it]   6%|▌         | 28/500 [35:53<10:24:28, 79.38s/it]Epoch: 28 done with learning rate 3.34E-03, Train loss: 3.02E+06, Train scatter: [0.4215 0.0679 0.3052 0.4866]
L1 regularization loss: 2.78E+00, L2 regularization loss: 5.86E-01
Test scatter: [0.4105 0.0676 0.3142 0.4903], Lowest was [0.4105 0.0671 0.298  0.4903]
Median for last 10 epochs: [0.4329 0.0714 0.3142 0.5071], Epochs since improvement 0
  6%|▌         | 29/500 [36:54<9:40:21, 73.93s/it] Epoch: 30 done with learning rate 3.72E-03, Train loss: 3.02E+06, Train scatter: [0.4879 0.0672 0.2971 0.4824]
L1 regularization loss: 2.83E+00, L2 regularization loss: 6.12E-01
Test scatter: [0.4926 0.0663 0.2995 0.4793], Lowest was [0.4105 0.0663 0.298  0.4793]
Median for last 10 epochs: [0.4329 0.0676 0.3098 0.5049], Epochs since improvement 0
  6%|▌         | 30/500 [38:32<10:36:19, 81.23s/it]  6%|▌         | 31/500 [39:33<9:48:24, 75.28s/it]   6%|▋         | 32/500 [41:05<10:25:37, 80.21s/it]Epoch: 32 done with learning rate 4.10E-03, Train loss: 2.91E+06, Train scatter: [0.4107 0.0671 0.2936 0.4861]
L1 regularization loss: 2.88E+00, L2 regularization loss: 6.48E-01
Test scatter: [0.4067 0.0661 0.2964 0.4853], Lowest was [0.4067 0.0661 0.2964 0.4793]
Median for last 10 epochs: [0.4285 0.0671 0.2995 0.4903], Epochs since improvement 0
  7%|▋         | 33/500 [42:06<9:39:59, 74.52s/it]   7%|▋         | 34/500 [43:38<10:18:11, 79.60s/it]Epoch: 34 done with learning rate 4.50E-03, Train loss: 2.68E+06, Train scatter: [0.4109 0.0687 0.3095 0.4719]
L1 regularization loss: 2.94E+00, L2 regularization loss: 6.96E-01
Test scatter: [0.4026 0.0678 0.3057 0.4741], Lowest was [0.4026 0.0661 0.2964 0.4741]
Median for last 10 epochs: [0.4105 0.0671 0.2995 0.4853], Epochs since improvement 0
  7%|▋         | 35/500 [44:39<9:34:06, 74.08s/it]   7%|▋         | 36/500 [46:10<10:12:42, 79.23s/it]Epoch: 36 done with learning rate 4.90E-03, Train loss: 2.69E+06, Train scatter: [0.3965 0.064  0.2923 0.4735]
L1 regularization loss: 3.00E+00, L2 regularization loss: 7.49E-01
Test scatter: [0.3895 0.0627 0.2927 0.4683], Lowest was [0.3895 0.0627 0.2927 0.4683]
Median for last 10 epochs: [0.4067 0.0663 0.2995 0.4793], Epochs since improvement 0
  7%|▋         | 37/500 [47:11<9:29:22, 73.79s/it]   8%|▊         | 38/500 [48:43<10:09:12, 79.12s/it]Epoch: 38 done with learning rate 5.30E-03, Train loss: 2.51E+06, Train scatter: [0.3305 0.0649 0.2934 0.4683]
L1 regularization loss: 3.07E+00, L2 regularization loss: 7.99E-01
Test scatter: [0.3333 0.064  0.2908 0.4662], Lowest was [0.3333 0.0627 0.2908 0.4662]
Median for last 10 epochs: [0.4026 0.0661 0.2964 0.4741], Epochs since improvement 0
  8%|▊         | 39/500 [49:44<9:26:44, 73.76s/it] Epoch: 40 done with learning rate 5.70E-03, Train loss: 3.06E+06, Train scatter: [0.4056 0.0725 0.5389 0.4807]
L1 regularization loss: 3.17E+00, L2 regularization loss: 8.69E-01
Test scatter: [0.3998 0.0718 0.5305 0.4805], Lowest was [0.3333 0.0627 0.2908 0.4662]
Median for last 10 epochs: [0.3998 0.0661 0.2964 0.4741], Epochs since improvement 2
  8%|▊         | 40/500 [51:23<10:23:31, 81.33s/it]  8%|▊         | 41/500 [52:25<9:36:54, 75.41s/it]   8%|▊         | 42/500 [53:56<10:12:38, 80.26s/it]Epoch: 42 done with learning rate 6.10E-03, Train loss: 2.52E+06, Train scatter: [0.4022 0.0618 0.3087 0.4632]
L1 regularization loss: 3.23E+00, L2 regularization loss: 9.48E-01
Test scatter: [0.3972 0.0612 0.3095 0.4645], Lowest was [0.3333 0.0612 0.2908 0.4645]
Median for last 10 epochs: [0.3972 0.064  0.3057 0.4683], Epochs since improvement 0
  9%|▊         | 43/500 [54:58<9:28:17, 74.61s/it]   9%|▉         | 44/500 [56:29<10:04:59, 79.60s/it]Epoch: 44 done with learning rate 6.49E-03, Train loss: 2.28E+06, Train scatter: [0.4194 0.0625 0.3031 0.4631]
L1 regularization loss: 3.29E+00, L2 regularization loss: 1.01E+00
Test scatter: [0.411  0.0615 0.3083 0.4641], Lowest was [0.3333 0.0612 0.2908 0.4641]
Median for last 10 epochs: [0.3972 0.0627 0.3083 0.4662], Epochs since improvement 0
  9%|▉         | 45/500 [57:31<9:22:18, 74.15s/it]   9%|▉         | 46/500 [59:02<9:59:32, 79.24s/it]Epoch: 46 done with learning rate 6.87E-03, Train loss: 2.20E+06, Train scatter: [0.3662 0.0603 0.2805 0.444 ]
L1 regularization loss: 3.37E+00, L2 regularization loss: 1.08E+00
Test scatter: [0.3509 0.0603 0.2794 0.445 ], Lowest was [0.3333 0.0603 0.2794 0.445 ]
Median for last 10 epochs: [0.3972 0.0615 0.3083 0.4645], Epochs since improvement 0
  9%|▉         | 47/500 [1:00:03<9:17:32, 73.85s/it] 10%|▉         | 48/500 [1:01:34<9:54:27, 78.91s/it]Epoch: 48 done with learning rate 7.24E-03, Train loss: 2.11E+06, Train scatter: [0.4217 0.0632 0.2798 0.4742]
L1 regularization loss: 3.45E+00, L2 regularization loss: 1.17E+00
Test scatter: [0.4161 0.0634 0.2832 0.4749], Lowest was [0.3333 0.0603 0.2794 0.445 ]
Median for last 10 epochs: [0.3998 0.0615 0.3083 0.4645], Epochs since improvement 2
 10%|▉         | 49/500 [1:02:35<9:13:38, 73.66s/it]Epoch: 50 done with learning rate 7.60E-03, Train loss: 1.93E+06, Train scatter: [0.5825 0.0665 0.296  0.4643]
L1 regularization loss: 3.60E+00, L2 regularization loss: 1.26E+00
Test scatter: [0.5731 0.0659 0.2962 0.4624], Lowest was [0.3333 0.0603 0.2794 0.445 ]
Median for last 10 epochs: [0.411  0.0615 0.2962 0.4641], Epochs since improvement 4
 10%|█         | 50/500 [1:04:13<10:08:12, 81.09s/it] 10%|█         | 51/500 [1:05:15<9:22:40, 75.19s/it]  10%|█         | 52/500 [1:06:46<9:56:17, 79.86s/it]Epoch: 52 done with learning rate 7.94E-03, Train loss: 3.51E+06, Train scatter: [0.9352 0.1728 0.5441 0.9954]
L1 regularization loss: 1.06E+01, L2 regularization loss: 3.64E+00
Test scatter: [0.9196 0.169  0.5355 0.985 ], Lowest was [0.3333 0.0603 0.2794 0.445 ]
Median for last 10 epochs: [0.4161 0.0634 0.2962 0.4641], Epochs since improvement 6
 11%|█         | 53/500 [1:07:47<9:13:54, 74.35s/it] 11%|█         | 54/500 [1:09:18<9:49:01, 79.24s/it]Epoch: 54 done with learning rate 8.26E-03, Train loss: 3.34E+06, Train scatter: [0.9352 0.1728 0.5441 0.9954]
L1 regularization loss: 1.06E+01, L2 regularization loss: 3.70E+00
Test scatter: [0.9196 0.1689 0.5355 0.985 ], Lowest was [0.3333 0.0603 0.2794 0.445 ]
Median for last 10 epochs: [0.5731 0.0659 0.2962 0.4749], Epochs since improvement 8
 11%|█         | 55/500 [1:10:19<9:08:03, 73.90s/it] 11%|█         | 56/500 [1:11:50<9:44:54, 79.04s/it]Epoch: 56 done with learning rate 8.56E-03, Train loss: 3.28E+06, Train scatter: [0.9352 0.1728 0.5441 0.9954]
L1 regularization loss: 1.06E+01, L2 regularization loss: 3.77E+00
Test scatter: [0.9196 0.1689 0.5355 0.985 ], Lowest was [0.3333 0.0603 0.2794 0.445 ]
Median for last 10 epochs: [0.9196 0.1689 0.5355 0.985 ], Epochs since improvement 10
 11%|█▏        | 57/500 [1:12:52<9:04:53, 73.80s/it] 12%|█▏        | 58/500 [1:14:22<9:40:06, 78.75s/it]Epoch: 58 done with learning rate 8.83E-03, Train loss: 3.24E+06, Train scatter: [0.9352 0.1728 0.5441 0.9954]
L1 regularization loss: 1.06E+01, L2 regularization loss: 3.84E+00
Test scatter: [0.9196 0.1689 0.5355 0.985 ], Lowest was [0.3333 0.0603 0.2794 0.445 ]
Median for last 10 epochs: [0.9196 0.1689 0.5355 0.985 ], Epochs since improvement 12
 12%|█▏        | 59/500 [1:15:23<8:59:58, 73.47s/it]Epoch: 60 done with learning rate 9.08E-03, Train loss: 3.14E+06, Train scatter: [0.9352 0.1724 0.5441 0.9951]
L1 regularization loss: 1.06E+01, L2 regularization loss: 3.95E+00
Test scatter: [0.9196 0.1686 0.5355 0.9848], Lowest was [0.3333 0.0603 0.2794 0.445 ]
Median for last 10 epochs: [0.9196 0.1689 0.5355 0.985 ], Epochs since improvement 14
 12%|█▏        | 60/500 [1:17:03<9:55:55, 81.26s/it] 12%|█▏        | 61/500 [1:18:04<9:11:04, 75.32s/it] 12%|█▏        | 62/500 [1:19:35<9:44:34, 80.08s/it]Epoch: 62 done with learning rate 9.31E-03, Train loss: 2.48E+06, Train scatter: [0.9352 0.1724 0.5441 0.9952]
L1 regularization loss: 1.06E+01, L2 regularization loss: 4.09E+00
Test scatter: [0.9196 0.1686 0.5355 0.9849], Lowest was [0.3333 0.0603 0.2794 0.445 ]
Median for last 10 epochs: [0.9196 0.1689 0.5355 0.985 ], Epochs since improvement 16
 13%|█▎        | 63/500 [1:20:37<9:02:27, 74.48s/it] 13%|█▎        | 64/500 [1:22:09<9:39:06, 79.69s/it]Epoch: 64 done with learning rate 9.50E-03, Train loss: 1.18E+06, Train scatter: [0.9352 0.1727 0.5441 0.9953]
L1 regularization loss: 1.06E+01, L2 regularization loss: 4.26E+00
Test scatter: [0.9196 0.1689 0.5355 0.985 ], Lowest was [0.3333 0.0603 0.2794 0.445 ]
Median for last 10 epochs: [0.9196 0.1689 0.5355 0.985 ], Epochs since improvement 18
 13%|█▎        | 65/500 [1:23:10<8:58:25, 74.27s/it] 13%|█▎        | 66/500 [1:24:42<9:34:46, 79.46s/it]Epoch: 66 done with learning rate 9.66E-03, Train loss: 6.45E+05, Train scatter: [0.9352 0.1728 0.5441 0.9954]
L1 regularization loss: 1.06E+01, L2 regularization loss: 4.35E+00
Test scatter: [0.9196 0.169  0.5355 0.9851], Lowest was [0.3333 0.0603 0.2794 0.445 ]
Median for last 10 epochs: [0.9196 0.1689 0.5355 0.985 ], Epochs since improvement 20
 13%|█▎        | 67/500 [1:25:43<8:54:04, 74.01s/it] 13%|█▎        | 67/500 [1:27:13<9:23:44, 78.12s/it]
Epoch: 68 done with learning rate 9.80E-03, Train loss: 6.29E+05, Train scatter: [0.9352 0.1728 0.5441 0.9953]
L1 regularization loss: 1.06E+01, L2 regularization loss: 4.41E+00
Test scatter: [0.9196 0.169  0.5355 0.985 ], Lowest was [0.3333 0.0603 0.2794 0.445 ]
Median for last 10 epochs: [0.9196 0.1689 0.5355 0.985 ], Epochs since improvement 22
Exited after 68 epochs due to early stopping
5233.76 seconds spent training, 10.468 seconds per epoch. Processed 6653 trees per second
[0.919542   0.16895641 0.5354723  0.9849636 ]
{'epoch_exit': 67, 'scatter_m_star': 0.919542, 'lowest_m_star': 0.33330148, 'last20_m_star': 0.9195834, 'last10_m_star': 0.91957647, 'scatter_v_disk': 0.16895641, 'lowest_v_disk': 0.060277123, 'last20_v_disk': 0.16894075, 'last10_v_disk': 0.168897, 'scatter_m_cold': 0.5354723, 'lowest_m_cold': 0.27939937, 'last20_m_cold': 0.5354859, 'last10_m_cold': 0.5354872, 'scatter_sfr_100': 0.9849636, 'lowest_sfr_100': 0.44498253, 'last20_sfr_100': 0.9850209, 'last10_sfr_100': 0.9849801}
Finished 1/1
Exited training after None epochs
Experiment 5 done: 5 / 6
Exploring ['conv_layers', 'batch_size']
Currently doing [  3 512]
{'experiment': 'GraphMerge', 'group': 'meta_pysr', 'move': False, 'model': 'MetaEdge', 'log': True, 'run_params': {'n_epochs': 500, 'n_trials': 1, 'batch_size': 512, 'val_epoch': 2, 'early_stopping': True, 'patience': 20, 'l1_lambda': 0.0001, 'l2_lambda': 0.0001, 'loss_func': 'GaussNd', 'metrics': 'test_multi_varrho', 'performance_plot': 'multi_base', 'save': True, 'seed': True, 'num_workers': 4}, 'learn_params': {'learning_rate': 0.01, 'schedule': 'onecycle', 'g_up': 1, 'g_down': 0.95, 'warmup': 4, 'period': 5, 'eta_min': 1e-05}, 'hyper_params': {'hidden_channels': 128, 'conv_layers': 3, 'conv_activation': 'relu', 'decode_activation': 'leakyrelu', 'decode_layers': 2, 'layernorm': True, 'agg': 'sum', 'variance': True, 'rho': 0, 'in_channels': 43, 'out_channels': 4}, 'data_params': {'case': 'vlarge_all_4t_z0.0_quantile_raw', 'targets': [0, 1, 2, 3], 'del_feats': [], 'split': 0.8, 'test': 0, 'scale': 0}, 'experiment_name': 'con3bat512'}
/home/cj1223/../../scratch/gpfs/cj1223/GraphResults/results_meta_pysr_120322/MetaEdge_vlarge_all_4t_z0.0_quantile_raw_xiujrn
RelU conv activation
LeakyRelU decode activation
N_params 546504
Made folder for saving model
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:54<7:30:45, 54.20s/it]  0%|          | 2/500 [02:14<9:38:16, 69.67s/it]Epoch: 2 done with learning rate 4.17E-04, Train loss: 1.20E+08, Train scatter: [0.9352 0.1725 0.5441 0.9953]
L1 regularization loss: 2.44E+00, L2 regularization loss: 4.23E-01
Test scatter: [0.9196 0.1679 0.5355 0.985 ], Lowest was [0.9196 0.1679 0.5355 0.985 ]
Median for last 10 epochs: [0.9196 0.1679 0.5355 0.985 ], Epochs since improvement 2
  1%|          | 3/500 [03:08<8:36:34, 62.36s/it]  1%|          | 4/500 [04:29<9:36:32, 69.74s/it]Epoch: 4 done with learning rate 4.67E-04, Train loss: 8.74E+07, Train scatter: [0.9352 0.131  0.5441 0.9955]
L1 regularization loss: 2.46E+00, L2 regularization loss: 4.25E-01
Test scatter: [0.9195 0.1282 0.5355 0.9852], Lowest was [0.9195 0.1282 0.5355 0.985 ]
Median for last 10 epochs: [0.9195 0.1282 0.5355 0.9851], Epochs since improvement 0
  1%|          | 5/500 [05:23<8:47:49, 63.98s/it]  1%|          | 6/500 [06:44<9:34:47, 69.81s/it]Epoch: 6 done with learning rate 5.51E-04, Train loss: 8.31E+07, Train scatter: [0.9348 0.111  0.5441 0.9955]
L1 regularization loss: 2.47E+00, L2 regularization loss: 4.28E-01
Test scatter: [0.9192 0.1093 0.5355 0.9851], Lowest was [0.9192 0.1093 0.5355 0.985 ]
Median for last 10 epochs: [0.9192 0.1093 0.5355 0.9851], Epochs since improvement 0
  1%|▏         | 7/500 [07:37<8:49:57, 64.50s/it]  2%|▏         | 8/500 [08:59<9:32:28, 69.81s/it]Epoch: 8 done with learning rate 6.67E-04, Train loss: 8.04E+07, Train scatter: [0.9326 0.1013 0.544  0.9955]
L1 regularization loss: 2.48E+00, L2 regularization loss: 4.32E-01
Test scatter: [0.9171 0.0993 0.5354 0.9852], Lowest was [0.9171 0.0993 0.5354 0.985 ]
Median for last 10 epochs: [0.9181 0.1043 0.5355 0.9851], Epochs since improvement 0
  2%|▏         | 9/500 [09:52<8:50:04, 64.77s/it]Epoch: 10 done with learning rate 8.15E-04, Train loss: 7.89E+07, Train scatter: [0.798  0.092  0.5439 0.9955]
L1 regularization loss: 2.49E+00, L2 regularization loss: 4.38E-01
Test scatter: [0.788  0.0929 0.5353 0.9851], Lowest was [0.788  0.0929 0.5353 0.985 ]
Median for last 10 epochs: [0.9171 0.0993 0.5354 0.9851], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
  2%|▏         | 10/500 [11:21<9:48:51, 72.10s/it]  2%|▏         | 11/500 [12:15<9:01:52, 66.49s/it]  2%|▏         | 12/500 [13:36<9:37:13, 70.97s/it]Epoch: 12 done with learning rate 9.94E-04, Train loss: 7.79E+07, Train scatter: [0.6994 0.0899 0.5438 0.9954]
L1 regularization loss: 2.50E+00, L2 regularization loss: 4.48E-01
Test scatter: [0.6929 0.0894 0.5353 0.9851], Lowest was [0.6929 0.0894 0.5353 0.985 ]
Median for last 10 epochs: [0.9171 0.0993 0.5354 0.9851], Epochs since improvement 0
  3%|▎         | 13/500 [14:30<8:53:51, 65.77s/it]  3%|▎         | 14/500 [15:51<9:30:40, 70.45s/it]Epoch: 14 done with learning rate 1.20E-03, Train loss: 7.66E+07, Train scatter: [0.5853 0.0855 0.5421 0.9953]
L1 regularization loss: 2.51E+00, L2 regularization loss: 4.57E-01
Test scatter: [0.5719 0.0852 0.5338 0.985 ], Lowest was [0.5719 0.0852 0.5338 0.985 ]
Median for last 10 epochs: [0.788  0.0929 0.5353 0.9851], Epochs since improvement 0
  3%|▎         | 15/500 [16:44<8:48:21, 65.36s/it]  3%|▎         | 16/500 [18:06<9:26:52, 70.27s/it]Epoch: 16 done with learning rate 1.44E-03, Train loss: 7.57E+07, Train scatter: [0.5006 0.0839 0.5341 0.9953]
L1 regularization loss: 2.53E+00, L2 regularization loss: 4.66E-01
Test scatter: [0.5007 0.0832 0.5265 0.985 ], Lowest was [0.5007 0.0832 0.5265 0.985 ]
Median for last 10 epochs: [0.6929 0.0894 0.5353 0.9851], Epochs since improvement 0
  3%|▎         | 17/500 [19:00<8:46:41, 65.43s/it]  4%|▎         | 18/500 [20:22<9:23:56, 70.20s/it]Epoch: 18 done with learning rate 1.70E-03, Train loss: 7.51E+07, Train scatter: [0.4504 0.0816 0.5257 0.9953]
L1 regularization loss: 2.54E+00, L2 regularization loss: 4.71E-01
Test scatter: [0.4451 0.0815 0.518  0.985 ], Lowest was [0.4451 0.0815 0.518  0.985 ]
Median for last 10 epochs: [0.5719 0.0852 0.5338 0.985 ], Epochs since improvement 0
  4%|▍         | 19/500 [21:16<8:44:11, 65.39s/it]Epoch: 20 done with learning rate 1.99E-03, Train loss: 7.47E+07, Train scatter: [0.7498 0.0966 0.5147 0.9954]
L1 regularization loss: 2.56E+00, L2 regularization loss: 4.79E-01
Test scatter: [0.7397 0.0948 0.5077 0.985 ], Lowest was [0.4451 0.0815 0.5077 0.985 ]
Median for last 10 epochs: [0.5719 0.0852 0.5265 0.985 ], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
  4%|▍         | 20/500 [22:45<9:39:55, 72.49s/it]  4%|▍         | 21/500 [23:38<8:53:28, 66.82s/it]  4%|▍         | 22/500 [25:01<9:29:20, 71.47s/it]Epoch: 22 done with learning rate 2.30E-03, Train loss: 7.35E+07, Train scatter: [0.5899 0.0993 0.4122 0.9953]
L1 regularization loss: 2.57E+00, L2 regularization loss: 4.88E-01
Test scatter: [0.5945 0.1003 0.4145 0.985 ], Lowest was [0.4451 0.0815 0.4145 0.985 ]
Median for last 10 epochs: [0.5719 0.0852 0.518  0.985 ], Epochs since improvement 0
  5%|▍         | 23/500 [25:54<8:45:30, 66.10s/it]  5%|▍         | 24/500 [27:16<9:21:03, 70.72s/it]Epoch: 24 done with learning rate 2.63E-03, Train loss: 7.27E+07, Train scatter: [0.5682 0.0922 0.3791 0.9954]
L1 regularization loss: 2.59E+00, L2 regularization loss: 4.93E-01
Test scatter: [0.5801 0.0923 0.3749 0.985 ], Lowest was [0.4451 0.0815 0.3749 0.985 ]
Median for last 10 epochs: [0.5801 0.0923 0.5077 0.985 ], Epochs since improvement 0
  5%|▌         | 25/500 [28:10<8:40:02, 65.69s/it]  5%|▌         | 26/500 [29:32<9:17:12, 70.53s/it]Epoch: 26 done with learning rate 2.98E-03, Train loss: 7.16E+07, Train scatter: [0.6168 0.0874 0.3628 0.9953]
L1 regularization loss: 2.60E+00, L2 regularization loss: 4.98E-01
Test scatter: [0.6197 0.088  0.3607 0.985 ], Lowest was [0.4451 0.0815 0.3607 0.985 ]
Median for last 10 epochs: [0.5945 0.0923 0.4145 0.985 ], Epochs since improvement 0
  5%|▌         | 27/500 [30:25<8:36:38, 65.54s/it]  6%|▌         | 28/500 [31:47<9:13:14, 70.33s/it]Epoch: 28 done with learning rate 3.34E-03, Train loss: 7.15E+07, Train scatter: [0.5231 0.0866 0.337  0.9953]
L1 regularization loss: 2.61E+00, L2 regularization loss: 5.04E-01
Test scatter: [0.5297 0.087  0.3379 0.985 ], Lowest was [0.4451 0.0815 0.3379 0.985 ]
Median for last 10 epochs: [0.5945 0.0923 0.3749 0.985 ], Epochs since improvement 0
  6%|▌         | 29/500 [32:41<8:32:43, 65.31s/it]Epoch: 30 done with learning rate 3.72E-03, Train loss: 7.09E+07, Train scatter: [0.4938 0.0809 0.3348 0.9954]
L1 regularization loss: 2.63E+00, L2 regularization loss: 5.09E-01
Test scatter: [0.5046 0.0806 0.3339 0.985 ], Lowest was [0.4451 0.0806 0.3339 0.985 ]
Median for last 10 epochs: [0.5801 0.088  0.3607 0.985 ], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
  6%|▌         | 30/500 [34:09<9:26:20, 72.30s/it]  6%|▌         | 31/500 [35:03<8:42:35, 66.86s/it]  6%|▋         | 32/500 [36:25<9:15:35, 71.23s/it]Epoch: 32 done with learning rate 4.10E-03, Train loss: 7.06E+07, Train scatter: [0.4692 0.0785 0.3182 0.9954]
L1 regularization loss: 2.65E+00, L2 regularization loss: 5.17E-01
Test scatter: [0.4739 0.0786 0.3194 0.985 ], Lowest was [0.4451 0.0786 0.3194 0.985 ]
Median for last 10 epochs: [0.5297 0.087  0.3379 0.985 ], Epochs since improvement 0
  7%|▋         | 33/500 [37:18<8:33:35, 65.99s/it]  7%|▋         | 34/500 [38:40<9:07:43, 70.52s/it]Epoch: 34 done with learning rate 4.50E-03, Train loss: 7.02E+07, Train scatter: [0.6008 0.0786 0.3946 0.9954]
L1 regularization loss: 2.66E+00, L2 regularization loss: 5.24E-01
Test scatter: [0.6135 0.0775 0.3989 0.9851], Lowest was [0.4451 0.0775 0.3194 0.985 ]
Median for last 10 epochs: [0.5297 0.0806 0.3379 0.985 ], Epochs since improvement 0
  7%|▋         | 35/500 [39:33<8:27:15, 65.45s/it]  7%|▋         | 36/500 [40:55<9:03:08, 70.23s/it]Epoch: 36 done with learning rate 4.90E-03, Train loss: 6.96E+07, Train scatter: [0.5439 0.0784 0.3805 0.9954]
L1 regularization loss: 2.69E+00, L2 regularization loss: 5.33E-01
Test scatter: [0.5514 0.0776 0.3751 0.9851], Lowest was [0.4451 0.0775 0.3194 0.985 ]
Median for last 10 epochs: [0.5297 0.0786 0.3379 0.985 ], Epochs since improvement 2
  7%|▋         | 37/500 [41:48<8:23:48, 65.29s/it]  8%|▊         | 38/500 [43:10<9:00:07, 70.15s/it]Epoch: 38 done with learning rate 5.30E-03, Train loss: 6.12E+07, Train scatter: [0.4773 0.0774 0.3017 0.9953]
L1 regularization loss: 2.73E+00, L2 regularization loss: 5.50E-01
Test scatter: [0.4798 0.0768 0.3042 0.985 ], Lowest was [0.4451 0.0768 0.3042 0.985 ]
Median for last 10 epochs: [0.5046 0.0776 0.3339 0.985 ], Epochs since improvement 0
  8%|▊         | 39/500 [44:03<8:20:42, 65.17s/it]Epoch: 40 done with learning rate 5.70E-03, Train loss: 1.67E+07, Train scatter: [0.508  0.0782 0.454  0.9951]
L1 regularization loss: 2.74E+00, L2 regularization loss: 5.62E-01
Test scatter: [0.5026 0.0771 0.4577 0.9848], Lowest was [0.4451 0.0768 0.3042 0.9848]
Median for last 10 epochs: [0.5026 0.0775 0.3751 0.985 ], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
  8%|▊         | 40/500 [45:33<9:15:28, 72.45s/it]  8%|▊         | 41/500 [46:26<8:31:06, 66.81s/it]  8%|▊         | 42/500 [47:49<9:06:30, 71.60s/it]Epoch: 42 done with learning rate 6.10E-03, Train loss: 6.88E+07, Train scatter: [0.9319 0.1792 0.5441 0.9961]
L1 regularization loss: 3.52E+00, L2 regularization loss: 8.22E-01
Test scatter: [0.9166 0.1761 0.5355 0.986 ], Lowest was [0.4451 0.0768 0.3042 0.9848]
Median for last 10 epochs: [0.5514 0.0775 0.3989 0.9851], Epochs since improvement 2
  9%|▊         | 43/500 [48:43<8:24:46, 66.27s/it]  9%|▉         | 44/500 [50:04<8:58:04, 70.80s/it]Epoch: 44 done with learning rate 6.49E-03, Train loss: 1.62E+07, Train scatter: [0.867  0.1264 0.5442 0.7793]
L1 regularization loss: 3.67E+00, L2 regularization loss: 9.35E-01
Test scatter: [0.8556 0.1298 0.5356 0.7799], Lowest was [0.4451 0.0768 0.3042 0.7799]
Median for last 10 epochs: [0.5514 0.0776 0.4577 0.985 ], Epochs since improvement 0
  9%|▉         | 45/500 [50:58<8:17:40, 65.63s/it]  9%|▉         | 46/500 [52:19<8:52:31, 70.38s/it]Epoch: 46 done with learning rate 6.87E-03, Train loss: 9.52E+06, Train scatter: [0.7363 0.0977 0.5442 0.6381]
L1 regularization loss: 3.72E+00, L2 regularization loss: 1.01E+00
Test scatter: [0.7424 0.0979 0.5356 0.6324], Lowest was [0.4451 0.0768 0.3042 0.6324]
Median for last 10 epochs: [0.7424 0.0979 0.5355 0.9848], Epochs since improvement 0
  9%|▉         | 47/500 [53:13<8:13:31, 65.37s/it] 10%|▉         | 48/500 [54:35<8:50:19, 70.40s/it]Epoch: 48 done with learning rate 7.24E-03, Train loss: 9.71E+06, Train scatter: [0.7029 0.1036 0.5442 0.6814]
L1 regularization loss: 3.75E+00, L2 regularization loss: 1.05E+00
Test scatter: [0.7299 0.1042 0.5356 0.6814], Lowest was [0.4451 0.0768 0.3042 0.6324]
Median for last 10 epochs: [0.7424 0.1042 0.5356 0.7799], Epochs since improvement 2
 10%|▉         | 49/500 [55:29<8:11:30, 65.39s/it]Epoch: 50 done with learning rate 7.60E-03, Train loss: 7.72E+06, Train scatter: [0.6502 0.0955 0.5442 0.6614]
L1 regularization loss: 3.77E+00, L2 regularization loss: 1.08E+00
Test scatter: [0.6856 0.0951 0.5356 0.6579], Lowest was [0.4451 0.0768 0.3042 0.6324]
Median for last 10 epochs: [0.7424 0.1042 0.5356 0.6814], Epochs since improvement 4
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 10%|█         | 50/500 [56:59<9:04:46, 72.64s/it] 10%|█         | 51/500 [57:52<8:20:59, 66.95s/it] 10%|█         | 52/500 [59:13<8:51:30, 71.18s/it]Epoch: 52 done with learning rate 7.94E-03, Train loss: 9.28E+06, Train scatter: [0.8544 0.1271 0.5441 0.9801]
L1 regularization loss: 3.79E+00, L2 regularization loss: 1.10E+00
Test scatter: [0.8427 0.1251 0.5356 0.9698], Lowest was [0.4451 0.0768 0.3042 0.6324]
Median for last 10 epochs: [0.7424 0.1042 0.5356 0.6814], Epochs since improvement 6
 11%|█         | 53/500 [1:00:07<8:11:14, 65.94s/it] 11%|█         | 54/500 [1:01:29<8:45:10, 70.65s/it]Epoch: 54 done with learning rate 8.26E-03, Train loss: 6.46E+06, Train scatter: [0.615  0.0876 0.5441 0.6052]
L1 regularization loss: 3.81E+00, L2 regularization loss: 1.14E+00
Test scatter: [0.64   0.0878 0.5355 0.6049], Lowest was [0.4451 0.0768 0.3042 0.6049]
Median for last 10 epochs: [0.7299 0.0979 0.5356 0.6579], Epochs since improvement 0
 11%|█         | 55/500 [1:02:22<8:05:59, 65.53s/it] 11%|█         | 56/500 [1:03:44<8:40:30, 70.34s/it]Epoch: 56 done with learning rate 8.56E-03, Train loss: 5.86E+06, Train scatter: [0.692  0.1165 0.5439 0.6625]
L1 regularization loss: 3.81E+00, L2 regularization loss: 1.16E+00
Test scatter: [0.7106 0.1192 0.5353 0.6621], Lowest was [0.4451 0.0768 0.3042 0.6049]
Median for last 10 epochs: [0.7106 0.1042 0.5356 0.6621], Epochs since improvement 2
 11%|█▏        | 57/500 [1:04:38<8:02:38, 65.37s/it] 12%|█▏        | 58/500 [1:05:59<8:36:20, 70.09s/it]Epoch: 58 done with learning rate 8.83E-03, Train loss: 5.64E+06, Train scatter: [0.5849 0.0808 0.5437 0.5469]
L1 regularization loss: 3.83E+00, L2 regularization loss: 1.17E+00
Test scatter: [0.6093 0.0804 0.5351 0.5446], Lowest was [0.4451 0.0768 0.3042 0.5446]
Median for last 10 epochs: [0.6856 0.0951 0.5355 0.6579], Epochs since improvement 0
 12%|█▏        | 59/500 [1:06:52<7:59:08, 65.19s/it]Epoch: 60 done with learning rate 9.08E-03, Train loss: 5.42E+06, Train scatter: [0.5408 0.0807 0.5434 0.5527]
L1 regularization loss: 3.84E+00, L2 regularization loss: 1.20E+00
Test scatter: [0.5711 0.0811 0.5348 0.554 ], Lowest was [0.4451 0.0768 0.3042 0.5446]
Median for last 10 epochs: [0.64   0.0878 0.5353 0.6049], Epochs since improvement 2
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 12%|█▏        | 60/500 [1:08:22<8:51:07, 72.43s/it] 12%|█▏        | 61/500 [1:09:15<8:08:57, 66.83s/it] 12%|█▏        | 62/500 [1:10:38<8:41:38, 71.46s/it]Epoch: 62 done with learning rate 9.31E-03, Train loss: 5.60E+06, Train scatter: [0.5229 0.0808 0.5427 0.5377]
L1 regularization loss: 3.85E+00, L2 regularization loss: 1.21E+00
Test scatter: [0.5497 0.0798 0.5342 0.5355], Lowest was [0.4451 0.0768 0.3042 0.5355]
Median for last 10 epochs: [0.6093 0.0811 0.5351 0.554 ], Epochs since improvement 0
 13%|█▎        | 63/500 [1:11:32<8:01:57, 66.17s/it] 13%|█▎        | 64/500 [1:12:53<8:33:36, 70.68s/it]Epoch: 64 done with learning rate 9.50E-03, Train loss: 5.01E+06, Train scatter: [0.5527 0.0816 0.5096 0.5388]
L1 regularization loss: 3.86E+00, L2 regularization loss: 1.22E+00
Test scatter: [0.5762 0.0816 0.5022 0.5348], Lowest was [0.4451 0.0768 0.3042 0.5348]
Median for last 10 epochs: [0.5762 0.0811 0.5348 0.5446], Epochs since improvement 0
 13%|█▎        | 65/500 [1:13:47<7:55:48, 65.63s/it] 13%|█▎        | 66/500 [1:15:08<8:28:45, 70.33s/it]Epoch: 66 done with learning rate 9.66E-03, Train loss: 4.70E+06, Train scatter: [0.3879 0.0773 0.4651 0.5276]
L1 regularization loss: 3.89E+00, L2 regularization loss: 1.25E+00
Test scatter: [0.4071 0.0767 0.4592 0.5275], Lowest was [0.4071 0.0767 0.3042 0.5275]
Median for last 10 epochs: [0.5711 0.0804 0.5342 0.5355], Epochs since improvement 0
 13%|█▎        | 67/500 [1:16:02<7:51:17, 65.31s/it] 14%|█▎        | 68/500 [1:17:24<8:26:27, 70.34s/it]Epoch: 68 done with learning rate 9.80E-03, Train loss: 5.08E+06, Train scatter: [0.4739 0.0762 0.5012 0.5167]
L1 regularization loss: 3.90E+00, L2 regularization loss: 1.27E+00
Test scatter: [0.4865 0.0754 0.4976 0.5131], Lowest was [0.4071 0.0754 0.3042 0.5131]
Median for last 10 epochs: [0.5497 0.0798 0.5022 0.5348], Epochs since improvement 0
 14%|█▍        | 69/500 [1:18:18<7:49:54, 65.42s/it]Epoch: 70 done with learning rate 9.90E-03, Train loss: 8.02E+06, Train scatter: [0.5685 0.109  0.544  0.6663]
L1 regularization loss: 4.00E+00, L2 regularization loss: 1.34E+00
Test scatter: [0.5957 0.108  0.5354 0.6725], Lowest was [0.4071 0.0754 0.3042 0.5131]
Median for last 10 epochs: [0.5497 0.0798 0.5022 0.5348], Epochs since improvement 2
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 14%|█▍        | 70/500 [1:19:46<8:38:05, 72.29s/it] 14%|█▍        | 71/500 [1:20:40<7:57:09, 66.73s/it] 14%|█▍        | 72/500 [1:22:02<8:28:47, 71.33s/it]Epoch: 72 done with learning rate 9.96E-03, Train loss: 5.80E+06, Train scatter: [0.4927 0.0995 0.544  0.5647]
L1 regularization loss: 4.01E+00, L2 regularization loss: 1.36E+00
Test scatter: [0.4882 0.0984 0.5354 0.5593], Lowest was [0.4071 0.0754 0.3042 0.5131]
Median for last 10 epochs: [0.4882 0.0816 0.5022 0.5348], Epochs since improvement 4
 15%|█▍        | 73/500 [1:22:55<7:49:40, 66.00s/it] 15%|█▍        | 74/500 [1:24:17<8:22:52, 70.83s/it]Epoch: 74 done with learning rate 1.00E-02, Train loss: 5.19E+06, Train scatter: [0.7089 0.0887 0.5437 0.5503]
L1 regularization loss: 4.03E+00, L2 regularization loss: 1.39E+00
Test scatter: [0.71   0.0882 0.5351 0.5447], Lowest was [0.4071 0.0754 0.3042 0.5131]
Median for last 10 epochs: [0.4882 0.0882 0.5351 0.5447], Epochs since improvement 6
 15%|█▌        | 75/500 [1:25:11<7:45:11, 65.67s/it] 15%|█▌        | 76/500 [1:26:32<8:16:26, 70.25s/it]Epoch: 76 done with learning rate 1.00E-02, Train loss: 4.85E+06, Train scatter: [0.4576 0.0858 0.5428 0.5297]
L1 regularization loss: 4.04E+00, L2 regularization loss: 1.42E+00
Test scatter: [0.4806 0.0862 0.5343 0.5228], Lowest was [0.4071 0.0754 0.3042 0.5131]
Median for last 10 epochs: [0.4882 0.0882 0.5351 0.5447], Epochs since improvement 8
 15%|█▌        | 77/500 [1:27:26<7:40:06, 65.26s/it] 16%|█▌        | 78/500 [1:28:47<8:13:32, 70.17s/it]Epoch: 78 done with learning rate 1.00E-02, Train loss: 4.71E+06, Train scatter: [0.4127 0.0826 0.5416 0.5819]
L1 regularization loss: 4.05E+00, L2 regularization loss: 1.46E+00
Test scatter: [0.4163 0.0828 0.5332 0.5813], Lowest was [0.4071 0.0754 0.3042 0.5131]
Median for last 10 epochs: [0.4882 0.0882 0.5351 0.5593], Epochs since improvement 10
 16%|█▌        | 79/500 [1:29:41<7:38:06, 65.29s/it]Epoch: 80 done with learning rate 1.00E-02, Train loss: 4.33E+06, Train scatter: [0.4686 0.0823 0.5397 0.5186]
L1 regularization loss: 4.06E+00, L2 regularization loss: 1.48E+00
Test scatter: [0.4518 0.0828 0.5314 0.5171], Lowest was [0.4071 0.0754 0.3042 0.5131]
Median for last 10 epochs: [0.4806 0.0862 0.5343 0.5447], Epochs since improvement 12
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 16%|█▌        | 80/500 [1:31:10<8:26:22, 72.34s/it] 16%|█▌        | 81/500 [1:32:03<7:46:01, 66.73s/it] 16%|█▋        | 82/500 [1:33:26<8:17:02, 71.34s/it]Epoch: 82 done with learning rate 9.99E-03, Train loss: 4.18E+06, Train scatter: [0.4139 0.0747 0.5367 0.5038]
L1 regularization loss: 4.07E+00, L2 regularization loss: 1.51E+00
Test scatter: [0.4065 0.0744 0.5287 0.5   ], Lowest was [0.4065 0.0744 0.3042 0.5   ]
Median for last 10 epochs: [0.4518 0.0828 0.5332 0.5228], Epochs since improvement 0
 17%|█▋        | 83/500 [1:34:19<7:38:50, 66.02s/it] 17%|█▋        | 84/500 [1:35:40<8:08:36, 70.47s/it]Epoch: 84 done with learning rate 9.99E-03, Train loss: 3.89E+06, Train scatter: [0.4033 0.0764 0.5218 0.6005]
L1 regularization loss: 4.07E+00, L2 regularization loss: 1.54E+00
Test scatter: [0.4128 0.0763 0.5149 0.6011], Lowest was [0.4065 0.0744 0.3042 0.5   ]
Median for last 10 epochs: [0.4163 0.0828 0.5314 0.5228], Epochs since improvement 2
 17%|█▋        | 85/500 [1:36:34<7:32:28, 65.42s/it] 17%|█▋        | 86/500 [1:37:55<8:05:07, 70.31s/it]Epoch: 86 done with learning rate 9.98E-03, Train loss: 3.55E+06, Train scatter: [0.2818 0.0738 0.4885 0.518 ]
L1 regularization loss: 4.08E+00, L2 regularization loss: 1.56E+00
Test scatter: [0.3007 0.0739 0.4827 0.5152], Lowest was [0.3007 0.0739 0.3042 0.5   ]
Median for last 10 epochs: [0.4128 0.0763 0.5287 0.5171], Epochs since improvement 0
 17%|█▋        | 87/500 [1:38:49<7:29:44, 65.34s/it] 18%|█▊        | 88/500 [1:40:10<8:00:29, 69.97s/it]Epoch: 88 done with learning rate 9.98E-03, Train loss: 3.09E+06, Train scatter: [0.2751 0.0688 0.4417 0.4941]
L1 regularization loss: 4.09E+00, L2 regularization loss: 1.58E+00
Test scatter: [0.2714 0.0685 0.4401 0.4894], Lowest was [0.2714 0.0685 0.3042 0.4894]
Median for last 10 epochs: [0.4065 0.0744 0.5149 0.5152], Epochs since improvement 0
 18%|█▊        | 89/500 [1:41:04<7:25:38, 65.06s/it]Epoch: 90 done with learning rate 9.97E-03, Train loss: 3.06E+06, Train scatter: [0.369  0.0705 0.388  0.4948]
L1 regularization loss: 4.10E+00, L2 regularization loss: 1.60E+00
Test scatter: [0.3498 0.0704 0.3871 0.4907], Lowest was [0.2714 0.0685 0.3042 0.4894]
Median for last 10 epochs: [0.3498 0.0739 0.4827 0.5   ], Epochs since improvement 2
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 18%|█▊        | 90/500 [1:42:32<8:12:49, 72.12s/it] 18%|█▊        | 91/500 [1:43:26<7:34:05, 66.62s/it] 18%|█▊        | 92/500 [1:44:48<8:04:37, 71.27s/it]Epoch: 92 done with learning rate 9.96E-03, Train loss: 2.98E+06, Train scatter: [0.2672 0.0662 0.4612 0.4957]
L1 regularization loss: 4.12E+00, L2 regularization loss: 1.62E+00
Test scatter: [0.2731 0.0658 0.4649 0.4907], Lowest was [0.2714 0.0658 0.3042 0.4894]
Median for last 10 epochs: [0.3007 0.0704 0.4649 0.4907], Epochs since improvement 0
 19%|█▊        | 93/500 [1:45:42<7:27:22, 65.95s/it] 19%|█▉        | 94/500 [1:47:03<7:57:01, 70.50s/it]Epoch: 94 done with learning rate 9.95E-03, Train loss: 2.87E+06, Train scatter: [0.2556 0.0638 0.3522 0.4927]
L1 regularization loss: 4.12E+00, L2 regularization loss: 1.63E+00
Test scatter: [0.2578 0.0638 0.3543 0.4887], Lowest was [0.2578 0.0638 0.3042 0.4887]
Median for last 10 epochs: [0.2731 0.0685 0.4401 0.4907], Epochs since improvement 0
 19%|█▉        | 95/500 [1:47:56<7:21:59, 65.48s/it] 19%|█▉        | 96/500 [1:49:18<7:53:29, 70.32s/it]Epoch: 96 done with learning rate 9.94E-03, Train loss: 2.47E+06, Train scatter: [0.3039 0.0644 0.3914 0.4997]
L1 regularization loss: 4.13E+00, L2 regularization loss: 1.65E+00
Test scatter: [0.2983 0.0647 0.3947 0.4981], Lowest was [0.2578 0.0638 0.3042 0.4887]
Median for last 10 epochs: [0.2731 0.0658 0.3947 0.4907], Epochs since improvement 2
 19%|█▉        | 97/500 [1:50:12<7:18:28, 65.28s/it] 20%|█▉        | 98/500 [1:51:33<7:49:46, 70.11s/it]Epoch: 98 done with learning rate 9.93E-03, Train loss: 2.41E+06, Train scatter: [0.2678 0.0626 0.3341 0.4915]
L1 regularization loss: 4.15E+00, L2 regularization loss: 1.67E+00
Test scatter: [0.2664 0.0625 0.3383 0.4853], Lowest was [0.2578 0.0625 0.3042 0.4853]
Median for last 10 epochs: [0.2731 0.0647 0.3871 0.4907], Epochs since improvement 0
 20%|█▉        | 99/500 [1:52:27<7:15:33, 65.17s/it]Epoch: 100 done with learning rate 9.91E-03, Train loss: 2.37E+06, Train scatter: [0.5875 0.0651 0.3651 0.5076]
L1 regularization loss: 4.16E+00, L2 regularization loss: 1.69E+00
Test scatter: [0.576  0.0643 0.3692 0.503 ], Lowest was [0.2578 0.0625 0.3042 0.4853]
Median for last 10 epochs: [0.2731 0.0643 0.3692 0.4907], Epochs since improvement 2
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 20%|██        | 100/500 [1:53:56<8:02:30, 72.38s/it] 20%|██        | 101/500 [1:54:50<7:24:15, 66.81s/it] 20%|██        | 102/500 [1:56:11<7:52:59, 71.30s/it]Epoch: 102 done with learning rate 9.90E-03, Train loss: 2.24E+06, Train scatter: [0.263  0.0807 0.3382 0.4968]
L1 regularization loss: 4.18E+00, L2 regularization loss: 1.72E+00
Test scatter: [0.2646 0.0791 0.3397 0.4914], Lowest was [0.2578 0.0625 0.3042 0.4853]
Median for last 10 epochs: [0.2664 0.0643 0.3543 0.4914], Epochs since improvement 4
 21%|██        | 103/500 [1:57:05<7:15:43, 65.85s/it] 21%|██        | 104/500 [1:58:25<7:42:38, 70.10s/it]Epoch: 104 done with learning rate 9.89E-03, Train loss: 2.11E+06, Train scatter: [0.2374 0.0601 0.3453 0.4777]
L1 regularization loss: 4.18E+00, L2 regularization loss: 1.74E+00
Test scatter: [0.2384 0.0604 0.3526 0.4739], Lowest was [0.2384 0.0604 0.3042 0.4739]
Median for last 10 epochs: [0.2664 0.0643 0.3526 0.4914], Epochs since improvement 0
 21%|██        | 105/500 [1:59:18<7:07:59, 65.01s/it] 21%|██        | 106/500 [2:00:38<7:36:07, 69.46s/it]Epoch: 106 done with learning rate 9.87E-03, Train loss: 2.08E+06, Train scatter: [0.2459 0.0627 0.3232 0.478 ]
L1 regularization loss: 4.19E+00, L2 regularization loss: 1.76E+00
Test scatter: [0.244  0.063  0.3315 0.477 ], Lowest was [0.2384 0.0604 0.3042 0.4739]
Median for last 10 epochs: [0.2646 0.063  0.3397 0.4853], Epochs since improvement 2
 21%|██▏       | 107/500 [2:01:31<7:02:47, 64.55s/it] 22%|██▏       | 108/500 [2:02:52<7:34:29, 69.57s/it]Epoch: 108 done with learning rate 9.85E-03, Train loss: 2.10E+06, Train scatter: [0.253  0.0626 0.3655 0.4928]
L1 regularization loss: 4.21E+00, L2 regularization loss: 1.78E+00
Test scatter: [0.2457 0.0633 0.371  0.4924], Lowest was [0.2384 0.0604 0.3042 0.4739]
Median for last 10 epochs: [0.2457 0.0633 0.3526 0.4914], Epochs since improvement 4
 22%|██▏       | 109/500 [2:03:45<7:01:10, 64.63s/it]Epoch: 110 done with learning rate 9.83E-03, Train loss: 1.83E+06, Train scatter: [0.289  0.0725 0.3418 0.5067]
L1 regularization loss: 4.21E+00, L2 regularization loss: 1.80E+00
Test scatter: [0.2861 0.0716 0.3517 0.5077], Lowest was [0.2384 0.0604 0.3042 0.4739]
Median for last 10 epochs: [0.2457 0.0633 0.3517 0.4914], Epochs since improvement 6
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 22%|██▏       | 110/500 [2:05:13<7:45:35, 71.63s/it] 22%|██▏       | 111/500 [2:06:06<7:08:41, 66.12s/it] 22%|██▏       | 112/500 [2:07:27<7:35:45, 70.48s/it]Epoch: 112 done with learning rate 9.81E-03, Train loss: 1.77E+06, Train scatter: [0.22   0.0589 0.3139 0.4657]
L1 regularization loss: 4.23E+00, L2 regularization loss: 1.84E+00
Test scatter: [0.2212 0.059  0.3152 0.4631], Lowest was [0.2212 0.059  0.3042 0.4631]
Median for last 10 epochs: [0.244  0.063  0.3517 0.477 ], Epochs since improvement 0
 23%|██▎       | 113/500 [2:08:20<7:01:18, 65.32s/it] 23%|██▎       | 114/500 [2:09:41<7:29:37, 69.89s/it]Epoch: 114 done with learning rate 9.79E-03, Train loss: 1.66E+06, Train scatter: [0.2553 0.0569 0.313  0.4576]
L1 regularization loss: 4.24E+00, L2 regularization loss: 1.86E+00
Test scatter: [0.2523 0.0578 0.3246 0.4591], Lowest was [0.2212 0.0578 0.3042 0.4591]
Median for last 10 epochs: [0.2457 0.063  0.3315 0.477 ], Epochs since improvement 0
 23%|██▎       | 115/500 [2:10:34<6:56:09, 64.85s/it] 23%|██▎       | 116/500 [2:11:53<7:23:14, 69.26s/it]Epoch: 116 done with learning rate 9.77E-03, Train loss: 1.63E+06, Train scatter: [0.2508 0.0564 0.3205 0.4543]
L1 regularization loss: 4.24E+00, L2 regularization loss: 1.89E+00
Test scatter: [0.2505 0.0568 0.3304 0.4551], Lowest was [0.2212 0.0568 0.3042 0.4551]
Median for last 10 epochs: [0.2505 0.059  0.3304 0.4631], Epochs since improvement 0
 23%|██▎       | 117/500 [2:12:47<6:51:29, 64.46s/it] 24%|██▎       | 118/500 [2:14:07<7:20:31, 69.19s/it]Epoch: 118 done with learning rate 9.75E-03, Train loss: 1.41E+06, Train scatter: [0.1986 0.0552 0.3142 0.446 ]
L1 regularization loss: 4.25E+00, L2 regularization loss: 1.91E+00
Test scatter: [0.2029 0.0561 0.3154 0.4486], Lowest was [0.2029 0.0561 0.3042 0.4486]
Median for last 10 epochs: [0.2505 0.0578 0.3246 0.4591], Epochs since improvement 0
 24%|██▍       | 119/500 [2:15:00<6:48:47, 64.38s/it]Epoch: 120 done with learning rate 9.73E-03, Train loss: 1.39E+06, Train scatter: [0.3695 0.0622 0.3123 0.4712]
L1 regularization loss: 4.26E+00, L2 regularization loss: 1.93E+00
Test scatter: [0.3553 0.0613 0.3198 0.4606], Lowest was [0.2029 0.0561 0.3042 0.4486]
Median for last 10 epochs: [0.2505 0.0578 0.3198 0.4591], Epochs since improvement 2
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 24%|██▍       | 120/500 [2:16:27<7:30:03, 71.06s/it] 24%|██▍       | 121/500 [2:17:20<6:55:10, 65.73s/it] 24%|██▍       | 122/500 [2:18:41<7:23:10, 70.35s/it]Epoch: 122 done with learning rate 9.70E-03, Train loss: 1.34E+06, Train scatter: [0.2408 0.0565 0.2937 0.4464]
L1 regularization loss: 4.27E+00, L2 regularization loss: 1.95E+00
Test scatter: [0.2352 0.0564 0.2969 0.4466], Lowest was [0.2029 0.0561 0.2969 0.4466]
Median for last 10 epochs: [0.2505 0.0568 0.3198 0.4551], Epochs since improvement 0
 25%|██▍       | 123/500 [2:19:34<6:49:34, 65.18s/it] 25%|██▍       | 124/500 [2:20:54<7:16:06, 69.59s/it]Epoch: 124 done with learning rate 9.68E-03, Train loss: 1.33E+06, Train scatter: [0.2019 0.0541 0.3138 0.445 ]
L1 regularization loss: 4.28E+00, L2 regularization loss: 1.98E+00
Test scatter: [0.201  0.0539 0.3166 0.4432], Lowest was [0.201  0.0539 0.2969 0.4432]
Median for last 10 epochs: [0.2352 0.0564 0.3166 0.4486], Epochs since improvement 0
 25%|██▌       | 125/500 [2:21:47<6:44:04, 64.65s/it] 25%|██▌       | 126/500 [2:23:07<7:12:14, 69.34s/it]Epoch: 126 done with learning rate 9.65E-03, Train loss: 1.26E+06, Train scatter: [0.2451 0.0553 0.2992 0.4495]
L1 regularization loss: 4.29E+00, L2 regularization loss: 2.01E+00
Test scatter: [0.244  0.0549 0.2997 0.448 ], Lowest was [0.201  0.0539 0.2969 0.4432]
Median for last 10 epochs: [0.2352 0.0561 0.3154 0.448 ], Epochs since improvement 2
 25%|██▌       | 127/500 [2:24:01<6:40:59, 64.50s/it] 26%|██▌       | 128/500 [2:25:21<7:08:50, 69.17s/it]Epoch: 128 done with learning rate 9.62E-03, Train loss: 1.25E+06, Train scatter: [0.2187 0.0526 0.298  0.4319]
L1 regularization loss: 4.30E+00, L2 regularization loss: 2.04E+00
Test scatter: [0.2232 0.0524 0.3012 0.4311], Lowest was [0.201  0.0524 0.2969 0.4311]
Median for last 10 epochs: [0.2352 0.0549 0.3012 0.4466], Epochs since improvement 0
 26%|██▌       | 129/500 [2:26:14<6:37:58, 64.36s/it]Epoch: 130 done with learning rate 9.59E-03, Train loss: 1.24E+06, Train scatter: [0.2352 0.0524 0.2889 0.4351]
L1 regularization loss: 4.32E+00, L2 regularization loss: 2.08E+00
Test scatter: [0.229  0.0522 0.2902 0.4309], Lowest was [0.201  0.0522 0.2902 0.4309]
Median for last 10 epochs: [0.229  0.0539 0.2997 0.4432], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 26%|██▌       | 130/500 [2:27:44<7:23:58, 72.00s/it] 26%|██▌       | 131/500 [2:28:37<6:47:59, 66.34s/it] 26%|██▋       | 132/500 [2:29:58<7:13:59, 70.76s/it]Epoch: 132 done with learning rate 9.56E-03, Train loss: 1.13E+06, Train scatter: [0.2335 0.0588 0.3318 0.4762]
L1 regularization loss: 4.32E+00, L2 regularization loss: 2.11E+00
Test scatter: [0.2323 0.0582 0.3328 0.472 ], Lowest was [0.201  0.0522 0.2902 0.4309]
Median for last 10 epochs: [0.229  0.0539 0.3012 0.4432], Epochs since improvement 2
 27%|██▋       | 133/500 [2:30:51<6:40:48, 65.53s/it] 27%|██▋       | 134/500 [2:32:12<7:08:27, 70.24s/it]Epoch: 134 done with learning rate 9.53E-03, Train loss: 1.05E+06, Train scatter: [0.1768 0.0528 0.283  0.4269]
L1 regularization loss: 4.33E+00, L2 regularization loss: 2.14E+00
Test scatter: [0.1757 0.0526 0.2829 0.4231], Lowest was [0.1757 0.0522 0.2829 0.4231]
Median for last 10 epochs: [0.229  0.0526 0.2997 0.4311], Epochs since improvement 0
 27%|██▋       | 135/500 [2:33:06<6:36:29, 65.18s/it] 27%|██▋       | 136/500 [2:34:26<7:03:03, 69.74s/it]Epoch: 136 done with learning rate 9.50E-03, Train loss: 1.04E+06, Train scatter: [0.2149 0.0535 0.2816 0.442 ]
L1 regularization loss: 4.33E+00, L2 regularization loss: 2.18E+00
Test scatter: [0.2178 0.0533 0.284  0.4457], Lowest was [0.1757 0.0522 0.2829 0.4231]
Median for last 10 epochs: [0.2232 0.0526 0.2902 0.4311], Epochs since improvement 2
 27%|██▋       | 137/500 [2:35:19<6:31:55, 64.78s/it] 28%|██▊       | 138/500 [2:36:40<7:00:13, 69.65s/it]Epoch: 138 done with learning rate 9.47E-03, Train loss: 9.17E+05, Train scatter: [0.2007 0.0504 0.2952 0.4203]
L1 regularization loss: 4.34E+00, L2 regularization loss: 2.22E+00
Test scatter: [0.1973 0.0502 0.2976 0.4195], Lowest was [0.1757 0.0502 0.2829 0.4195]
Median for last 10 epochs: [0.2178 0.0526 0.2902 0.4309], Epochs since improvement 0
 28%|██▊       | 139/500 [2:37:34<6:29:24, 64.72s/it]Epoch: 140 done with learning rate 9.43E-03, Train loss: 8.75E+05, Train scatter: [0.1929 0.0516 0.2963 0.4198]
L1 regularization loss: 4.35E+00, L2 regularization loss: 2.25E+00
Test scatter: [0.1865 0.051  0.3003 0.4141], Lowest was [0.1757 0.0502 0.2829 0.4141]
Median for last 10 epochs: [0.1973 0.0526 0.2976 0.4231], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 28%|██▊       | 140/500 [2:39:01<7:09:50, 71.64s/it] 28%|██▊       | 141/500 [2:39:55<6:35:28, 66.10s/it] 28%|██▊       | 142/500 [2:41:15<7:00:06, 70.41s/it]Epoch: 142 done with learning rate 9.40E-03, Train loss: 8.42E+05, Train scatter: [0.1989 0.0518 0.2902 0.444 ]
L1 regularization loss: 4.36E+00, L2 regularization loss: 2.30E+00
Test scatter: [0.1956 0.0513 0.2901 0.4352], Lowest was [0.1757 0.0502 0.2829 0.4141]
Median for last 10 epochs: [0.1956 0.0513 0.2901 0.4231], Epochs since improvement 2
 29%|██▊       | 143/500 [2:42:08<6:28:06, 65.23s/it] 29%|██▉       | 144/500 [2:43:29<6:54:21, 69.84s/it]Epoch: 144 done with learning rate 9.36E-03, Train loss: 8.03E+05, Train scatter: [0.1748 0.0531 0.2866 0.4257]
L1 regularization loss: 4.36E+00, L2 regularization loss: 2.33E+00
Test scatter: [0.1824 0.0531 0.2895 0.4276], Lowest was [0.1757 0.0502 0.2829 0.4141]
Median for last 10 epochs: [0.1956 0.0513 0.2901 0.4276], Epochs since improvement 4
 29%|██▉       | 145/500 [2:44:22<6:23:56, 64.89s/it] 29%|██▉       | 146/500 [2:45:44<6:52:08, 69.85s/it]Epoch: 146 done with learning rate 9.33E-03, Train loss: 7.59E+05, Train scatter: [0.1612 0.0485 0.2757 0.4146]
L1 regularization loss: 4.36E+00, L2 regularization loss: 2.36E+00
Test scatter: [0.1607 0.0482 0.2789 0.4101], Lowest was [0.1607 0.0482 0.2789 0.4101]
Median for last 10 epochs: [0.1865 0.051  0.2901 0.4195], Epochs since improvement 0
 29%|██▉       | 147/500 [2:46:37<6:21:23, 64.83s/it] 30%|██▉       | 148/500 [2:47:57<6:48:09, 69.57s/it]Epoch: 148 done with learning rate 9.29E-03, Train loss: 7.35E+05, Train scatter: [0.186  0.0533 0.2878 0.4306]
L1 regularization loss: 4.37E+00, L2 regularization loss: 2.39E+00
Test scatter: [0.1925 0.0535 0.2939 0.4329], Lowest was [0.1607 0.0482 0.2789 0.4101]
Median for last 10 epochs: [0.1865 0.0513 0.2901 0.4276], Epochs since improvement 2
 30%|██▉       | 149/500 [2:48:51<6:18:29, 64.70s/it]Epoch: 150 done with learning rate 9.25E-03, Train loss: 6.38E+05, Train scatter: [0.1621 0.05   0.2685 0.4144]
L1 regularization loss: 4.37E+00, L2 regularization loss: 2.41E+00
Test scatter: [0.1591 0.0498 0.2702 0.4107], Lowest was [0.1591 0.0482 0.2702 0.4101]
Median for last 10 epochs: [0.1824 0.0513 0.2895 0.4276], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 30%|███       | 150/500 [2:50:19<6:58:18, 71.71s/it] 30%|███       | 151/500 [2:51:12<6:25:07, 66.21s/it] 30%|███       | 152/500 [2:52:33<6:48:50, 70.49s/it]Epoch: 152 done with learning rate 9.21E-03, Train loss: 6.01E+05, Train scatter: [0.1735 0.0504 0.2878 0.4148]
L1 regularization loss: 4.37E+00, L2 regularization loss: 2.44E+00
Test scatter: [0.1738 0.0499 0.2897 0.4097], Lowest was [0.1591 0.0482 0.2702 0.4097]
Median for last 10 epochs: [0.1738 0.0499 0.2895 0.4107], Epochs since improvement 0
 31%|███       | 153/500 [2:53:26<6:17:51, 65.34s/it] 31%|███       | 154/500 [2:54:46<6:43:06, 69.90s/it]Epoch: 154 done with learning rate 9.17E-03, Train loss: 5.52E+05, Train scatter: [0.2128 0.0524 0.3061 0.4161]
L1 regularization loss: 4.38E+00, L2 regularization loss: 2.47E+00
Test scatter: [0.2162 0.0519 0.3089 0.4147], Lowest was [0.1591 0.0482 0.2702 0.4097]
Median for last 10 epochs: [0.1738 0.0499 0.2897 0.4107], Epochs since improvement 2
 31%|███       | 155/500 [2:55:40<6:13:18, 64.92s/it] 31%|███       | 156/500 [2:57:00<6:38:39, 69.53s/it]Epoch: 156 done with learning rate 9.13E-03, Train loss: 5.23E+05, Train scatter: [0.1591 0.0469 0.2566 0.4108]
L1 regularization loss: 4.39E+00, L2 regularization loss: 2.51E+00
Test scatter: [0.16   0.0472 0.2614 0.4089], Lowest was [0.1591 0.0472 0.2614 0.4089]
Median for last 10 epochs: [0.1738 0.0499 0.2897 0.4107], Epochs since improvement 0
 31%|███▏      | 157/500 [2:57:53<6:09:35, 64.65s/it] 32%|███▏      | 158/500 [2:59:14<6:35:17, 69.35s/it]Epoch: 158 done with learning rate 9.09E-03, Train loss: 4.31E+05, Train scatter: [0.1573 0.0503 0.2922 0.4342]
L1 regularization loss: 4.40E+00, L2 regularization loss: 2.54E+00
Test scatter: [0.1584 0.0503 0.2965 0.4346], Lowest was [0.1584 0.0472 0.2614 0.4089]
Median for last 10 epochs: [0.16   0.0499 0.2897 0.4107], Epochs since improvement 0
 32%|███▏      | 159/500 [3:00:07<6:06:38, 64.51s/it]Epoch: 160 done with learning rate 9.04E-03, Train loss: 3.82E+05, Train scatter: [0.1492 0.0488 0.2679 0.408 ]
L1 regularization loss: 4.43E+00, L2 regularization loss: 2.58E+00
Test scatter: [0.1488 0.048  0.2712 0.4005], Lowest was [0.1488 0.0472 0.2614 0.4005]
Median for last 10 epochs: [0.16   0.0499 0.2897 0.4097], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 32%|███▏      | 160/500 [3:01:35<6:45:02, 71.48s/it] 32%|███▏      | 161/500 [3:02:28<6:13:05, 66.03s/it] 32%|███▏      | 162/500 [3:03:49<6:37:18, 70.53s/it]Epoch: 162 done with learning rate 9.00E-03, Train loss: 3.12E+05, Train scatter: [0.2118 0.0525 0.2967 0.4133]
L1 regularization loss: 4.44E+00, L2 regularization loss: 2.61E+00
Test scatter: [0.2109 0.0518 0.2937 0.416 ], Lowest was [0.1488 0.0472 0.2614 0.4005]
Median for last 10 epochs: [0.16   0.0503 0.2937 0.4147], Epochs since improvement 2
 33%|███▎      | 163/500 [3:04:42<6:07:23, 65.41s/it] 33%|███▎      | 164/500 [3:06:04<6:32:54, 70.16s/it]Epoch: 164 done with learning rate 8.96E-03, Train loss: 2.44E+05, Train scatter: [0.2644 0.0484 0.2895 0.4059]
L1 regularization loss: 4.45E+00, L2 regularization loss: 2.64E+00
Test scatter: [0.2502 0.0479 0.2933 0.4032], Lowest was [0.1488 0.0472 0.2614 0.4005]
Median for last 10 epochs: [0.16   0.048  0.2933 0.4089], Epochs since improvement 4
 33%|███▎      | 165/500 [3:06:57<6:03:43, 65.14s/it] 33%|███▎      | 166/500 [3:08:19<6:30:52, 70.22s/it]Epoch: 166 done with learning rate 8.91E-03, Train loss: 1.52E+05, Train scatter: [0.1488 0.0451 0.2467 0.3965]
L1 regularization loss: 4.48E+00, L2 regularization loss: 2.68E+00
Test scatter: [0.1464 0.0444 0.2514 0.3929], Lowest was [0.1464 0.0444 0.2514 0.3929]
Median for last 10 epochs: [0.1584 0.048  0.2933 0.4032], Epochs since improvement 0
 33%|███▎      | 167/500 [3:09:12<6:01:12, 65.08s/it] 34%|███▎      | 168/500 [3:10:33<6:26:54, 69.92s/it]Epoch: 168 done with learning rate 8.86E-03, Train loss: 6.94E+04, Train scatter: [0.1818 0.0513 0.2833 0.4222]
L1 regularization loss: 4.47E+00, L2 regularization loss: 2.71E+00
Test scatter: [0.1907 0.0519 0.2884 0.4244], Lowest was [0.1464 0.0444 0.2514 0.3929]
Median for last 10 epochs: [0.1907 0.048  0.2884 0.4032], Epochs since improvement 2
 34%|███▍      | 169/500 [3:11:27<5:57:52, 64.87s/it]Epoch: 170 done with learning rate 8.82E-03, Train loss: -9.46E+03, Train scatter: [0.2308 0.0489 0.2956 0.4086]
L1 regularization loss: 4.48E+00, L2 regularization loss: 2.74E+00
Test scatter: [0.2335 0.0495 0.2996 0.4082], Lowest was [0.1464 0.0444 0.2514 0.3929]
Median for last 10 epochs: [0.2109 0.0495 0.2933 0.4082], Epochs since improvement 4
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 34%|███▍      | 170/500 [3:12:53<6:33:13, 71.50s/it] 34%|███▍      | 171/500 [3:13:47<6:02:10, 66.05s/it] 34%|███▍      | 172/500 [3:15:08<6:25:29, 70.52s/it]Epoch: 172 done with learning rate 8.77E-03, Train loss: -6.81E+04, Train scatter: [0.1753 0.0459 0.2654 0.408 ]
L1 regularization loss: 4.49E+00, L2 regularization loss: 2.77E+00
Test scatter: [0.1692 0.045  0.2692 0.4003], Lowest was [0.1464 0.0444 0.2514 0.3929]
Median for last 10 epochs: [0.1907 0.0479 0.2884 0.4032], Epochs since improvement 6
 35%|███▍      | 173/500 [3:16:01<5:55:54, 65.30s/it] 35%|███▍      | 174/500 [3:17:21<6:18:19, 69.63s/it]Epoch: 174 done with learning rate 8.72E-03, Train loss: -1.25E+05, Train scatter: [0.1527 0.047  0.2395 0.3891]
L1 regularization loss: 4.49E+00, L2 regularization loss: 2.80E+00
Test scatter: [0.1516 0.0458 0.2414 0.3864], Lowest was [0.1464 0.0444 0.2414 0.3864]
Median for last 10 epochs: [0.1692 0.0458 0.2692 0.4003], Epochs since improvement 0
 35%|███▌      | 175/500 [3:18:14<5:50:03, 64.63s/it] 35%|███▌      | 176/500 [3:19:34<6:15:05, 69.46s/it]Epoch: 176 done with learning rate 8.67E-03, Train loss: -1.73E+05, Train scatter: [0.1374 0.0462 0.2464 0.3944]
L1 regularization loss: 4.49E+00, L2 regularization loss: 2.83E+00
Test scatter: [0.1384 0.0453 0.2454 0.389 ], Lowest was [0.1384 0.0444 0.2414 0.3864]
Median for last 10 epochs: [0.1692 0.0458 0.2692 0.4003], Epochs since improvement 0
 35%|███▌      | 177/500 [3:20:27<5:47:32, 64.56s/it] 36%|███▌      | 178/500 [3:21:48<6:12:56, 69.49s/it]Epoch: 178 done with learning rate 8.62E-03, Train loss: -2.17E+05, Train scatter: [0.2041 0.044  0.2339 0.399 ]
L1 regularization loss: 4.51E+00, L2 regularization loss: 2.87E+00
Test scatter: [0.2004 0.0435 0.2365 0.3968], Lowest was [0.1384 0.0435 0.2365 0.3864]
Median for last 10 epochs: [0.1692 0.0453 0.2454 0.3968], Epochs since improvement 0
 36%|███▌      | 179/500 [3:22:41<5:45:21, 64.55s/it]Epoch: 180 done with learning rate 8.57E-03, Train loss: -2.67E+05, Train scatter: [0.1348 0.0472 0.2882 0.4137]
L1 regularization loss: 4.50E+00, L2 regularization loss: 2.89E+00
Test scatter: [0.1338 0.0464 0.2934 0.4088], Lowest was [0.1338 0.0435 0.2365 0.3864]
Median for last 10 epochs: [0.1516 0.0453 0.2454 0.3968], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 36%|███▌      | 180/500 [3:24:09<6:21:20, 71.50s/it] 36%|███▌      | 181/500 [3:25:02<5:50:56, 66.01s/it] 36%|███▋      | 182/500 [3:26:24<6:15:20, 70.82s/it]Epoch: 182 done with learning rate 8.52E-03, Train loss: -2.89E+05, Train scatter: [0.1957 0.0428 0.2274 0.3917]
L1 regularization loss: 4.52E+00, L2 regularization loss: 2.93E+00
Test scatter: [0.1911 0.0421 0.2301 0.3864], Lowest was [0.1338 0.0421 0.2301 0.3864]
Median for last 10 epochs: [0.1516 0.0453 0.2414 0.389 ], Epochs since improvement 0
 37%|███▋      | 183/500 [3:27:18<5:46:12, 65.53s/it] 37%|███▋      | 184/500 [3:28:38<6:08:31, 69.97s/it]Epoch: 184 done with learning rate 8.46E-03, Train loss: -3.11E+05, Train scatter: [0.1479 0.0553 0.2446 0.393 ]
L1 regularization loss: 4.52E+00, L2 regularization loss: 2.97E+00
Test scatter: [0.1494 0.0547 0.2499 0.3919], Lowest was [0.1338 0.0421 0.2301 0.3864]
Median for last 10 epochs: [0.1494 0.0453 0.2454 0.3919], Epochs since improvement 2
 37%|███▋      | 185/500 [3:29:31<5:40:34, 64.87s/it] 37%|███▋      | 186/500 [3:30:52<6:04:25, 69.64s/it]Epoch: 186 done with learning rate 8.41E-03, Train loss: -3.24E+05, Train scatter: [0.1507 0.0425 0.2276 0.3962]
L1 regularization loss: 4.52E+00, L2 regularization loss: 3.00E+00
Test scatter: [0.1497 0.042  0.2306 0.3907], Lowest was [0.1338 0.042  0.2301 0.3864]
Median for last 10 epochs: [0.1497 0.0435 0.2365 0.3919], Epochs since improvement 0
 37%|███▋      | 187/500 [3:31:45<5:37:38, 64.72s/it] 38%|███▊      | 188/500 [3:33:06<6:02:15, 69.66s/it]Epoch: 188 done with learning rate 8.35E-03, Train loss: -3.47E+05, Train scatter: [0.1288 0.0432 0.2287 0.3916]
L1 regularization loss: 4.53E+00, L2 regularization loss: 3.02E+00
Test scatter: [0.1306 0.0425 0.231  0.3881], Lowest was [0.1306 0.042  0.2301 0.3864]
Median for last 10 epochs: [0.1494 0.0425 0.231  0.3907], Epochs since improvement 0
 38%|███▊      | 189/500 [3:33:59<5:35:19, 64.69s/it]Epoch: 190 done with learning rate 8.30E-03, Train loss: -3.55E+05, Train scatter: [0.1344 0.0468 0.2185 0.3849]
L1 regularization loss: 4.59E+00, L2 regularization loss: 3.07E+00
Test scatter: [0.1354 0.0455 0.2211 0.3821], Lowest was [0.1306 0.042  0.2211 0.3821]
Median for last 10 epochs: [0.1494 0.0425 0.2306 0.3881], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 38%|███▊      | 190/500 [3:35:27<6:10:17, 71.67s/it] 38%|███▊      | 191/500 [3:36:20<5:40:38, 66.14s/it] 38%|███▊      | 192/500 [3:37:41<6:01:39, 70.45s/it]Epoch: 192 done with learning rate 8.24E-03, Train loss: -3.74E+05, Train scatter: [0.1339 0.0478 0.233  0.3957]
L1 regularization loss: 4.57E+00, L2 regularization loss: 3.09E+00
Test scatter: [0.1352 0.0469 0.2361 0.3906], Lowest was [0.1306 0.042  0.2211 0.3821]
Median for last 10 epochs: [0.1354 0.0455 0.231  0.3906], Epochs since improvement 2
 39%|███▊      | 193/500 [3:38:34<5:33:41, 65.22s/it] 39%|███▉      | 194/500 [3:39:54<5:55:59, 69.80s/it]Epoch: 194 done with learning rate 8.19E-03, Train loss: -3.81E+05, Train scatter: [0.139  0.0426 0.2313 0.3869]
L1 regularization loss: 4.57E+00, L2 regularization loss: 3.12E+00
Test scatter: [0.1387 0.0425 0.2336 0.384 ], Lowest was [0.1306 0.042  0.2211 0.3821]
Median for last 10 epochs: [0.1354 0.0425 0.231  0.3881], Epochs since improvement 4
 39%|███▉      | 195/500 [3:40:47<5:29:15, 64.77s/it] 39%|███▉      | 196/500 [3:42:08<5:52:47, 69.63s/it]Epoch: 196 done with learning rate 8.13E-03, Train loss: -3.21E+05, Train scatter: [0.1825 0.0439 0.2602 0.4034]
L1 regularization loss: 4.68E+00, L2 regularization loss: 3.21E+00
Test scatter: [0.1736 0.0439 0.2621 0.4012], Lowest was [0.1306 0.042  0.2211 0.3821]
Median for last 10 epochs: [0.1354 0.0439 0.2336 0.3881], Epochs since improvement 6
 39%|███▉      | 197/500 [3:43:01<5:26:27, 64.64s/it] 40%|███▉      | 198/500 [3:44:23<5:50:28, 69.63s/it]Epoch: 198 done with learning rate 8.07E-03, Train loss: -3.66E+05, Train scatter: [0.3504 0.0421 0.2253 0.3923]
L1 regularization loss: 4.83E+00, L2 regularization loss: 3.31E+00
Test scatter: [0.3398 0.0416 0.2281 0.3868], Lowest was [0.1306 0.0416 0.2211 0.3821]
Median for last 10 epochs: [0.1387 0.0439 0.2336 0.3868], Epochs since improvement 0
 40%|███▉      | 199/500 [3:45:16<5:24:34, 64.70s/it]Epoch: 200 done with learning rate 8.01E-03, Train loss: -4.00E+05, Train scatter: [0.1673 0.0415 0.2174 0.3909]
L1 regularization loss: 4.74E+00, L2 regularization loss: 3.31E+00
Test scatter: [0.1677 0.0411 0.2207 0.3874], Lowest was [0.1306 0.0411 0.2207 0.3821]
Median for last 10 epochs: [0.1677 0.0425 0.2336 0.3874], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 40%|████      | 200/500 [3:46:44<5:58:20, 71.67s/it] 40%|████      | 201/500 [3:47:37<5:29:26, 66.11s/it] 40%|████      | 202/500 [3:48:57<5:49:23, 70.35s/it]Epoch: 202 done with learning rate 7.95E-03, Train loss: -3.95E+05, Train scatter: [0.1255 0.0414 0.2168 0.3897]
L1 regularization loss: 4.71E+00, L2 regularization loss: 3.33E+00
Test scatter: [0.1253 0.0408 0.2197 0.3839], Lowest was [0.1253 0.0408 0.2197 0.3821]
Median for last 10 epochs: [0.1677 0.0416 0.2281 0.3868], Epochs since improvement 0
 41%|████      | 203/500 [3:49:50<5:22:38, 65.18s/it] 41%|████      | 204/500 [3:51:11<5:44:24, 69.81s/it]Epoch: 204 done with learning rate 7.89E-03, Train loss: -3.64E+05, Train scatter: [0.1509 0.0482 0.2371 0.4081]
L1 regularization loss: 4.85E+00, L2 regularization loss: 3.40E+00
Test scatter: [0.1516 0.0469 0.2388 0.4016], Lowest was [0.1253 0.0408 0.2197 0.3821]
Median for last 10 epochs: [0.1677 0.0416 0.2281 0.3874], Epochs since improvement 2
 41%|████      | 205/500 [3:52:04<5:18:41, 64.82s/it] 41%|████      | 206/500 [3:53:25<5:41:39, 69.73s/it]Epoch: 206 done with learning rate 7.83E-03, Train loss: -4.05E+05, Train scatter: [0.1515 0.0434 0.2295 0.4099]
L1 regularization loss: 4.82E+00, L2 regularization loss: 3.43E+00
Test scatter: [0.153  0.0426 0.232  0.4034], Lowest was [0.1253 0.0408 0.2197 0.3821]
Median for last 10 epochs: [0.153  0.0416 0.2281 0.3874], Epochs since improvement 4
 41%|████▏     | 207/500 [3:54:18<5:16:06, 64.73s/it] 42%|████▏     | 208/500 [3:55:39<5:37:53, 69.43s/it]Epoch: 208 done with learning rate 7.77E-03, Train loss: -2.86E+05, Train scatter: [0.4418 0.0848 0.4402 0.5278]
L1 regularization loss: 5.05E+00, L2 regularization loss: 3.59E+00
Test scatter: [0.4275 0.0839 0.4339 0.5204], Lowest was [0.1253 0.0408 0.2197 0.3821]
Median for last 10 epochs: [0.153  0.0426 0.232  0.4016], Epochs since improvement 6
 42%|████▏     | 209/500 [3:56:32<5:12:53, 64.51s/it]Epoch: 210 done with learning rate 7.71E-03, Train loss: -3.16E+05, Train scatter: [0.1498 0.0498 0.2553 0.4196]
L1 regularization loss: 5.24E+00, L2 regularization loss: 3.78E+00
Test scatter: [0.1444 0.0487 0.2538 0.4109], Lowest was [0.1253 0.0408 0.2197 0.3821]
Median for last 10 epochs: [0.1516 0.0469 0.2388 0.4034], Epochs since improvement 8
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 42%|████▏     | 210/500 [3:57:59<5:45:09, 71.41s/it] 42%|████▏     | 211/500 [3:58:52<5:17:35, 65.93s/it] 42%|████▏     | 212/500 [4:00:14<5:38:18, 70.48s/it]Epoch: 212 done with learning rate 7.65E-03, Train loss: -4.08E+05, Train scatter: [0.1569 0.0432 0.223  0.4052]
L1 regularization loss: 5.19E+00, L2 regularization loss: 3.78E+00
Test scatter: [0.1559 0.0428 0.2256 0.4015], Lowest was [0.1253 0.0408 0.2197 0.3821]
Median for last 10 epochs: [0.153  0.0469 0.2388 0.4034], Epochs since improvement 10
 43%|████▎     | 213/500 [4:01:07<5:12:07, 65.25s/it] 43%|████▎     | 214/500 [4:02:27<5:32:20, 69.72s/it]Epoch: 214 done with learning rate 7.58E-03, Train loss: -3.98E+05, Train scatter: [0.1391 0.0445 0.2246 0.4058]
L1 regularization loss: 5.17E+00, L2 regularization loss: 3.79E+00
Test scatter: [0.1398 0.0444 0.2286 0.4019], Lowest was [0.1253 0.0408 0.2197 0.3821]
Median for last 10 epochs: [0.153  0.0444 0.232  0.4034], Epochs since improvement 12
 43%|████▎     | 215/500 [4:03:20<5:07:16, 64.69s/it] 43%|████▎     | 216/500 [4:04:41<5:29:39, 69.65s/it]Epoch: 216 done with learning rate 7.52E-03, Train loss: 1.07E+05, Train scatter: [0.9352 0.1707 0.5441 0.9952]
L1 regularization loss: 8.93E+00, L2 regularization loss: 5.09E+00
Test scatter: [0.9196 0.167  0.5355 0.9849], Lowest was [0.1253 0.0408 0.2197 0.3821]
Median for last 10 epochs: [0.1559 0.0487 0.2538 0.4109], Epochs since improvement 14
 43%|████▎     | 217/500 [4:05:34<5:04:57, 64.65s/it] 44%|████▎     | 218/500 [4:06:55<5:27:26, 69.67s/it]Epoch: 218 done with learning rate 7.46E-03, Train loss: 5.05E+04, Train scatter: [0.9351 0.166  0.5442 0.9952]
L1 regularization loss: 8.88E+00, L2 regularization loss: 5.08E+00
Test scatter: [0.9195 0.1624 0.5356 0.9848], Lowest was [0.1253 0.0408 0.2197 0.3821]
Median for last 10 epochs: [0.1559 0.0487 0.2538 0.4109], Epochs since improvement 16
 44%|████▍     | 219/500 [4:07:48<5:02:45, 64.65s/it]Epoch: 220 done with learning rate 7.39E-03, Train loss: 2.73E+04, Train scatter: [0.9351 0.1563 0.5444 0.9948]
L1 regularization loss: 8.84E+00, L2 regularization loss: 5.09E+00
Test scatter: [0.9195 0.153  0.5358 0.9844], Lowest was [0.1253 0.0408 0.2197 0.3821]
Median for last 10 epochs: [0.9195 0.153  0.5355 0.9844], Epochs since improvement 18
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 44%|████▍     | 220/500 [4:09:19<5:37:42, 72.37s/it] 44%|████▍     | 221/500 [4:10:12<5:09:32, 66.57s/it] 44%|████▍     | 222/500 [4:11:33<5:28:32, 70.91s/it]Epoch: 222 done with learning rate 7.33E-03, Train loss: 8.67E+03, Train scatter: [0.935  0.1191 0.5454 0.998 ]
L1 regularization loss: 8.80E+00, L2 regularization loss: 5.14E+00
Test scatter: [0.9194 0.1178 0.5368 0.9875], Lowest was [0.1253 0.0408 0.2197 0.3821]
Median for last 10 epochs: [0.9195 0.153  0.5356 0.9848], Epochs since improvement 20
 45%|████▍     | 223/500 [4:12:26<5:02:32, 65.53s/it] 45%|████▍     | 223/500 [4:13:47<5:15:15, 68.29s/it]
Epoch: 224 done with learning rate 7.26E-03, Train loss: -1.45E+04, Train scatter: [0.9308 0.1233 0.5455 0.9791]
L1 regularization loss: 8.79E+00, L2 regularization loss: 5.32E+00
Test scatter: [0.9153 0.1216 0.5369 0.969 ], Lowest was [0.1253 0.0408 0.2197 0.3821]
Median for last 10 epochs: [0.9195 0.153  0.5358 0.9848], Epochs since improvement 22
Exited after 224 epochs due to early stopping
15227.80 seconds spent training, 30.456 seconds per epoch. Processed 2286 trees per second
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
[0.9152671  0.12158522 0.5368502  0.96893483]
{'epoch_exit': 223, 'scatter_m_star': 0.9152671, 'lowest_m_star': 0.12534042, 'last20_m_star': 0.671396, 'last10_m_star': 0.91948146, 'scatter_v_disk': 0.12158522, 'lowest_v_disk': 0.040764663, 'last20_v_disk': 0.10081741, 'last10_v_disk': 0.15304609, 'scatter_m_cold': 0.5368502, 'lowest_m_cold': 0.21968369, 'last20_m_cold': 0.48470438, 'last10_m_cold': 0.5358019, 'scatter_sfr_100': 0.96893483, 'lowest_sfr_100': 0.38210326, 'last20_sfr_100': 0.74469554, 'last10_sfr_100': 0.98484284}
Finished 1/1
Exited training after None epochs
Experiment 6 done: 6 / 6
