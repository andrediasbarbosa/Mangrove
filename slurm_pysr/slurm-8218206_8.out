Starting process with 6 experiments
{'conv_layers': [1, 2, 3], 'batch_size': [256, 512]}
Exploring ['conv_layers', 'batch_size']
Currently doing [  1 256]
{'experiment': 'GraphMerge', 'group': 'meta_pysr', 'move': False, 'model': 'MetaEdge', 'log': True, 'run_params': {'n_epochs': 500, 'n_trials': 1, 'batch_size': 256, 'val_epoch': 2, 'early_stopping': True, 'patience': 20, 'l1_lambda': 0.0001, 'l2_lambda': 0.0001, 'loss_func': 'GaussNd', 'metrics': 'test_multi_varrho', 'performance_plot': 'multi_base', 'save': True, 'seed': True, 'num_workers': 4}, 'learn_params': {'learning_rate': 0.01, 'schedule': 'onecycle', 'g_up': 1, 'g_down': 0.95, 'warmup': 4, 'period': 5, 'eta_min': 1e-05}, 'hyper_params': {'hidden_channels': 128, 'conv_layers': 1, 'conv_activation': 'relu', 'decode_activation': 'leakyrelu', 'decode_layers': 2, 'layernorm': True, 'agg': 'sum', 'variance': True, 'rho': 0}, 'data_params': {'case': 'vlarge_all_4t_z0.0_quantile_raw', 'targets': [0, 1, 2, 3], 'del_feats': [], 'split': 0.8, 'test': 0, 'scale': 0}, 'experiment_name': 'con1bat256'}
/home/cj1223/../../scratch/gpfs/cj1223/GraphResults/results_meta_pysr_120322/MetaEdge_vlarge_all_4t_z0.0_quantile_raw_rbeane
RelU conv activation
LeakyRelU decode activation
N_params 298056
Made folder for saving model
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:32<4:26:21, 32.03s/it]  0%|          | 2/500 [01:21<5:48:45, 42.02s/it]Epoch: 2 done with learning rate 4.17E-04, Train loss: 1.27E+07, Train scatter: [0.9352 0.1669 0.5441 0.9954]
L1 regularization loss: 1.52E+00, L2 regularization loss: 3.31E-01
Test scatter: [0.9196 0.1661 0.5355 0.9851], Lowest was [0.9196 0.1661 0.5355 0.9851]
Median for last 10 epochs: [0.9196 0.1661 0.5355 0.9851], Epochs since improvement 2
  1%|          | 3/500 [01:53<5:12:03, 37.67s/it]  1%|          | 4/500 [02:44<5:53:36, 42.77s/it]Epoch: 4 done with learning rate 4.67E-04, Train loss: 6.87E+06, Train scatter: [0.9351 0.1487 0.5439 0.9954]
L1 regularization loss: 1.53E+00, L2 regularization loss: 3.54E-01
Test scatter: [0.9196 0.1476 0.5353 0.985 ], Lowest was [0.9196 0.1476 0.5353 0.985 ]
Median for last 10 epochs: [0.9196 0.1476 0.5353 0.985 ], Epochs since improvement 0
  1%|          | 5/500 [03:17<5:24:25, 39.32s/it]  1%|          | 6/500 [04:08<5:56:11, 43.26s/it]Epoch: 6 done with learning rate 5.51E-04, Train loss: 5.22E+06, Train scatter: [0.9347 0.1277 0.5423 0.7241]
L1 regularization loss: 1.57E+00, L2 regularization loss: 3.90E-01
Test scatter: [0.9191 0.1274 0.5337 0.7167], Lowest was [0.9191 0.1274 0.5337 0.7167]
Median for last 10 epochs: [0.9191 0.1274 0.5337 0.7167], Epochs since improvement 0
  1%|▏         | 7/500 [04:41<5:28:28, 39.98s/it]  2%|▏         | 8/500 [05:31<5:53:13, 43.08s/it]Epoch: 8 done with learning rate 6.67E-04, Train loss: 4.57E+06, Train scatter: [0.9104 0.114  0.5333 0.617 ]
L1 regularization loss: 1.59E+00, L2 regularization loss: 4.02E-01
Test scatter: [0.8958 0.1147 0.5253 0.6088], Lowest was [0.8958 0.1147 0.5253 0.6088]
Median for last 10 epochs: [0.9074 0.1211 0.5295 0.6627], Epochs since improvement 0
  2%|▏         | 9/500 [06:03<5:24:31, 39.66s/it]Epoch: 10 done with learning rate 8.15E-04, Train loss: 3.98E+06, Train scatter: [0.7391 0.1028 0.5242 0.5946]
L1 regularization loss: 1.61E+00, L2 regularization loss: 4.13E-01
Test scatter: [0.7248 0.1045 0.5159 0.594 ], Lowest was [0.7248 0.1045 0.5159 0.594 ]
Median for last 10 epochs: [0.8958 0.1147 0.5253 0.6088], Epochs since improvement 0
  2%|▏         | 10/500 [06:57<6:01:31, 44.27s/it]  2%|▏         | 11/500 [07:30<5:30:37, 40.57s/it]  2%|▏         | 12/500 [08:20<5:53:51, 43.51s/it]Epoch: 12 done with learning rate 9.94E-04, Train loss: 2.40E+06, Train scatter: [0.5951 0.0973 0.4153 0.6086]
L1 regularization loss: 1.62E+00, L2 regularization loss: 4.24E-01
Test scatter: [0.6058 0.1003 0.4146 0.6152], Lowest was [0.6058 0.1003 0.4146 0.594 ]
Median for last 10 epochs: [0.8958 0.1147 0.5253 0.6152], Epochs since improvement 0
  3%|▎         | 13/500 [08:53<5:28:02, 40.42s/it]  3%|▎         | 14/500 [09:43<5:51:32, 43.40s/it]Epoch: 14 done with learning rate 1.20E-03, Train loss: 1.59E+06, Train scatter: [0.5353 0.0919 0.3869 0.5897]
L1 regularization loss: 1.64E+00, L2 regularization loss: 4.31E-01
Test scatter: [0.5383 0.0945 0.3898 0.5917], Lowest was [0.5383 0.0945 0.3898 0.5917]
Median for last 10 epochs: [0.7248 0.1045 0.5159 0.6088], Epochs since improvement 0
  3%|▎         | 15/500 [10:15<5:22:21, 39.88s/it]  3%|▎         | 16/500 [11:04<5:44:19, 42.68s/it]Epoch: 16 done with learning rate 1.44E-03, Train loss: 1.29E+06, Train scatter: [0.518  0.0887 0.3595 0.5838]
L1 regularization loss: 1.65E+00, L2 regularization loss: 4.38E-01
Test scatter: [0.5227 0.0908 0.3653 0.5889], Lowest was [0.5227 0.0908 0.3653 0.5889]
Median for last 10 epochs: [0.6058 0.1003 0.4146 0.594 ], Epochs since improvement 0
  3%|▎         | 17/500 [11:37<5:19:22, 39.67s/it]  4%|▎         | 18/500 [12:27<5:44:45, 42.92s/it]Epoch: 18 done with learning rate 1.70E-03, Train loss: 1.10E+06, Train scatter: [0.5294 0.0863 0.3471 0.5766]
L1 regularization loss: 1.67E+00, L2 regularization loss: 4.46E-01
Test scatter: [0.5207 0.0875 0.3512 0.581 ], Lowest was [0.5207 0.0875 0.3512 0.581 ]
Median for last 10 epochs: [0.5383 0.0945 0.3898 0.5917], Epochs since improvement 0
  4%|▍         | 19/500 [12:59<5:16:43, 39.51s/it]Epoch: 20 done with learning rate 1.99E-03, Train loss: 2.13E+07, Train scatter: [0.9139 0.1747 0.5321 1.0049]
L1 regularization loss: 1.81E+00, L2 regularization loss: 4.78E-01
Test scatter: [0.9007 0.1698 0.5241 0.9946], Lowest was [0.5207 0.0875 0.3512 0.581 ]
Median for last 10 epochs: [0.5383 0.0945 0.3898 0.5917], Epochs since improvement 2
  4%|▍         | 20/500 [13:54<5:53:34, 44.20s/it]  4%|▍         | 21/500 [14:26<5:22:55, 40.45s/it]  4%|▍         | 22/500 [15:15<5:43:04, 43.06s/it]Epoch: 22 done with learning rate 2.30E-03, Train loss: 1.67E+07, Train scatter: [0.8276 0.1683 0.5158 0.9955]
L1 regularization loss: 1.83E+00, L2 regularization loss: 5.01E-01
Test scatter: [0.8202 0.1635 0.511  0.9861], Lowest was [0.5207 0.0875 0.3512 0.581 ]
Median for last 10 epochs: [0.5383 0.0945 0.3898 0.5917], Epochs since improvement 4
  5%|▍         | 23/500 [15:48<5:18:01, 40.00s/it]  5%|▍         | 24/500 [16:38<5:42:01, 43.11s/it]Epoch: 24 done with learning rate 2.63E-03, Train loss: 1.35E+07, Train scatter: [0.5902 0.1559 0.5103 0.9205]
L1 regularization loss: 1.85E+00, L2 regularization loss: 5.36E-01
Test scatter: [0.5899 0.1511 0.5044 0.9186], Lowest was [0.5207 0.0875 0.3512 0.581 ]
Median for last 10 epochs: [0.5899 0.1511 0.5044 0.9186], Epochs since improvement 6
  5%|▌         | 25/500 [17:12<5:18:02, 40.17s/it]  5%|▌         | 26/500 [18:01<5:38:35, 42.86s/it]Epoch: 26 done with learning rate 2.98E-03, Train loss: 8.22E+06, Train scatter: [0.6034 0.1246 0.4763 0.6826]
L1 regularization loss: 1.88E+00, L2 regularization loss: 5.74E-01
Test scatter: [0.6165 0.1233 0.4705 0.6803], Lowest was [0.5207 0.0875 0.3512 0.581 ]
Median for last 10 epochs: [0.6165 0.1511 0.5044 0.9186], Epochs since improvement 8
  5%|▌         | 27/500 [18:32<5:11:03, 39.46s/it]  6%|▌         | 28/500 [19:21<5:33:01, 42.33s/it]Epoch: 28 done with learning rate 3.34E-03, Train loss: 6.52E+06, Train scatter: [0.5096 0.1119 0.4584 0.6453]
L1 regularization loss: 1.90E+00, L2 regularization loss: 5.96E-01
Test scatter: [0.5198 0.112  0.4512 0.641 ], Lowest was [0.5198 0.0875 0.3512 0.581 ]
Median for last 10 epochs: [0.6165 0.1511 0.5044 0.9186], Epochs since improvement 0
  6%|▌         | 29/500 [19:53<5:07:12, 39.14s/it]Epoch: 30 done with learning rate 3.72E-03, Train loss: 5.40E+06, Train scatter: [0.4903 0.1039 0.4428 0.6237]
L1 regularization loss: 1.94E+00, L2 regularization loss: 6.23E-01
Test scatter: [0.4888 0.1042 0.4388 0.6205], Lowest was [0.4888 0.0875 0.3512 0.581 ]
Median for last 10 epochs: [0.5899 0.1233 0.4705 0.6803], Epochs since improvement 0
  6%|▌         | 30/500 [20:48<5:44:08, 43.93s/it]  6%|▌         | 31/500 [21:20<5:15:07, 40.31s/it]  6%|▋         | 32/500 [22:09<5:35:57, 43.07s/it]Epoch: 32 done with learning rate 4.10E-03, Train loss: 3.65E+06, Train scatter: [0.6205 0.0992 0.4338 0.611 ]
L1 regularization loss: 2.08E+00, L2 regularization loss: 7.18E-01
Test scatter: [0.5956 0.0995 0.4309 0.6063], Lowest was [0.4888 0.0875 0.3512 0.581 ]
Median for last 10 epochs: [0.5899 0.112  0.4512 0.641 ], Epochs since improvement 2
  7%|▋         | 33/500 [22:42<5:09:50, 39.81s/it]  7%|▋         | 34/500 [23:32<5:33:04, 42.89s/it]Epoch: 34 done with learning rate 4.50E-03, Train loss: 3.26E+06, Train scatter: [0.4329 0.0958 0.4319 0.601 ]
L1 regularization loss: 2.10E+00, L2 regularization loss: 7.38E-01
Test scatter: [0.4412 0.0966 0.4299 0.5988], Lowest was [0.4412 0.0875 0.3512 0.581 ]
Median for last 10 epochs: [0.5198 0.1042 0.4388 0.6205], Epochs since improvement 0
  7%|▋         | 35/500 [24:05<5:09:32, 39.94s/it]  7%|▋         | 36/500 [24:55<5:31:51, 42.91s/it]Epoch: 36 done with learning rate 4.90E-03, Train loss: 3.07E+06, Train scatter: [0.4536 0.0925 0.4212 0.5871]
L1 regularization loss: 2.13E+00, L2 regularization loss: 7.68E-01
Test scatter: [0.4613 0.0916 0.4188 0.5851], Lowest was [0.4412 0.0875 0.3512 0.581 ]
Median for last 10 epochs: [0.4888 0.0995 0.4309 0.6063], Epochs since improvement 2
  7%|▋         | 37/500 [25:28<5:08:37, 40.00s/it]  8%|▊         | 38/500 [26:18<5:32:20, 43.16s/it]Epoch: 38 done with learning rate 5.30E-03, Train loss: 2.68E+06, Train scatter: [0.4124 0.0888 0.4135 0.5951]
L1 regularization loss: 2.15E+00, L2 regularization loss: 7.98E-01
Test scatter: [0.4155 0.089  0.4096 0.5881], Lowest was [0.4155 0.0875 0.3512 0.581 ]
Median for last 10 epochs: [0.4613 0.0966 0.4299 0.5988], Epochs since improvement 0
  8%|▊         | 39/500 [26:51<5:07:22, 40.01s/it]Epoch: 40 done with learning rate 5.70E-03, Train loss: 2.41E+06, Train scatter: [0.2902 0.0851 0.4098 0.5604]
L1 regularization loss: 2.16E+00, L2 regularization loss: 8.23E-01
Test scatter: [0.3063 0.0856 0.409  0.5583], Lowest was [0.3063 0.0856 0.3512 0.5583]
Median for last 10 epochs: [0.4412 0.0916 0.4188 0.5881], Epochs since improvement 0
  8%|▊         | 40/500 [27:45<5:39:52, 44.33s/it]  8%|▊         | 41/500 [28:18<5:12:04, 40.79s/it]  8%|▊         | 42/500 [29:07<5:30:39, 43.32s/it]Epoch: 42 done with learning rate 6.10E-03, Train loss: 2.18E+06, Train scatter: [0.2746 0.0817 0.4007 0.5599]
L1 regularization loss: 2.17E+00, L2 regularization loss: 8.42E-01
Test scatter: [0.2903 0.0818 0.3995 0.5565], Lowest was [0.2903 0.0818 0.3512 0.5565]
Median for last 10 epochs: [0.4155 0.089  0.4096 0.5851], Epochs since improvement 0
  9%|▊         | 43/500 [29:40<5:05:50, 40.15s/it]  9%|▉         | 44/500 [30:28<5:24:00, 42.63s/it]Epoch: 44 done with learning rate 6.49E-03, Train loss: 2.07E+06, Train scatter: [0.3148 0.0861 0.3915 0.54  ]
L1 regularization loss: 2.18E+00, L2 regularization loss: 8.67E-01
Test scatter: [0.3275 0.0872 0.3917 0.536 ], Lowest was [0.2903 0.0818 0.3512 0.536 ]
Median for last 10 epochs: [0.3275 0.0872 0.409  0.5583], Epochs since improvement 0
  9%|▉         | 45/500 [31:02<5:02:29, 39.89s/it]  9%|▉         | 46/500 [31:51<5:23:29, 42.75s/it]Epoch: 46 done with learning rate 6.87E-03, Train loss: 1.90E+06, Train scatter: [0.2635 0.0787 0.3855 0.5296]
L1 regularization loss: 2.19E+00, L2 regularization loss: 8.92E-01
Test scatter: [0.2769 0.079  0.3868 0.5276], Lowest was [0.2769 0.079  0.3512 0.5276]
Median for last 10 epochs: [0.3063 0.0856 0.3995 0.5565], Epochs since improvement 0
  9%|▉         | 47/500 [32:25<5:01:33, 39.94s/it] 10%|▉         | 48/500 [33:15<5:23:37, 42.96s/it]Epoch: 48 done with learning rate 7.24E-03, Train loss: 1.81E+06, Train scatter: [0.2562 0.0791 0.3968 0.5345]
L1 regularization loss: 2.21E+00, L2 regularization loss: 9.22E-01
Test scatter: [0.2705 0.0786 0.388  0.5335], Lowest was [0.2705 0.0786 0.3512 0.5276]
Median for last 10 epochs: [0.2903 0.0818 0.3917 0.536 ], Epochs since improvement 0
 10%|▉         | 49/500 [33:48<5:00:20, 39.96s/it]Epoch: 50 done with learning rate 7.60E-03, Train loss: 1.62E+06, Train scatter: [0.3364 0.0821 0.3849 0.5453]
L1 regularization loss: 2.21E+00, L2 regularization loss: 9.49E-01
Test scatter: [0.3456 0.0827 0.3832 0.5447], Lowest was [0.2705 0.0786 0.3512 0.5276]
Median for last 10 epochs: [0.2903 0.0818 0.388  0.536 ], Epochs since improvement 2
 10%|█         | 50/500 [34:42<5:32:49, 44.38s/it] 10%|█         | 51/500 [35:15<5:05:48, 40.86s/it] 10%|█         | 52/500 [36:05<5:25:30, 43.60s/it]Epoch: 52 done with learning rate 7.94E-03, Train loss: 1.43E+06, Train scatter: [0.4008 0.0769 0.3725 0.5248]
L1 regularization loss: 2.24E+00, L2 regularization loss: 9.90E-01
Test scatter: [0.3938 0.0757 0.3693 0.523 ], Lowest was [0.2705 0.0757 0.3512 0.523 ]
Median for last 10 epochs: [0.3275 0.079  0.3868 0.5335], Epochs since improvement 0
 11%|█         | 53/500 [36:37<4:59:55, 40.26s/it] 11%|█         | 54/500 [37:27<5:20:45, 43.15s/it]Epoch: 54 done with learning rate 8.26E-03, Train loss: 1.33E+06, Train scatter: [0.398  0.078  0.3783 0.5273]
L1 regularization loss: 2.33E+00, L2 regularization loss: 1.06E+00
Test scatter: [0.3951 0.078  0.3804 0.5231], Lowest was [0.2705 0.0757 0.3512 0.523 ]
Median for last 10 epochs: [0.3456 0.0786 0.3832 0.5276], Epochs since improvement 2
 11%|█         | 55/500 [38:00<4:56:18, 39.95s/it] 11%|█         | 56/500 [38:49<5:17:11, 42.86s/it]Epoch: 56 done with learning rate 8.56E-03, Train loss: 1.88E+11, Train scatter: [0.9352 0.1727 0.5441 0.9953]
L1 regularization loss: 5.27E+00, L2 regularization loss: 2.14E+00
Test scatter: [0.9196 0.1689 0.5355 0.985 ], Lowest was [0.2705 0.0757 0.3512 0.523 ]
Median for last 10 epochs: [0.3938 0.0786 0.3832 0.5335], Epochs since improvement 4
 11%|█▏        | 57/500 [39:21<4:51:45, 39.51s/it] 12%|█▏        | 58/500 [40:11<5:14:32, 42.70s/it]Epoch: 58 done with learning rate 8.83E-03, Train loss: 1.02E+06, Train scatter: [0.9352 0.1727 0.5441 0.9953]
L1 regularization loss: 5.27E+00, L2 regularization loss: 2.15E+00
Test scatter: [0.9196 0.1689 0.5355 0.985 ], Lowest was [0.2705 0.0757 0.3512 0.523 ]
Median for last 10 epochs: [0.3951 0.0827 0.3832 0.5447], Epochs since improvement 6
 12%|█▏        | 59/500 [40:44<4:52:41, 39.82s/it]Epoch: 60 done with learning rate 9.08E-03, Train loss: 9.88E+05, Train scatter: [0.9352 0.1727 0.5441 0.9953]
L1 regularization loss: 5.27E+00, L2 regularization loss: 2.16E+00
Test scatter: [0.9196 0.1689 0.5355 0.985 ], Lowest was [0.2705 0.0757 0.3512 0.523 ]
Median for last 10 epochs: [0.9196 0.1689 0.5355 0.985 ], Epochs since improvement 8
 12%|█▏        | 60/500 [41:39<5:24:09, 44.20s/it] 12%|█▏        | 61/500 [42:12<4:59:16, 40.90s/it] 12%|█▏        | 62/500 [43:02<5:17:50, 43.54s/it]Epoch: 62 done with learning rate 9.31E-03, Train loss: 9.75E+05, Train scatter: [0.9352 0.173  0.5441 0.9954]
L1 regularization loss: 5.27E+00, L2 regularization loss: 2.17E+00
Test scatter: [0.9196 0.1692 0.5355 0.9851], Lowest was [0.2705 0.0757 0.3512 0.523 ]
Median for last 10 epochs: [0.9196 0.1689 0.5355 0.985 ], Epochs since improvement 10
 13%|█▎        | 63/500 [43:34<4:51:45, 40.06s/it] 13%|█▎        | 64/500 [44:23<5:11:21, 42.85s/it]Epoch: 64 done with learning rate 9.50E-03, Train loss: 9.60E+05, Train scatter: [0.9352 0.1729 0.5441 0.9954]
L1 regularization loss: 5.27E+00, L2 regularization loss: 2.18E+00
Test scatter: [0.9196 0.1691 0.5355 0.9851], Lowest was [0.2705 0.0757 0.3512 0.523 ]
Median for last 10 epochs: [0.9196 0.1689 0.5355 0.985 ], Epochs since improvement 12
 13%|█▎        | 65/500 [44:56<4:49:40, 39.96s/it] 13%|█▎        | 66/500 [45:46<5:10:02, 42.86s/it]Epoch: 66 done with learning rate 9.66E-03, Train loss: 9.34E+05, Train scatter: [0.935  0.1723 0.544  0.9954]
L1 regularization loss: 5.27E+00, L2 regularization loss: 2.24E+00
Test scatter: [0.9194 0.1685 0.5354 0.985 ], Lowest was [0.2705 0.0757 0.3512 0.523 ]
Median for last 10 epochs: [0.9196 0.1689 0.5355 0.985 ], Epochs since improvement 14
 13%|█▎        | 67/500 [46:19<4:48:22, 39.96s/it] 14%|█▎        | 68/500 [47:09<5:09:43, 43.02s/it]Epoch: 68 done with learning rate 9.80E-03, Train loss: 9.11E+05, Train scatter: [0.9351 0.1696 0.544  0.9955]
L1 regularization loss: 5.27E+00, L2 regularization loss: 2.27E+00
Test scatter: [0.9194 0.1659 0.5354 0.9852], Lowest was [0.2705 0.0757 0.3512 0.523 ]
Median for last 10 epochs: [0.9196 0.1689 0.5355 0.9851], Epochs since improvement 16
 14%|█▍        | 69/500 [47:41<4:44:57, 39.67s/it]Epoch: 70 done with learning rate 9.90E-03, Train loss: 8.85E+05, Train scatter: [0.9352 0.1557 0.544  0.9957]
L1 regularization loss: 5.27E+00, L2 regularization loss: 2.35E+00
Test scatter: [0.9196 0.1527 0.5354 0.9854], Lowest was [0.2705 0.0757 0.3512 0.523 ]
Median for last 10 epochs: [0.9196 0.1685 0.5354 0.9851], Epochs since improvement 18
 14%|█▍        | 70/500 [48:36<5:17:54, 44.36s/it] 14%|█▍        | 71/500 [49:10<4:53:32, 41.05s/it] 14%|█▍        | 72/500 [50:01<5:14:44, 44.12s/it]Epoch: 72 done with learning rate 9.96E-03, Train loss: 8.62E+05, Train scatter: [0.9353 0.1493 0.5439 0.9957]
L1 regularization loss: 5.26E+00, L2 regularization loss: 2.39E+00
Test scatter: [0.9197 0.1467 0.5353 0.9853], Lowest was [0.2705 0.0757 0.3512 0.523 ]
Median for last 10 epochs: [0.9196 0.1659 0.5354 0.9852], Epochs since improvement 20
 15%|█▍        | 73/500 [50:34<4:50:40, 40.84s/it] 15%|█▍        | 73/500 [51:25<5:00:47, 42.26s/it]
Epoch: 74 done with learning rate 1.00E-02, Train loss: 8.41E+05, Train scatter: [0.9353 0.1459 0.5437 0.9957]
L1 regularization loss: 5.25E+00, L2 regularization loss: 2.42E+00
Test scatter: [0.9197 0.1437 0.5351 0.9854], Lowest was [0.2705 0.0757 0.3512 0.523 ]
Median for last 10 epochs: [0.9196 0.1527 0.5354 0.9853], Epochs since improvement 22
Exited after 74 epochs due to early stopping
3085.34 seconds spent training, 6.171 seconds per epoch. Processed 11285 trees per second
[0.91967493 0.14372563 0.53508055 0.98532593]
{'epoch_exit': 73, 'scatter_m_star': 0.91967493, 'lowest_m_star': 0.2704701, 'last20_m_star': 0.9195889, 'last10_m_star': 0.9196375, 'scatter_v_disk': 0.14372563, 'lowest_v_disk': 0.075669035, 'last20_v_disk': 0.16868314, 'last10_v_disk': 0.15267158, 'scatter_m_cold': 0.53508055, 'lowest_m_cold': 0.35120234, 'last20_m_cold': 0.53546065, 'last10_m_cold': 0.5353894, 'scatter_sfr_100': 0.98532593, 'lowest_sfr_100': 0.5229856, 'last20_sfr_100': 0.98506105, 'last10_sfr_100': 0.9853391}
Finished 1/1
Exited training after None epochs
Experiment 1 done: 1 / 6
Exploring ['conv_layers', 'batch_size']
Currently doing [  1 512]
{'experiment': 'GraphMerge', 'group': 'meta_pysr', 'move': False, 'model': 'MetaEdge', 'log': True, 'run_params': {'n_epochs': 500, 'n_trials': 1, 'batch_size': 512, 'val_epoch': 2, 'early_stopping': True, 'patience': 20, 'l1_lambda': 0.0001, 'l2_lambda': 0.0001, 'loss_func': 'GaussNd', 'metrics': 'test_multi_varrho', 'performance_plot': 'multi_base', 'save': True, 'seed': True, 'num_workers': 4}, 'learn_params': {'learning_rate': 0.01, 'schedule': 'onecycle', 'g_up': 1, 'g_down': 0.95, 'warmup': 4, 'period': 5, 'eta_min': 1e-05}, 'hyper_params': {'hidden_channels': 128, 'conv_layers': 1, 'conv_activation': 'relu', 'decode_activation': 'leakyrelu', 'decode_layers': 2, 'layernorm': True, 'agg': 'sum', 'variance': True, 'rho': 0, 'in_channels': 43, 'out_channels': 4}, 'data_params': {'case': 'vlarge_all_4t_z0.0_quantile_raw', 'targets': [0, 1, 2, 3], 'del_feats': [], 'split': 0.8, 'test': 0, 'scale': 0}, 'experiment_name': 'con1bat512'}
/home/cj1223/../../scratch/gpfs/cj1223/GraphResults/results_meta_pysr_120322/MetaEdge_vlarge_all_4t_z0.0_quantile_raw_aegerf
RelU conv activation
LeakyRelU decode activation
N_params 298056
Made folder for saving model
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:28<3:56:03, 28.38s/it]  0%|          | 2/500 [01:13<5:15:56, 38.07s/it]Epoch: 2 done with learning rate 4.17E-04, Train loss: 2.36E+07, Train scatter: [0.9353 0.164  0.5442 0.9954]
L1 regularization loss: 1.51E+00, L2 regularization loss: 3.26E-01
Test scatter: [0.9197 0.1662 0.5356 0.9851], Lowest was [0.9197 0.1662 0.5356 0.9851]
Median for last 10 epochs: [0.9197 0.1662 0.5356 0.9851], Epochs since improvement 2
  1%|          | 3/500 [01:41<4:38:58, 33.68s/it]  1%|          | 4/500 [02:28<5:20:57, 38.83s/it]Epoch: 4 done with learning rate 4.67E-04, Train loss: 1.19E+07, Train scatter: [0.9353 0.1768 0.5441 0.9954]
L1 regularization loss: 1.52E+00, L2 regularization loss: 3.32E-01
Test scatter: [0.9197 0.1782 0.5355 0.9851], Lowest was [0.9197 0.1662 0.5355 0.9851]
Median for last 10 epochs: [0.9197 0.1722 0.5355 0.9851], Epochs since improvement 0
  1%|          | 5/500 [02:56<4:47:15, 34.82s/it]  1%|          | 6/500 [03:42<5:18:45, 38.72s/it]Epoch: 6 done with learning rate 5.51E-04, Train loss: 9.60E+06, Train scatter: [0.9352 0.1699 0.5442 0.9954]
L1 regularization loss: 1.53E+00, L2 regularization loss: 3.43E-01
Test scatter: [0.9196 0.1651 0.5356 0.9851], Lowest was [0.9196 0.1651 0.5355 0.9851]
Median for last 10 epochs: [0.9196 0.1651 0.5355 0.9851], Epochs since improvement 0
  1%|▏         | 7/500 [04:10<4:48:35, 35.12s/it]  2%|▏         | 8/500 [04:57<5:19:04, 38.91s/it]Epoch: 8 done with learning rate 6.67E-04, Train loss: 8.59E+06, Train scatter: [0.9352 0.1509 0.5441 0.995 ]
L1 regularization loss: 1.55E+00, L2 regularization loss: 3.65E-01
Test scatter: [0.9196 0.1438 0.5355 0.9846], Lowest was [0.9196 0.1438 0.5355 0.9846]
Median for last 10 epochs: [0.9196 0.1544 0.5355 0.9848], Epochs since improvement 0
  2%|▏         | 9/500 [05:24<4:49:38, 35.39s/it]Epoch: 10 done with learning rate 8.15E-04, Train loss: 7.72E+06, Train scatter: [0.9348 0.1318 0.544  0.7155]
L1 regularization loss: 1.58E+00, L2 regularization loss: 3.94E-01
Test scatter: [0.9192 0.1265 0.5354 0.7318], Lowest was [0.9192 0.1265 0.5354 0.7318]
Median for last 10 epochs: [0.9196 0.1438 0.5355 0.9846], Epochs since improvement 0
  2%|▏         | 10/500 [06:16<5:29:17, 40.32s/it]  2%|▏         | 11/500 [06:43<4:57:15, 36.47s/it]  2%|▏         | 12/500 [07:29<5:20:17, 39.38s/it]Epoch: 12 done with learning rate 9.94E-04, Train loss: 6.90E+06, Train scatter: [0.9299 0.123  0.5438 0.638 ]
L1 regularization loss: 1.60E+00, L2 regularization loss: 4.03E-01
Test scatter: [0.914  0.1182 0.5352 0.6316], Lowest was [0.914  0.1182 0.5352 0.6316]
Median for last 10 epochs: [0.9196 0.1438 0.5355 0.9846], Epochs since improvement 0
  3%|▎         | 13/500 [07:57<4:50:13, 35.76s/it]  3%|▎         | 14/500 [08:43<5:15:41, 38.97s/it]Epoch: 14 done with learning rate 1.20E-03, Train loss: 6.55E+06, Train scatter: [0.893  0.1153 0.5423 0.607 ]
L1 regularization loss: 1.61E+00, L2 regularization loss: 4.11E-01
Test scatter: [0.8784 0.1112 0.5337 0.6024], Lowest was [0.8784 0.1112 0.5337 0.6024]
Median for last 10 epochs: [0.9192 0.1265 0.5354 0.7318], Epochs since improvement 0
  3%|▎         | 15/500 [09:11<4:47:26, 35.56s/it]  3%|▎         | 16/500 [09:57<5:12:18, 38.72s/it]Epoch: 16 done with learning rate 1.44E-03, Train loss: 6.14E+06, Train scatter: [0.7632 0.1085 0.5398 0.5955]
L1 regularization loss: 1.62E+00, L2 regularization loss: 4.21E-01
Test scatter: [0.753  0.1057 0.5311 0.5879], Lowest was [0.753  0.1057 0.5311 0.5879]
Median for last 10 epochs: [0.914  0.1182 0.5352 0.6316], Epochs since improvement 0
  3%|▎         | 17/500 [10:25<4:44:56, 35.40s/it]  4%|▎         | 18/500 [11:10<5:08:55, 38.46s/it]Epoch: 18 done with learning rate 1.70E-03, Train loss: 5.76E+06, Train scatter: [0.573  0.1026 0.536  0.5719]
L1 regularization loss: 1.64E+00, L2 regularization loss: 4.31E-01
Test scatter: [0.5743 0.1011 0.5273 0.5688], Lowest was [0.5743 0.1011 0.5273 0.5688]
Median for last 10 epochs: [0.8784 0.1112 0.5337 0.6024], Epochs since improvement 0
  4%|▍         | 19/500 [11:38<4:41:50, 35.16s/it]Epoch: 20 done with learning rate 1.99E-03, Train loss: 5.39E+06, Train scatter: [0.4819 0.0987 0.5318 0.5575]
L1 regularization loss: 1.66E+00, L2 regularization loss: 4.42E-01
Test scatter: [0.5074 0.0974 0.5229 0.5553], Lowest was [0.5074 0.0974 0.5229 0.5553]
Median for last 10 epochs: [0.753  0.1057 0.5311 0.5879], Epochs since improvement 0
  4%|▍         | 20/500 [12:29<5:19:09, 39.90s/it]  4%|▍         | 21/500 [12:56<4:49:20, 36.24s/it]  4%|▍         | 22/500 [13:43<5:13:04, 39.30s/it]Epoch: 22 done with learning rate 2.30E-03, Train loss: 5.02E+06, Train scatter: [0.4761 0.0922 0.5285 0.5881]
L1 regularization loss: 1.67E+00, L2 regularization loss: 4.52E-01
Test scatter: [0.4837 0.0908 0.5191 0.5879], Lowest was [0.4837 0.0908 0.5191 0.5553]
Median for last 10 epochs: [0.5743 0.1011 0.5273 0.5879], Epochs since improvement 0
  5%|▍         | 23/500 [14:11<4:46:06, 35.99s/it]  5%|▍         | 24/500 [14:57<5:09:10, 38.97s/it]Epoch: 24 done with learning rate 2.63E-03, Train loss: 4.30E+06, Train scatter: [0.5546 0.0974 0.4871 0.5904]
L1 regularization loss: 1.69E+00, L2 regularization loss: 4.63E-01
Test scatter: [0.5729 0.0997 0.484  0.6011], Lowest was [0.4837 0.0908 0.484  0.5553]
Median for last 10 epochs: [0.5729 0.0997 0.5229 0.5879], Epochs since improvement 0
  5%|▌         | 25/500 [15:25<4:42:45, 35.72s/it]  5%|▌         | 26/500 [16:11<5:06:32, 38.80s/it]Epoch: 26 done with learning rate 2.98E-03, Train loss: 4.19E+06, Train scatter: [0.6026 0.1256 0.5438 0.783 ]
L1 regularization loss: 1.72E+00, L2 regularization loss: 4.76E-01
Test scatter: [0.6593 0.1244 0.5352 0.7838], Lowest was [0.4837 0.0908 0.484  0.5553]
Median for last 10 epochs: [0.5729 0.0997 0.5229 0.5879], Epochs since improvement 2
  5%|▌         | 27/500 [16:39<4:41:14, 35.68s/it]  6%|▌         | 28/500 [17:26<5:05:46, 38.87s/it]Epoch: 28 done with learning rate 3.34E-03, Train loss: 3.98E+06, Train scatter: [0.5145 0.0964 0.387  0.6267]
L1 regularization loss: 1.74E+00, L2 regularization loss: 4.91E-01
Test scatter: [0.5153 0.0966 0.3827 0.6253], Lowest was [0.4837 0.0908 0.3827 0.5553]
Median for last 10 epochs: [0.5153 0.0974 0.5191 0.6011], Epochs since improvement 0
  6%|▌         | 29/500 [17:54<4:38:58, 35.54s/it]Epoch: 30 done with learning rate 3.72E-03, Train loss: 2.07E+06, Train scatter: [0.504  0.0917 0.3626 0.588 ]
L1 regularization loss: 1.75E+00, L2 regularization loss: 4.97E-01
Test scatter: [0.5091 0.0936 0.3676 0.5841], Lowest was [0.4837 0.0908 0.3676 0.5553]
Median for last 10 epochs: [0.5153 0.0966 0.484  0.6011], Epochs since improvement 0
  6%|▌         | 30/500 [18:45<5:15:24, 40.27s/it]  6%|▌         | 31/500 [19:13<4:45:26, 36.52s/it]  6%|▋         | 32/500 [19:59<5:08:39, 39.57s/it]Epoch: 32 done with learning rate 4.10E-03, Train loss: 1.86E+06, Train scatter: [0.554  0.0878 0.3446 0.5764]
L1 regularization loss: 1.77E+00, L2 regularization loss: 5.07E-01
Test scatter: [0.5509 0.0886 0.3422 0.5719], Lowest was [0.4837 0.0886 0.3422 0.5553]
Median for last 10 epochs: [0.5509 0.0966 0.3827 0.6011], Epochs since improvement 0
  7%|▋         | 33/500 [20:27<4:40:46, 36.07s/it]  7%|▋         | 34/500 [21:14<5:04:49, 39.25s/it]Epoch: 34 done with learning rate 4.50E-03, Train loss: 1.56E+06, Train scatter: [0.4256 0.0839 0.327  0.5514]
L1 regularization loss: 1.80E+00, L2 regularization loss: 5.21E-01
Test scatter: [0.4713 0.0847 0.3316 0.5524], Lowest was [0.4713 0.0847 0.3316 0.5524]
Median for last 10 epochs: [0.5153 0.0936 0.3676 0.5841], Epochs since improvement 0
  7%|▋         | 35/500 [21:42<4:37:35, 35.82s/it]  7%|▋         | 36/500 [22:27<4:59:55, 38.78s/it]Epoch: 36 done with learning rate 4.90E-03, Train loss: 1.54E+06, Train scatter: [0.3545 0.0839 0.3338 0.5512]
L1 regularization loss: 1.83E+00, L2 regularization loss: 5.37E-01
Test scatter: [0.37   0.0859 0.3365 0.5512], Lowest was [0.37   0.0847 0.3316 0.5512]
Median for last 10 epochs: [0.5091 0.0886 0.3422 0.5719], Epochs since improvement 0
  7%|▋         | 37/500 [22:56<4:34:51, 35.62s/it]  8%|▊         | 38/500 [23:42<4:59:34, 38.91s/it]Epoch: 38 done with learning rate 5.30E-03, Train loss: 1.42E+06, Train scatter: [0.4255 0.0818 0.3603 0.5527]
L1 regularization loss: 1.86E+00, L2 regularization loss: 5.52E-01
Test scatter: [0.4266 0.0836 0.361  0.5536], Lowest was [0.37   0.0836 0.3316 0.5512]
Median for last 10 epochs: [0.4713 0.0859 0.3422 0.5536], Epochs since improvement 0
  8%|▊         | 39/500 [24:10<4:32:38, 35.48s/it]Epoch: 40 done with learning rate 5.70E-03, Train loss: 1.19E+06, Train scatter: [0.4246 0.0802 0.3132 0.5306]
L1 regularization loss: 1.88E+00, L2 regularization loss: 5.65E-01
Test scatter: [0.4239 0.0813 0.3154 0.5249], Lowest was [0.37   0.0813 0.3154 0.5249]
Median for last 10 epochs: [0.4266 0.0847 0.3365 0.5524], Epochs since improvement 0
  8%|▊         | 40/500 [25:02<5:10:13, 40.47s/it]  8%|▊         | 41/500 [25:31<4:42:50, 36.97s/it]  8%|▊         | 42/500 [26:18<5:06:29, 40.15s/it]Epoch: 42 done with learning rate 6.10E-03, Train loss: 1.24E+06, Train scatter: [0.3009 0.0836 0.3354 0.5361]
L1 regularization loss: 1.91E+00, L2 regularization loss: 5.81E-01
Test scatter: [0.3182 0.0845 0.3414 0.5335], Lowest was [0.3182 0.0813 0.3154 0.5249]
Median for last 10 epochs: [0.4239 0.0845 0.3365 0.5512], Epochs since improvement 0
  9%|▊         | 43/500 [26:46<4:37:18, 36.41s/it]  9%|▉         | 44/500 [27:33<5:01:56, 39.73s/it]Epoch: 44 done with learning rate 6.49E-03, Train loss: 1.23E+06, Train scatter: [0.3473 0.0812 0.321  0.5414]
L1 regularization loss: 1.94E+00, L2 regularization loss: 5.98E-01
Test scatter: [0.3599 0.0821 0.324  0.5314], Lowest was [0.3182 0.0813 0.3154 0.5249]
Median for last 10 epochs: [0.37   0.0836 0.3365 0.5335], Epochs since improvement 2
  9%|▉         | 45/500 [28:02<4:35:56, 36.39s/it]  9%|▉         | 46/500 [28:48<4:58:07, 39.40s/it]Epoch: 46 done with learning rate 6.87E-03, Train loss: 1.03E+06, Train scatter: [0.4194 0.0802 0.3025 0.5201]
L1 regularization loss: 1.97E+00, L2 regularization loss: 6.22E-01
Test scatter: [0.4239 0.0808 0.3028 0.5154], Lowest was [0.3182 0.0808 0.3028 0.5154]
Median for last 10 epochs: [0.4239 0.0821 0.324  0.5314], Epochs since improvement 0
  9%|▉         | 47/500 [29:16<4:30:38, 35.85s/it] 10%|▉         | 48/500 [30:03<4:54:40, 39.12s/it]Epoch: 48 done with learning rate 7.24E-03, Train loss: 1.03E+06, Train scatter: [0.2953 0.0775 0.3006 0.5172]
L1 regularization loss: 1.99E+00, L2 regularization loss: 6.37E-01
Test scatter: [0.3009 0.0789 0.3108 0.5185], Lowest was [0.3009 0.0789 0.3028 0.5154]
Median for last 10 epochs: [0.3599 0.0813 0.3154 0.5249], Epochs since improvement 0
 10%|▉         | 49/500 [30:31<4:28:40, 35.74s/it]Epoch: 50 done with learning rate 7.60E-03, Train loss: 9.74E+05, Train scatter: [0.4031 0.077  0.3119 0.523 ]
L1 regularization loss: 2.01E+00, L2 regularization loss: 6.52E-01
Test scatter: [0.4049 0.079  0.3165 0.5229], Lowest was [0.3009 0.0789 0.3028 0.5154]
Median for last 10 epochs: [0.3599 0.0808 0.3165 0.5229], Epochs since improvement 2
 10%|█         | 50/500 [31:25<5:11:07, 41.48s/it] 10%|█         | 51/500 [31:53<4:39:29, 37.35s/it] 10%|█         | 52/500 [32:40<5:00:12, 40.21s/it]Epoch: 52 done with learning rate 7.94E-03, Train loss: 9.06E+05, Train scatter: [0.2873 0.0774 0.3149 0.5277]
L1 regularization loss: 2.03E+00, L2 regularization loss: 6.71E-01
Test scatter: [0.2907 0.0782 0.319  0.5221], Lowest was [0.2907 0.0782 0.3028 0.5154]
Median for last 10 epochs: [0.3599 0.079  0.3165 0.5221], Epochs since improvement 0
 11%|█         | 53/500 [33:08<4:32:59, 36.64s/it] 11%|█         | 54/500 [33:55<4:54:37, 39.64s/it]Epoch: 54 done with learning rate 8.26E-03, Train loss: 9.12E+05, Train scatter: [0.3046 0.0782 0.3048 0.5211]
L1 regularization loss: 2.05E+00, L2 regularization loss: 6.89E-01
Test scatter: [0.3139 0.0848 0.3149 0.5249], Lowest was [0.2907 0.0782 0.3028 0.5154]
Median for last 10 epochs: [0.3139 0.079  0.3149 0.5221], Epochs since improvement 2
 11%|█         | 55/500 [34:23<4:28:30, 36.20s/it] 11%|█         | 56/500 [35:09<4:49:44, 39.15s/it]Epoch: 56 done with learning rate 8.56E-03, Train loss: 8.45E+05, Train scatter: [0.2656 0.0718 0.3032 0.5072]
L1 regularization loss: 2.06E+00, L2 regularization loss: 7.06E-01
Test scatter: [0.2745 0.0748 0.3121 0.507 ], Lowest was [0.2745 0.0748 0.3028 0.507 ]
Median for last 10 epochs: [0.3009 0.0789 0.3149 0.5221], Epochs since improvement 0
 11%|█▏        | 57/500 [35:37<4:24:37, 35.84s/it] 12%|█▏        | 58/500 [36:25<4:49:30, 39.30s/it]Epoch: 58 done with learning rate 8.83E-03, Train loss: 7.99E+05, Train scatter: [0.2729 0.0713 0.2923 0.5144]
L1 regularization loss: 2.08E+00, L2 regularization loss: 7.25E-01
Test scatter: [0.28   0.0729 0.2979 0.5099], Lowest was [0.2745 0.0729 0.2979 0.507 ]
Median for last 10 epochs: [0.2907 0.0782 0.3149 0.5221], Epochs since improvement 0
 12%|█▏        | 59/500 [36:53<4:24:49, 36.03s/it]Epoch: 60 done with learning rate 9.08E-03, Train loss: 8.01E+05, Train scatter: [0.2837 0.0712 0.2886 0.4997]
L1 regularization loss: 2.10E+00, L2 regularization loss: 7.45E-01
Test scatter: [0.2903 0.0722 0.2989 0.4944], Lowest was [0.2745 0.0722 0.2979 0.4944]
Median for last 10 epochs: [0.2903 0.0748 0.3121 0.5099], Epochs since improvement 0
 12%|█▏        | 60/500 [37:46<5:01:26, 41.10s/it] 12%|█▏        | 61/500 [38:14<4:31:36, 37.12s/it] 12%|█▏        | 62/500 [39:01<4:52:56, 40.13s/it]Epoch: 62 done with learning rate 9.31E-03, Train loss: 7.06E+05, Train scatter: [0.2915 0.0699 0.2773 0.4947]
L1 regularization loss: 2.12E+00, L2 regularization loss: 7.70E-01
Test scatter: [0.2943 0.0721 0.2873 0.4941], Lowest was [0.2745 0.0721 0.2873 0.4941]
Median for last 10 epochs: [0.2903 0.0729 0.2989 0.507 ], Epochs since improvement 0
 13%|█▎        | 63/500 [39:29<4:26:20, 36.57s/it] 13%|█▎        | 64/500 [40:16<4:48:09, 39.66s/it]Epoch: 64 done with learning rate 9.50E-03, Train loss: 6.46E+05, Train scatter: [0.3332 0.0891 0.312  0.5274]
L1 regularization loss: 2.14E+00, L2 regularization loss: 7.92E-01
Test scatter: [0.3485 0.0947 0.3252 0.5363], Lowest was [0.2745 0.0721 0.2873 0.4941]
Median for last 10 epochs: [0.2903 0.0729 0.2989 0.507 ], Epochs since improvement 2
 13%|█▎        | 65/500 [40:44<4:21:44, 36.10s/it] 13%|█▎        | 66/500 [41:32<4:47:42, 39.78s/it]Epoch: 66 done with learning rate 9.66E-03, Train loss: 6.35E+05, Train scatter: [0.2893 0.0692 0.2808 0.4923]
L1 regularization loss: 2.17E+00, L2 regularization loss: 8.19E-01
Test scatter: [0.2984 0.071  0.2849 0.4927], Lowest was [0.2745 0.071  0.2849 0.4927]
Median for last 10 epochs: [0.2943 0.0722 0.2979 0.4944], Epochs since improvement 0
 13%|█▎        | 67/500 [42:01<4:22:25, 36.36s/it] 14%|█▎        | 68/500 [42:48<4:45:50, 39.70s/it]Epoch: 68 done with learning rate 9.80E-03, Train loss: 5.77E+05, Train scatter: [0.2736 0.0705 0.2787 0.5088]
L1 regularization loss: 2.19E+00, L2 regularization loss: 8.42E-01
Test scatter: [0.2781 0.0736 0.2869 0.5088], Lowest was [0.2745 0.071  0.2849 0.4927]
Median for last 10 epochs: [0.2943 0.0722 0.2873 0.4944], Epochs since improvement 2
 14%|█▍        | 69/500 [43:17<4:20:59, 36.33s/it]Epoch: 70 done with learning rate 9.90E-03, Train loss: 1.60E+06, Train scatter: [0.3447 0.068  0.3715 0.4963]
L1 regularization loss: 2.28E+00, L2 regularization loss: 8.92E-01
Test scatter: [0.3486 0.069  0.3676 0.4949], Lowest was [0.2745 0.069  0.2849 0.4927]
Median for last 10 epochs: [0.2984 0.0721 0.2873 0.4949], Epochs since improvement 0
 14%|█▍        | 70/500 [44:09<4:54:12, 41.05s/it] 14%|█▍        | 71/500 [44:37<4:25:26, 37.12s/it] 14%|█▍        | 72/500 [45:23<4:45:02, 39.96s/it]Epoch: 72 done with learning rate 9.96E-03, Train loss: 5.25E+05, Train scatter: [0.3631 0.0679 0.3168 0.5636]
L1 regularization loss: 2.28E+00, L2 regularization loss: 9.14E-01
Test scatter: [0.353  0.0702 0.3102 0.561 ], Lowest was [0.2745 0.069  0.2849 0.4927]
Median for last 10 epochs: [0.3485 0.071  0.3102 0.5088], Epochs since improvement 2
 15%|█▍        | 73/500 [45:51<4:18:35, 36.34s/it] 15%|█▍        | 74/500 [46:38<4:40:35, 39.52s/it]Epoch: 74 done with learning rate 1.00E-02, Train loss: 1.37E+06, Train scatter: [0.263  0.068  0.3761 0.4973]
L1 regularization loss: 2.39E+00, L2 regularization loss: 9.99E-01
Test scatter: [0.2689 0.0678 0.3741 0.496 ], Lowest was [0.2689 0.0678 0.2849 0.4927]
Median for last 10 epochs: [0.2984 0.0702 0.3102 0.496 ], Epochs since improvement 0
 15%|█▌        | 75/500 [47:06<4:15:36, 36.08s/it] 15%|█▌        | 76/500 [47:54<4:39:15, 39.52s/it]Epoch: 76 done with learning rate 1.00E-02, Train loss: 4.72E+05, Train scatter: [0.2494 0.0653 0.2781 0.4882]
L1 regularization loss: 2.40E+00, L2 regularization loss: 1.02E+00
Test scatter: [0.2585 0.0668 0.2881 0.489 ], Lowest was [0.2585 0.0668 0.2849 0.489 ]
Median for last 10 epochs: [0.2781 0.069  0.3102 0.496 ], Epochs since improvement 0
 15%|█▌        | 77/500 [48:22<4:14:32, 36.11s/it] 16%|█▌        | 78/500 [49:09<4:37:36, 39.47s/it]Epoch: 78 done with learning rate 1.00E-02, Train loss: 2.19E+07, Train scatter: [0.9585 0.1725 0.5441 0.9944]
L1 regularization loss: 3.89E+00, L2 regularization loss: 1.82E+00
Test scatter: [0.9426 0.1687 0.5355 0.9841], Lowest was [0.2585 0.0668 0.2849 0.489 ]
Median for last 10 epochs: [0.3486 0.069  0.3676 0.496 ], Epochs since improvement 2
 16%|█▌        | 79/500 [49:37<4:11:57, 35.91s/it]Epoch: 80 done with learning rate 1.00E-02, Train loss: 6.92E+06, Train scatter: [0.9556 0.1717 0.544  0.9917]
L1 regularization loss: 3.89E+00, L2 regularization loss: 1.86E+00
Test scatter: [0.9399 0.1679 0.5354 0.9816], Lowest was [0.2585 0.0668 0.2849 0.489 ]
Median for last 10 epochs: [0.353  0.0702 0.3741 0.561 ], Epochs since improvement 4
 16%|█▌        | 80/500 [50:29<4:45:41, 40.81s/it] 16%|█▌        | 81/500 [50:57<4:17:29, 36.87s/it] 16%|█▋        | 82/500 [51:43<4:37:19, 39.81s/it]Epoch: 82 done with learning rate 9.99E-03, Train loss: 5.83E+06, Train scatter: [0.9519 0.1704 0.5439 0.9891]
L1 regularization loss: 3.89E+00, L2 regularization loss: 1.90E+00
Test scatter: [0.9365 0.1667 0.5353 0.9792], Lowest was [0.2585 0.0668 0.2849 0.489 ]
Median for last 10 epochs: [0.9365 0.1667 0.5353 0.9792], Epochs since improvement 6
 17%|█▋        | 83/500 [52:11<4:11:24, 36.17s/it] 17%|█▋        | 84/500 [52:58<4:33:10, 39.40s/it]Epoch: 84 done with learning rate 9.99E-03, Train loss: 5.24E+06, Train scatter: [0.9446 0.1688 0.5437 0.9834]
L1 regularization loss: 3.88E+00, L2 regularization loss: 1.93E+00
Test scatter: [0.9295 0.1652 0.5352 0.9738], Lowest was [0.2585 0.0668 0.2849 0.489 ]
Median for last 10 epochs: [0.9365 0.1667 0.5353 0.9792], Epochs since improvement 8
 17%|█▋        | 85/500 [53:26<4:09:48, 36.12s/it] 17%|█▋        | 86/500 [54:13<4:30:33, 39.21s/it]Epoch: 86 done with learning rate 9.98E-03, Train loss: 4.70E+06, Train scatter: [0.9329 0.1667 0.5433 0.9594]
L1 regularization loss: 3.89E+00, L2 regularization loss: 1.98E+00
Test scatter: [0.9181 0.1632 0.5348 0.9511], Lowest was [0.2585 0.0668 0.2849 0.489 ]
Median for last 10 epochs: [0.9365 0.1667 0.5353 0.9792], Epochs since improvement 10
 17%|█▋        | 87/500 [54:41<4:07:43, 35.99s/it] 18%|█▊        | 88/500 [55:28<4:29:34, 39.26s/it]Epoch: 88 done with learning rate 9.98E-03, Train loss: 3.95E+06, Train scatter: [0.9196 0.164  0.5418 0.791 ]
L1 regularization loss: 3.91E+00, L2 regularization loss: 2.09E+00
Test scatter: [0.9052 0.1606 0.5334 0.7875], Lowest was [0.2585 0.0668 0.2849 0.489 ]
Median for last 10 epochs: [0.9295 0.1652 0.5352 0.9738], Epochs since improvement 12
 18%|█▊        | 89/500 [55:56<4:05:32, 35.85s/it]Epoch: 90 done with learning rate 9.97E-03, Train loss: 2.97E+06, Train scatter: [0.9047 0.1602 0.5339 0.7333]
L1 regularization loss: 3.92E+00, L2 regularization loss: 2.19E+00
Test scatter: [0.8909 0.1571 0.5262 0.7293], Lowest was [0.2585 0.0668 0.2849 0.489 ]
Median for last 10 epochs: [0.9181 0.1632 0.5348 0.9511], Epochs since improvement 14
 18%|█▊        | 90/500 [56:48<4:38:32, 40.76s/it] 18%|█▊        | 91/500 [57:16<4:11:12, 36.85s/it] 18%|█▊        | 92/500 [58:03<4:31:17, 39.89s/it]Epoch: 92 done with learning rate 9.96E-03, Train loss: 2.19E+06, Train scatter: [0.8852 0.1551 0.5395 0.7321]
L1 regularization loss: 3.94E+00, L2 regularization loss: 2.32E+00
Test scatter: [0.872  0.1524 0.5313 0.7318], Lowest was [0.2585 0.0668 0.2849 0.489 ]
Median for last 10 epochs: [0.9052 0.1606 0.5334 0.7875], Epochs since improvement 16
 19%|█▊        | 93/500 [58:31<4:05:57, 36.26s/it] 19%|█▉        | 94/500 [59:17<4:26:11, 39.34s/it]Epoch: 94 done with learning rate 9.95E-03, Train loss: 1.61E+06, Train scatter: [0.8556 0.1414 0.517  0.6992]
L1 regularization loss: 3.96E+00, L2 regularization loss: 2.46E+00
Test scatter: [0.8431 0.139  0.5091 0.7048], Lowest was [0.2585 0.0668 0.2849 0.489 ]
Median for last 10 epochs: [0.8909 0.1571 0.5313 0.7318], Epochs since improvement 18
 19%|█▉        | 95/500 [59:46<4:03:09, 36.02s/it] 19%|█▉        | 96/500 [1:00:32<4:24:29, 39.28s/it]Epoch: 96 done with learning rate 9.94E-03, Train loss: 1.10E+06, Train scatter: [0.8036 0.1285 0.4893 0.6769]
L1 regularization loss: 3.97E+00, L2 regularization loss: 2.62E+00
Test scatter: [0.7921 0.1266 0.4842 0.6696], Lowest was [0.2585 0.0668 0.2849 0.489 ]
Median for last 10 epochs: [0.872  0.1524 0.5262 0.7293], Epochs since improvement 20
 19%|█▉        | 97/500 [1:01:01<4:01:40, 35.98s/it] 19%|█▉        | 97/500 [1:01:47<4:16:45, 38.23s/it]
Epoch: 98 done with learning rate 9.93E-03, Train loss: 8.60E+05, Train scatter: [0.7703 0.1232 0.4696 0.6538]
L1 regularization loss: 3.98E+00, L2 regularization loss: 2.75E+00
Test scatter: [0.7599 0.1219 0.4732 0.6502], Lowest was [0.2585 0.0668 0.2849 0.489 ]
Median for last 10 epochs: [0.8431 0.139  0.5091 0.7048], Epochs since improvement 22
Exited after 98 epochs due to early stopping
3707.95 seconds spent training, 7.416 seconds per epoch. Processed 9390 trees per second
[0.7598844  0.12193028 0.47314283 0.65022963]
{'epoch_exit': 97, 'scatter_m_star': 0.7598844, 'lowest_m_star': 0.2584901, 'last20_m_star': 0.89806736, 'last10_m_star': 0.8430691, 'scatter_v_disk': 0.12193028, 'lowest_v_disk': 0.06680727, 'last20_v_disk': 0.15889126, 'last10_v_disk': 0.1389684, 'scatter_m_cold': 0.47314283, 'lowest_m_cold': 0.28491232, 'last20_m_cold': 0.5323283, 'last10_m_cold': 0.5091029, 'scatter_sfr_100': 0.65022963, 'lowest_sfr_100': 0.48895267, 'last20_sfr_100': 0.7596314, 'last10_sfr_100': 0.70484984}
Finished 1/1
Exited training after None epochs
Experiment 2 done: 2 / 6
Exploring ['conv_layers', 'batch_size']
Currently doing [  2 256]
{'experiment': 'GraphMerge', 'group': 'meta_pysr', 'move': False, 'model': 'MetaEdge', 'log': True, 'run_params': {'n_epochs': 500, 'n_trials': 1, 'batch_size': 256, 'val_epoch': 2, 'early_stopping': True, 'patience': 20, 'l1_lambda': 0.0001, 'l2_lambda': 0.0001, 'loss_func': 'GaussNd', 'metrics': 'test_multi_varrho', 'performance_plot': 'multi_base', 'save': True, 'seed': True, 'num_workers': 4}, 'learn_params': {'learning_rate': 0.01, 'schedule': 'onecycle', 'g_up': 1, 'g_down': 0.95, 'warmup': 4, 'period': 5, 'eta_min': 1e-05}, 'hyper_params': {'hidden_channels': 128, 'conv_layers': 2, 'conv_activation': 'relu', 'decode_activation': 'leakyrelu', 'decode_layers': 2, 'layernorm': True, 'agg': 'sum', 'variance': True, 'rho': 0, 'in_channels': 43, 'out_channels': 4}, 'data_params': {'case': 'vlarge_all_4t_z0.0_quantile_raw', 'targets': [0, 1, 2, 3], 'del_feats': [], 'split': 0.8, 'test': 0, 'scale': 0}, 'experiment_name': 'con2bat256'}
/home/cj1223/../../scratch/gpfs/cj1223/GraphResults/results_meta_pysr_120322/MetaEdge_vlarge_all_4t_z0.0_quantile_raw_gumeax
RelU conv activation
LeakyRelU decode activation
N_params 422280
Made folder for saving model
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:47<6:34:54, 47.48s/it]  0%|          | 2/500 [02:00<8:40:03, 62.66s/it]Epoch: 2 done with learning rate 4.17E-04, Train loss: 2.29E+07, Train scatter: [0.9352 0.1363 0.5441 0.9954]
L1 regularization loss: 1.99E+00, L2 regularization loss: 3.83E-01
Test scatter: [0.9196 0.1336 0.5355 0.9851], Lowest was [0.9196 0.1336 0.5355 0.9851]
Median for last 10 epochs: [0.9196 0.1336 0.5355 0.9851], Epochs since improvement 2
  1%|          | 3/500 [02:48<7:42:21, 55.82s/it]  1%|          | 4/500 [04:00<8:35:36, 62.37s/it]Epoch: 4 done with learning rate 4.67E-04, Train loss: 1.48E+07, Train scatter: [0.932  0.1038 0.5439 0.9952]
L1 regularization loss: 2.01E+00, L2 regularization loss: 4.11E-01
Test scatter: [0.9163 0.1024 0.5354 0.9848], Lowest was [0.9163 0.1024 0.5354 0.9848]
Median for last 10 epochs: [0.9163 0.1024 0.5354 0.9848], Epochs since improvement 0
  1%|          | 5/500 [04:49<7:54:05, 57.47s/it]  1%|          | 6/500 [06:01<8:34:46, 62.52s/it]Epoch: 6 done with learning rate 5.51E-04, Train loss: 9.23E+06, Train scatter: [0.6316 0.088  0.5438 0.6397]
L1 regularization loss: 2.05E+00, L2 regularization loss: 4.53E-01
Test scatter: [0.6124 0.0874 0.5353 0.6368], Lowest was [0.6124 0.0874 0.5353 0.6368]
Median for last 10 epochs: [0.6124 0.0874 0.5353 0.6368], Epochs since improvement 0
  1%|▏         | 7/500 [06:51<7:58:02, 58.18s/it]  2%|▏         | 8/500 [08:02<8:30:36, 62.27s/it]Epoch: 8 done with learning rate 6.67E-04, Train loss: 5.52E+06, Train scatter: [0.4982 0.0774 0.5438 0.5615]
L1 regularization loss: 2.07E+00, L2 regularization loss: 4.69E-01
Test scatter: [0.4922 0.0776 0.5352 0.5569], Lowest was [0.4922 0.0776 0.5352 0.5569]
Median for last 10 epochs: [0.5523 0.0825 0.5352 0.5969], Epochs since improvement 0
  2%|▏         | 9/500 [08:49<7:52:27, 57.73s/it]Epoch: 10 done with learning rate 8.15E-04, Train loss: 4.13E+06, Train scatter: [0.3996 0.0743 0.5438 0.5384]
L1 regularization loss: 2.08E+00, L2 regularization loss: 4.78E-01
Test scatter: [0.4043 0.0736 0.5352 0.5344], Lowest was [0.4043 0.0736 0.5352 0.5344]
Median for last 10 epochs: [0.4922 0.0776 0.5352 0.5569], Epochs since improvement 0
  2%|▏         | 10/500 [10:10<8:49:32, 64.84s/it]  2%|▏         | 11/500 [10:59<8:08:35, 59.95s/it]  2%|▏         | 12/500 [12:12<8:38:28, 63.75s/it]Epoch: 12 done with learning rate 9.94E-04, Train loss: 3.64E+06, Train scatter: [0.2471 0.0768 0.5437 0.5373]
L1 regularization loss: 2.08E+00, L2 regularization loss: 4.85E-01
Test scatter: [0.2545 0.0757 0.5352 0.5288], Lowest was [0.2545 0.0736 0.5352 0.5288]
Median for last 10 epochs: [0.4922 0.0776 0.5352 0.5569], Epochs since improvement 0
  3%|▎         | 13/500 [13:00<8:00:15, 59.17s/it]  3%|▎         | 14/500 [14:12<8:30:13, 62.99s/it]Epoch: 14 done with learning rate 1.20E-03, Train loss: 3.46E+06, Train scatter: [0.2757 0.0748 0.5437 0.5267]
L1 regularization loss: 2.09E+00, L2 regularization loss: 4.90E-01
Test scatter: [0.281  0.0746 0.5351 0.5203], Lowest was [0.2545 0.0736 0.5351 0.5203]
Median for last 10 epochs: [0.4043 0.0757 0.5352 0.5344], Epochs since improvement 0
  3%|▎         | 15/500 [15:01<7:55:43, 58.85s/it]  3%|▎         | 16/500 [16:15<8:31:08, 63.36s/it]Epoch: 16 done with learning rate 1.44E-03, Train loss: 3.42E+06, Train scatter: [0.24   0.0699 0.5437 0.5133]
L1 regularization loss: 2.10E+00, L2 regularization loss: 4.95E-01
Test scatter: [0.2441 0.07   0.5351 0.5086], Lowest was [0.2441 0.07   0.5351 0.5086]
Median for last 10 epochs: [0.281  0.0746 0.5352 0.5288], Epochs since improvement 0
  3%|▎         | 17/500 [17:05<7:57:13, 59.28s/it]  4%|▎         | 18/500 [18:18<8:30:46, 63.58s/it]Epoch: 18 done with learning rate 1.70E-03, Train loss: 3.28E+06, Train scatter: [0.2258 0.0693 0.5436 0.515 ]
L1 regularization loss: 2.11E+00, L2 regularization loss: 4.99E-01
Test scatter: [0.2332 0.0694 0.535  0.5089], Lowest was [0.2332 0.0694 0.535  0.5086]
Median for last 10 epochs: [0.2545 0.0736 0.5351 0.5203], Epochs since improvement 0
  4%|▍         | 19/500 [19:08<7:55:16, 59.29s/it]Epoch: 20 done with learning rate 1.99E-03, Train loss: 3.20E+06, Train scatter: [0.2111 0.0672 0.5435 0.5178]
L1 regularization loss: 2.12E+00, L2 regularization loss: 5.04E-01
Test scatter: [0.2221 0.0682 0.535  0.5151], Lowest was [0.2221 0.0682 0.535  0.5086]
Median for last 10 epochs: [0.2441 0.07   0.5351 0.5151], Epochs since improvement 0
  4%|▍         | 20/500 [20:28<8:44:22, 65.55s/it]  4%|▍         | 21/500 [21:18<8:05:31, 60.82s/it]  4%|▍         | 22/500 [22:31<8:35:06, 64.66s/it]Epoch: 22 done with learning rate 2.30E-03, Train loss: 3.17E+06, Train scatter: [0.2095 0.0655 0.5434 0.4974]
L1 regularization loss: 2.13E+00, L2 regularization loss: 5.09E-01
Test scatter: [0.2178 0.0663 0.5349 0.492 ], Lowest was [0.2178 0.0663 0.5349 0.492 ]
Median for last 10 epochs: [0.2332 0.0694 0.535  0.5089], Epochs since improvement 0
  5%|▍         | 23/500 [23:21<7:58:01, 60.13s/it]  5%|▍         | 24/500 [24:33<8:26:27, 63.84s/it]Epoch: 24 done with learning rate 2.63E-03, Train loss: 3.13E+06, Train scatter: [0.2244 0.0657 0.5433 0.5187]
L1 regularization loss: 2.14E+00, L2 regularization loss: 5.15E-01
Test scatter: [0.2303 0.0666 0.5347 0.5199], Lowest was [0.2178 0.0663 0.5347 0.492 ]
Median for last 10 epochs: [0.2303 0.0682 0.535  0.5089], Epochs since improvement 0
  5%|▌         | 25/500 [25:22<7:49:42, 59.33s/it]  5%|▌         | 26/500 [26:36<8:23:34, 63.74s/it]Epoch: 26 done with learning rate 2.98E-03, Train loss: 3.10E+06, Train scatter: [0.1864 0.0619 0.5431 0.4913]
L1 regularization loss: 2.16E+00, L2 regularization loss: 5.21E-01
Test scatter: [0.1967 0.063  0.5346 0.4888], Lowest was [0.1967 0.063  0.5346 0.4888]
Median for last 10 epochs: [0.2221 0.0666 0.5349 0.5089], Epochs since improvement 0
  5%|▌         | 27/500 [27:26<7:50:09, 59.64s/it]  6%|▌         | 28/500 [28:38<8:17:10, 63.20s/it]Epoch: 28 done with learning rate 3.34E-03, Train loss: 3.07E+06, Train scatter: [0.2057 0.0624 0.543  0.5024]
L1 regularization loss: 2.19E+00, L2 regularization loss: 5.29E-01
Test scatter: [0.2122 0.063  0.5345 0.5017], Lowest was [0.1967 0.063  0.5345 0.4888]
Median for last 10 epochs: [0.2178 0.0663 0.5347 0.5017], Epochs since improvement 0
  6%|▌         | 29/500 [29:25<7:38:59, 58.47s/it]Epoch: 30 done with learning rate 3.72E-03, Train loss: 3.03E+06, Train scatter: [0.1877 0.0583 0.543  0.486 ]
L1 regularization loss: 2.20E+00, L2 regularization loss: 5.36E-01
Test scatter: [0.1975 0.0587 0.5345 0.4838], Lowest was [0.1967 0.0587 0.5345 0.4838]
Median for last 10 epochs: [0.2122 0.063  0.5346 0.492 ], Epochs since improvement 0
  6%|▌         | 30/500 [30:45<8:27:02, 64.73s/it]  6%|▌         | 31/500 [31:34<7:51:05, 60.27s/it]  6%|▋         | 32/500 [32:48<8:20:46, 64.20s/it]Epoch: 32 done with learning rate 4.10E-03, Train loss: 3.01E+06, Train scatter: [0.2821 0.0645 0.5428 0.4886]
L1 regularization loss: 2.22E+00, L2 regularization loss: 5.45E-01
Test scatter: [0.2798 0.0642 0.5343 0.4816], Lowest was [0.1967 0.0587 0.5343 0.4816]
Median for last 10 epochs: [0.2122 0.063  0.5345 0.4888], Epochs since improvement 0
  7%|▋         | 33/500 [33:38<7:45:53, 59.86s/it]  7%|▋         | 34/500 [34:52<8:18:44, 64.22s/it]Epoch: 34 done with learning rate 4.50E-03, Train loss: 2.98E+06, Train scatter: [0.3988 0.087  0.543  0.5735]
L1 regularization loss: 2.25E+00, L2 regularization loss: 5.56E-01
Test scatter: [0.3958 0.0875 0.5345 0.5739], Lowest was [0.1967 0.0587 0.5343 0.4816]
Median for last 10 epochs: [0.2122 0.063  0.5345 0.4888], Epochs since improvement 2
  7%|▋         | 35/500 [35:39<7:38:25, 59.15s/it]  7%|▋         | 36/500 [36:52<8:10:05, 63.37s/it]Epoch: 36 done with learning rate 4.90E-03, Train loss: 2.93E+06, Train scatter: [0.3184 0.0668 0.5426 0.4978]
L1 regularization loss: 2.27E+00, L2 regularization loss: 5.67E-01
Test scatter: [0.3125 0.0659 0.5341 0.4881], Lowest was [0.1967 0.0587 0.5341 0.4816]
Median for last 10 epochs: [0.2798 0.0642 0.5345 0.4881], Epochs since improvement 0
  7%|▋         | 37/500 [37:42<7:37:01, 59.23s/it]  8%|▊         | 38/500 [38:56<8:09:42, 63.60s/it]Epoch: 38 done with learning rate 5.30E-03, Train loss: 2.95E+06, Train scatter: [0.2134 0.0617 0.5421 0.4896]
L1 regularization loss: 2.32E+00, L2 regularization loss: 5.84E-01
Test scatter: [0.2224 0.0625 0.5336 0.4867], Lowest was [0.1967 0.0587 0.5336 0.4816]
Median for last 10 epochs: [0.2798 0.0642 0.5343 0.4867], Epochs since improvement 0
  8%|▊         | 39/500 [39:44<7:32:04, 58.84s/it]Epoch: 40 done with learning rate 5.70E-03, Train loss: 2.91E+06, Train scatter: [0.2008 0.0586 0.5409 0.4832]
L1 regularization loss: 2.33E+00, L2 regularization loss: 5.96E-01
Test scatter: [0.209  0.0599 0.5324 0.4806], Lowest was [0.1967 0.0587 0.5324 0.4806]
Median for last 10 epochs: [0.2798 0.0642 0.5341 0.4867], Epochs since improvement 0
  8%|▊         | 40/500 [41:02<8:16:53, 64.81s/it]  8%|▊         | 41/500 [41:51<7:39:46, 60.10s/it]  8%|▊         | 42/500 [43:05<8:10:09, 64.21s/it]Epoch: 42 done with learning rate 6.10E-03, Train loss: 3.12E+06, Train scatter: [0.9071 0.1256 0.544  0.8027]
L1 regularization loss: 2.51E+00, L2 regularization loss: 6.64E-01
Test scatter: [0.8936 0.1257 0.5355 0.8042], Lowest was [0.1967 0.0587 0.5324 0.4806]
Median for last 10 epochs: [0.3125 0.0659 0.5341 0.4881], Epochs since improvement 2
  9%|▊         | 43/500 [43:55<7:35:12, 59.77s/it]  9%|▉         | 44/500 [45:08<8:04:57, 63.81s/it]Epoch: 44 done with learning rate 6.49E-03, Train loss: 3.02E+06, Train scatter: [0.7078 0.1024 0.5439 0.6701]
L1 regularization loss: 2.52E+00, L2 regularization loss: 6.96E-01
Test scatter: [0.6989 0.1013 0.5353 0.6676], Lowest was [0.1967 0.0587 0.5324 0.4806]
Median for last 10 epochs: [0.3125 0.0659 0.5341 0.4881], Epochs since improvement 4
  9%|▉         | 45/500 [45:57<7:30:06, 59.35s/it]  9%|▉         | 46/500 [47:09<7:59:07, 63.32s/it]Epoch: 46 done with learning rate 6.87E-03, Train loss: 3.03E+06, Train scatter: [0.8279 0.0998 0.5439 0.7887]
L1 regularization loss: 2.62E+00, L2 regularization loss: 7.51E-01
Test scatter: [0.8182 0.1005 0.5353 0.7821], Lowest was [0.1967 0.0587 0.5324 0.4806]
Median for last 10 epochs: [0.6989 0.1005 0.5353 0.6676], Epochs since improvement 6
  9%|▉         | 47/500 [47:58<7:25:19, 58.98s/it] 10%|▉         | 48/500 [49:10<7:54:19, 62.96s/it]Epoch: 48 done with learning rate 7.24E-03, Train loss: 2.91E+06, Train scatter: [0.5695 0.0913 0.5437 0.5752]
L1 regularization loss: 2.60E+00, L2 regularization loss: 7.91E-01
Test scatter: [0.5548 0.0915 0.5351 0.5709], Lowest was [0.1967 0.0587 0.5324 0.4806]
Median for last 10 epochs: [0.6989 0.1005 0.5353 0.6676], Epochs since improvement 8
 10%|▉         | 49/500 [50:00<7:23:53, 59.05s/it]Epoch: 50 done with learning rate 7.60E-03, Train loss: 6.00E+06, Train scatter: [0.9456 0.1678 0.5441 0.9959]
L1 regularization loss: 3.29E+00, L2 regularization loss: 1.12E+00
Test scatter: [0.9295 0.1643 0.5355 0.9855], Lowest was [0.1967 0.0587 0.5324 0.4806]
Median for last 10 epochs: [0.8182 0.1013 0.5353 0.7821], Epochs since improvement 10
 10%|█         | 50/500 [51:19<8:07:04, 64.94s/it] 10%|█         | 51/500 [52:08<7:30:29, 60.20s/it] 10%|█         | 52/500 [53:22<7:59:49, 64.26s/it]Epoch: 52 done with learning rate 7.94E-03, Train loss: 4.65E+06, Train scatter: [0.9253 0.1658 0.5441 0.9956]
L1 regularization loss: 3.34E+00, L2 regularization loss: 1.22E+00
Test scatter: [0.9099 0.1625 0.5355 0.9852], Lowest was [0.1967 0.0587 0.5324 0.4806]
Median for last 10 epochs: [0.8182 0.1013 0.5353 0.7821], Epochs since improvement 12
 11%|█         | 53/500 [54:11<7:25:34, 59.81s/it] 11%|█         | 54/500 [55:23<7:51:18, 63.41s/it]Epoch: 54 done with learning rate 8.26E-03, Train loss: 4.08E+06, Train scatter: [0.934  0.1676 0.544  0.9927]
L1 regularization loss: 3.38E+00, L2 regularization loss: 1.28E+00
Test scatter: [0.9182 0.1636 0.5354 0.9821], Lowest was [0.1967 0.0587 0.5324 0.4806]
Median for last 10 epochs: [0.9099 0.1625 0.5354 0.9821], Epochs since improvement 14
 11%|█         | 55/500 [56:13<7:20:04, 59.34s/it] 11%|█         | 56/500 [57:26<7:49:25, 63.43s/it]Epoch: 56 done with learning rate 8.56E-03, Train loss: 3.88E+06, Train scatter: [0.919  0.1362 0.544  0.9741]
L1 regularization loss: 3.43E+00, L2 regularization loss: 1.41E+00
Test scatter: [0.9036 0.1333 0.5354 0.9632], Lowest was [0.1967 0.0587 0.5324 0.4806]
Median for last 10 epochs: [0.9099 0.1625 0.5354 0.9821], Epochs since improvement 16
 11%|█▏        | 57/500 [58:14<7:13:46, 58.75s/it] 12%|█▏        | 58/500 [59:29<7:48:01, 63.53s/it]Epoch: 58 done with learning rate 8.83E-03, Train loss: 3.76E+06, Train scatter: [0.9331 0.1157 0.544  0.7731]
L1 regularization loss: 3.50E+00, L2 regularization loss: 1.58E+00
Test scatter: [0.9176 0.1144 0.5354 0.7674], Lowest was [0.1967 0.0587 0.5324 0.4806]
Median for last 10 epochs: [0.9176 0.1625 0.5354 0.9821], Epochs since improvement 18
 12%|█▏        | 59/500 [1:00:17<7:13:29, 58.98s/it]Epoch: 60 done with learning rate 9.08E-03, Train loss: 3.69E+06, Train scatter: [0.9331 0.1084 0.5425 0.6947]
L1 regularization loss: 3.51E+00, L2 regularization loss: 1.62E+00
Test scatter: [0.9176 0.1074 0.534  0.689 ], Lowest was [0.1967 0.0587 0.5324 0.4806]
Median for last 10 epochs: [0.9176 0.1333 0.5354 0.9632], Epochs since improvement 20
 12%|█▏        | 60/500 [1:01:38<8:01:48, 65.70s/it] 12%|█▏        | 61/500 [1:02:27<7:23:49, 60.66s/it] 12%|█▏        | 62/500 [1:03:40<7:50:01, 64.39s/it]Epoch: 62 done with learning rate 9.31E-03, Train loss: 3.59E+06, Train scatter: [0.9303 0.1003 0.5381 0.6478]
L1 regularization loss: 3.50E+00, L2 regularization loss: 1.65E+00
Test scatter: [0.9147 0.0996 0.5295 0.646 ], Lowest was [0.1967 0.0587 0.5295 0.4806]
Median for last 10 epochs: [0.9176 0.1144 0.5354 0.7674], Epochs since improvement 0
 13%|█▎        | 63/500 [1:04:29<7:15:49, 59.84s/it] 13%|█▎        | 64/500 [1:05:42<7:42:01, 63.58s/it]Epoch: 64 done with learning rate 9.50E-03, Train loss: 3.48E+06, Train scatter: [0.9127 0.0971 0.534  0.6238]
L1 regularization loss: 3.51E+00, L2 regularization loss: 1.67E+00
Test scatter: [0.8972 0.0959 0.5255 0.6131], Lowest was [0.1967 0.0587 0.5255 0.4806]
Median for last 10 epochs: [0.9147 0.1074 0.534  0.689 ], Epochs since improvement 0
 13%|█▎        | 65/500 [1:06:29<7:04:51, 58.60s/it] 13%|█▎        | 66/500 [1:07:42<7:35:30, 62.97s/it]Epoch: 66 done with learning rate 9.66E-03, Train loss: 3.42E+06, Train scatter: [0.6106 0.1003 0.5366 0.6563]
L1 regularization loss: 3.61E+00, L2 regularization loss: 1.75E+00
Test scatter: [0.6045 0.1    0.528  0.6533], Lowest was [0.1967 0.0587 0.5255 0.4806]
Median for last 10 epochs: [0.9147 0.1    0.5295 0.6533], Epochs since improvement 2
 13%|█▎        | 67/500 [1:08:31<7:03:38, 58.70s/it] 14%|█▎        | 68/500 [1:09:43<7:31:36, 62.72s/it]Epoch: 68 done with learning rate 9.80E-03, Train loss: 3.34E+06, Train scatter: [0.5278 0.0941 0.532  0.6402]
L1 regularization loss: 3.59E+00, L2 regularization loss: 1.79E+00
Test scatter: [0.5169 0.0944 0.5241 0.6416], Lowest was [0.1967 0.0587 0.5241 0.4806]
Median for last 10 epochs: [0.8972 0.0996 0.528  0.646 ], Epochs since improvement 0
 14%|█▍        | 69/500 [1:10:32<7:00:51, 58.59s/it]Epoch: 70 done with learning rate 9.90E-03, Train loss: 3.35E+06, Train scatter: [0.6657 0.1129 0.5443 0.6853]
L1 regularization loss: 3.62E+00, L2 regularization loss: 1.84E+00
Test scatter: [0.6583 0.1128 0.5356 0.6858], Lowest was [0.1967 0.0587 0.5241 0.4806]
Median for last 10 epochs: [0.6583 0.0996 0.528  0.646 ], Epochs since improvement 2
 14%|█▍        | 70/500 [1:11:52<7:45:23, 64.94s/it] 14%|█▍        | 71/500 [1:12:38<7:05:45, 59.55s/it] 14%|█▍        | 72/500 [1:13:52<7:33:49, 63.62s/it]Epoch: 72 done with learning rate 9.96E-03, Train loss: 3.47E+06, Train scatter: [0.5537 0.0942 0.5415 0.6487]
L1 regularization loss: 3.65E+00, L2 regularization loss: 1.92E+00
Test scatter: [0.5536 0.0946 0.5332 0.6573], Lowest was [0.1967 0.0587 0.5241 0.4806]
Median for last 10 epochs: [0.6045 0.0959 0.528  0.6533], Epochs since improvement 4
 15%|█▍        | 73/500 [1:14:40<7:00:34, 59.10s/it] 15%|█▍        | 74/500 [1:15:53<7:28:22, 63.15s/it]Epoch: 74 done with learning rate 1.00E-02, Train loss: 4.49E+06, Train scatter: [0.9357 0.1741 0.5441 0.9948]
L1 regularization loss: 4.47E+00, L2 regularization loss: 2.51E+00
Test scatter: [0.9201 0.17   0.5355 0.9846], Lowest was [0.1967 0.0587 0.5241 0.4806]
Median for last 10 epochs: [0.6045 0.1    0.5332 0.6573], Epochs since improvement 6
 15%|█▌        | 75/500 [1:16:41<6:56:28, 58.80s/it] 15%|█▌        | 76/500 [1:17:56<7:29:59, 63.68s/it]Epoch: 76 done with learning rate 1.00E-02, Train loss: 3.69E+06, Train scatter: [0.9318 0.1226 0.544  0.9872]
L1 regularization loss: 4.44E+00, L2 regularization loss: 2.61E+00
Test scatter: [0.9164 0.1232 0.5355 0.9773], Lowest was [0.1967 0.0587 0.5241 0.4806]
Median for last 10 epochs: [0.6583 0.1128 0.5355 0.6858], Epochs since improvement 8
 15%|█▌        | 77/500 [1:18:46<6:58:58, 59.43s/it] 16%|█▌        | 78/500 [1:19:59<7:26:20, 63.46s/it]Epoch: 78 done with learning rate 1.00E-02, Train loss: 3.47E+06, Train scatter: [0.9318 0.1137 0.544  0.943 ]
L1 regularization loss: 4.43E+00, L2 regularization loss: 2.78E+00
Test scatter: [0.9163 0.1146 0.5354 0.9351], Lowest was [0.1967 0.0587 0.5241 0.4806]
Median for last 10 epochs: [0.9163 0.1146 0.5355 0.9351], Epochs since improvement 10
 16%|█▌        | 79/500 [1:20:47<6:52:25, 58.78s/it]Epoch: 80 done with learning rate 1.00E-02, Train loss: 3.22E+06, Train scatter: [0.9288 0.1036 0.5436 0.6909]
L1 regularization loss: 4.43E+00, L2 regularization loss: 2.94E+00
Test scatter: [0.9135 0.1034 0.535  0.7025], Lowest was [0.1967 0.0587 0.5241 0.4806]
Median for last 10 epochs: [0.9163 0.1146 0.5354 0.9351], Epochs since improvement 12
 16%|█▌        | 80/500 [1:22:08<7:38:26, 65.49s/it] 16%|█▌        | 81/500 [1:22:57<7:02:35, 60.51s/it] 16%|█▋        | 82/500 [1:24:10<7:28:11, 64.33s/it]Epoch: 82 done with learning rate 9.99E-03, Train loss: 2.65E+06, Train scatter: [0.8995 0.1074 0.4887 0.734 ]
L1 regularization loss: 4.45E+00, L2 regularization loss: 3.18E+00
Test scatter: [0.8843 0.1045 0.4829 0.715 ], Lowest was [0.1967 0.0587 0.4829 0.4806]
Median for last 10 epochs: [0.9163 0.1146 0.5354 0.9351], Epochs since improvement 0
 17%|█▋        | 83/500 [1:24:59<6:54:48, 59.68s/it] 17%|█▋        | 84/500 [1:26:13<7:24:04, 64.05s/it]Epoch: 84 done with learning rate 9.99E-03, Train loss: 1.52E+06, Train scatter: [0.7038 0.0947 0.3732 0.6825]
L1 regularization loss: 4.58E+00, L2 regularization loss: 3.75E+00
Test scatter: [0.7024 0.0946 0.3782 0.6742], Lowest was [0.1967 0.0587 0.3782 0.4806]
Median for last 10 epochs: [0.9135 0.1045 0.535  0.715 ], Epochs since improvement 0
 17%|█▋        | 85/500 [1:27:02<6:52:15, 59.60s/it] 17%|█▋        | 86/500 [1:28:14<7:16:50, 63.31s/it]Epoch: 86 done with learning rate 9.98E-03, Train loss: 9.16E+05, Train scatter: [0.5756 0.0811 0.3251 0.6266]
L1 regularization loss: 4.61E+00, L2 regularization loss: 3.96E+00
Test scatter: [0.5735 0.0832 0.3359 0.6239], Lowest was [0.1967 0.0587 0.3359 0.4806]
Median for last 10 epochs: [0.8843 0.1034 0.4829 0.7025], Epochs since improvement 0
 17%|█▋        | 87/500 [1:29:03<6:45:18, 58.88s/it] 18%|█▊        | 88/500 [1:30:16<7:13:56, 63.20s/it]Epoch: 88 done with learning rate 9.98E-03, Train loss: 7.97E+05, Train scatter: [0.5566 0.0715 0.3122 0.6167]
L1 regularization loss: 4.65E+00, L2 regularization loss: 4.23E+00
Test scatter: [0.5597 0.0717 0.3196 0.6105], Lowest was [0.1967 0.0587 0.3196 0.4806]
Median for last 10 epochs: [0.7024 0.0946 0.3782 0.6742], Epochs since improvement 0
 18%|█▊        | 89/500 [1:31:04<6:42:31, 58.76s/it]Epoch: 90 done with learning rate 9.97E-03, Train loss: 1.02E+06, Train scatter: [0.4432 0.0876 0.3712 0.5836]
L1 regularization loss: 4.70E+00, L2 regularization loss: 4.45E+00
Test scatter: [0.4399 0.0891 0.3762 0.5776], Lowest was [0.1967 0.0587 0.3196 0.4806]
Median for last 10 epochs: [0.5735 0.0891 0.3762 0.6239], Epochs since improvement 2
 18%|█▊        | 90/500 [1:32:23<7:21:41, 64.64s/it] 18%|█▊        | 91/500 [1:33:13<6:51:06, 60.31s/it] 18%|█▊        | 92/500 [1:34:27<7:17:55, 64.40s/it]Epoch: 92 done with learning rate 9.96E-03, Train loss: 7.90E+05, Train scatter: [0.4878 0.0639 0.314  0.5251]
L1 regularization loss: 4.66E+00, L2 regularization loss: 4.61E+00
Test scatter: [0.4638 0.065  0.3197 0.5204], Lowest was [0.1967 0.0587 0.3196 0.4806]
Median for last 10 epochs: [0.5597 0.0832 0.3359 0.6105], Epochs since improvement 4
 19%|█▊        | 93/500 [1:35:15<6:42:31, 59.34s/it] 19%|█▉        | 94/500 [1:36:29<7:11:31, 63.77s/it]Epoch: 94 done with learning rate 9.95E-03, Train loss: 6.11E+05, Train scatter: [0.4122 0.0581 0.2915 0.5058]
L1 regularization loss: 4.56E+00, L2 regularization loss: 4.68E+00
Test scatter: [0.4054 0.06   0.3065 0.5077], Lowest was [0.1967 0.0587 0.3065 0.4806]
Median for last 10 epochs: [0.4638 0.0717 0.3197 0.5776], Epochs since improvement 0
 19%|█▉        | 95/500 [1:37:16<6:38:12, 58.99s/it] 19%|█▉        | 96/500 [1:38:28<7:01:41, 62.63s/it]Epoch: 96 done with learning rate 9.94E-03, Train loss: 5.74E+05, Train scatter: [0.4007 0.0623 0.2912 0.496 ]
L1 regularization loss: 4.49E+00, L2 regularization loss: 4.72E+00
Test scatter: [0.394  0.0646 0.304  0.4957], Lowest was [0.1967 0.0587 0.304  0.4806]
Median for last 10 epochs: [0.4399 0.065  0.3196 0.5204], Epochs since improvement 0
 19%|█▉        | 97/500 [1:39:17<6:34:15, 58.70s/it] 20%|█▉        | 98/500 [1:40:31<7:03:36, 63.22s/it]Epoch: 98 done with learning rate 9.93E-03, Train loss: 5.16E+05, Train scatter: [0.529  0.0636 0.3247 0.527 ]
L1 regularization loss: 4.44E+00, L2 regularization loss: 4.77E+00
Test scatter: [0.531  0.0661 0.3323 0.5394], Lowest was [0.1967 0.0587 0.304  0.4806]
Median for last 10 epochs: [0.4399 0.065  0.3197 0.5204], Epochs since improvement 2
 20%|█▉        | 99/500 [1:41:20<6:34:07, 58.97s/it]Epoch: 100 done with learning rate 9.91E-03, Train loss: 4.78E+05, Train scatter: [0.4205 0.0562 0.2763 0.4899]
L1 regularization loss: 4.46E+00, L2 regularization loss: 4.88E+00
Test scatter: [0.4171 0.0578 0.2844 0.4896], Lowest was [0.1967 0.0578 0.2844 0.4806]
Median for last 10 epochs: [0.4171 0.0646 0.3065 0.5077], Epochs since improvement 0
 20%|██        | 100/500 [1:42:38<7:10:38, 64.60s/it] 20%|██        | 101/500 [1:43:28<6:40:27, 60.22s/it] 20%|██        | 102/500 [1:44:40<7:03:31, 63.85s/it]Epoch: 102 done with learning rate 9.90E-03, Train loss: 4.67E+05, Train scatter: [0.3988 0.0552 0.2712 0.4846]
L1 regularization loss: 4.38E+00, L2 regularization loss: 4.89E+00
Test scatter: [0.395  0.0563 0.2805 0.4852], Lowest was [0.1967 0.0563 0.2805 0.4806]
Median for last 10 epochs: [0.4054 0.06   0.304  0.4957], Epochs since improvement 0
 21%|██        | 103/500 [1:45:30<6:35:17, 59.74s/it] 21%|██        | 104/500 [1:46:42<6:57:31, 63.26s/it]Epoch: 104 done with learning rate 9.89E-03, Train loss: 4.11E+05, Train scatter: [0.6847 0.0569 0.2699 0.4922]
L1 regularization loss: 4.49E+00, L2 regularization loss: 4.99E+00
Test scatter: [0.6726 0.0576 0.2755 0.4941], Lowest was [0.1967 0.0563 0.2755 0.4806]
Median for last 10 epochs: [0.4171 0.0578 0.2844 0.4941], Epochs since improvement 0
 21%|██        | 105/500 [1:47:31<6:28:07, 58.96s/it] 21%|██        | 106/500 [1:48:44<6:56:36, 63.44s/it]Epoch: 106 done with learning rate 9.87E-03, Train loss: 2.86E+05, Train scatter: [0.4448 0.06   0.2713 0.481 ]
L1 regularization loss: 4.41E+00, L2 regularization loss: 5.03E+00
Test scatter: [0.4353 0.0601 0.2845 0.4797], Lowest was [0.1967 0.0563 0.2755 0.4797]
Median for last 10 epochs: [0.4353 0.0578 0.2844 0.4896], Epochs since improvement 0
 21%|██▏       | 107/500 [1:49:33<6:26:23, 58.99s/it] 22%|██▏       | 108/500 [1:50:45<6:51:36, 63.00s/it]Epoch: 108 done with learning rate 9.85E-03, Train loss: 3.05E+05, Train scatter: [0.4266 0.053  0.2633 0.4789]
L1 regularization loss: 4.40E+00, L2 regularization loss: 5.10E+00
Test scatter: [0.4153 0.0541 0.2741 0.4773], Lowest was [0.1967 0.0541 0.2741 0.4773]
Median for last 10 epochs: [0.4171 0.0576 0.2805 0.4852], Epochs since improvement 0
 22%|██▏       | 109/500 [1:51:34<6:23:19, 58.82s/it]Epoch: 110 done with learning rate 9.83E-03, Train loss: 2.63E+05, Train scatter: [0.3992 0.0514 0.2542 0.4694]
L1 regularization loss: 4.35E+00, L2 regularization loss: 5.13E+00
Test scatter: [0.3903 0.0523 0.2632 0.4675], Lowest was [0.1967 0.0523 0.2632 0.4675]
Median for last 10 epochs: [0.4153 0.0563 0.2755 0.4797], Epochs since improvement 0
 22%|██▏       | 110/500 [1:52:54<7:03:37, 65.17s/it] 22%|██▏       | 111/500 [1:53:44<6:32:38, 60.56s/it] 22%|██▏       | 112/500 [1:54:57<6:55:53, 64.31s/it]Epoch: 112 done with learning rate 9.81E-03, Train loss: 1.97E+05, Train scatter: [0.4008 0.0544 0.259  0.4626]
L1 regularization loss: 4.39E+00, L2 regularization loss: 5.21E+00
Test scatter: [0.3915 0.0555 0.264  0.4612], Lowest was [0.1967 0.0523 0.2632 0.4612]
Median for last 10 epochs: [0.4153 0.0555 0.2741 0.4773], Epochs since improvement 0
 23%|██▎       | 113/500 [1:55:47<6:26:28, 59.92s/it] 23%|██▎       | 114/500 [1:56:59<6:48:10, 63.45s/it]Epoch: 114 done with learning rate 9.79E-03, Train loss: 1.90E+05, Train scatter: [0.4275 0.0501 0.2455 0.4609]
L1 regularization loss: 4.42E+00, L2 regularization loss: 5.27E+00
Test scatter: [0.417  0.051  0.256  0.4615], Lowest was [0.1967 0.051  0.256  0.4612]
Median for last 10 epochs: [0.4153 0.0541 0.264  0.4675], Epochs since improvement 0
 23%|██▎       | 115/500 [1:57:47<6:17:32, 58.84s/it] 23%|██▎       | 116/500 [1:58:58<6:39:37, 62.44s/it]Epoch: 116 done with learning rate 9.77E-03, Train loss: 1.68E+05, Train scatter: [0.4915 0.0539 0.2602 0.4742]
L1 regularization loss: 4.36E+00, L2 regularization loss: 5.30E+00
Test scatter: [0.4783 0.0533 0.2675 0.4711], Lowest was [0.1967 0.051  0.256  0.4612]
Median for last 10 epochs: [0.4153 0.0533 0.264  0.4675], Epochs since improvement 2
 23%|██▎       | 117/500 [1:59:46<6:11:26, 58.19s/it] 24%|██▎       | 118/500 [2:00:57<6:35:18, 62.09s/it]Epoch: 118 done with learning rate 9.75E-03, Train loss: 1.51E+05, Train scatter: [0.4904 0.0651 0.3928 0.4821]
L1 regularization loss: 4.32E+00, L2 regularization loss: 5.36E+00
Test scatter: [0.478  0.064  0.3925 0.4754], Lowest was [0.1967 0.051  0.256  0.4612]
Median for last 10 epochs: [0.417  0.0533 0.264  0.4675], Epochs since improvement 4
 24%|██▍       | 119/500 [2:01:44<6:05:39, 57.58s/it]Epoch: 120 done with learning rate 9.73E-03, Train loss: 1.24E+05, Train scatter: [0.3722 0.0523 0.2505 0.4841]
L1 regularization loss: 4.33E+00, L2 regularization loss: 5.42E+00
Test scatter: [0.3651 0.0535 0.2622 0.4858], Lowest was [0.1967 0.051  0.256  0.4612]
Median for last 10 epochs: [0.417  0.0535 0.264  0.4711], Epochs since improvement 6
 24%|██▍       | 120/500 [2:03:04<6:47:44, 64.38s/it] 24%|██▍       | 121/500 [2:03:53<6:16:20, 59.58s/it] 24%|██▍       | 122/500 [2:05:06<6:40:34, 63.58s/it]Epoch: 122 done with learning rate 9.70E-03, Train loss: 7.21E+04, Train scatter: [0.3709 0.0536 0.2761 0.4635]
L1 regularization loss: 4.28E+00, L2 regularization loss: 5.44E+00
Test scatter: [0.3609 0.0539 0.2863 0.4616], Lowest was [0.1967 0.051  0.256  0.4612]
Median for last 10 epochs: [0.417  0.0535 0.2675 0.4711], Epochs since improvement 8
 25%|██▍       | 123/500 [2:05:55<6:11:53, 59.19s/it] 25%|██▍       | 124/500 [2:07:08<6:38:23, 63.57s/it]Epoch: 124 done with learning rate 9.68E-03, Train loss: 5.71E+04, Train scatter: [0.4504 0.0549 0.2478 0.4641]
L1 regularization loss: 4.26E+00, L2 regularization loss: 5.50E+00
Test scatter: [0.4375 0.0543 0.2525 0.4641], Lowest was [0.1967 0.051  0.2525 0.4612]
Median for last 10 epochs: [0.4375 0.0539 0.2675 0.4711], Epochs since improvement 0
 25%|██▌       | 125/500 [2:07:57<6:08:45, 59.00s/it] 25%|██▌       | 126/500 [2:09:09<6:32:41, 63.00s/it]Epoch: 126 done with learning rate 9.65E-03, Train loss: 6.89E+02, Train scatter: [0.4116 0.0483 0.2441 0.4504]
L1 regularization loss: 4.27E+00, L2 regularization loss: 5.58E+00
Test scatter: [0.4004 0.048  0.2515 0.4454], Lowest was [0.1967 0.048  0.2515 0.4454]
Median for last 10 epochs: [0.4004 0.0539 0.2622 0.4641], Epochs since improvement 0
 25%|██▌       | 127/500 [2:09:56<6:02:10, 58.26s/it] 26%|██▌       | 128/500 [2:11:10<6:29:22, 62.80s/it]Epoch: 128 done with learning rate 9.62E-03, Train loss: -6.48E+04, Train scatter: [0.3848 0.0552 0.2299 0.4586]
L1 regularization loss: 4.25E+00, L2 regularization loss: 5.66E+00
Test scatter: [0.3815 0.0552 0.2348 0.457 ], Lowest was [0.1967 0.048  0.2348 0.4454]
Median for last 10 epochs: [0.3815 0.0539 0.2525 0.4616], Epochs since improvement 0
 26%|██▌       | 129/500 [2:11:58<6:02:10, 58.57s/it]Epoch: 130 done with learning rate 9.59E-03, Train loss: -1.61E+05, Train scatter: [0.4041 0.0475 0.2223 0.442 ]
L1 regularization loss: 4.25E+00, L2 regularization loss: 5.73E+00
Test scatter: [0.3954 0.0472 0.2308 0.4375], Lowest was [0.1967 0.0472 0.2308 0.4375]
Median for last 10 epochs: [0.3954 0.0539 0.2515 0.457 ], Epochs since improvement 0
 26%|██▌       | 130/500 [2:13:19<6:41:38, 65.13s/it] 26%|██▌       | 131/500 [2:14:07<6:10:05, 60.18s/it] 26%|██▋       | 132/500 [2:15:22<6:34:57, 64.40s/it]Epoch: 132 done with learning rate 9.56E-03, Train loss: -2.58E+05, Train scatter: [0.415  0.0461 0.2205 0.44  ]
L1 regularization loss: 4.30E+00, L2 regularization loss: 5.83E+00
Test scatter: [0.4028 0.0457 0.2262 0.4325], Lowest was [0.1967 0.0457 0.2262 0.4325]
Median for last 10 epochs: [0.4004 0.048  0.2348 0.4454], Epochs since improvement 0
 27%|██▋       | 133/500 [2:16:10<6:05:09, 59.70s/it] 27%|██▋       | 134/500 [2:17:24<6:29:13, 63.81s/it]Epoch: 134 done with learning rate 9.53E-03, Train loss: -3.19E+05, Train scatter: [0.4864 0.0473 0.2224 0.4609]
L1 regularization loss: 4.32E+00, L2 regularization loss: 5.93E+00
Test scatter: [0.4746 0.0469 0.2314 0.461 ], Lowest was [0.1967 0.0457 0.2262 0.4325]
Median for last 10 epochs: [0.4004 0.0472 0.2314 0.4454], Epochs since improvement 2
 27%|██▋       | 135/500 [2:18:12<5:59:31, 59.10s/it] 27%|██▋       | 136/500 [2:19:25<6:23:30, 63.22s/it]Epoch: 136 done with learning rate 9.50E-03, Train loss: -3.35E+05, Train scatter: [0.4089 0.0483 0.2158 0.4414]
L1 regularization loss: 4.78E+00, L2 regularization loss: 6.31E+00
Test scatter: [0.3961 0.0474 0.2229 0.4347], Lowest was [0.1967 0.0457 0.2229 0.4325]
Median for last 10 epochs: [0.3961 0.0472 0.2308 0.4375], Epochs since improvement 0
 27%|██▋       | 137/500 [2:20:12<5:54:06, 58.53s/it] 28%|██▊       | 138/500 [2:21:26<6:20:22, 63.05s/it]Epoch: 138 done with learning rate 9.47E-03, Train loss: -3.43E+05, Train scatter: [0.351  0.0518 0.2264 0.4399]
L1 regularization loss: 4.75E+00, L2 regularization loss: 6.39E+00
Test scatter: [0.3403 0.051  0.2308 0.4343], Lowest was [0.1967 0.0457 0.2229 0.4325]
Median for last 10 epochs: [0.3961 0.0472 0.2308 0.4347], Epochs since improvement 2
 28%|██▊       | 139/500 [2:22:13<5:50:10, 58.20s/it]Epoch: 140 done with learning rate 9.43E-03, Train loss: -3.66E+05, Train scatter: [0.3542 0.0435 0.2184 0.4248]
L1 regularization loss: 4.70E+00, L2 regularization loss: 6.48E+00
Test scatter: [0.3464 0.0432 0.2248 0.4169], Lowest was [0.1967 0.0432 0.2229 0.4169]
Median for last 10 epochs: [0.3961 0.0469 0.2262 0.4343], Epochs since improvement 0
 28%|██▊       | 140/500 [2:23:32<6:26:14, 64.37s/it] 28%|██▊       | 141/500 [2:24:20<5:57:22, 59.73s/it] 28%|██▊       | 142/500 [2:25:33<6:20:05, 63.70s/it]Epoch: 142 done with learning rate 9.40E-03, Train loss: -3.66E+05, Train scatter: [0.376  0.0478 0.2355 0.4415]
L1 regularization loss: 4.66E+00, L2 regularization loss: 6.48E+00
Test scatter: [0.3681 0.0472 0.2382 0.4368], Lowest was [0.1967 0.0432 0.2229 0.4169]
Median for last 10 epochs: [0.3681 0.0472 0.2308 0.4347], Epochs since improvement 2
 29%|██▊       | 143/500 [2:26:22<5:52:16, 59.20s/it] 29%|██▉       | 144/500 [2:27:36<6:17:41, 63.66s/it]Epoch: 144 done with learning rate 9.36E-03, Train loss: -3.87E+05, Train scatter: [0.2233 0.0474 0.22   0.4221]
L1 regularization loss: 4.56E+00, L2 regularization loss: 6.48E+00
Test scatter: [0.2142 0.0473 0.2249 0.4175], Lowest was [0.1967 0.0432 0.2229 0.4169]
Median for last 10 epochs: [0.3464 0.0473 0.2249 0.4343], Epochs since improvement 4
 29%|██▉       | 145/500 [2:28:24<5:48:47, 58.95s/it] 29%|██▉       | 146/500 [2:29:39<6:15:35, 63.66s/it]Epoch: 146 done with learning rate 9.33E-03, Train loss: -1.24E+05, Train scatter: [0.459  0.1093 0.5387 0.6096]
L1 regularization loss: 5.44E+00, L2 regularization loss: 7.66E+00
Test scatter: [0.4546 0.1074 0.5301 0.5998], Lowest was [0.1967 0.0432 0.2229 0.4169]
Median for last 10 epochs: [0.3464 0.0473 0.2308 0.4343], Epochs since improvement 6
 29%|██▉       | 147/500 [2:30:28<5:48:32, 59.24s/it] 30%|██▉       | 148/500 [2:31:40<6:09:40, 63.01s/it]Epoch: 148 done with learning rate 9.29E-03, Train loss: -1.99E+05, Train scatter: [0.3074 0.0925 0.543  0.5471]
L1 regularization loss: 5.35E+00, L2 regularization loss: 7.77E+00
Test scatter: [0.3048 0.0895 0.5344 0.5369], Lowest was [0.1967 0.0432 0.2229 0.4169]
Median for last 10 epochs: [0.3464 0.0473 0.2382 0.4368], Epochs since improvement 8
 30%|██▉       | 149/500 [2:32:28<5:42:21, 58.52s/it]Epoch: 150 done with learning rate 9.25E-03, Train loss: -2.59E+05, Train scatter: [0.2061 0.0624 0.5424 0.4962]
L1 regularization loss: 5.53E+00, L2 regularization loss: 7.90E+00
Test scatter: [0.1998 0.0613 0.5338 0.4861], Lowest was [0.1967 0.0432 0.2229 0.4169]
Median for last 10 epochs: [0.3048 0.0613 0.5301 0.4861], Epochs since improvement 10
 30%|███       | 150/500 [2:33:47<6:17:59, 64.80s/it] 30%|███       | 151/500 [2:34:35<5:47:52, 59.81s/it] 30%|███       | 152/500 [2:35:50<6:13:12, 64.35s/it]Epoch: 152 done with learning rate 9.21E-03, Train loss: -2.77E+05, Train scatter: [0.1847 0.0561 0.5409 0.4817]
L1 regularization loss: 5.52E+00, L2 regularization loss: 7.94E+00
Test scatter: [0.1789 0.0559 0.5324 0.4757], Lowest was [0.1789 0.0432 0.2229 0.4169]
Median for last 10 epochs: [0.2142 0.0613 0.5324 0.4861], Epochs since improvement 0
 31%|███       | 153/500 [2:36:37<5:42:04, 59.15s/it] 31%|███       | 154/500 [2:37:49<6:03:41, 63.07s/it]Epoch: 154 done with learning rate 9.17E-03, Train loss: -1.05E+05, Train scatter: [0.4033 0.0846 0.5344 0.5948]
L1 regularization loss: 6.35E+00, L2 regularization loss: 8.71E+00
Test scatter: [0.3937 0.0811 0.5259 0.5891], Lowest was [0.1789 0.0432 0.2229 0.4169]
Median for last 10 epochs: [0.3048 0.0811 0.5324 0.5369], Epochs since improvement 2
 31%|███       | 155/500 [2:38:37<5:35:41, 58.38s/it] 31%|███       | 156/500 [2:39:50<6:00:41, 62.91s/it]Epoch: 156 done with learning rate 9.13E-03, Train loss: -2.37E+05, Train scatter: [0.3489 0.0713 0.4682 0.4964]
L1 regularization loss: 6.35E+00, L2 regularization loss: 9.38E+00
Test scatter: [0.3399 0.0703 0.462  0.4946], Lowest was [0.1789 0.0432 0.2229 0.4169]
Median for last 10 epochs: [0.3048 0.0703 0.5324 0.4946], Epochs since improvement 4
 31%|███▏      | 157/500 [2:40:38<5:33:13, 58.29s/it] 32%|███▏      | 158/500 [2:41:50<5:56:02, 62.46s/it]Epoch: 158 done with learning rate 9.09E-03, Train loss: -2.84E+05, Train scatter: [0.22   0.0527 0.3509 0.4792]
L1 regularization loss: 6.23E+00, L2 regularization loss: 9.44E+00
Test scatter: [0.2111 0.0523 0.3527 0.4745], Lowest was [0.1789 0.0432 0.2229 0.4169]
Median for last 10 epochs: [0.2111 0.0613 0.5259 0.4861], Epochs since improvement 6
 32%|███▏      | 159/500 [2:42:38<5:29:35, 57.99s/it]Epoch: 160 done with learning rate 9.05E-03, Train loss: -3.20E+05, Train scatter: [0.1906 0.0663 0.3749 0.4804]
L1 regularization loss: 6.08E+00, L2 regularization loss: 9.33E+00
Test scatter: [0.1847 0.0642 0.3719 0.4728], Lowest was [0.1789 0.0432 0.2229 0.4169]
Median for last 10 epochs: [0.2111 0.0642 0.462  0.4757], Epochs since improvement 8
 32%|███▏      | 160/500 [2:43:57<6:05:43, 64.54s/it] 32%|███▏      | 161/500 [2:44:47<5:38:46, 59.96s/it] 32%|███▏      | 162/500 [2:45:59<5:58:01, 63.56s/it]Epoch: 162 done with learning rate 9.00E-03, Train loss: -3.41E+05, Train scatter: [0.1815 0.0499 0.2867 0.4758]
L1 regularization loss: 5.99E+00, L2 regularization loss: 9.26E+00
Test scatter: [0.1787 0.0489 0.2874 0.4719], Lowest was [0.1787 0.0432 0.2229 0.4169]
Median for last 10 epochs: [0.2111 0.0642 0.3719 0.4745], Epochs since improvement 0
 33%|███▎      | 163/500 [2:46:47<5:31:09, 58.96s/it] 33%|███▎      | 164/500 [2:47:58<5:50:39, 62.62s/it]Epoch: 164 done with learning rate 8.96E-03, Train loss: -3.54E+05, Train scatter: [0.1586 0.0497 0.294  0.4627]
L1 regularization loss: 5.89E+00, L2 regularization loss: 9.19E+00
Test scatter: [0.1543 0.0492 0.2942 0.4579], Lowest was [0.1543 0.0432 0.2229 0.4169]
Median for last 10 epochs: [0.1847 0.0523 0.3527 0.4728], Epochs since improvement 0
 33%|███▎      | 165/500 [2:48:47<5:26:36, 58.50s/it] 33%|███▎      | 166/500 [2:49:59<5:48:56, 62.69s/it]Epoch: 166 done with learning rate 8.91E-03, Train loss: -3.61E+05, Train scatter: [0.1498 0.0459 0.3677 0.4555]
L1 regularization loss: 5.84E+00, L2 regularization loss: 9.15E+00
Test scatter: [0.1476 0.0451 0.3581 0.451 ], Lowest was [0.1476 0.0432 0.2229 0.4169]
Median for last 10 epochs: [0.1787 0.0492 0.3527 0.4719], Epochs since improvement 0
 33%|███▎      | 167/500 [2:50:48<5:24:42, 58.51s/it] 34%|███▎      | 168/500 [2:52:02<5:49:04, 63.09s/it]Epoch: 168 done with learning rate 8.86E-03, Train loss: -2.59E+05, Train scatter: [0.3403 0.0625 0.4906 0.498 ]
L1 regularization loss: 6.56E+00, L2 regularization loss: 9.90E+00
Test scatter: [0.3309 0.0621 0.4824 0.4928], Lowest was [0.1476 0.0432 0.2229 0.4169]
Median for last 10 epochs: [0.1787 0.0492 0.3581 0.4719], Epochs since improvement 2
 34%|███▍      | 169/500 [2:52:50<5:22:35, 58.48s/it]Epoch: 170 done with learning rate 8.82E-03, Train loss: -3.51E+05, Train scatter: [0.1556 0.0475 0.2816 0.4441]
L1 regularization loss: 6.50E+00, L2 regularization loss: 9.92E+00
Test scatter: [0.1541 0.0473 0.2833 0.4393], Lowest was [0.1476 0.0432 0.2229 0.4169]
Median for last 10 epochs: [0.1543 0.0489 0.2942 0.4579], Epochs since improvement 4
 34%|███▍      | 170/500 [2:54:10<5:57:39, 65.03s/it] 34%|███▍      | 171/500 [2:54:58<5:28:34, 59.92s/it] 34%|███▍      | 172/500 [2:56:11<5:49:45, 63.98s/it]Epoch: 172 done with learning rate 8.77E-03, Train loss: -3.44E+05, Train scatter: [0.1682 0.0462 0.2739 0.4425]
L1 regularization loss: 6.47E+00, L2 regularization loss: 9.90E+00
Test scatter: [0.1652 0.046  0.2752 0.4351], Lowest was [0.1476 0.0432 0.2229 0.4169]
Median for last 10 epochs: [0.1543 0.0473 0.2942 0.451 ], Epochs since improvement 6
 35%|███▍      | 173/500 [2:56:59<5:21:50, 59.05s/it] 35%|███▍      | 174/500 [2:58:12<5:44:07, 63.34s/it]Epoch: 174 done with learning rate 8.72E-03, Train loss: -3.72E+05, Train scatter: [0.1401 0.0466 0.2766 0.4437]
L1 regularization loss: 6.47E+00, L2 regularization loss: 9.91E+00
Test scatter: [0.1364 0.046  0.2744 0.4374], Lowest was [0.1364 0.0432 0.2229 0.4169]
Median for last 10 epochs: [0.1541 0.046  0.2833 0.4393], Epochs since improvement 0
 35%|███▌      | 175/500 [2:59:00<5:17:16, 58.57s/it] 35%|███▌      | 176/500 [3:00:12<5:38:54, 62.76s/it]Epoch: 176 done with learning rate 8.67E-03, Train loss: -3.96E+05, Train scatter: [0.355  0.0551 0.2577 0.4946]
L1 regularization loss: 6.47E+00, L2 regularization loss: 9.86E+00
Test scatter: [0.343  0.0545 0.2582 0.4816], Lowest was [0.1364 0.0432 0.2229 0.4169]
Median for last 10 epochs: [0.1652 0.0473 0.2752 0.4393], Epochs since improvement 2
 35%|███▌      | 177/500 [3:01:00<5:14:07, 58.35s/it] 36%|███▌      | 178/500 [3:02:12<5:34:57, 62.41s/it]Epoch: 178 done with learning rate 8.62E-03, Train loss: -3.51E+05, Train scatter: [0.2139 0.0429 0.2359 0.4364]
L1 regularization loss: 6.75E+00, L2 regularization loss: 1.01E+01
Test scatter: [0.2029 0.0423 0.2371 0.429 ], Lowest was [0.1364 0.0423 0.2229 0.4169]
Median for last 10 epochs: [0.1652 0.046  0.2744 0.4374], Epochs since improvement 0
 36%|███▌      | 179/500 [3:02:59<5:09:31, 57.86s/it]Epoch: 180 done with learning rate 8.57E-03, Train loss: -3.77E+05, Train scatter: [0.366  0.0443 0.235  0.4361]
L1 regularization loss: 6.76E+00, L2 regularization loss: 1.01E+01
Test scatter: [0.3545 0.0436 0.2363 0.4253], Lowest was [0.1364 0.0423 0.2229 0.4169]
Median for last 10 epochs: [0.2029 0.046  0.2582 0.4351], Epochs since improvement 2
 36%|███▌      | 180/500 [3:04:17<5:40:39, 63.87s/it] 36%|███▌      | 181/500 [3:05:05<5:14:25, 59.14s/it] 36%|███▋      | 182/500 [3:06:19<5:36:29, 63.49s/it]Epoch: 182 done with learning rate 8.52E-03, Train loss: -3.96E+05, Train scatter: [0.2862 0.043  0.2291 0.4267]
L1 regularization loss: 6.75E+00, L2 regularization loss: 1.01E+01
Test scatter: [0.2727 0.0425 0.2302 0.416 ], Lowest was [0.1364 0.0423 0.2229 0.416 ]
Median for last 10 epochs: [0.2727 0.0436 0.2371 0.429 ], Epochs since improvement 0
 37%|███▋      | 183/500 [3:07:09<5:14:02, 59.44s/it] 37%|███▋      | 184/500 [3:08:23<5:35:49, 63.76s/it]Epoch: 184 done with learning rate 8.46E-03, Train loss: -3.25E+05, Train scatter: [0.3518 0.0491 0.2727 0.4422]
L1 regularization loss: 7.31E+00, L2 regularization loss: 1.04E+01
Test scatter: [0.3423 0.0486 0.2725 0.4376], Lowest was [0.1364 0.0423 0.2229 0.416 ]
Median for last 10 epochs: [0.3423 0.0436 0.2371 0.429 ], Epochs since improvement 2
 37%|███▋      | 185/500 [3:09:13<5:12:29, 59.52s/it] 37%|███▋      | 186/500 [3:10:26<5:32:50, 63.60s/it]Epoch: 186 done with learning rate 8.41E-03, Train loss: -4.08E+05, Train scatter: [0.1638 0.0484 0.2493 0.4373]
L1 regularization loss: 7.14E+00, L2 regularization loss: 1.04E+01
Test scatter: [0.1584 0.047  0.2493 0.4268], Lowest was [0.1364 0.0423 0.2229 0.416 ]
Median for last 10 epochs: [0.2727 0.0436 0.2371 0.4268], Epochs since improvement 4
 37%|███▋      | 187/500 [3:11:13<5:07:01, 58.86s/it] 38%|███▊      | 188/500 [3:12:25<5:26:23, 62.77s/it]Epoch: 188 done with learning rate 8.35E-03, Train loss: -4.23E+05, Train scatter: [0.3639 0.0425 0.2269 0.4256]
L1 regularization loss: 7.09E+00, L2 regularization loss: 1.03E+01
Test scatter: [0.351  0.0421 0.2288 0.4166], Lowest was [0.1364 0.0421 0.2229 0.416 ]
Median for last 10 epochs: [0.3423 0.0436 0.2363 0.4253], Epochs since improvement 0
 38%|███▊      | 189/500 [3:13:13<5:02:24, 58.34s/it]Epoch: 190 done with learning rate 8.30E-03, Train loss: -4.38E+05, Train scatter: [0.2392 0.0421 0.2355 0.4149]
L1 regularization loss: 7.07E+00, L2 regularization loss: 1.02E+01
Test scatter: [0.2241 0.0418 0.2374 0.4052], Lowest was [0.1364 0.0418 0.2229 0.4052]
Median for last 10 epochs: [0.2727 0.0425 0.2374 0.4166], Epochs since improvement 0
 38%|███▊      | 190/500 [3:14:34<5:36:26, 65.12s/it] 38%|███▊      | 191/500 [3:15:24<5:11:31, 60.49s/it] 38%|███▊      | 192/500 [3:16:36<5:27:35, 63.82s/it]Epoch: 192 done with learning rate 8.24E-03, Train loss: -4.14E+05, Train scatter: [0.1358 0.0398 0.2181 0.4239]
L1 regularization loss: 7.17E+00, L2 regularization loss: 1.03E+01
Test scatter: [0.1336 0.0393 0.2207 0.4134], Lowest was [0.1336 0.0393 0.2207 0.4052]
Median for last 10 epochs: [0.2241 0.0421 0.2374 0.4166], Epochs since improvement 0
 39%|███▊      | 193/500 [3:17:26<5:05:21, 59.68s/it] 39%|███▉      | 194/500 [3:18:38<5:24:34, 63.64s/it]Epoch: 194 done with learning rate 8.19E-03, Train loss: -4.47E+05, Train scatter: [0.1934 0.0422 0.2154 0.419 ]
L1 regularization loss: 7.13E+00, L2 regularization loss: 1.02E+01
Test scatter: [0.1899 0.0417 0.2163 0.4065], Lowest was [0.1336 0.0393 0.2163 0.4052]
Median for last 10 epochs: [0.1899 0.0418 0.2288 0.4134], Epochs since improvement 0
 39%|███▉      | 195/500 [3:19:28<5:01:53, 59.39s/it] 39%|███▉      | 196/500 [3:20:41<5:22:15, 63.60s/it]Epoch: 196 done with learning rate 8.13E-03, Train loss: -4.55E+05, Train scatter: [0.1365 0.0406 0.2266 0.4265]
L1 regularization loss: 7.09E+00, L2 regularization loss: 1.01E+01
Test scatter: [0.1338 0.04   0.2293 0.4164], Lowest was [0.1336 0.0393 0.2163 0.4052]
Median for last 10 epochs: [0.1899 0.0417 0.2288 0.4134], Epochs since improvement 2
 39%|███▉      | 197/500 [3:21:29<4:57:29, 58.91s/it] 40%|███▉      | 198/500 [3:22:43<5:18:33, 63.29s/it]Epoch: 198 done with learning rate 8.07E-03, Train loss: -4.56E+05, Train scatter: [0.1369 0.0395 0.2241 0.4095]
L1 regularization loss: 7.06E+00, L2 regularization loss: 1.01E+01
Test scatter: [0.1305 0.0389 0.2258 0.3981], Lowest was [0.1305 0.0389 0.2163 0.3981]
Median for last 10 epochs: [0.1338 0.04   0.2258 0.4065], Epochs since improvement 0
 40%|███▉      | 199/500 [3:23:31<4:55:09, 58.84s/it]Epoch: 200 done with learning rate 8.01E-03, Train loss: -4.61E+05, Train scatter: [0.1231 0.042  0.2117 0.4056]
L1 regularization loss: 7.07E+00, L2 regularization loss: 1.00E+01
Test scatter: [0.1252 0.0417 0.2138 0.3971], Lowest was [0.1252 0.0389 0.2138 0.3971]
Median for last 10 epochs: [0.1336 0.04   0.2207 0.4065], Epochs since improvement 0
 40%|████      | 200/500 [3:24:51<5:24:58, 65.00s/it] 40%|████      | 201/500 [3:25:40<5:00:10, 60.24s/it] 40%|████      | 202/500 [3:26:51<5:15:54, 63.60s/it]Epoch: 202 done with learning rate 7.95E-03, Train loss: -4.55E+05, Train scatter: [0.1643 0.0398 0.2074 0.403 ]
L1 regularization loss: 7.07E+00, L2 regularization loss: 9.98E+00
Test scatter: [0.1542 0.039  0.2102 0.3924], Lowest was [0.1252 0.0389 0.2102 0.3924]
Median for last 10 epochs: [0.1338 0.04   0.2163 0.3981], Epochs since improvement 0
 41%|████      | 203/500 [3:27:39<4:51:53, 58.97s/it] 41%|████      | 204/500 [3:28:52<5:10:31, 62.95s/it]Epoch: 204 done with learning rate 7.89E-03, Train loss: -4.74E+05, Train scatter: [0.1241 0.0391 0.2116 0.4491]
L1 regularization loss: 7.10E+00, L2 regularization loss: 9.93E+00
Test scatter: [0.1226 0.0385 0.2143 0.4401], Lowest was [0.1226 0.0385 0.2102 0.3924]
Median for last 10 epochs: [0.1305 0.039  0.2143 0.3981], Epochs since improvement 0
 41%|████      | 205/500 [3:29:39<4:46:19, 58.24s/it] 41%|████      | 206/500 [3:30:53<5:07:58, 62.85s/it]Epoch: 206 done with learning rate 7.83E-03, Train loss: -4.75E+05, Train scatter: [0.1191 0.0375 0.2065 0.4019]
L1 regularization loss: 7.12E+00, L2 regularization loss: 9.92E+00
Test scatter: [0.117  0.0373 0.2101 0.394 ], Lowest was [0.117  0.0373 0.2101 0.3924]
Median for last 10 epochs: [0.1252 0.0389 0.2138 0.3971], Epochs since improvement 0
 41%|████▏     | 207/500 [3:31:42<4:47:21, 58.84s/it] 42%|████▏     | 208/500 [3:32:54<5:04:57, 62.66s/it]Epoch: 208 done with learning rate 7.77E-03, Train loss: -4.68E+05, Train scatter: [0.1714 0.0397 0.2093 0.4001]
L1 regularization loss: 7.22E+00, L2 regularization loss: 1.00E+01
Test scatter: [0.1667 0.0396 0.2122 0.3916], Lowest was [0.117  0.0373 0.2101 0.3916]
Median for last 10 epochs: [0.1252 0.039  0.2122 0.394 ], Epochs since improvement 0
 42%|████▏     | 209/500 [3:33:42<4:43:17, 58.41s/it]Epoch: 210 done with learning rate 7.71E-03, Train loss: -4.59E+05, Train scatter: [0.2623 0.0575 0.2776 0.4582]
L1 regularization loss: 7.70E+00, L2 regularization loss: 1.02E+01
Test scatter: [0.2449 0.056  0.2726 0.4449], Lowest was [0.117  0.0373 0.2101 0.3916]
Median for last 10 epochs: [0.1542 0.039  0.2122 0.394 ], Epochs since improvement 2
 42%|████▏     | 210/500 [3:35:03<5:14:24, 65.05s/it] 42%|████▏     | 211/500 [3:35:51<4:49:33, 60.11s/it] 42%|████▏     | 212/500 [3:37:06<5:09:22, 64.45s/it]Epoch: 212 done with learning rate 7.65E-03, Train loss: -4.59E+05, Train scatter: [0.116  0.038  0.2089 0.395 ]
L1 regularization loss: 7.77E+00, L2 regularization loss: 1.04E+01
Test scatter: [0.1132 0.0375 0.2128 0.3861], Lowest was [0.1132 0.0373 0.2101 0.3861]
Median for last 10 epochs: [0.1226 0.0385 0.2128 0.394 ], Epochs since improvement 0
 43%|████▎     | 213/500 [3:37:55<4:46:40, 59.93s/it] 43%|████▎     | 214/500 [3:39:08<5:03:55, 63.76s/it]Epoch: 214 done with learning rate 7.58E-03, Train loss: -4.86E+05, Train scatter: [0.1205 0.038  0.207  0.3926]
L1 regularization loss: 7.67E+00, L2 regularization loss: 1.03E+01
Test scatter: [0.1192 0.0374 0.2103 0.3817], Lowest was [0.1132 0.0373 0.2101 0.3817]
Median for last 10 epochs: [0.1192 0.0375 0.2122 0.3916], Epochs since improvement 0
 43%|████▎     | 215/500 [3:39:56<4:40:03, 58.96s/it] 43%|████▎     | 216/500 [3:41:10<5:00:17, 63.44s/it]Epoch: 216 done with learning rate 7.52E-03, Train loss: -4.91E+05, Train scatter: [0.1171 0.0379 0.204  0.3919]
L1 regularization loss: 7.64E+00, L2 regularization loss: 1.03E+01
Test scatter: [0.1159 0.0375 0.2073 0.3831], Lowest was [0.1132 0.0373 0.2073 0.3817]
Median for last 10 epochs: [0.1192 0.0375 0.2122 0.3861], Epochs since improvement 0
 43%|████▎     | 217/500 [3:41:58<4:37:40, 58.87s/it] 44%|████▎     | 218/500 [3:43:12<4:57:53, 63.38s/it]Epoch: 218 done with learning rate 7.46E-03, Train loss: -4.91E+05, Train scatter: [0.1246 0.0376 0.2089 0.4022]
L1 regularization loss: 7.66E+00, L2 regularization loss: 1.03E+01
Test scatter: [0.1246 0.0373 0.2116 0.3917], Lowest was [0.1132 0.0373 0.2073 0.3817]
Median for last 10 epochs: [0.1192 0.0375 0.2116 0.3861], Epochs since improvement 2
 44%|████▍     | 219/500 [3:44:01<4:36:47, 59.10s/it]Epoch: 220 done with learning rate 7.39E-03, Train loss: -4.95E+05, Train scatter: [0.1164 0.037  0.205  0.3882]
L1 regularization loss: 7.69E+00, L2 regularization loss: 1.04E+01
Test scatter: [0.1147 0.0367 0.2091 0.3792], Lowest was [0.1132 0.0367 0.2073 0.3792]
Median for last 10 epochs: [0.1159 0.0374 0.2103 0.3831], Epochs since improvement 0
 44%|████▍     | 220/500 [3:45:21<5:06:04, 65.59s/it] 44%|████▍     | 221/500 [3:46:10<4:40:36, 60.35s/it] 44%|████▍     | 222/500 [3:47:23<4:58:12, 64.36s/it]Epoch: 222 done with learning rate 7.33E-03, Train loss: -4.96E+05, Train scatter: [0.1333 0.0384 0.2051 0.3963]
L1 regularization loss: 7.73E+00, L2 regularization loss: 1.04E+01
Test scatter: [0.1348 0.0378 0.208  0.3894], Lowest was [0.1132 0.0367 0.2073 0.3792]
Median for last 10 epochs: [0.1192 0.0374 0.2091 0.3831], Epochs since improvement 2
 45%|████▍     | 223/500 [3:48:13<4:36:33, 59.90s/it] 45%|████▍     | 224/500 [3:49:28<4:56:29, 64.45s/it]Epoch: 224 done with learning rate 7.26E-03, Train loss: -5.03E+05, Train scatter: [0.1183 0.041  0.2062 0.3859]
L1 regularization loss: 7.78E+00, L2 regularization loss: 1.04E+01
Test scatter: [0.1184 0.0401 0.2078 0.3759], Lowest was [0.1132 0.0367 0.2073 0.3759]
Median for last 10 epochs: [0.1184 0.0375 0.208  0.3831], Epochs since improvement 0
 45%|████▌     | 225/500 [3:50:17<4:34:00, 59.78s/it] 45%|████▌     | 226/500 [3:51:31<4:52:14, 63.99s/it]Epoch: 226 done with learning rate 7.20E-03, Train loss: -4.96E+05, Train scatter: [0.1115 0.0369 0.1998 0.3905]
L1 regularization loss: 7.96E+00, L2 regularization loss: 1.06E+01
Test scatter: [0.1126 0.0367 0.2042 0.3791], Lowest was [0.1126 0.0367 0.2042 0.3759]
Median for last 10 epochs: [0.1184 0.0373 0.208  0.3792], Epochs since improvement 0
 45%|████▌     | 227/500 [3:52:19<4:30:03, 59.36s/it] 46%|████▌     | 228/500 [3:53:31<4:46:29, 63.20s/it]Epoch: 228 done with learning rate 7.13E-03, Train loss: -5.10E+05, Train scatter: [0.1159 0.0363 0.199  0.3852]
L1 regularization loss: 7.92E+00, L2 regularization loss: 1.06E+01
Test scatter: [0.1135 0.0361 0.2039 0.3753], Lowest was [0.1126 0.0361 0.2039 0.3753]
Median for last 10 epochs: [0.1147 0.0367 0.2078 0.3791], Epochs since improvement 0
 46%|████▌     | 229/500 [3:54:21<4:27:44, 59.28s/it]Epoch: 230 done with learning rate 7.06E-03, Train loss: -5.06E+05, Train scatter: [0.1071 0.0352 0.197  0.3861]
L1 regularization loss: 8.05E+00, L2 regularization loss: 1.07E+01
Test scatter: [0.1081 0.0355 0.2014 0.3769], Lowest was [0.1081 0.0355 0.2014 0.3753]
Median for last 10 epochs: [0.1135 0.0367 0.2042 0.3769], Epochs since improvement 0
 46%|████▌     | 230/500 [3:55:41<4:53:56, 65.32s/it] 46%|████▌     | 231/500 [3:56:30<4:30:37, 60.36s/it] 46%|████▋     | 232/500 [3:57:44<4:48:08, 64.51s/it]Epoch: 232 done with learning rate 6.99E-03, Train loss: -4.77E+05, Train scatter: [0.8154 0.1322 0.5075 0.8292]
L1 regularization loss: 9.43E+00, L2 regularization loss: 1.13E+01
Test scatter: [0.8082 0.1312 0.4973 0.83  ], Lowest was [0.1081 0.0355 0.2014 0.3753]
Median for last 10 epochs: [0.1135 0.0367 0.2042 0.3769], Epochs since improvement 2
 47%|████▋     | 233/500 [3:58:32<4:25:26, 59.65s/it] 47%|████▋     | 234/500 [3:59:47<4:44:38, 64.20s/it]Epoch: 234 done with learning rate 6.93E-03, Train loss: -3.87E+05, Train scatter: [0.1492 0.0422 0.2242 0.4221]
L1 regularization loss: 1.05E+01, L2 regularization loss: 1.23E+01
Test scatter: [0.1497 0.0413 0.2254 0.4107], Lowest was [0.1081 0.0355 0.2014 0.3753]
Median for last 10 epochs: [0.1135 0.0367 0.2042 0.3791], Epochs since improvement 4
 47%|████▋     | 235/500 [4:00:35<4:21:35, 59.23s/it] 47%|████▋     | 236/500 [4:01:48<4:39:27, 63.51s/it]Epoch: 236 done with learning rate 6.86E-03, Train loss: -4.40E+05, Train scatter: [0.1225 0.0398 0.2189 0.4247]
L1 regularization loss: 1.05E+01, L2 regularization loss: 1.24E+01
Test scatter: [0.1195 0.0394 0.2217 0.4145], Lowest was [0.1081 0.0355 0.2014 0.3753]
Median for last 10 epochs: [0.1195 0.0394 0.2217 0.4107], Epochs since improvement 6
 47%|████▋     | 237/500 [4:02:36<4:18:29, 58.97s/it] 48%|████▊     | 238/500 [4:03:49<4:35:37, 63.12s/it]Epoch: 238 done with learning rate 6.79E-03, Train loss: -4.59E+05, Train scatter: [0.158  0.0391 0.209  0.3986]
L1 regularization loss: 1.05E+01, L2 regularization loss: 1.24E+01
Test scatter: [0.1502 0.0385 0.2114 0.3879], Lowest was [0.1081 0.0355 0.2014 0.3753]
Median for last 10 epochs: [0.1497 0.0394 0.2217 0.4107], Epochs since improvement 8
 48%|████▊     | 239/500 [4:04:37<4:14:01, 58.40s/it]Epoch: 240 done with learning rate 6.72E-03, Train loss: -4.68E+05, Train scatter: [0.1496 0.0381 0.2038 0.3923]
L1 regularization loss: 1.05E+01, L2 regularization loss: 1.25E+01
Test scatter: [0.1504 0.0376 0.2076 0.3835], Lowest was [0.1081 0.0355 0.2014 0.3753]
Median for last 10 epochs: [0.1502 0.0394 0.2217 0.4107], Epochs since improvement 10
 48%|████▊     | 240/500 [4:05:56<4:40:14, 64.67s/it] 48%|████▊     | 241/500 [4:06:45<4:18:59, 60.00s/it] 48%|████▊     | 242/500 [4:07:57<4:33:26, 63.59s/it]Epoch: 242 done with learning rate 6.65E-03, Train loss: -4.82E+05, Train scatter: [0.1151 0.0377 0.2021 0.3886]
L1 regularization loss: 1.05E+01, L2 regularization loss: 1.26E+01
Test scatter: [0.1377 0.0371 0.2053 0.3806], Lowest was [0.1081 0.0355 0.2014 0.3753]
Median for last 10 epochs: [0.1497 0.0385 0.2114 0.3879], Epochs since improvement 12
 49%|████▊     | 243/500 [4:08:45<4:12:37, 58.98s/it] 49%|████▉     | 244/500 [4:09:57<4:27:41, 62.74s/it]Epoch: 244 done with learning rate 6.58E-03, Train loss: -4.92E+05, Train scatter: [0.1432 0.0396 0.2099 0.3922]
L1 regularization loss: 1.05E+01, L2 regularization loss: 1.26E+01
Test scatter: [0.1475 0.0395 0.2142 0.3828], Lowest was [0.1081 0.0355 0.2014 0.3753]
Median for last 10 epochs: [0.1475 0.0385 0.2114 0.3835], Epochs since improvement 14
 49%|████▉     | 245/500 [4:10:45<4:07:53, 58.33s/it] 49%|████▉     | 246/500 [4:11:58<4:25:27, 62.71s/it]Epoch: 246 done with learning rate 6.51E-03, Train loss: -5.00E+05, Train scatter: [0.1249 0.036  0.1993 0.3823]
L1 regularization loss: 1.05E+01, L2 regularization loss: 1.27E+01
Test scatter: [0.1229 0.036  0.2034 0.3742], Lowest was [0.1081 0.0355 0.2014 0.3742]
Median for last 10 epochs: [0.1475 0.0376 0.2076 0.3828], Epochs since improvement 0
 49%|████▉     | 247/500 [4:12:45<4:05:27, 58.21s/it] 50%|████▉     | 248/500 [4:13:59<4:23:25, 62.72s/it]Epoch: 248 done with learning rate 6.44E-03, Train loss: -4.27E+05, Train scatter: [0.3352 0.1037 0.4747 0.5643]
L1 regularization loss: 1.12E+01, L2 regularization loss: 1.32E+01
Test scatter: [0.3132 0.0994 0.4701 0.5509], Lowest was [0.1081 0.0355 0.2014 0.3742]
Median for last 10 epochs: [0.1475 0.0376 0.2076 0.3828], Epochs since improvement 2
 50%|████▉     | 249/500 [4:14:48<4:05:30, 58.69s/it]Epoch: 250 done with learning rate 6.37E-03, Train loss: -4.44E+05, Train scatter: [0.1216 0.0399 0.2067 0.3966]
L1 regularization loss: 1.11E+01, L2 regularization loss: 1.33E+01
Test scatter: [0.1169 0.0391 0.2096 0.3854], Lowest was [0.1081 0.0355 0.2014 0.3742]
Median for last 10 epochs: [0.1377 0.0391 0.2096 0.3828], Epochs since improvement 4
 50%|█████     | 250/500 [4:16:07<4:29:43, 64.73s/it] 50%|█████     | 251/500 [4:16:54<4:06:42, 59.45s/it] 50%|█████     | 252/500 [4:18:09<4:24:58, 64.11s/it]Epoch: 252 done with learning rate 6.30E-03, Train loss: -4.78E+05, Train scatter: [0.1109 0.0393 0.2063 0.3884]
L1 regularization loss: 1.10E+01, L2 regularization loss: 1.33E+01
Test scatter: [0.1105 0.0387 0.2088 0.3783], Lowest was [0.1081 0.0355 0.2014 0.3742]
Median for last 10 epochs: [0.1229 0.0391 0.2096 0.3828], Epochs since improvement 6
 51%|█████     | 253/500 [4:18:59<4:06:03, 59.77s/it] 51%|█████     | 254/500 [4:20:10<4:19:08, 63.21s/it]Epoch: 254 done with learning rate 6.23E-03, Train loss: -4.90E+05, Train scatter: [0.1139 0.037  0.2021 0.3858]
L1 regularization loss: 1.09E+01, L2 regularization loss: 1.32E+01
Test scatter: [0.1164 0.0367 0.2054 0.3769], Lowest was [0.1081 0.0355 0.2014 0.3742]
Median for last 10 epochs: [0.1169 0.0387 0.2088 0.3783], Epochs since improvement 8
 51%|█████     | 255/500 [4:20:59<4:00:47, 58.97s/it] 51%|█████     | 256/500 [4:22:10<4:14:12, 62.51s/it]Epoch: 256 done with learning rate 6.15E-03, Train loss: -4.96E+05, Train scatter: [0.1106 0.0367 0.199  0.3829]
L1 regularization loss: 1.09E+01, L2 regularization loss: 1.32E+01
Test scatter: [0.1116 0.0365 0.203  0.3745], Lowest was [0.1081 0.0355 0.2014 0.3742]
Median for last 10 epochs: [0.1164 0.0387 0.2088 0.3783], Epochs since improvement 10
 51%|█████▏    | 257/500 [4:22:58<3:55:51, 58.24s/it] 52%|█████▏    | 258/500 [4:24:10<4:11:25, 62.34s/it]Epoch: 258 done with learning rate 6.08E-03, Train loss: -5.08E+05, Train scatter: [0.1054 0.0357 0.2008 0.3819]
L1 regularization loss: 1.09E+01, L2 regularization loss: 1.32E+01
Test scatter: [0.1054 0.0355 0.2049 0.3732], Lowest was [0.1054 0.0355 0.2014 0.3732]
Median for last 10 epochs: [0.1116 0.0367 0.2054 0.3769], Epochs since improvement 0
 52%|█████▏    | 259/500 [4:24:57<3:52:05, 57.78s/it]Epoch: 260 done with learning rate 6.01E-03, Train loss: -5.10E+05, Train scatter: [0.1169 0.0363 0.1963 0.3765]
L1 regularization loss: 1.08E+01, L2 regularization loss: 1.32E+01
Test scatter: [0.1198 0.0363 0.2002 0.3694], Lowest was [0.1054 0.0355 0.2002 0.3694]
Median for last 10 epochs: [0.1116 0.0365 0.2049 0.3745], Epochs since improvement 0
 52%|█████▏    | 260/500 [4:26:18<4:19:12, 64.80s/it] 52%|█████▏    | 261/500 [4:27:08<3:59:57, 60.24s/it] 52%|█████▏    | 262/500 [4:28:21<4:14:35, 64.18s/it]Epoch: 262 done with learning rate 5.94E-03, Train loss: -5.12E+05, Train scatter: [0.1045 0.0351 0.2014 0.3764]
L1 regularization loss: 1.09E+01, L2 regularization loss: 1.33E+01
Test scatter: [0.1058 0.0351 0.2052 0.3667], Lowest was [0.1054 0.0351 0.2002 0.3667]
Median for last 10 epochs: [0.1116 0.0363 0.2049 0.3732], Epochs since improvement 0
 53%|█████▎    | 263/500 [4:29:09<3:54:35, 59.39s/it] 53%|█████▎    | 264/500 [4:30:22<4:09:32, 63.44s/it]Epoch: 264 done with learning rate 5.86E-03, Train loss: -4.82E+05, Train scatter: [0.1033 0.0358 0.1973 0.3794]
L1 regularization loss: 1.10E+01, L2 regularization loss: 1.34E+01
Test scatter: [0.1043 0.0358 0.2007 0.3699], Lowest was [0.1043 0.0351 0.2002 0.3667]
Median for last 10 epochs: [0.1058 0.0358 0.203  0.3699], Epochs since improvement 0
 53%|█████▎    | 265/500 [4:31:10<3:50:04, 58.74s/it] 53%|█████▎    | 266/500 [4:32:23<4:05:46, 63.02s/it]Epoch: 266 done with learning rate 5.79E-03, Train loss: -5.11E+05, Train scatter: [0.1218 0.0357 0.2003 0.3815]
L1 regularization loss: 1.10E+01, L2 regularization loss: 1.34E+01
Test scatter: [0.1236 0.0357 0.2033 0.3744], Lowest was [0.1043 0.0351 0.2002 0.3667]
Median for last 10 epochs: [0.1058 0.0357 0.2033 0.3699], Epochs since improvement 2
 53%|█████▎    | 267/500 [4:33:11<3:47:20, 58.54s/it] 54%|█████▎    | 268/500 [4:34:27<4:06:05, 63.65s/it]Epoch: 268 done with learning rate 5.72E-03, Train loss: -5.25E+05, Train scatter: [0.1125 0.0349 0.1977 0.3731]
L1 regularization loss: 1.09E+01, L2 regularization loss: 1.34E+01
Test scatter: [0.1143 0.035  0.2022 0.3666], Lowest was [0.1043 0.035  0.2002 0.3666]
Median for last 10 epochs: [0.1143 0.0357 0.2022 0.3694], Epochs since improvement 0
 54%|█████▍    | 269/500 [4:35:16<3:48:09, 59.26s/it]Epoch: 270 done with learning rate 5.65E-03, Train loss: -5.29E+05, Train scatter: [0.1035 0.0346 0.194  0.3702]
L1 regularization loss: 1.09E+01, L2 regularization loss: 1.35E+01
Test scatter: [0.1058 0.0348 0.1987 0.363 ], Lowest was [0.1043 0.0348 0.1987 0.363 ]
Median for last 10 epochs: [0.1058 0.0351 0.2022 0.3667], Epochs since improvement 0
 54%|█████▍    | 270/500 [4:36:36<4:11:38, 65.64s/it] 54%|█████▍    | 271/500 [4:37:24<3:49:49, 60.21s/it] 54%|█████▍    | 272/500 [4:38:37<4:03:22, 64.05s/it]Epoch: 272 done with learning rate 5.57E-03, Train loss: -5.24E+05, Train scatter: [0.1057 0.0353 0.1953 0.3757]
L1 regularization loss: 1.09E+01, L2 regularization loss: 1.35E+01
Test scatter: [0.1091 0.0356 0.1999 0.3689], Lowest was [0.1043 0.0348 0.1987 0.363 ]
Median for last 10 epochs: [0.1091 0.0356 0.2007 0.3689], Epochs since improvement 2
 55%|█████▍    | 273/500 [4:39:26<3:45:16, 59.54s/it] 55%|█████▍    | 274/500 [4:40:38<3:59:05, 63.48s/it]Epoch: 274 done with learning rate 5.50E-03, Train loss: 1.23E+07, Train scatter: [0.7537 0.09   0.3686 0.5668]
L1 regularization loss: 1.32E+01, L2 regularization loss: 1.47E+01
Test scatter: [0.7326 0.0902 0.3631 0.5578], Lowest was [0.1043 0.0348 0.1987 0.363 ]
Median for last 10 epochs: [0.1143 0.0356 0.2022 0.3689], Epochs since improvement 4
 55%|█████▌    | 275/500 [4:41:27<3:41:20, 59.02s/it] 55%|█████▌    | 276/500 [4:42:40<3:56:28, 63.34s/it]Epoch: 276 done with learning rate 5.42E-03, Train loss: -2.99E+05, Train scatter: [0.2424 0.0653 0.2767 0.4782]
L1 regularization loss: 1.35E+01, L2 regularization loss: 1.51E+01
Test scatter: [0.2445 0.0653 0.2776 0.4693], Lowest was [0.1043 0.0348 0.1987 0.363 ]
Median for last 10 epochs: [0.1143 0.0356 0.2022 0.3689], Epochs since improvement 6
 55%|█████▌    | 277/500 [4:43:28<3:38:07, 58.69s/it] 56%|█████▌    | 278/500 [4:44:43<3:55:18, 63.60s/it]Epoch: 278 done with learning rate 5.35E-03, Train loss: -3.37E+05, Train scatter: [0.1772 0.0584 0.2533 0.4492]
L1 regularization loss: 1.35E+01, L2 regularization loss: 1.53E+01
Test scatter: [0.1783 0.0588 0.2535 0.4407], Lowest was [0.1043 0.0348 0.1987 0.363 ]
Median for last 10 epochs: [0.1783 0.0588 0.2535 0.4407], Epochs since improvement 8
 56%|█████▌    | 279/500 [4:45:31<3:36:06, 58.67s/it]Epoch: 280 done with learning rate 5.28E-03, Train loss: -3.55E+05, Train scatter: [0.1493 0.0536 0.2488 0.4372]
L1 regularization loss: 1.35E+01, L2 regularization loss: 1.54E+01
Test scatter: [0.1508 0.0533 0.251  0.4288], Lowest was [0.1043 0.0348 0.1987 0.363 ]
Median for last 10 epochs: [0.1783 0.0588 0.2535 0.4407], Epochs since improvement 10
 56%|█████▌    | 280/500 [4:46:50<3:57:52, 64.87s/it] 56%|█████▌    | 281/500 [4:47:38<3:38:28, 59.86s/it] 56%|█████▋    | 282/500 [4:48:51<3:51:45, 63.79s/it]Epoch: 282 done with learning rate 5.20E-03, Train loss: -3.66E+05, Train scatter: [0.1406 0.0519 0.2367 0.425 ]
L1 regularization loss: 1.35E+01, L2 regularization loss: 1.54E+01
Test scatter: [0.1401 0.0509 0.2391 0.4153], Lowest was [0.1043 0.0348 0.1987 0.363 ]
Median for last 10 epochs: [0.1783 0.0588 0.2535 0.4407], Epochs since improvement 12
 57%|█████▋    | 283/500 [4:49:41<3:35:34, 59.61s/it] 57%|█████▋    | 284/500 [4:50:54<3:49:26, 63.74s/it]Epoch: 284 done with learning rate 5.13E-03, Train loss: -3.75E+05, Train scatter: [0.1549 0.0499 0.2305 0.4224]
L1 regularization loss: 1.35E+01, L2 regularization loss: 1.55E+01
Test scatter: [0.1516 0.0486 0.2324 0.4124], Lowest was [0.1043 0.0348 0.1987 0.363 ]
Median for last 10 epochs: [0.1516 0.0533 0.251  0.4288], Epochs since improvement 14
 57%|█████▋    | 285/500 [4:51:43<3:31:50, 59.12s/it] 57%|█████▋    | 286/500 [4:52:57<3:47:05, 63.67s/it]Epoch: 286 done with learning rate 5.06E-03, Train loss: -3.82E+05, Train scatter: [0.1422 0.0488 0.2308 0.417 ]
L1 regularization loss: 1.35E+01, L2 regularization loss: 1.55E+01
Test scatter: [0.1395 0.0473 0.2325 0.4066], Lowest was [0.1043 0.0348 0.1987 0.363 ]
Median for last 10 epochs: [0.1508 0.0509 0.2391 0.4153], Epochs since improvement 16
 57%|█████▋    | 287/500 [4:53:46<3:31:06, 59.47s/it] 58%|█████▊    | 288/500 [4:55:00<3:45:30, 63.82s/it]Epoch: 288 done with learning rate 4.98E-03, Train loss: -3.91E+05, Train scatter: [0.1351 0.0484 0.2268 0.415 ]
L1 regularization loss: 1.35E+01, L2 regularization loss: 1.55E+01
Test scatter: [0.1355 0.0467 0.2271 0.4068], Lowest was [0.1043 0.0348 0.1987 0.363 ]
Median for last 10 epochs: [0.1401 0.0486 0.2325 0.4124], Epochs since improvement 18
 58%|█████▊    | 289/500 [4:55:49<3:28:20, 59.24s/it]Epoch: 290 done with learning rate 4.91E-03, Train loss: -3.98E+05, Train scatter: [0.1282 0.0464 0.2237 0.4094]
L1 regularization loss: 1.35E+01, L2 regularization loss: 1.56E+01
Test scatter: [0.1277 0.0451 0.2253 0.3988], Lowest was [0.1043 0.0348 0.1987 0.363 ]
Median for last 10 epochs: [0.1395 0.0473 0.2324 0.4068], Epochs since improvement 20
 58%|█████▊    | 290/500 [4:57:09<3:49:19, 65.52s/it] 58%|█████▊    | 291/500 [4:57:58<3:30:42, 60.49s/it] 58%|█████▊    | 291/500 [4:59:12<3:34:53, 61.69s/it]
Epoch: 292 done with learning rate 4.83E-03, Train loss: -4.04E+05, Train scatter: [0.1334 0.0453 0.2201 0.4058]
L1 regularization loss: 1.35E+01, L2 regularization loss: 1.56E+01
Test scatter: [0.133  0.0443 0.2215 0.3956], Lowest was [0.1043 0.0348 0.1987 0.363 ]
Median for last 10 epochs: [0.1355 0.0467 0.2271 0.4066], Epochs since improvement 22
Exited after 292 epochs due to early stopping
17952.11 seconds spent training, 35.904 seconds per epoch. Processed 1939 trees per second
[0.13297497 0.04431082 0.22149538 0.39554638]
{'epoch_exit': 291, 'scatter_m_star': 0.13297497, 'lowest_m_star': 0.10426694, 'last20_m_star': 0.14549313, 'last10_m_star': 0.13548148, 'scatter_v_disk': 0.044310816, 'lowest_v_disk': 0.034768403, 'last20_v_disk': 0.049744755, 'last10_v_disk': 0.04669148, 'scatter_m_cold': 0.22149538, 'lowest_m_cold': 0.19870473, 'last20_m_cold': 0.23579091, 'last10_m_cold': 0.22712633, 'scatter_sfr_100': 0.39554638, 'lowest_sfr_100': 0.3629627, 'last20_sfr_100': 0.41384757, 'last10_sfr_100': 0.40662366}
Finished 1/1
Exited training after None epochs
Experiment 3 done: 3 / 6
Exploring ['conv_layers', 'batch_size']
Currently doing [  2 512]
{'experiment': 'GraphMerge', 'group': 'meta_pysr', 'move': False, 'model': 'MetaEdge', 'log': True, 'run_params': {'n_epochs': 500, 'n_trials': 1, 'batch_size': 512, 'val_epoch': 2, 'early_stopping': True, 'patience': 20, 'l1_lambda': 0.0001, 'l2_lambda': 0.0001, 'loss_func': 'GaussNd', 'metrics': 'test_multi_varrho', 'performance_plot': 'multi_base', 'save': True, 'seed': True, 'num_workers': 4}, 'learn_params': {'learning_rate': 0.01, 'schedule': 'onecycle', 'g_up': 1, 'g_down': 0.95, 'warmup': 4, 'period': 5, 'eta_min': 1e-05}, 'hyper_params': {'hidden_channels': 128, 'conv_layers': 2, 'conv_activation': 'relu', 'decode_activation': 'leakyrelu', 'decode_layers': 2, 'layernorm': True, 'agg': 'sum', 'variance': True, 'rho': 0, 'in_channels': 43, 'out_channels': 4}, 'data_params': {'case': 'vlarge_all_4t_z0.0_quantile_raw', 'targets': [0, 1, 2, 3], 'del_feats': [], 'split': 0.8, 'test': 0, 'scale': 0}, 'experiment_name': 'con2bat512'}
/home/cj1223/../../scratch/gpfs/cj1223/GraphResults/results_meta_pysr_120322/MetaEdge_vlarge_all_4t_z0.0_quantile_raw_ygdjav
RelU conv activation
LeakyRelU decode activation
N_params 422280
Made folder for saving model
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:41<5:43:48, 41.34s/it]  0%|          | 2/500 [01:45<7:33:13, 54.60s/it]Epoch: 2 done with learning rate 4.17E-04, Train loss: 3.01E+07, Train scatter: [0.9352 0.1713 0.5441 0.9954]
L1 regularization loss: 1.98E+00, L2 regularization loss: 3.76E-01
Test scatter: [0.9196 0.1674 0.5356 0.9851], Lowest was [0.9196 0.1674 0.5356 0.9851]
Median for last 10 epochs: [0.9196 0.1674 0.5356 0.9851], Epochs since improvement 2
  1%|          | 3/500 [02:25<6:38:33, 48.12s/it]  1%|          | 4/500 [03:29<7:28:07, 54.21s/it]Epoch: 4 done with learning rate 4.67E-04, Train loss: 2.21E+07, Train scatter: [0.9352 0.155  0.5441 0.9954]
L1 regularization loss: 1.99E+00, L2 regularization loss: 3.85E-01
Test scatter: [0.9196 0.1509 0.5355 0.9851], Lowest was [0.9196 0.1509 0.5355 0.9851]
Median for last 10 epochs: [0.9196 0.1509 0.5355 0.9851], Epochs since improvement 0
  1%|          | 5/500 [04:10<6:49:24, 49.62s/it]  1%|          | 6/500 [05:15<7:30:29, 54.72s/it]Epoch: 6 done with learning rate 5.51E-04, Train loss: 1.72E+07, Train scatter: [0.9349 0.1106 0.5441 0.9954]
L1 regularization loss: 2.01E+00, L2 regularization loss: 4.01E-01
Test scatter: [0.9193 0.1094 0.5355 0.985 ], Lowest was [0.9193 0.1094 0.5355 0.985 ]
Median for last 10 epochs: [0.9193 0.1094 0.5355 0.985 ], Epochs since improvement 0
  1%|▏         | 7/500 [05:56<6:53:26, 50.32s/it]  2%|▏         | 8/500 [07:00<7:29:13, 54.78s/it]Epoch: 8 done with learning rate 6.67E-04, Train loss: 1.34E+07, Train scatter: [0.9089 0.0958 0.544  0.8254]
L1 regularization loss: 2.04E+00, L2 regularization loss: 4.27E-01
Test scatter: [0.8945 0.0957 0.5354 0.8191], Lowest was [0.8945 0.0957 0.5354 0.8191]
Median for last 10 epochs: [0.9069 0.1026 0.5355 0.9021], Epochs since improvement 0
  2%|▏         | 9/500 [07:41<6:51:24, 50.27s/it]Epoch: 10 done with learning rate 8.15E-04, Train loss: 9.90E+06, Train scatter: [0.7782 0.0919 0.5439 0.6131]
L1 regularization loss: 2.07E+00, L2 regularization loss: 4.57E-01
Test scatter: [0.769  0.0915 0.5353 0.6132], Lowest was [0.769  0.0915 0.5353 0.6132]
Median for last 10 epochs: [0.8945 0.0957 0.5354 0.8191], Epochs since improvement 0
  2%|▏         | 10/500 [08:51<7:40:00, 56.33s/it]  2%|▏         | 11/500 [09:32<7:01:19, 51.70s/it]  2%|▏         | 12/500 [10:35<7:28:32, 55.15s/it]Epoch: 12 done with learning rate 9.94E-04, Train loss: 7.00E+06, Train scatter: [0.6919 0.0864 0.5439 0.5869]
L1 regularization loss: 2.07E+00, L2 regularization loss: 4.68E-01
Test scatter: [0.6875 0.0863 0.5353 0.5896], Lowest was [0.6875 0.0863 0.5353 0.5896]
Median for last 10 epochs: [0.8945 0.0957 0.5354 0.8191], Epochs since improvement 0
  3%|▎         | 13/500 [11:16<6:53:36, 50.96s/it]  3%|▎         | 14/500 [12:21<7:26:41, 55.15s/it]Epoch: 14 done with learning rate 1.20E-03, Train loss: 5.53E+06, Train scatter: [0.561  0.0839 0.5438 0.5594]
L1 regularization loss: 2.08E+00, L2 regularization loss: 4.79E-01
Test scatter: [0.5584 0.0836 0.5353 0.5592], Lowest was [0.5584 0.0836 0.5353 0.5592]
Median for last 10 epochs: [0.769  0.0915 0.5353 0.6132], Epochs since improvement 0
  3%|▎         | 15/500 [13:02<6:50:29, 50.78s/it]  3%|▎         | 16/500 [14:07<7:23:55, 55.03s/it]Epoch: 16 done with learning rate 1.44E-03, Train loss: 4.35E+06, Train scatter: [0.3768 0.0769 0.5436 0.5423]
L1 regularization loss: 2.10E+00, L2 regularization loss: 4.87E-01
Test scatter: [0.3834 0.0777 0.5351 0.54  ], Lowest was [0.3834 0.0777 0.5351 0.54  ]
Median for last 10 epochs: [0.6875 0.0863 0.5353 0.5896], Epochs since improvement 0
  3%|▎         | 17/500 [14:48<6:49:36, 50.88s/it]  4%|▎         | 18/500 [15:52<7:21:56, 55.01s/it]Epoch: 18 done with learning rate 1.70E-03, Train loss: 4.05E+06, Train scatter: [0.3098 0.0768 0.5435 0.5347]
L1 regularization loss: 2.11E+00, L2 regularization loss: 4.95E-01
Test scatter: [0.3177 0.0775 0.535  0.5336], Lowest was [0.3177 0.0775 0.535  0.5336]
Median for last 10 epochs: [0.5584 0.0836 0.5353 0.5592], Epochs since improvement 0
  4%|▍         | 19/500 [16:33<6:46:49, 50.75s/it]Epoch: 20 done with learning rate 1.99E-03, Train loss: 3.80E+06, Train scatter: [0.2397 0.0763 0.5435 0.5323]
L1 regularization loss: 2.11E+00, L2 regularization loss: 5.00E-01
Test scatter: [0.2445 0.0759 0.535  0.5207], Lowest was [0.2445 0.0759 0.535  0.5207]
Median for last 10 epochs: [0.3834 0.0777 0.5351 0.54  ], Epochs since improvement 0
  4%|▍         | 20/500 [17:43<7:32:13, 56.53s/it]  4%|▍         | 21/500 [18:25<6:55:05, 51.99s/it]  4%|▍         | 22/500 [19:29<7:24:38, 55.81s/it]Epoch: 22 done with learning rate 2.30E-03, Train loss: 3.73E+06, Train scatter: [0.2505 0.073  0.5435 0.5134]
L1 regularization loss: 2.12E+00, L2 regularization loss: 5.05E-01
Test scatter: [0.257  0.0734 0.535  0.5115], Lowest was [0.2445 0.0734 0.535  0.5115]
Median for last 10 epochs: [0.3177 0.0775 0.535  0.5336], Epochs since improvement 0
  5%|▍         | 23/500 [20:11<6:50:12, 51.60s/it]  5%|▍         | 24/500 [21:17<7:22:17, 55.75s/it]Epoch: 24 done with learning rate 2.63E-03, Train loss: 3.94E+06, Train scatter: [0.2449 0.074  0.5435 0.5165]
L1 regularization loss: 2.13E+00, L2 regularization loss: 5.10E-01
Test scatter: [0.2507 0.0736 0.535  0.5149], Lowest was [0.2445 0.0734 0.535  0.5115]
Median for last 10 epochs: [0.257  0.0759 0.535  0.5207], Epochs since improvement 2
  5%|▌         | 25/500 [21:59<6:49:31, 51.73s/it]  5%|▌         | 26/500 [23:03<7:17:42, 55.41s/it]Epoch: 26 done with learning rate 2.98E-03, Train loss: 3.63E+06, Train scatter: [0.2852 0.0722 0.5435 0.5457]
L1 regularization loss: 2.14E+00, L2 regularization loss: 5.14E-01
Test scatter: [0.288  0.0722 0.535  0.545 ], Lowest was [0.2445 0.0722 0.535  0.5115]
Median for last 10 epochs: [0.257  0.0736 0.535  0.5207], Epochs since improvement 0
  5%|▌         | 27/500 [23:44<6:43:25, 51.17s/it]  6%|▌         | 28/500 [24:49<7:13:30, 55.11s/it]Epoch: 28 done with learning rate 3.34E-03, Train loss: 3.60E+06, Train scatter: [0.2302 0.0715 0.5435 0.5085]
L1 regularization loss: 2.15E+00, L2 regularization loss: 5.18E-01
Test scatter: [0.2329 0.0714 0.535  0.5068], Lowest was [0.2329 0.0714 0.535  0.5068]
Median for last 10 epochs: [0.2507 0.0734 0.535  0.5149], Epochs since improvement 0
  6%|▌         | 29/500 [25:29<6:38:33, 50.77s/it]Epoch: 30 done with learning rate 3.72E-03, Train loss: 3.57E+06, Train scatter: [0.2283 0.0728 0.5435 0.5164]
L1 regularization loss: 2.17E+00, L2 regularization loss: 5.23E-01
Test scatter: [0.2325 0.0724 0.535  0.5134], Lowest was [0.2325 0.0714 0.535  0.5068]
Median for last 10 epochs: [0.2507 0.0724 0.535  0.5134], Epochs since improvement 0
  6%|▌         | 30/500 [26:40<7:25:24, 56.86s/it]  6%|▌         | 31/500 [27:22<6:49:21, 52.37s/it]  6%|▋         | 32/500 [28:26<7:15:18, 55.81s/it]Epoch: 32 done with learning rate 4.10E-03, Train loss: 3.32E+06, Train scatter: [0.3181 0.0732 0.5435 0.5311]
L1 regularization loss: 2.19E+00, L2 regularization loss: 5.29E-01
Test scatter: [0.3209 0.0732 0.5349 0.5308], Lowest was [0.2325 0.0714 0.5349 0.5068]
Median for last 10 epochs: [0.2507 0.0724 0.535  0.5149], Epochs since improvement 0
  7%|▋         | 33/500 [29:07<6:40:20, 51.44s/it]  7%|▋         | 34/500 [30:12<7:10:38, 55.45s/it]Epoch: 34 done with learning rate 4.50E-03, Train loss: 3.25E+06, Train scatter: [0.2002 0.071  0.5434 0.4993]
L1 regularization loss: 2.20E+00, L2 regularization loss: 5.36E-01
Test scatter: [0.2074 0.0711 0.5349 0.4987], Lowest was [0.2074 0.0711 0.5349 0.4987]
Median for last 10 epochs: [0.2329 0.0722 0.535  0.5134], Epochs since improvement 0
  7%|▋         | 35/500 [30:54<6:37:19, 51.27s/it]  7%|▋         | 36/500 [31:59<7:08:44, 55.44s/it]Epoch: 36 done with learning rate 4.90E-03, Train loss: 3.23E+06, Train scatter: [0.2709 0.0725 0.5434 0.5217]
L1 regularization loss: 2.22E+00, L2 regularization loss: 5.43E-01
Test scatter: [0.2765 0.0733 0.5348 0.5236], Lowest was [0.2074 0.0711 0.5348 0.4987]
Median for last 10 epochs: [0.2329 0.0724 0.5349 0.5134], Epochs since improvement 0
  7%|▋         | 37/500 [32:41<6:36:48, 51.42s/it]  8%|▊         | 38/500 [33:46<7:07:08, 55.47s/it]Epoch: 38 done with learning rate 5.30E-03, Train loss: 3.19E+06, Train scatter: [0.2269 0.075  0.5433 0.5181]
L1 regularization loss: 2.24E+00, L2 regularization loss: 5.55E-01
Test scatter: [0.2389 0.0744 0.5348 0.5071], Lowest was [0.2074 0.0711 0.5348 0.4987]
Median for last 10 epochs: [0.2389 0.0732 0.5349 0.5134], Epochs since improvement 0
  8%|▊         | 39/500 [34:26<6:31:44, 50.99s/it]Epoch: 40 done with learning rate 5.70E-03, Train loss: 3.23E+06, Train scatter: [0.2301 0.077  0.5435 0.509 ]
L1 regularization loss: 2.26E+00, L2 regularization loss: 5.65E-01
Test scatter: [0.2329 0.0757 0.5349 0.5035], Lowest was [0.2074 0.0711 0.5348 0.4987]
Median for last 10 epochs: [0.2389 0.0733 0.5349 0.5071], Epochs since improvement 2
  8%|▊         | 40/500 [35:38<7:19:54, 57.38s/it]  8%|▊         | 41/500 [36:19<6:41:00, 52.42s/it]  8%|▊         | 42/500 [37:25<7:10:09, 56.35s/it]Epoch: 42 done with learning rate 6.10E-03, Train loss: 3.08E+06, Train scatter: [0.2072 0.072  0.5434 0.5037]
L1 regularization loss: 2.27E+00, L2 regularization loss: 5.69E-01
Test scatter: [0.2104 0.0713 0.5349 0.498 ], Lowest was [0.2074 0.0711 0.5348 0.498 ]
Median for last 10 epochs: [0.2329 0.0733 0.5349 0.5035], Epochs since improvement 0
  9%|▊         | 43/500 [38:07<6:36:33, 52.06s/it]  9%|▉         | 44/500 [39:10<7:00:18, 55.30s/it]Epoch: 44 done with learning rate 6.49E-03, Train loss: 3.06E+06, Train scatter: [0.2107 0.0711 0.5433 0.5104]
L1 regularization loss: 2.27E+00, L2 regularization loss: 5.74E-01
Test scatter: [0.2148 0.0701 0.5348 0.5089], Lowest was [0.2074 0.0701 0.5348 0.498 ]
Median for last 10 epochs: [0.2329 0.0733 0.5348 0.5071], Epochs since improvement 0
  9%|▉         | 45/500 [39:51<6:26:20, 50.95s/it]  9%|▉         | 46/500 [40:55<6:56:34, 55.05s/it]Epoch: 46 done with learning rate 6.87E-03, Train loss: 3.04E+06, Train scatter: [0.2598 0.0735 0.5433 0.5533]
L1 regularization loss: 2.28E+00, L2 regularization loss: 5.79E-01
Test scatter: [0.2571 0.0721 0.5347 0.5545], Lowest was [0.2074 0.0701 0.5347 0.498 ]
Median for last 10 epochs: [0.2329 0.0721 0.5348 0.5071], Epochs since improvement 0
  9%|▉         | 47/500 [41:37<6:25:42, 51.09s/it] 10%|▉         | 48/500 [42:41<6:54:07, 54.97s/it]Epoch: 48 done with learning rate 7.24E-03, Train loss: 3.04E+06, Train scatter: [0.2525 0.0701 0.5432 0.5149]
L1 regularization loss: 2.30E+00, L2 regularization loss: 5.87E-01
Test scatter: [0.2577 0.0695 0.5346 0.5155], Lowest was [0.2074 0.0695 0.5346 0.498 ]
Median for last 10 epochs: [0.2329 0.0713 0.5348 0.5089], Epochs since improvement 0
 10%|▉         | 49/500 [43:23<6:23:44, 51.05s/it]Epoch: 50 done with learning rate 7.60E-03, Train loss: 3.03E+06, Train scatter: [0.2653 0.0707 0.5431 0.5159]
L1 regularization loss: 2.30E+00, L2 regularization loss: 5.91E-01
Test scatter: [0.2699 0.07   0.5346 0.5107], Lowest was [0.2074 0.0695 0.5346 0.498 ]
Median for last 10 epochs: [0.2571 0.0701 0.5347 0.5107], Epochs since improvement 0
 10%|█         | 50/500 [44:34<7:07:28, 57.00s/it] 10%|█         | 51/500 [45:16<6:32:33, 52.46s/it] 10%|█         | 52/500 [46:20<6:57:48, 55.96s/it]Epoch: 52 done with learning rate 7.94E-03, Train loss: 3.02E+06, Train scatter: [0.2087 0.069  0.5431 0.497 ]
L1 regularization loss: 2.31E+00, L2 regularization loss: 5.97E-01
Test scatter: [0.2109 0.0682 0.5345 0.4931], Lowest was [0.2074 0.0682 0.5345 0.4931]
Median for last 10 epochs: [0.2571 0.07   0.5346 0.5107], Epochs since improvement 0
 11%|█         | 53/500 [47:01<6:23:39, 51.50s/it] 11%|█         | 54/500 [48:06<6:52:34, 55.50s/it]Epoch: 54 done with learning rate 8.26E-03, Train loss: 3.08E+06, Train scatter: [0.3954 0.0771 0.5432 0.4981]
L1 regularization loss: 2.32E+00, L2 regularization loss: 6.07E-01
Test scatter: [0.3881 0.0758 0.5346 0.4903], Lowest was [0.2074 0.0682 0.5345 0.4903]
Median for last 10 epochs: [0.2577 0.07   0.5346 0.5107], Epochs since improvement 0
 11%|█         | 55/500 [48:47<6:19:30, 51.17s/it] 11%|█         | 56/500 [49:51<6:47:14, 55.03s/it]Epoch: 56 done with learning rate 8.56E-03, Train loss: 3.06E+06, Train scatter: [0.4277 0.0708 0.5431 0.4964]
L1 regularization loss: 2.35E+00, L2 regularization loss: 6.17E-01
Test scatter: [0.4202 0.0703 0.5345 0.4894], Lowest was [0.2074 0.0682 0.5345 0.4894]
Median for last 10 epochs: [0.2699 0.07   0.5346 0.4931], Epochs since improvement 0
 11%|█▏        | 57/500 [50:32<6:15:19, 50.83s/it] 12%|█▏        | 58/500 [51:36<6:44:33, 54.92s/it]Epoch: 58 done with learning rate 8.83E-03, Train loss: 3.05E+06, Train scatter: [0.4276 0.0725 0.543  0.5035]
L1 regularization loss: 2.37E+00, L2 regularization loss: 6.28E-01
Test scatter: [0.4195 0.0715 0.5344 0.4982], Lowest was [0.2074 0.0682 0.5344 0.4894]
Median for last 10 epochs: [0.3881 0.0703 0.5345 0.4931], Epochs since improvement 0
 12%|█▏        | 59/500 [52:17<6:12:04, 50.62s/it]Epoch: 60 done with learning rate 9.08E-03, Train loss: 3.08E+06, Train scatter: [0.43   0.0871 0.5432 0.5304]
L1 regularization loss: 2.42E+00, L2 regularization loss: 6.52E-01
Test scatter: [0.4427 0.0864 0.5346 0.5251], Lowest was [0.2074 0.0682 0.5344 0.4894]
Median for last 10 epochs: [0.4195 0.0715 0.5345 0.4931], Epochs since improvement 2
 12%|█▏        | 60/500 [53:27<6:53:00, 56.32s/it] 12%|█▏        | 61/500 [54:08<6:18:41, 51.76s/it] 12%|█▏        | 62/500 [55:12<6:45:34, 55.56s/it]Epoch: 62 done with learning rate 9.31E-03, Train loss: 3.03E+06, Train scatter: [0.3439 0.0735 0.5429 0.5379]
L1 regularization loss: 2.47E+00, L2 regularization loss: 6.73E-01
Test scatter: [0.3412 0.0721 0.5344 0.5399], Lowest was [0.2074 0.0682 0.5344 0.4894]
Median for last 10 epochs: [0.4195 0.0721 0.5345 0.4982], Epochs since improvement 0
 13%|█▎        | 63/500 [55:53<6:13:06, 51.23s/it] 13%|█▎        | 64/500 [56:57<6:40:01, 55.05s/it]Epoch: 64 done with learning rate 9.50E-03, Train loss: 3.00E+06, Train scatter: [0.3124 0.0785 0.5427 0.5056]
L1 regularization loss: 2.47E+00, L2 regularization loss: 6.82E-01
Test scatter: [0.3302 0.0769 0.5342 0.4978], Lowest was [0.2074 0.0682 0.5342 0.4894]
Median for last 10 epochs: [0.4195 0.0721 0.5344 0.4982], Epochs since improvement 0
 13%|█▎        | 65/500 [57:38<6:07:44, 50.72s/it] 13%|█▎        | 66/500 [58:41<6:33:36, 54.42s/it]Epoch: 66 done with learning rate 9.66E-03, Train loss: 3.02E+06, Train scatter: [0.2409 0.0718 0.5428 0.5173]
L1 regularization loss: 2.49E+00, L2 regularization loss: 6.98E-01
Test scatter: [0.2428 0.0708 0.5343 0.5166], Lowest was [0.2074 0.0682 0.5342 0.4894]
Median for last 10 epochs: [0.3412 0.0721 0.5344 0.5166], Epochs since improvement 2
 13%|█▎        | 67/500 [59:22<6:03:04, 50.31s/it] 14%|█▎        | 68/500 [1:00:26<6:33:31, 54.66s/it]Epoch: 68 done with learning rate 9.80E-03, Train loss: 3.00E+06, Train scatter: [0.4336 0.0721 0.5426 0.4945]
L1 regularization loss: 2.51E+00, L2 regularization loss: 7.10E-01
Test scatter: [0.4243 0.0718 0.534  0.4883], Lowest was [0.2074 0.0682 0.534  0.4883]
Median for last 10 epochs: [0.3412 0.0721 0.5343 0.5166], Epochs since improvement 0
 14%|█▍        | 69/500 [1:01:07<6:03:09, 50.56s/it]Epoch: 70 done with learning rate 9.90E-03, Train loss: 3.01E+06, Train scatter: [0.4413 0.0704 0.5426 0.4995]
L1 regularization loss: 2.56E+00, L2 regularization loss: 7.33E-01
Test scatter: [0.4363 0.0703 0.534  0.4938], Lowest was [0.2074 0.0682 0.534  0.4883]
Median for last 10 epochs: [0.3412 0.0718 0.5342 0.4978], Epochs since improvement 2
 14%|█▍        | 70/500 [1:02:18<6:45:19, 56.56s/it] 14%|█▍        | 71/500 [1:02:59<6:10:56, 51.88s/it] 14%|█▍        | 72/500 [1:04:04<6:38:07, 55.81s/it]Epoch: 72 done with learning rate 9.96E-03, Train loss: 2.99E+06, Train scatter: [0.3879 0.0875 0.5423 0.5187]
L1 regularization loss: 2.55E+00, L2 regularization loss: 7.38E-01
Test scatter: [0.3826 0.0848 0.5337 0.5144], Lowest was [0.2074 0.0682 0.5337 0.4883]
Median for last 10 epochs: [0.3826 0.0718 0.534  0.4978], Epochs since improvement 0
 15%|█▍        | 73/500 [1:04:46<6:07:26, 51.63s/it] 15%|█▍        | 74/500 [1:05:50<6:33:33, 55.43s/it]Epoch: 74 done with learning rate 1.00E-02, Train loss: 2.97E+06, Train scatter: [0.4265 0.1025 0.5425 0.5059]
L1 regularization loss: 2.56E+00, L2 regularization loss: 7.56E-01
Test scatter: [0.4171 0.0998 0.5339 0.5007], Lowest was [0.2074 0.0682 0.5337 0.4883]
Median for last 10 epochs: [0.4171 0.0718 0.534  0.5007], Epochs since improvement 2
 15%|█▌        | 75/500 [1:06:31<6:01:58, 51.10s/it] 15%|█▌        | 76/500 [1:07:35<6:28:38, 55.00s/it]Epoch: 76 done with learning rate 1.00E-02, Train loss: 2.96E+06, Train scatter: [0.2569 0.0676 0.5424 0.5012]
L1 regularization loss: 2.58E+00, L2 regularization loss: 7.70E-01
Test scatter: [0.2467 0.0671 0.5338 0.4961], Lowest was [0.2074 0.0671 0.5337 0.4883]
Median for last 10 epochs: [0.4171 0.0718 0.5339 0.4961], Epochs since improvement 0
 15%|█▌        | 77/500 [1:08:17<6:00:29, 51.13s/it] 16%|█▌        | 78/500 [1:09:21<6:25:47, 54.85s/it]Epoch: 78 done with learning rate 1.00E-02, Train loss: 2.96E+06, Train scatter: [0.3094 0.0737 0.5425 0.5276]
L1 regularization loss: 2.60E+00, L2 regularization loss: 7.87E-01
Test scatter: [0.3    0.0724 0.5339 0.5261], Lowest was [0.2074 0.0671 0.5337 0.4883]
Median for last 10 epochs: [0.3826 0.0724 0.5339 0.5007], Epochs since improvement 2
 16%|█▌        | 79/500 [1:10:01<5:54:36, 50.54s/it]Epoch: 80 done with learning rate 1.00E-02, Train loss: 2.96E+06, Train scatter: [0.4445 0.0668 0.5422 0.5107]
L1 regularization loss: 2.65E+00, L2 regularization loss: 8.10E-01
Test scatter: [0.4352 0.066  0.5336 0.5019], Lowest was [0.2074 0.066  0.5336 0.4883]
Median for last 10 epochs: [0.3826 0.0724 0.5338 0.5019], Epochs since improvement 0
 16%|█▌        | 80/500 [1:11:13<6:37:32, 56.79s/it] 16%|█▌        | 81/500 [1:11:54<6:04:21, 52.18s/it] 16%|█▋        | 82/500 [1:12:58<6:27:40, 55.65s/it]Epoch: 82 done with learning rate 9.99E-03, Train loss: 2.96E+06, Train scatter: [0.6832 0.1558 0.543  0.936 ]
L1 regularization loss: 2.71E+00, L2 regularization loss: 8.44E-01
Test scatter: [0.6684 0.1517 0.5344 0.9274], Lowest was [0.2074 0.066  0.5336 0.4883]
Median for last 10 epochs: [0.4171 0.0724 0.5339 0.5019], Epochs since improvement 2
 17%|█▋        | 83/500 [1:13:40<5:58:55, 51.64s/it] 17%|█▋        | 84/500 [1:14:43<6:22:17, 55.14s/it]Epoch: 84 done with learning rate 9.99E-03, Train loss: 2.98E+06, Train scatter: [0.3954 0.0835 0.5424 0.5426]
L1 regularization loss: 2.76E+00, L2 regularization loss: 9.03E-01
Test scatter: [0.3905 0.0829 0.5339 0.5365], Lowest was [0.2074 0.066  0.5336 0.4883]
Median for last 10 epochs: [0.3905 0.0724 0.5339 0.5261], Epochs since improvement 4
 17%|█▋        | 85/500 [1:15:24<5:51:32, 50.83s/it] 17%|█▋        | 86/500 [1:16:29<6:19:14, 54.96s/it]Epoch: 86 done with learning rate 9.98E-03, Train loss: 2.96E+06, Train scatter: [0.3379 0.0977 0.5423 0.5299]
L1 regularization loss: 2.78E+00, L2 regularization loss: 9.25E-01
Test scatter: [0.3497 0.0966 0.5338 0.5257], Lowest was [0.2074 0.066  0.5336 0.4883]
Median for last 10 epochs: [0.3905 0.0829 0.5339 0.5261], Epochs since improvement 6
 17%|█▋        | 87/500 [1:17:11<5:51:24, 51.05s/it] 18%|█▊        | 88/500 [1:18:17<6:21:14, 55.52s/it]Epoch: 88 done with learning rate 9.98E-03, Train loss: 2.94E+06, Train scatter: [0.3858 0.0676 0.542  0.5067]
L1 regularization loss: 2.80E+00, L2 regularization loss: 9.41E-01
Test scatter: [0.3831 0.068  0.5334 0.5026], Lowest was [0.2074 0.066  0.5334 0.4883]
Median for last 10 epochs: [0.3905 0.0829 0.5338 0.5257], Epochs since improvement 0
 18%|█▊        | 89/500 [1:18:59<5:52:32, 51.47s/it]Epoch: 90 done with learning rate 9.97E-03, Train loss: 3.47E+06, Train scatter: [0.8396 0.1274 0.5438 0.8454]
L1 regularization loss: 3.58E+00, L2 regularization loss: 1.45E+00
Test scatter: [0.825  0.1249 0.5353 0.8366], Lowest was [0.2074 0.066  0.5334 0.4883]
Median for last 10 epochs: [0.3905 0.0966 0.5339 0.5365], Epochs since improvement 2
 18%|█▊        | 90/500 [1:20:08<6:29:15, 56.97s/it] 18%|█▊        | 91/500 [1:20:50<5:56:06, 52.24s/it] 18%|█▊        | 92/500 [1:21:55<6:21:23, 56.09s/it]Epoch: 92 done with learning rate 9.96E-03, Train loss: 3.13E+06, Train scatter: [0.6593 0.1125 0.5439 0.7255]
L1 regularization loss: 3.62E+00, L2 regularization loss: 1.56E+00
Test scatter: [0.6561 0.1108 0.5353 0.7173], Lowest was [0.2074 0.066  0.5334 0.4883]
Median for last 10 epochs: [0.3905 0.0966 0.5339 0.5365], Epochs since improvement 4
 19%|█▊        | 93/500 [1:22:36<5:49:46, 51.56s/it] 19%|█▉        | 94/500 [1:23:40<6:15:30, 55.49s/it]Epoch: 94 done with learning rate 9.95E-03, Train loss: 3.08E+06, Train scatter: [0.4615 0.1037 0.5438 0.6487]
L1 regularization loss: 3.63E+00, L2 regularization loss: 1.59E+00
Test scatter: [0.4597 0.101  0.5352 0.6312], Lowest was [0.2074 0.066  0.5334 0.4883]
Median for last 10 epochs: [0.4597 0.101  0.5352 0.6312], Epochs since improvement 6
 19%|█▉        | 95/500 [1:24:22<5:47:10, 51.43s/it] 19%|█▉        | 96/500 [1:25:27<6:12:42, 55.35s/it]Epoch: 96 done with learning rate 9.94E-03, Train loss: 3.16E+06, Train scatter: [0.5718 0.1332 0.5437 0.6768]
L1 regularization loss: 3.64E+00, L2 regularization loss: 1.71E+00
Test scatter: [0.5601 0.129  0.5351 0.6691], Lowest was [0.2074 0.066  0.5334 0.4883]
Median for last 10 epochs: [0.5601 0.1108 0.5352 0.6691], Epochs since improvement 8
 19%|█▉        | 97/500 [1:26:08<5:42:19, 50.97s/it] 20%|█▉        | 98/500 [1:27:11<6:06:13, 54.66s/it]Epoch: 98 done with learning rate 9.93E-03, Train loss: 3.05E+06, Train scatter: [0.5201 0.0887 0.5437 0.5979]
L1 regularization loss: 3.66E+00, L2 regularization loss: 1.75E+00
Test scatter: [0.5107 0.0865 0.5351 0.5893], Lowest was [0.2074 0.066  0.5334 0.4883]
Median for last 10 epochs: [0.5601 0.1108 0.5352 0.6691], Epochs since improvement 10
 20%|█▉        | 99/500 [1:27:51<5:36:55, 50.41s/it]Epoch: 100 done with learning rate 9.91E-03, Train loss: 3.24E+06, Train scatter: [0.8681 0.1331 0.5439 0.9411]
L1 regularization loss: 3.77E+00, L2 regularization loss: 1.82E+00
Test scatter: [0.8502 0.1301 0.5353 0.934 ], Lowest was [0.2074 0.066  0.5334 0.4883]
Median for last 10 epochs: [0.5601 0.1108 0.5352 0.6691], Epochs since improvement 12
 20%|██        | 100/500 [1:29:02<6:16:51, 56.53s/it] 20%|██        | 101/500 [1:29:44<5:45:41, 51.98s/it] 20%|██        | 102/500 [1:30:46<6:05:45, 55.14s/it]Epoch: 102 done with learning rate 9.90E-03, Train loss: 3.09E+06, Train scatter: [0.6054 0.1128 0.544  0.7253]
L1 regularization loss: 3.79E+00, L2 regularization loss: 1.90E+00
Test scatter: [0.6228 0.1121 0.5354 0.7175], Lowest was [0.2074 0.066  0.5334 0.4883]
Median for last 10 epochs: [0.5601 0.1121 0.5352 0.6691], Epochs since improvement 14
 21%|██        | 103/500 [1:31:28<5:37:41, 51.04s/it] 21%|██        | 104/500 [1:32:31<6:02:08, 54.87s/it]Epoch: 104 done with learning rate 9.89E-03, Train loss: 3.06E+06, Train scatter: [0.5193 0.1095 0.5439 0.7132]
L1 regularization loss: 3.78E+00, L2 regularization loss: 1.91E+00
Test scatter: [0.5276 0.1086 0.5354 0.7105], Lowest was [0.2074 0.066  0.5334 0.4883]
Median for last 10 epochs: [0.5601 0.1121 0.5353 0.7105], Epochs since improvement 16
 21%|██        | 105/500 [1:33:13<5:34:57, 50.88s/it] 21%|██        | 106/500 [1:34:18<6:02:40, 55.23s/it]Epoch: 106 done with learning rate 9.87E-03, Train loss: 3.05E+06, Train scatter: [0.4808 0.1091 0.5439 0.687 ]
L1 regularization loss: 3.77E+00, L2 regularization loss: 1.92E+00
Test scatter: [0.4867 0.1083 0.5353 0.6804], Lowest was [0.2074 0.066  0.5334 0.4883]
Median for last 10 epochs: [0.5276 0.1086 0.5353 0.7105], Epochs since improvement 18
 21%|██▏       | 107/500 [1:34:59<5:32:55, 50.83s/it] 22%|██▏       | 108/500 [1:36:04<5:59:19, 55.00s/it]Epoch: 108 done with learning rate 9.85E-03, Train loss: 3.04E+06, Train scatter: [0.4767 0.1037 0.5439 0.6651]
L1 regularization loss: 3.75E+00, L2 regularization loss: 1.93E+00
Test scatter: [0.4785 0.1026 0.5353 0.6563], Lowest was [0.2074 0.066  0.5334 0.4883]
Median for last 10 epochs: [0.5276 0.1086 0.5353 0.7105], Epochs since improvement 20
 22%|██▏       | 109/500 [1:36:44<5:30:47, 50.76s/it]Epoch: 110 done with learning rate 9.83E-03, Train loss: 3.04E+06, Train scatter: [0.486  0.1014 0.5439 0.645 ]
L1 regularization loss: 3.74E+00, L2 regularization loss: 1.95E+00
Test scatter: [0.4818 0.1002 0.5353 0.6334], Lowest was [0.2074 0.066  0.5334 0.4883]
Median for last 10 epochs: [0.4867 0.1083 0.5353 0.6804], Epochs since improvement 22
 22%|██▏       | 109/500 [1:37:57<5:51:23, 53.92s/it]
Exited after 110 epochs due to early stopping
5877.44 seconds spent training, 11.755 seconds per epoch. Processed 5924 trees per second
[0.4818008  0.10016099 0.53528357 0.63339245]
{'epoch_exit': 109, 'scatter_m_star': 0.4818008, 'lowest_m_star': 0.20742504, 'last20_m_star': 0.51915085, 'last10_m_star': 0.48674524, 'scatter_v_disk': 0.100160986, 'lowest_v_disk': 0.065954074, 'last20_v_disk': 0.10844801, 'last10_v_disk': 0.108311325, 'scatter_m_cold': 0.53528357, 'lowest_m_cold': 0.5333931, 'last20_m_cold': 0.53531075, 'last10_m_cold': 0.5353389, 'scatter_sfr_100': 0.63339245, 'lowest_sfr_100': 0.4883439, 'last20_sfr_100': 0.6747507, 'last10_sfr_100': 0.6803902}
Finished 1/1
Exited training after None epochs
Experiment 4 done: 4 / 6
Exploring ['conv_layers', 'batch_size']
Currently doing [  3 256]
{'experiment': 'GraphMerge', 'group': 'meta_pysr', 'move': False, 'model': 'MetaEdge', 'log': True, 'run_params': {'n_epochs': 500, 'n_trials': 1, 'batch_size': 256, 'val_epoch': 2, 'early_stopping': True, 'patience': 20, 'l1_lambda': 0.0001, 'l2_lambda': 0.0001, 'loss_func': 'GaussNd', 'metrics': 'test_multi_varrho', 'performance_plot': 'multi_base', 'save': True, 'seed': True, 'num_workers': 4}, 'learn_params': {'learning_rate': 0.01, 'schedule': 'onecycle', 'g_up': 1, 'g_down': 0.95, 'warmup': 4, 'period': 5, 'eta_min': 1e-05}, 'hyper_params': {'hidden_channels': 128, 'conv_layers': 3, 'conv_activation': 'relu', 'decode_activation': 'leakyrelu', 'decode_layers': 2, 'layernorm': True, 'agg': 'sum', 'variance': True, 'rho': 0, 'in_channels': 43, 'out_channels': 4}, 'data_params': {'case': 'vlarge_all_4t_z0.0_quantile_raw', 'targets': [0, 1, 2, 3], 'del_feats': [], 'split': 0.8, 'test': 0, 'scale': 0}, 'experiment_name': 'con3bat256'}
/home/cj1223/../../scratch/gpfs/cj1223/GraphResults/results_meta_pysr_120322/MetaEdge_vlarge_all_4t_z0.0_quantile_raw_aggfzz
RelU conv activation
LeakyRelU decode activation
N_params 546504
Made folder for saving model
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [01:01<8:31:45, 61.53s/it]  0%|          | 2/500 [02:32<10:53:43, 78.76s/it]Epoch: 2 done with learning rate 4.17E-04, Train loss: 9.23E+07, Train scatter: [0.9351 0.1375 0.5441 0.9955]
L1 regularization loss: 2.45E+00, L2 regularization loss: 4.24E-01
Test scatter: [0.9195 0.1345 0.5355 0.9851], Lowest was [0.9195 0.1345 0.5355 0.9851]
Median for last 10 epochs: [0.9195 0.1345 0.5355 0.9851], Epochs since improvement 2
  1%|          | 3/500 [03:34<9:47:40, 70.95s/it]   1%|          | 4/500 [05:06<10:58:22, 79.64s/it]Epoch: 4 done with learning rate 4.67E-04, Train loss: 8.07E+07, Train scatter: [0.9311 0.1011 0.5438 0.9955]
L1 regularization loss: 2.47E+00, L2 regularization loss: 4.29E-01
Test scatter: [0.9155 0.0997 0.5352 0.9852], Lowest was [0.9155 0.0997 0.5352 0.9851]
Median for last 10 epochs: [0.9155 0.0997 0.5352 0.9852], Epochs since improvement 0
  1%|          | 5/500 [06:09<10:06:47, 73.55s/it]  1%|          | 6/500 [07:44<11:03:53, 80.63s/it]Epoch: 6 done with learning rate 5.51E-04, Train loss: 7.68E+07, Train scatter: [0.8458 0.0899 0.5397 0.9955]
L1 regularization loss: 2.49E+00, L2 regularization loss: 4.38E-01
Test scatter: [0.8411 0.0896 0.5315 0.9851], Lowest was [0.8411 0.0896 0.5315 0.9851]
Median for last 10 epochs: [0.8411 0.0896 0.5315 0.9851], Epochs since improvement 0
  1%|▏         | 7/500 [08:48<10:18:16, 75.25s/it]  2%|▏         | 8/500 [10:20<11:00:13, 80.51s/it]Epoch: 8 done with learning rate 6.67E-04, Train loss: 7.43E+07, Train scatter: [0.7753 0.0926 0.4835 0.9955]
L1 regularization loss: 2.51E+00, L2 regularization loss: 4.53E-01
Test scatter: [0.7804 0.0917 0.4827 0.9852], Lowest was [0.7804 0.0896 0.4827 0.9851]
Median for last 10 epochs: [0.8107 0.0906 0.5071 0.9851], Epochs since improvement 0
  2%|▏         | 9/500 [11:23<10:15:10, 75.17s/it]Epoch: 10 done with learning rate 8.15E-04, Train loss: 7.18E+07, Train scatter: [0.6457 0.0897 0.3684 0.9953]
L1 regularization loss: 2.53E+00, L2 regularization loss: 4.65E-01
Test scatter: [0.6328 0.0893 0.3696 0.985 ], Lowest was [0.6328 0.0893 0.3696 0.985 ]
Median for last 10 epochs: [0.7804 0.0896 0.4827 0.9851], Epochs since improvement 0
  2%|▏         | 10/500 [13:05<11:21:22, 83.43s/it]  2%|▏         | 11/500 [14:10<10:34:03, 77.80s/it]  2%|▏         | 12/500 [15:42<11:08:23, 82.18s/it]Epoch: 12 done with learning rate 9.94E-04, Train loss: 6.82E+07, Train scatter: [0.5958 0.0853 0.3602 0.9954]
L1 regularization loss: 2.54E+00, L2 regularization loss: 4.71E-01
Test scatter: [0.5918 0.0856 0.3643 0.9851], Lowest was [0.5918 0.0856 0.3643 0.985 ]
Median for last 10 epochs: [0.7804 0.0896 0.4827 0.9851], Epochs since improvement 0
  3%|▎         | 13/500 [16:44<10:16:40, 75.98s/it]  3%|▎         | 14/500 [18:15<10:52:04, 80.50s/it]Epoch: 14 done with learning rate 1.20E-03, Train loss: 6.74E+07, Train scatter: [0.5435 0.0813 0.3401 0.9954]
L1 regularization loss: 2.55E+00, L2 regularization loss: 4.76E-01
Test scatter: [0.5392 0.0808 0.3459 0.9851], Lowest was [0.5392 0.0808 0.3459 0.985 ]
Median for last 10 epochs: [0.6328 0.0893 0.3696 0.9851], Epochs since improvement 0
  3%|▎         | 15/500 [19:19<10:10:56, 75.58s/it]  3%|▎         | 16/500 [20:53<10:53:24, 81.00s/it]Epoch: 16 done with learning rate 1.44E-03, Train loss: 6.55E+07, Train scatter: [0.625  0.0805 0.3752 0.9954]
L1 regularization loss: 2.57E+00, L2 regularization loss: 4.84E-01
Test scatter: [0.6268 0.08   0.3839 0.9851], Lowest was [0.5392 0.08   0.3459 0.985 ]
Median for last 10 epochs: [0.6268 0.0856 0.3696 0.9851], Epochs since improvement 0
  3%|▎         | 17/500 [21:55<10:06:12, 75.31s/it]  4%|▎         | 18/500 [23:29<10:50:14, 80.94s/it]Epoch: 18 done with learning rate 1.70E-03, Train loss: 3.61E+07, Train scatter: [0.4913 0.0792 0.3486 0.9472]
L1 regularization loss: 2.59E+00, L2 regularization loss: 4.99E-01
Test scatter: [0.493  0.0781 0.3525 0.9422], Lowest was [0.493  0.0781 0.3459 0.9422]
Median for last 10 epochs: [0.5918 0.0808 0.3643 0.9851], Epochs since improvement 0
  4%|▍         | 19/500 [24:32<10:05:41, 75.55s/it]Epoch: 20 done with learning rate 1.99E-03, Train loss: 5.12E+06, Train scatter: [0.4583 0.0764 0.3171 0.5609]
L1 regularization loss: 2.63E+00, L2 regularization loss: 5.17E-01
Test scatter: [0.4491 0.0761 0.3226 0.5629], Lowest was [0.4491 0.0761 0.3226 0.5629]
Median for last 10 epochs: [0.5392 0.08   0.3525 0.9851], Epochs since improvement 0
  4%|▍         | 20/500 [26:13<11:05:24, 83.18s/it]  4%|▍         | 21/500 [27:16<10:16:53, 77.27s/it]  4%|▍         | 22/500 [28:48<10:49:32, 81.53s/it]Epoch: 22 done with learning rate 2.30E-03, Train loss: 3.58E+06, Train scatter: [0.4421 0.0715 0.3022 0.5204]
L1 regularization loss: 2.68E+00, L2 regularization loss: 5.36E-01
Test scatter: [0.4406 0.0715 0.3073 0.5197], Lowest was [0.4406 0.0715 0.3073 0.5197]
Median for last 10 epochs: [0.493  0.0781 0.3459 0.9422], Epochs since improvement 0
  5%|▍         | 23/500 [29:50<10:02:03, 75.73s/it]  5%|▍         | 24/500 [31:22<10:39:05, 80.56s/it]Epoch: 24 done with learning rate 2.63E-03, Train loss: 3.15E+06, Train scatter: [0.4327 0.07   0.2974 0.5034]
L1 regularization loss: 2.73E+00, L2 regularization loss: 5.61E-01
Test scatter: [0.4307 0.0703 0.3    0.5088], Lowest was [0.4307 0.0703 0.3    0.5088]
Median for last 10 epochs: [0.4491 0.0761 0.3226 0.5629], Epochs since improvement 0
  5%|▌         | 25/500 [32:23<9:52:45, 74.87s/it]   5%|▌         | 26/500 [33:55<10:31:05, 79.89s/it]Epoch: 26 done with learning rate 2.98E-03, Train loss: 3.05E+06, Train scatter: [0.4266 0.0688 0.2955 0.492 ]
L1 regularization loss: 2.78E+00, L2 regularization loss: 5.91E-01
Test scatter: [0.422  0.0686 0.2958 0.4939], Lowest was [0.422  0.0686 0.2958 0.4939]
Median for last 10 epochs: [0.4406 0.0715 0.3073 0.5197], Epochs since improvement 0
  5%|▌         | 27/500 [34:57<9:48:41, 74.67s/it]   6%|▌         | 28/500 [36:32<10:34:15, 80.63s/it]Epoch: 28 done with learning rate 3.34E-03, Train loss: 2.93E+06, Train scatter: [0.3752 0.0692 0.2888 0.4822]
L1 regularization loss: 2.84E+00, L2 regularization loss: 6.24E-01
Test scatter: [0.369  0.0694 0.2923 0.4836], Lowest was [0.369  0.0686 0.2923 0.4836]
Median for last 10 epochs: [0.4307 0.0703 0.3    0.5088], Epochs since improvement 0
  6%|▌         | 29/500 [37:35<9:51:10, 75.31s/it] Epoch: 30 done with learning rate 3.72E-03, Train loss: 2.84E+06, Train scatter: [0.3907 0.0674 0.2941 0.4821]
L1 regularization loss: 2.89E+00, L2 regularization loss: 6.58E-01
Test scatter: [0.3865 0.0671 0.2966 0.4851], Lowest was [0.369  0.0671 0.2923 0.4836]
Median for last 10 epochs: [0.422  0.0694 0.2966 0.4939], Epochs since improvement 0
  6%|▌         | 30/500 [39:14<10:47:09, 82.62s/it]  6%|▌         | 31/500 [40:19<10:04:40, 77.36s/it]  6%|▋         | 32/500 [41:50<10:34:45, 81.38s/it]Epoch: 32 done with learning rate 4.10E-03, Train loss: 2.79E+06, Train scatter: [0.4335 0.0696 0.2977 0.486 ]
L1 regularization loss: 2.95E+00, L2 regularization loss: 7.06E-01
Test scatter: [0.4262 0.0694 0.2987 0.487 ], Lowest was [0.369  0.0671 0.2923 0.4836]
Median for last 10 epochs: [0.422  0.0694 0.2966 0.487 ], Epochs since improvement 2
  7%|▋         | 33/500 [42:52<9:48:01, 75.55s/it]   7%|▋         | 34/500 [44:23<10:23:02, 80.22s/it]Epoch: 34 done with learning rate 4.50E-03, Train loss: 2.70E+06, Train scatter: [0.3281 0.0692 0.2904 0.4814]
L1 regularization loss: 3.01E+00, L2 regularization loss: 7.51E-01
Test scatter: [0.3209 0.0689 0.2909 0.4817], Lowest was [0.3209 0.0671 0.2909 0.4817]
Median for last 10 epochs: [0.3865 0.0689 0.2958 0.4851], Epochs since improvement 0
  7%|▋         | 35/500 [45:26<9:40:49, 74.95s/it]   7%|▋         | 36/500 [47:00<10:23:04, 80.57s/it]Epoch: 36 done with learning rate 4.90E-03, Train loss: 2.69E+06, Train scatter: [0.363  0.0641 0.2881 0.4735]
L1 regularization loss: 3.06E+00, L2 regularization loss: 7.82E-01
Test scatter: [0.361  0.0635 0.289  0.4689], Lowest was [0.3209 0.0635 0.289  0.4689]
Median for last 10 epochs: [0.369  0.0689 0.2923 0.4836], Epochs since improvement 0
  7%|▋         | 37/500 [48:01<9:37:49, 74.88s/it]   8%|▊         | 38/500 [49:33<10:15:34, 79.94s/it]Epoch: 38 done with learning rate 5.30E-03, Train loss: 2.45E+06, Train scatter: [0.2869 0.0633 0.2892 0.472 ]
L1 regularization loss: 3.08E+00, L2 regularization loss: 7.97E-01
Test scatter: [0.2869 0.0634 0.291  0.4723], Lowest was [0.2869 0.0634 0.289  0.4689]
Median for last 10 epochs: [0.361  0.0671 0.291  0.4817], Epochs since improvement 0
  8%|▊         | 39/500 [50:35<9:33:00, 74.58s/it] Epoch: 40 done with learning rate 5.70E-03, Train loss: 2.51E+06, Train scatter: [0.366  0.0639 0.2946 0.5358]
L1 regularization loss: 3.12E+00, L2 regularization loss: 8.19E-01
Test scatter: [0.3652 0.0642 0.2925 0.5459], Lowest was [0.2869 0.0634 0.289  0.4689]
Median for last 10 epochs: [0.361  0.0642 0.291  0.4817], Epochs since improvement 2
  8%|▊         | 40/500 [52:17<10:33:53, 82.68s/it]  8%|▊         | 41/500 [53:20<9:48:22, 76.91s/it]   8%|▊         | 42/500 [54:53<10:24:28, 81.81s/it]Epoch: 42 done with learning rate 6.10E-03, Train loss: 2.35E+06, Train scatter: [0.37   0.0608 0.3254 0.4965]
L1 regularization loss: 3.17E+00, L2 regularization loss: 8.43E-01
Test scatter: [0.3689 0.0613 0.3311 0.4956], Lowest was [0.2869 0.0613 0.289  0.4689]
Median for last 10 epochs: [0.361  0.0635 0.291  0.4817], Epochs since improvement 0
  9%|▊         | 43/500 [55:57<9:40:40, 76.24s/it]   9%|▉         | 44/500 [57:30<10:17:47, 81.29s/it]Epoch: 44 done with learning rate 6.49E-03, Train loss: 2.26E+06, Train scatter: [0.3932 0.062  0.2849 0.4631]
L1 regularization loss: 3.22E+00, L2 regularization loss: 8.76E-01
Test scatter: [0.3842 0.0625 0.2875 0.4636], Lowest was [0.2869 0.0613 0.2875 0.4636]
Median for last 10 epochs: [0.3652 0.0634 0.291  0.4723], Epochs since improvement 0
  9%|▉         | 45/500 [58:31<9:31:52, 75.41s/it]   9%|▉         | 46/500 [1:00:03<10:07:34, 80.30s/it]Epoch: 46 done with learning rate 6.87E-03, Train loss: 2.62E+07, Train scatter: [0.9351 0.1801 0.5441 0.9945]
L1 regularization loss: 7.72E+00, L2 regularization loss: 2.23E+00
Test scatter: [0.9195 0.1764 0.5355 0.9842], Lowest was [0.2869 0.0613 0.2875 0.4636]
Median for last 10 epochs: [0.3689 0.0634 0.2925 0.4956], Epochs since improvement 2
  9%|▉         | 47/500 [1:01:05<9:24:31, 74.77s/it]  10%|▉         | 48/500 [1:02:37<10:02:11, 79.94s/it]Epoch: 48 done with learning rate 7.24E-03, Train loss: 1.47E+07, Train scatter: [0.9431 0.1258 0.5442 0.8966]
L1 regularization loss: 7.75E+00, L2 regularization loss: 2.35E+00
Test scatter: [0.9274 0.1246 0.5356 0.8882], Lowest was [0.2869 0.0613 0.2875 0.4636]
Median for last 10 epochs: [0.3842 0.0642 0.3311 0.5459], Epochs since improvement 4
 10%|▉         | 49/500 [1:03:41<9:24:42, 75.13s/it] Epoch: 50 done with learning rate 7.60E-03, Train loss: 9.24E+06, Train scatter: [0.8016 0.1259 0.5434 0.7897]
L1 regularization loss: 7.80E+00, L2 regularization loss: 2.42E+00
Test scatter: [0.7891 0.125  0.5348 0.785 ], Lowest was [0.2869 0.0613 0.2875 0.4636]
Median for last 10 epochs: [0.7891 0.1246 0.5348 0.785 ], Epochs since improvement 6
 10%|█         | 50/500 [1:05:22<10:22:00, 82.94s/it] 10%|█         | 51/500 [1:06:24<9:33:40, 76.66s/it]  10%|█         | 52/500 [1:07:56<10:07:05, 81.31s/it]Epoch: 52 done with learning rate 7.94E-03, Train loss: 8.02E+06, Train scatter: [0.7132 0.126  0.5425 0.7769]
L1 regularization loss: 7.81E+00, L2 regularization loss: 2.47E+00
Test scatter: [0.7037 0.1249 0.534  0.7721], Lowest was [0.2869 0.0613 0.2875 0.4636]
Median for last 10 epochs: [0.7891 0.1249 0.5348 0.785 ], Epochs since improvement 8
 11%|█         | 53/500 [1:08:59<9:25:20, 75.88s/it]  11%|█         | 54/500 [1:10:33<10:03:56, 81.25s/it]Epoch: 54 done with learning rate 8.26E-03, Train loss: 7.33E+06, Train scatter: [0.7072 0.126  0.5415 0.778 ]
L1 regularization loss: 7.82E+00, L2 regularization loss: 2.49E+00
Test scatter: [0.6986 0.1248 0.5331 0.7734], Lowest was [0.2869 0.0613 0.2875 0.4636]
Median for last 10 epochs: [0.7891 0.1249 0.5348 0.785 ], Epochs since improvement 10
 11%|█         | 55/500 [1:11:36<9:22:04, 75.78s/it]  11%|█         | 56/500 [1:13:09<9:59:29, 81.01s/it]Epoch: 56 done with learning rate 8.56E-03, Train loss: 6.61E+06, Train scatter: [0.656  0.1203 0.5394 0.7782]
L1 regularization loss: 7.82E+00, L2 regularization loss: 2.52E+00
Test scatter: [0.6487 0.1194 0.531  0.7716], Lowest was [0.2869 0.0613 0.2875 0.4636]
Median for last 10 epochs: [0.7037 0.1248 0.534  0.7734], Epochs since improvement 12
 11%|█▏        | 57/500 [1:14:11<9:16:00, 75.31s/it] 12%|█▏        | 58/500 [1:15:41<9:47:05, 79.70s/it]Epoch: 58 done with learning rate 8.83E-03, Train loss: 6.01E+06, Train scatter: [0.6518 0.1193 0.5331 0.7567]
L1 regularization loss: 7.83E+00, L2 regularization loss: 2.59E+00
Test scatter: [0.6446 0.1183 0.5251 0.7497], Lowest was [0.2869 0.0613 0.2875 0.4636]
Median for last 10 epochs: [0.6986 0.1248 0.5331 0.7721], Epochs since improvement 14
 12%|█▏        | 59/500 [1:16:44<9:07:46, 74.53s/it]Epoch: 60 done with learning rate 9.08E-03, Train loss: 5.36E+06, Train scatter: [0.6732 0.1194 0.5257 0.7596]
L1 regularization loss: 7.85E+00, L2 regularization loss: 2.69E+00
Test scatter: [0.6659 0.1184 0.5182 0.753 ], Lowest was [0.2869 0.0613 0.2875 0.4636]
Median for last 10 epochs: [0.6659 0.1194 0.531  0.7716], Epochs since improvement 16
 12%|█▏        | 60/500 [1:18:24<10:02:12, 82.12s/it] 12%|█▏        | 61/500 [1:19:27<9:19:34, 76.48s/it]  12%|█▏        | 62/500 [1:20:58<9:50:49, 80.94s/it]Epoch: 62 done with learning rate 9.31E-03, Train loss: 5.16E+06, Train scatter: [0.6508 0.1194 0.5216 0.7547]
L1 regularization loss: 7.89E+00, L2 regularization loss: 2.82E+00
Test scatter: [0.6431 0.1183 0.5144 0.7487], Lowest was [0.2869 0.0613 0.2875 0.4636]
Median for last 10 epochs: [0.6487 0.1184 0.5251 0.753 ], Epochs since improvement 18
 13%|█▎        | 63/500 [1:22:03<9:13:36, 76.01s/it] 13%|█▎        | 64/500 [1:23:36<9:49:10, 81.08s/it]Epoch: 64 done with learning rate 9.50E-03, Train loss: 4.68E+06, Train scatter: [0.5911 0.1166 0.5193 0.7287]
L1 regularization loss: 7.92E+00, L2 regularization loss: 3.02E+00
Test scatter: [0.5836 0.1153 0.512  0.7219], Lowest was [0.2869 0.0613 0.2875 0.4636]
Median for last 10 epochs: [0.6446 0.1183 0.5182 0.7497], Epochs since improvement 20
 13%|█▎        | 65/500 [1:24:39<9:10:02, 75.87s/it] 13%|█▎        | 65/500 [1:26:10<9:36:43, 79.55s/it]
Epoch: 66 done with learning rate 9.66E-03, Train loss: 3.85E+06, Train scatter: [0.5282 0.1145 0.5015 0.683 ]
L1 regularization loss: 7.93E+00, L2 regularization loss: 3.07E+00
Test scatter: [0.519  0.1124 0.4945 0.6723], Lowest was [0.2869 0.0613 0.2875 0.4636]
Median for last 10 epochs: [0.6431 0.1183 0.5144 0.7487], Epochs since improvement 22
Exited after 66 epochs due to early stopping
5170.68 seconds spent training, 10.341 seconds per epoch. Processed 6734 trees per second
[0.5189553  0.11241711 0.49447104 0.67225873]
{'epoch_exit': 65, 'scatter_m_star': 0.5189553, 'lowest_m_star': 0.28686926, 'last20_m_star': 0.6573062, 'last10_m_star': 0.643117, 'scatter_v_disk': 0.11241711, 'lowest_v_disk': 0.061282184, 'last20_v_disk': 0.11889364, 'last10_v_disk': 0.118256405, 'scatter_m_cold': 0.49447104, 'lowest_m_cold': 0.28747365, 'last20_m_cold': 0.52804494, 'last10_m_cold': 0.5143897, 'scatter_sfr_100': 0.67225873, 'lowest_sfr_100': 0.46363133, 'last20_sfr_100': 0.7623012, 'last10_sfr_100': 0.7486858}
Finished 1/1
Exited training after None epochs
Experiment 5 done: 5 / 6
Exploring ['conv_layers', 'batch_size']
Currently doing [  3 512]
{'experiment': 'GraphMerge', 'group': 'meta_pysr', 'move': False, 'model': 'MetaEdge', 'log': True, 'run_params': {'n_epochs': 500, 'n_trials': 1, 'batch_size': 512, 'val_epoch': 2, 'early_stopping': True, 'patience': 20, 'l1_lambda': 0.0001, 'l2_lambda': 0.0001, 'loss_func': 'GaussNd', 'metrics': 'test_multi_varrho', 'performance_plot': 'multi_base', 'save': True, 'seed': True, 'num_workers': 4}, 'learn_params': {'learning_rate': 0.01, 'schedule': 'onecycle', 'g_up': 1, 'g_down': 0.95, 'warmup': 4, 'period': 5, 'eta_min': 1e-05}, 'hyper_params': {'hidden_channels': 128, 'conv_layers': 3, 'conv_activation': 'relu', 'decode_activation': 'leakyrelu', 'decode_layers': 2, 'layernorm': True, 'agg': 'sum', 'variance': True, 'rho': 0, 'in_channels': 43, 'out_channels': 4}, 'data_params': {'case': 'vlarge_all_4t_z0.0_quantile_raw', 'targets': [0, 1, 2, 3], 'del_feats': [], 'split': 0.8, 'test': 0, 'scale': 0}, 'experiment_name': 'con3bat512'}
/home/cj1223/../../scratch/gpfs/cj1223/GraphResults/results_meta_pysr_120322/MetaEdge_vlarge_all_4t_z0.0_quantile_raw_kmkfrx
RelU conv activation
LeakyRelU decode activation
N_params 546504
Made folder for saving model
  0%|          | 0/500 [00:00<?, ?it/s]  0%|          | 1/500 [00:54<7:33:42, 54.55s/it]  0%|          | 2/500 [02:16<9:48:13, 70.87s/it]Epoch: 2 done with learning rate 4.17E-04, Train loss: 1.19E+08, Train scatter: [0.9352 0.173  0.5441 0.9953]
L1 regularization loss: 2.44E+00, L2 regularization loss: 4.23E-01
Test scatter: [0.9196 0.1677 0.5355 0.985 ], Lowest was [0.9196 0.1677 0.5355 0.985 ]
Median for last 10 epochs: [0.9196 0.1677 0.5355 0.985 ], Epochs since improvement 2
  1%|          | 3/500 [03:10<8:41:12, 62.92s/it]  1%|          | 4/500 [04:33<9:45:02, 70.77s/it]Epoch: 4 done with learning rate 4.67E-04, Train loss: 8.84E+07, Train scatter: [0.9352 0.1355 0.5441 0.9955]
L1 regularization loss: 2.46E+00, L2 regularization loss: 4.25E-01
Test scatter: [0.9195 0.1313 0.5355 0.9851], Lowest was [0.9195 0.1313 0.5355 0.985 ]
Median for last 10 epochs: [0.9195 0.1313 0.5355 0.9851], Epochs since improvement 0
  1%|          | 5/500 [05:28<8:57:30, 65.15s/it]  1%|          | 6/500 [06:51<9:46:46, 71.27s/it]Epoch: 6 done with learning rate 5.51E-04, Train loss: 8.26E+07, Train scatter: [0.9349 0.1135 0.5441 0.9954]
L1 regularization loss: 2.47E+00, L2 regularization loss: 4.29E-01
Test scatter: [0.9193 0.111  0.5355 0.9851], Lowest was [0.9193 0.111  0.5355 0.985 ]
Median for last 10 epochs: [0.9193 0.111  0.5355 0.9851], Epochs since improvement 0
  1%|▏         | 7/500 [07:46<9:01:41, 65.93s/it]  2%|▏         | 8/500 [09:07<9:40:05, 70.74s/it]Epoch: 8 done with learning rate 6.67E-04, Train loss: 7.99E+07, Train scatter: [0.934  0.1039 0.544  0.9954]
L1 regularization loss: 2.48E+00, L2 regularization loss: 4.33E-01
Test scatter: [0.9183 0.1016 0.5354 0.9851], Lowest was [0.9183 0.1016 0.5354 0.985 ]
Median for last 10 epochs: [0.9188 0.1063 0.5355 0.9851], Epochs since improvement 0
  2%|▏         | 9/500 [10:02<8:58:21, 65.79s/it]Epoch: 10 done with learning rate 8.15E-04, Train loss: 7.87E+07, Train scatter: [0.7694 0.0921 0.5439 0.9955]
L1 regularization loss: 2.49E+00, L2 regularization loss: 4.40E-01
Test scatter: [0.7577 0.0923 0.5353 0.9851], Lowest was [0.7577 0.0923 0.5353 0.985 ]
Median for last 10 epochs: [0.9183 0.1016 0.5354 0.9851], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
  2%|▏         | 10/500 [11:31<9:55:34, 72.93s/it]  2%|▏         | 11/500 [12:25<9:08:36, 67.31s/it]  2%|▏         | 12/500 [13:47<9:43:16, 71.71s/it]Epoch: 12 done with learning rate 9.94E-04, Train loss: 7.80E+07, Train scatter: [0.6355 0.0875 0.5438 0.9954]
L1 regularization loss: 2.50E+00, L2 regularization loss: 4.50E-01
Test scatter: [0.6286 0.0875 0.5353 0.9851], Lowest was [0.6286 0.0875 0.5353 0.985 ]
Median for last 10 epochs: [0.9183 0.1016 0.5354 0.9851], Epochs since improvement 0
  3%|▎         | 13/500 [14:41<8:58:13, 66.31s/it]  3%|▎         | 14/500 [16:03<9:35:57, 71.11s/it]Epoch: 14 done with learning rate 1.20E-03, Train loss: 7.73E+07, Train scatter: [0.5466 0.0851 0.5434 0.9954]
L1 regularization loss: 2.51E+00, L2 regularization loss: 4.60E-01
Test scatter: [0.5431 0.0851 0.5349 0.9851], Lowest was [0.5431 0.0851 0.5349 0.985 ]
Median for last 10 epochs: [0.7577 0.0923 0.5353 0.9851], Epochs since improvement 0
  3%|▎         | 15/500 [16:58<8:55:58, 66.31s/it]  3%|▎         | 16/500 [18:21<9:34:25, 71.21s/it]Epoch: 16 done with learning rate 1.44E-03, Train loss: 7.60E+07, Train scatter: [0.5738 0.0837 0.5368 0.9954]
L1 regularization loss: 2.53E+00, L2 regularization loss: 4.67E-01
Test scatter: [0.5632 0.0827 0.5286 0.9851], Lowest was [0.5431 0.0827 0.5286 0.985 ]
Median for last 10 epochs: [0.6286 0.0875 0.5353 0.9851], Epochs since improvement 0
  3%|▎         | 17/500 [19:16<8:54:25, 66.39s/it]  4%|▎         | 18/500 [20:39<9:33:16, 71.36s/it]Epoch: 18 done with learning rate 1.70E-03, Train loss: 7.51E+07, Train scatter: [0.4672 0.0842 0.5269 0.9954]
L1 regularization loss: 2.54E+00, L2 regularization loss: 4.73E-01
Test scatter: [0.4729 0.0864 0.5191 0.985 ], Lowest was [0.4729 0.0827 0.5191 0.985 ]
Median for last 10 epochs: [0.5632 0.0864 0.5349 0.9851], Epochs since improvement 0
  4%|▍         | 19/500 [21:33<8:48:59, 65.99s/it]Epoch: 20 done with learning rate 1.99E-03, Train loss: 7.43E+07, Train scatter: [0.5085 0.092  0.4905 0.9954]
L1 regularization loss: 2.56E+00, L2 regularization loss: 4.82E-01
Test scatter: [0.4978 0.0893 0.4783 0.985 ], Lowest was [0.4729 0.0827 0.4783 0.985 ]
Median for last 10 epochs: [0.5431 0.0864 0.5286 0.9851], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
  4%|▍         | 20/500 [23:02<9:45:29, 73.19s/it]  4%|▍         | 21/500 [23:56<8:57:28, 67.32s/it]  4%|▍         | 22/500 [25:20<9:35:41, 72.26s/it]Epoch: 22 done with learning rate 2.30E-03, Train loss: 7.31E+07, Train scatter: [0.6193 0.0934 0.3687 0.9954]
L1 regularization loss: 2.59E+00, L2 regularization loss: 4.94E-01
Test scatter: [0.6127 0.0933 0.3692 0.9851], Lowest was [0.4729 0.0827 0.3692 0.985 ]
Median for last 10 epochs: [0.5431 0.0864 0.5191 0.9851], Epochs since improvement 0
  5%|▍         | 23/500 [26:14<8:50:29, 66.73s/it]  5%|▍         | 24/500 [27:36<9:26:14, 71.38s/it]Epoch: 24 done with learning rate 2.63E-03, Train loss: 7.20E+07, Train scatter: [0.6274 0.0932 0.3557 0.9953]
L1 regularization loss: 2.60E+00, L2 regularization loss: 5.02E-01
Test scatter: [0.6338 0.0949 0.3591 0.985 ], Lowest was [0.4729 0.0827 0.3591 0.985 ]
Median for last 10 epochs: [0.5632 0.0893 0.4783 0.985 ], Epochs since improvement 0
  5%|▌         | 25/500 [28:31<8:46:15, 66.48s/it]  5%|▌         | 26/500 [29:52<9:19:38, 70.84s/it]Epoch: 26 done with learning rate 2.98E-03, Train loss: 7.12E+07, Train scatter: [0.5819 0.0809 0.3224 0.9954]
L1 regularization loss: 2.62E+00, L2 regularization loss: 5.10E-01
Test scatter: [0.5763 0.0798 0.323  0.985 ], Lowest was [0.4729 0.0798 0.323  0.985 ]
Median for last 10 epochs: [0.5763 0.0893 0.3692 0.985 ], Epochs since improvement 0
  5%|▌         | 27/500 [30:46<8:37:28, 65.64s/it]  6%|▌         | 28/500 [32:06<9:11:13, 70.07s/it]Epoch: 28 done with learning rate 3.34E-03, Train loss: 7.12E+07, Train scatter: [0.5779 0.0823 0.3669 0.9954]
L1 regularization loss: 2.63E+00, L2 regularization loss: 5.15E-01
Test scatter: [0.573  0.0815 0.371  0.9851], Lowest was [0.4729 0.0798 0.323  0.985 ]
Median for last 10 epochs: [0.5763 0.0893 0.3692 0.985 ], Epochs since improvement 2
  6%|▌         | 29/500 [33:01<8:34:07, 65.49s/it]Epoch: 30 done with learning rate 3.72E-03, Train loss: 7.08E+07, Train scatter: [0.5249 0.0783 0.3036 0.9954]
L1 regularization loss: 2.65E+00, L2 regularization loss: 5.21E-01
Test scatter: [0.5245 0.0788 0.3075 0.985 ], Lowest was [0.4729 0.0788 0.3075 0.985 ]
Median for last 10 epochs: [0.5763 0.0815 0.3591 0.985 ], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
  6%|▌         | 30/500 [34:30<9:28:48, 72.61s/it]  6%|▌         | 31/500 [35:25<8:45:38, 67.25s/it]  6%|▋         | 32/500 [36:46<9:18:28, 71.60s/it]Epoch: 32 done with learning rate 4.10E-03, Train loss: 7.03E+07, Train scatter: [0.4437 0.0772 0.2938 0.9954]
L1 regularization loss: 2.67E+00, L2 regularization loss: 5.30E-01
Test scatter: [0.4438 0.0772 0.2969 0.985 ], Lowest was [0.4438 0.0772 0.2969 0.985 ]
Median for last 10 epochs: [0.573  0.0798 0.323  0.985 ], Epochs since improvement 0
  7%|▋         | 33/500 [37:42<8:38:38, 66.64s/it]  7%|▋         | 34/500 [39:03<9:11:24, 71.00s/it]Epoch: 34 done with learning rate 4.50E-03, Train loss: 6.97E+07, Train scatter: [0.5316 0.0787 0.3605 0.9954]
L1 regularization loss: 2.68E+00, L2 regularization loss: 5.36E-01
Test scatter: [0.5197 0.0779 0.3659 0.9851], Lowest was [0.4438 0.0772 0.2969 0.985 ]
Median for last 10 epochs: [0.5245 0.0788 0.323  0.985 ], Epochs since improvement 2
  7%|▋         | 35/500 [39:56<8:30:09, 65.83s/it]  7%|▋         | 36/500 [41:17<9:03:50, 70.32s/it]Epoch: 36 done with learning rate 4.90E-03, Train loss: 6.92E+07, Train scatter: [0.3598 0.0728 0.2863 0.9954]
L1 regularization loss: 2.69E+00, L2 regularization loss: 5.43E-01
Test scatter: [0.365  0.072  0.2933 0.985 ], Lowest was [0.365  0.072  0.2933 0.985 ]
Median for last 10 epochs: [0.5197 0.0779 0.3075 0.985 ], Epochs since improvement 0
  7%|▋         | 37/500 [42:12<8:26:08, 65.59s/it]  8%|▊         | 38/500 [43:34<9:03:26, 70.58s/it]Epoch: 38 done with learning rate 5.30E-03, Train loss: 6.80E+07, Train scatter: [0.512  0.0766 0.3129 0.9953]
L1 regularization loss: 2.73E+00, L2 regularization loss: 5.57E-01
Test scatter: [0.4992 0.0756 0.3138 0.985 ], Lowest was [0.365  0.072  0.2933 0.985 ]
Median for last 10 epochs: [0.4992 0.0772 0.3075 0.985 ], Epochs since improvement 2
  8%|▊         | 39/500 [44:28<8:24:14, 65.63s/it]Epoch: 40 done with learning rate 5.70E-03, Train loss: 3.18E+07, Train scatter: [0.4135 0.079  0.3608 0.9953]
L1 regularization loss: 2.74E+00, L2 regularization loss: 5.71E-01
Test scatter: [0.4146 0.0777 0.3657 0.9849], Lowest was [0.365  0.072  0.2933 0.9849]
Median for last 10 epochs: [0.4438 0.0772 0.3138 0.985 ], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
  8%|▊         | 40/500 [45:57<9:17:32, 72.72s/it]  8%|▊         | 41/500 [46:52<8:35:35, 67.40s/it]  8%|▊         | 42/500 [48:14<9:07:04, 71.67s/it]Epoch: 42 done with learning rate 6.10E-03, Train loss: 1.02E+07, Train scatter: [1.5124 0.135  0.4544 0.7133]
L1 regularization loss: 2.77E+00, L2 regularization loss: 5.89E-01
Test scatter: [1.5774 0.1346 0.4477 0.7031], Lowest was [0.365  0.072  0.2933 0.7031]
Median for last 10 epochs: [0.4992 0.0777 0.3657 0.985 ], Epochs since improvement 0
  9%|▊         | 43/500 [49:08<8:24:24, 66.23s/it]  9%|▉         | 44/500 [50:29<8:59:07, 70.94s/it]Epoch: 44 done with learning rate 6.49E-03, Train loss: 1.36E+07, Train scatter: [0.9029 0.1507 0.544  0.9824]
L1 regularization loss: 2.87E+00, L2 regularization loss: 6.38E-01
Test scatter: [0.8841 0.1463 0.5355 0.9722], Lowest was [0.365  0.072  0.2933 0.7031]
Median for last 10 epochs: [0.4992 0.0777 0.3657 0.9849], Epochs since improvement 2
  9%|▉         | 45/500 [51:24<8:20:01, 65.94s/it]  9%|▉         | 46/500 [52:45<8:53:10, 70.46s/it]Epoch: 46 done with learning rate 6.87E-03, Train loss: 1.71E+07, Train scatter: [0.9353 0.1716 0.5441 0.9947]
L1 regularization loss: 3.48E+00, L2 regularization loss: 8.69E-01
Test scatter: [0.9194 0.1676 0.5355 0.9843], Lowest was [0.365  0.072  0.2933 0.7031]
Median for last 10 epochs: [0.8841 0.1346 0.4477 0.9843], Epochs since improvement 4
  9%|▉         | 47/500 [53:40<8:17:19, 65.87s/it] 10%|▉         | 48/500 [55:04<8:56:23, 71.20s/it]Epoch: 48 done with learning rate 7.24E-03, Train loss: 1.30E+07, Train scatter: [0.9233 0.1643 0.5436 0.9638]
L1 regularization loss: 3.50E+00, L2 regularization loss: 9.21E-01
Test scatter: [0.9081 0.1607 0.5351 0.9539], Lowest was [0.365  0.072  0.2933 0.7031]
Median for last 10 epochs: [0.9081 0.1463 0.5351 0.9722], Epochs since improvement 6
 10%|▉         | 49/500 [55:57<8:15:10, 65.88s/it]Epoch: 50 done with learning rate 7.60E-03, Train loss: 8.93E+06, Train scatter: [0.7247 0.129  0.5352 0.6984]
L1 regularization loss: 3.55E+00, L2 regularization loss: 1.02E+00
Test scatter: [0.7119 0.1264 0.5266 0.678 ], Lowest was [0.365  0.072  0.2933 0.678 ]
Median for last 10 epochs: [0.9081 0.1463 0.5351 0.9539], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 10%|█         | 50/500 [57:27<9:08:28, 73.13s/it] 10%|█         | 51/500 [58:22<8:25:29, 67.55s/it] 10%|█         | 52/500 [59:44<8:57:00, 71.92s/it]Epoch: 52 done with learning rate 7.94E-03, Train loss: 6.93E+06, Train scatter: [0.5738 0.1034 0.5426 0.6487]
L1 regularization loss: 3.62E+00, L2 regularization loss: 1.09E+00
Test scatter: [0.5635 0.1027 0.5341 0.642 ], Lowest was [0.365  0.072  0.2933 0.642 ]
Median for last 10 epochs: [0.8841 0.1463 0.5351 0.9539], Epochs since improvement 0
 11%|█         | 53/500 [1:00:38<8:15:23, 66.50s/it] 11%|█         | 54/500 [1:01:59<8:47:12, 70.92s/it]Epoch: 54 done with learning rate 8.26E-03, Train loss: 6.27E+06, Train scatter: [0.5295 0.0979 0.4669 0.5894]
L1 regularization loss: 3.62E+00, L2 regularization loss: 1.12E+00
Test scatter: [0.524  0.0973 0.4622 0.5869], Lowest was [0.365  0.072  0.2933 0.5869]
Median for last 10 epochs: [0.7119 0.1264 0.5341 0.678 ], Epochs since improvement 0
 11%|█         | 55/500 [1:02:54<8:11:13, 66.23s/it] 11%|█         | 56/500 [1:04:16<8:45:08, 70.96s/it]Epoch: 56 done with learning rate 8.56E-03, Train loss: 6.01E+06, Train scatter: [0.5525 0.0964 0.5251 0.5805]
L1 regularization loss: 3.62E+00, L2 regularization loss: 1.14E+00
Test scatter: [0.5484 0.0956 0.517  0.5765], Lowest was [0.365  0.072  0.2933 0.5765]
Median for last 10 epochs: [0.5635 0.1027 0.5266 0.642 ], Epochs since improvement 0
 11%|█▏        | 57/500 [1:05:10<8:05:52, 65.81s/it] 12%|█▏        | 58/500 [1:06:32<8:40:33, 70.66s/it]Epoch: 58 done with learning rate 8.83E-03, Train loss: 5.02E+06, Train scatter: [0.4814 0.0889 0.4764 0.5488]
L1 regularization loss: 3.64E+00, L2 regularization loss: 1.18E+00
Test scatter: [0.4778 0.0885 0.4701 0.5478], Lowest was [0.365  0.072  0.2933 0.5478]
Median for last 10 epochs: [0.5484 0.0973 0.517  0.5869], Epochs since improvement 0
 12%|█▏        | 59/500 [1:07:26<8:02:05, 65.59s/it]Epoch: 60 done with learning rate 9.08E-03, Train loss: 4.65E+06, Train scatter: [0.4685 0.0873 0.456  0.5374]
L1 regularization loss: 3.67E+00, L2 regularization loss: 1.22E+00
Test scatter: [0.471  0.087  0.4517 0.5374], Lowest was [0.365  0.072  0.2933 0.5374]
Median for last 10 epochs: [0.524  0.0956 0.4701 0.5765], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 12%|█▏        | 60/500 [1:08:55<8:53:34, 72.76s/it] 12%|█▏        | 61/500 [1:09:49<8:11:28, 67.17s/it] 12%|█▏        | 62/500 [1:11:12<8:44:11, 71.81s/it]Epoch: 62 done with learning rate 9.31E-03, Train loss: 4.68E+06, Train scatter: [0.4619 0.0846 0.4633 0.5303]
L1 regularization loss: 3.70E+00, L2 regularization loss: 1.25E+00
Test scatter: [0.4626 0.0846 0.459  0.5313], Lowest was [0.365  0.072  0.2933 0.5313]
Median for last 10 epochs: [0.4778 0.0885 0.4622 0.5478], Epochs since improvement 0
 13%|█▎        | 63/500 [1:12:06<8:04:58, 66.59s/it] 13%|█▎        | 64/500 [1:13:28<8:36:23, 71.06s/it]Epoch: 64 done with learning rate 9.50E-03, Train loss: 3.97E+06, Train scatter: [0.459  0.0862 0.3973 0.5405]
L1 regularization loss: 3.70E+00, L2 regularization loss: 1.28E+00
Test scatter: [0.4554 0.0852 0.3945 0.5342], Lowest was [0.365  0.072  0.2933 0.5313]
Median for last 10 epochs: [0.471  0.087  0.459  0.5374], Epochs since improvement 2
 13%|█▎        | 65/500 [1:14:22<7:58:05, 65.94s/it] 13%|█▎        | 66/500 [1:15:45<8:34:21, 71.11s/it]Epoch: 66 done with learning rate 9.66E-03, Train loss: 3.57E+06, Train scatter: [0.4655 0.0818 0.3671 0.5206]
L1 regularization loss: 3.71E+00, L2 regularization loss: 1.31E+00
Test scatter: [0.4624 0.0818 0.3702 0.5191], Lowest was [0.365  0.072  0.2933 0.5191]
Median for last 10 epochs: [0.4626 0.0852 0.4517 0.5342], Epochs since improvement 0
 13%|█▎        | 67/500 [1:16:39<7:55:58, 65.96s/it] 14%|█▎        | 68/500 [1:18:02<8:31:53, 71.10s/it]Epoch: 68 done with learning rate 9.80E-03, Train loss: 3.36E+06, Train scatter: [0.4751 0.0809 0.3654 0.5205]
L1 regularization loss: 3.72E+00, L2 regularization loss: 1.33E+00
Test scatter: [0.4762 0.0811 0.3704 0.5239], Lowest was [0.365  0.072  0.2933 0.5191]
Median for last 10 epochs: [0.4626 0.0846 0.3945 0.5313], Epochs since improvement 2
 14%|█▍        | 69/500 [1:18:56<7:53:03, 65.86s/it]Epoch: 70 done with learning rate 9.90E-03, Train loss: 3.59E+06, Train scatter: [0.5249 0.0798 0.354  0.5143]
L1 regularization loss: 3.75E+00, L2 regularization loss: 1.37E+00
Test scatter: [0.4785 0.0796 0.3598 0.5132], Lowest was [0.365  0.072  0.2933 0.5132]
Median for last 10 epochs: [0.4626 0.0818 0.3704 0.5239], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 14%|█▍        | 70/500 [1:20:25<8:43:01, 72.98s/it] 14%|█▍        | 71/500 [1:21:21<8:04:05, 67.70s/it] 14%|█▍        | 72/500 [1:22:41<8:30:32, 71.57s/it]Epoch: 72 done with learning rate 9.96E-03, Train loss: 3.20E+06, Train scatter: [0.4073 0.0805 0.344  0.5145]
L1 regularization loss: 3.78E+00, L2 regularization loss: 1.40E+00
Test scatter: [0.4238 0.0804 0.3515 0.5178], Lowest was [0.365  0.072  0.2933 0.5132]
Median for last 10 epochs: [0.4624 0.0811 0.3702 0.5191], Epochs since improvement 2
 15%|█▍        | 73/500 [1:23:35<7:52:05, 66.34s/it] 15%|█▍        | 74/500 [1:24:58<8:25:19, 71.17s/it]Epoch: 74 done with learning rate 1.00E-02, Train loss: 3.01E+06, Train scatter: [0.4506 0.0766 0.333  0.5023]
L1 regularization loss: 3.82E+00, L2 regularization loss: 1.46E+00
Test scatter: [0.4494 0.0763 0.3422 0.5019], Lowest was [0.365  0.072  0.2933 0.5019]
Median for last 10 epochs: [0.4624 0.0804 0.3598 0.5178], Epochs since improvement 0
 15%|█▌        | 75/500 [1:25:51<7:46:57, 65.92s/it] 15%|█▌        | 76/500 [1:27:13<8:18:53, 70.60s/it]Epoch: 76 done with learning rate 1.00E-02, Train loss: 2.85E+06, Train scatter: [0.4623 0.073  0.3408 0.542 ]
L1 regularization loss: 3.86E+00, L2 regularization loss: 1.51E+00
Test scatter: [0.4592 0.0726 0.3378 0.5435], Lowest was [0.365  0.072  0.2933 0.5019]
Median for last 10 epochs: [0.4592 0.0796 0.3515 0.5178], Epochs since improvement 2
 15%|█▌        | 77/500 [1:28:07<7:41:56, 65.52s/it] 16%|█▌        | 78/500 [1:29:28<8:13:41, 70.19s/it]Epoch: 78 done with learning rate 1.00E-02, Train loss: 2.76E+06, Train scatter: [0.4814 0.0688 0.3122 0.4911]
L1 regularization loss: 3.89E+00, L2 regularization loss: 1.56E+00
Test scatter: [0.4462 0.0687 0.3181 0.4886], Lowest was [0.365  0.0687 0.2933 0.4886]
Median for last 10 epochs: [0.4494 0.0763 0.3422 0.5132], Epochs since improvement 0
 16%|█▌        | 79/500 [1:30:22<7:39:24, 65.47s/it]Epoch: 80 done with learning rate 1.00E-02, Train loss: 2.72E+06, Train scatter: [0.4469 0.0662 0.3156 0.4882]
L1 regularization loss: 3.91E+00, L2 regularization loss: 1.61E+00
Test scatter: [0.445  0.066  0.3197 0.4882], Lowest was [0.365  0.066  0.2933 0.4882]
Median for last 10 epochs: [0.4462 0.0726 0.3378 0.5019], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 16%|█▌        | 80/500 [1:31:53<8:31:59, 73.14s/it] 16%|█▌        | 81/500 [1:32:47<7:50:22, 67.36s/it] 16%|█▋        | 82/500 [1:34:10<8:21:51, 72.04s/it]Epoch: 82 done with learning rate 9.99E-03, Train loss: 2.71E+06, Train scatter: [0.4375 0.0727 0.3717 0.5451]
L1 regularization loss: 3.94E+00, L2 regularization loss: 1.66E+00
Test scatter: [0.437  0.0719 0.3684 0.549 ], Lowest was [0.365  0.066  0.2933 0.4882]
Median for last 10 epochs: [0.4462 0.0719 0.3378 0.5019], Epochs since improvement 2
 17%|█▋        | 83/500 [1:35:05<7:46:00, 67.05s/it] 17%|█▋        | 84/500 [1:36:28<8:17:24, 71.74s/it]Epoch: 84 done with learning rate 9.99E-03, Train loss: 2.58E+06, Train scatter: [0.4362 0.0694 0.3447 0.612 ]
L1 regularization loss: 3.97E+00, L2 regularization loss: 1.71E+00
Test scatter: [0.4341 0.0691 0.3432 0.6157], Lowest was [0.365  0.066  0.2933 0.4882]
Median for last 10 epochs: [0.445  0.0691 0.3378 0.5435], Epochs since improvement 4
 17%|█▋        | 85/500 [1:37:22<7:39:21, 66.41s/it] 17%|█▋        | 86/500 [1:38:43<8:08:20, 70.77s/it]Epoch: 86 done with learning rate 9.98E-03, Train loss: 2.60E+06, Train scatter: [0.3651 0.0671 0.3154 0.4878]
L1 regularization loss: 4.00E+00, L2 regularization loss: 1.76E+00
Test scatter: [0.3666 0.0682 0.3196 0.4857], Lowest was [0.365  0.066  0.2933 0.4857]
Median for last 10 epochs: [0.437  0.0687 0.3197 0.4886], Epochs since improvement 0
 17%|█▋        | 87/500 [1:39:38<7:33:50, 65.93s/it] 18%|█▊        | 88/500 [1:41:00<8:07:27, 70.99s/it]Epoch: 88 done with learning rate 9.98E-03, Train loss: 2.43E+06, Train scatter: [0.337  0.0626 0.3038 0.4878]
L1 regularization loss: 4.03E+00, L2 regularization loss: 1.82E+00
Test scatter: [0.3392 0.0624 0.3051 0.4915], Lowest was [0.3392 0.0624 0.2933 0.4857]
Median for last 10 epochs: [0.4341 0.0682 0.3197 0.4915], Epochs since improvement 0
 18%|█▊        | 89/500 [1:41:56<7:34:44, 66.39s/it]Epoch: 90 done with learning rate 9.97E-03, Train loss: 2.80E+06, Train scatter: [0.3422 0.0646 0.3138 0.4867]
L1 regularization loss: 4.08E+00, L2 regularization loss: 1.89E+00
Test scatter: [0.3525 0.0659 0.3144 0.4855], Lowest was [0.3392 0.0624 0.2933 0.4855]
Median for last 10 epochs: [0.3666 0.0682 0.3196 0.4915], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 18%|█▊        | 90/500 [1:43:26<8:21:36, 73.41s/it] 18%|█▊        | 91/500 [1:44:20<7:40:49, 67.60s/it] 18%|█▊        | 92/500 [1:45:43<8:10:15, 72.10s/it]Epoch: 92 done with learning rate 9.96E-03, Train loss: 2.40E+06, Train scatter: [0.4049 0.061  0.2988 0.4892]
L1 regularization loss: 4.10E+00, L2 regularization loss: 1.95E+00
Test scatter: [0.3797 0.0616 0.3047 0.4852], Lowest was [0.3392 0.0616 0.2933 0.4852]
Median for last 10 epochs: [0.3666 0.0659 0.3144 0.4857], Epochs since improvement 0
 19%|█▊        | 93/500 [1:46:38<7:34:35, 67.02s/it] 19%|█▉        | 94/500 [1:48:00<8:05:03, 71.68s/it]Epoch: 94 done with learning rate 9.95E-03, Train loss: 2.37E+06, Train scatter: [0.4099 0.0614 0.2982 0.4746]
L1 regularization loss: 4.14E+00, L2 regularization loss: 2.02E+00
Test scatter: [0.4047 0.06   0.2989 0.4754], Lowest was [0.3392 0.06   0.2933 0.4754]
Median for last 10 epochs: [0.3666 0.0624 0.3051 0.4855], Epochs since improvement 0
 19%|█▉        | 95/500 [1:48:55<7:28:37, 66.46s/it] 19%|█▉        | 96/500 [1:50:16<7:57:25, 70.90s/it]Epoch: 96 done with learning rate 9.94E-03, Train loss: 2.15E+06, Train scatter: [0.4194 0.059  0.2869 0.4588]
L1 regularization loss: 4.16E+00, L2 regularization loss: 2.08E+00
Test scatter: [0.4128 0.0596 0.291  0.4591], Lowest was [0.3392 0.0596 0.291  0.4591]
Median for last 10 epochs: [0.3797 0.0616 0.3047 0.4852], Epochs since improvement 0
 19%|█▉        | 97/500 [1:51:10<7:22:09, 65.83s/it] 20%|█▉        | 98/500 [1:52:32<7:52:57, 70.59s/it]Epoch: 98 done with learning rate 9.93E-03, Train loss: 2.16E+06, Train scatter: [0.3955 0.0577 0.2814 0.4656]
L1 regularization loss: 4.19E+00, L2 regularization loss: 2.16E+00
Test scatter: [0.3895 0.0588 0.2859 0.4595], Lowest was [0.3392 0.0588 0.2859 0.4591]
Median for last 10 epochs: [0.3895 0.06   0.2989 0.4754], Epochs since improvement 0
 20%|█▉        | 99/500 [1:53:26<7:18:32, 65.62s/it]Epoch: 100 done with learning rate 9.91E-03, Train loss: 2.09E+06, Train scatter: [0.2873 0.0587 0.2897 0.4671]
L1 regularization loss: 4.21E+00, L2 regularization loss: 2.24E+00
Test scatter: [0.2858 0.0589 0.2918 0.4642], Lowest was [0.2858 0.0588 0.2859 0.4591]
Median for last 10 epochs: [0.3895 0.0596 0.2918 0.4642], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 20%|██        | 100/500 [1:54:54<8:02:47, 72.42s/it] 20%|██        | 101/500 [1:55:49<7:26:34, 67.15s/it] 20%|██        | 102/500 [1:57:11<7:56:34, 71.85s/it]Epoch: 102 done with learning rate 9.90E-03, Train loss: 1.85E+06, Train scatter: [0.3237 0.0659 0.3153 0.5124]
L1 regularization loss: 4.23E+00, L2 regularization loss: 2.30E+00
Test scatter: [0.3276 0.0658 0.3139 0.5168], Lowest was [0.2858 0.0588 0.2859 0.4591]
Median for last 10 epochs: [0.3895 0.0596 0.2918 0.4642], Epochs since improvement 2
 21%|██        | 103/500 [1:58:07<7:22:04, 66.81s/it] 21%|██        | 104/500 [1:59:27<7:48:40, 71.01s/it]Epoch: 104 done with learning rate 9.89E-03, Train loss: 1.83E+06, Train scatter: [0.4042 0.0591 0.2923 0.458 ]
L1 regularization loss: 4.25E+00, L2 regularization loss: 2.37E+00
Test scatter: [0.4067 0.0609 0.2992 0.4562], Lowest was [0.2858 0.0588 0.2859 0.4562]
Median for last 10 epochs: [0.3895 0.0596 0.2918 0.4595], Epochs since improvement 0
 21%|██        | 105/500 [2:00:23<7:16:23, 66.29s/it] 21%|██        | 106/500 [2:01:45<7:46:43, 71.07s/it]Epoch: 106 done with learning rate 9.87E-03, Train loss: 1.74E+06, Train scatter: [0.4562 0.0655 0.2938 0.4806]
L1 regularization loss: 4.27E+00, L2 regularization loss: 2.44E+00
Test scatter: [0.4389 0.0649 0.2982 0.4785], Lowest was [0.2858 0.0588 0.2859 0.4562]
Median for last 10 epochs: [0.3895 0.0609 0.2982 0.4642], Epochs since improvement 2
 21%|██▏       | 107/500 [2:02:39<7:13:10, 66.13s/it] 22%|██▏       | 108/500 [2:04:01<7:42:08, 70.74s/it]Epoch: 108 done with learning rate 9.85E-03, Train loss: 1.74E+06, Train scatter: [0.3083 0.0622 0.2922 0.4503]
L1 regularization loss: 4.29E+00, L2 regularization loss: 2.52E+00
Test scatter: [0.2982 0.062  0.2905 0.4496], Lowest was [0.2858 0.0588 0.2859 0.4496]
Median for last 10 epochs: [0.3276 0.062  0.2982 0.4642], Epochs since improvement 0
 22%|██▏       | 109/500 [2:04:55<7:08:22, 65.73s/it]Epoch: 110 done with learning rate 9.83E-03, Train loss: 1.65E+06, Train scatter: [0.2608 0.0649 0.273  0.4571]
L1 regularization loss: 4.31E+00, L2 regularization loss: 2.59E+00
Test scatter: [0.2565 0.065  0.2783 0.4583], Lowest was [0.2565 0.0588 0.2783 0.4496]
Median for last 10 epochs: [0.3276 0.0649 0.2982 0.4583], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 22%|██▏       | 110/500 [2:06:24<7:53:20, 72.82s/it] 22%|██▏       | 111/500 [2:07:20<7:19:01, 67.72s/it] 22%|██▏       | 112/500 [2:08:41<7:44:07, 71.77s/it]Epoch: 112 done with learning rate 9.81E-03, Train loss: 1.72E+06, Train scatter: [0.3205 0.0607 0.2921 0.4682]
L1 regularization loss: 4.34E+00, L2 regularization loss: 2.65E+00
Test scatter: [0.3126 0.0597 0.2907 0.4673], Lowest was [0.2565 0.0588 0.2783 0.4496]
Median for last 10 epochs: [0.3126 0.062  0.2907 0.4583], Epochs since improvement 2
 23%|██▎       | 113/500 [2:09:36<7:09:04, 66.52s/it] 23%|██▎       | 114/500 [2:10:57<7:37:03, 71.05s/it]Epoch: 114 done with learning rate 9.79E-03, Train loss: 1.41E+06, Train scatter: [0.2288 0.0544 0.267  0.437 ]
L1 regularization loss: 4.35E+00, L2 regularization loss: 2.71E+00
Test scatter: [0.2289 0.0543 0.2708 0.4363], Lowest was [0.2289 0.0543 0.2708 0.4363]
Median for last 10 epochs: [0.2982 0.062  0.2905 0.4583], Epochs since improvement 0
 23%|██▎       | 115/500 [2:11:52<7:03:54, 66.06s/it] 23%|██▎       | 116/500 [2:13:14<7:34:31, 71.02s/it]Epoch: 116 done with learning rate 9.77E-03, Train loss: 1.40E+06, Train scatter: [0.3213 0.0594 0.2702 0.4451]
L1 regularization loss: 4.36E+00, L2 regularization loss: 2.76E+00
Test scatter: [0.3081 0.0583 0.2714 0.442 ], Lowest was [0.2289 0.0543 0.2708 0.4363]
Median for last 10 epochs: [0.2982 0.0597 0.2783 0.4496], Epochs since improvement 2
 23%|██▎       | 117/500 [2:14:09<7:02:57, 66.26s/it] 24%|██▎       | 118/500 [2:15:32<7:32:27, 71.07s/it]Epoch: 118 done with learning rate 9.75E-03, Train loss: 1.32E+06, Train scatter: [0.2113 0.0541 0.2751 0.4396]
L1 regularization loss: 4.38E+00, L2 regularization loss: 2.82E+00
Test scatter: [0.2158 0.0544 0.2794 0.4397], Lowest was [0.2158 0.0543 0.2708 0.4363]
Median for last 10 epochs: [0.2565 0.0583 0.2783 0.442 ], Epochs since improvement 0
 24%|██▍       | 119/500 [2:16:27<7:01:01, 66.30s/it]Epoch: 120 done with learning rate 9.73E-03, Train loss: 1.31E+06, Train scatter: [0.3901 0.0566 0.284  0.4567]
L1 regularization loss: 4.40E+00, L2 regularization loss: 2.88E+00
Test scatter: [0.3814 0.0567 0.287  0.4481], Lowest was [0.2158 0.0543 0.2708 0.4363]
Median for last 10 epochs: [0.3081 0.0567 0.2794 0.442 ], Epochs since improvement 2
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 24%|██▍       | 120/500 [2:17:55<7:40:43, 72.75s/it] 24%|██▍       | 121/500 [2:18:50<7:06:32, 67.53s/it] 24%|██▍       | 122/500 [2:20:13<7:35:17, 72.27s/it]Epoch: 122 done with learning rate 9.70E-03, Train loss: 1.21E+06, Train scatter: [0.2607 0.0575 0.2716 0.4352]
L1 regularization loss: 4.41E+00, L2 regularization loss: 2.92E+00
Test scatter: [0.255  0.0572 0.2746 0.4345], Lowest was [0.2158 0.0543 0.2708 0.4345]
Median for last 10 epochs: [0.255  0.0567 0.2746 0.4397], Epochs since improvement 0
 25%|██▍       | 123/500 [2:21:08<7:01:24, 67.07s/it] 25%|██▍       | 124/500 [2:22:30<7:27:27, 71.40s/it]Epoch: 124 done with learning rate 9.68E-03, Train loss: 1.29E+06, Train scatter: [0.2816 0.0532 0.2676 0.4309]
L1 regularization loss: 4.43E+00, L2 regularization loss: 2.98E+00
Test scatter: [0.2727 0.0539 0.2703 0.4318], Lowest was [0.2158 0.0539 0.2703 0.4318]
Median for last 10 epochs: [0.2727 0.0567 0.2746 0.4397], Epochs since improvement 0
 25%|██▌       | 125/500 [2:23:24<6:54:07, 66.26s/it] 25%|██▌       | 126/500 [2:24:46<7:22:00, 70.91s/it]Epoch: 126 done with learning rate 9.65E-03, Train loss: 1.16E+06, Train scatter: [0.2157 0.0524 0.2583 0.4317]
L1 regularization loss: 4.44E+00, L2 regularization loss: 3.02E+00
Test scatter: [0.2139 0.0522 0.2607 0.4311], Lowest was [0.2139 0.0522 0.2607 0.4311]
Median for last 10 epochs: [0.255  0.0544 0.2746 0.4345], Epochs since improvement 0
 25%|██▌       | 127/500 [2:25:41<6:52:19, 66.33s/it] 26%|██▌       | 128/500 [2:27:03<7:19:11, 70.84s/it]Epoch: 128 done with learning rate 9.62E-03, Train loss: 1.09E+06, Train scatter: [0.1996 0.0529 0.2618 0.4278]
L1 regularization loss: 4.46E+00, L2 regularization loss: 3.06E+00
Test scatter: [0.203  0.053  0.2647 0.4248], Lowest was [0.203  0.0522 0.2607 0.4248]
Median for last 10 epochs: [0.255  0.0539 0.2703 0.4318], Epochs since improvement 0
 26%|██▌       | 129/500 [2:27:58<6:48:34, 66.08s/it]Epoch: 130 done with learning rate 9.59E-03, Train loss: 1.11E+06, Train scatter: [0.1912 0.0512 0.2567 0.4253]
L1 regularization loss: 4.47E+00, L2 regularization loss: 3.09E+00
Test scatter: [0.1906 0.0512 0.2589 0.4214], Lowest was [0.1906 0.0512 0.2589 0.4214]
Median for last 10 epochs: [0.2139 0.053  0.2647 0.4311], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 26%|██▌       | 130/500 [2:29:26<7:27:44, 72.61s/it] 26%|██▌       | 131/500 [2:30:20<6:51:56, 66.98s/it] 26%|██▋       | 132/500 [2:31:44<7:22:15, 72.11s/it]Epoch: 132 done with learning rate 9.56E-03, Train loss: 1.03E+06, Train scatter: [0.2706 0.0564 0.2763 0.4451]
L1 regularization loss: 4.49E+00, L2 regularization loss: 3.14E+00
Test scatter: [0.2627 0.0568 0.285  0.4464], Lowest was [0.1906 0.0512 0.2589 0.4214]
Median for last 10 epochs: [0.2139 0.053  0.2647 0.4311], Epochs since improvement 2
 27%|██▋       | 133/500 [2:32:38<6:48:16, 66.75s/it] 27%|██▋       | 134/500 [2:33:59<7:13:47, 71.11s/it]Epoch: 134 done with learning rate 9.53E-03, Train loss: 1.00E+06, Train scatter: [0.1876 0.0497 0.2556 0.4146]
L1 regularization loss: 4.51E+00, L2 regularization loss: 3.17E+00
Test scatter: [0.1903 0.0493 0.2562 0.4112], Lowest was [0.1903 0.0493 0.2562 0.4112]
Median for last 10 epochs: [0.203  0.0522 0.2607 0.4248], Epochs since improvement 0
 27%|██▋       | 135/500 [2:34:54<6:42:19, 66.13s/it] 27%|██▋       | 136/500 [2:36:17<7:12:19, 71.26s/it]Epoch: 136 done with learning rate 9.50E-03, Train loss: 1.01E+06, Train scatter: [0.2224 0.0541 0.2598 0.4259]
L1 regularization loss: 4.52E+00, L2 regularization loss: 3.21E+00
Test scatter: [0.2238 0.0547 0.2617 0.4232], Lowest was [0.1903 0.0493 0.2562 0.4112]
Median for last 10 epochs: [0.203  0.053  0.2617 0.4232], Epochs since improvement 2
 27%|██▋       | 137/500 [2:37:12<6:42:28, 66.52s/it] 28%|██▊       | 138/500 [2:38:34<7:08:36, 71.04s/it]Epoch: 138 done with learning rate 9.47E-03, Train loss: 9.21E+05, Train scatter: [0.2373 0.0504 0.2536 0.4148]
L1 regularization loss: 4.53E+00, L2 regularization loss: 3.24E+00
Test scatter: [0.2351 0.0502 0.2568 0.4134], Lowest was [0.1903 0.0493 0.2562 0.4112]
Median for last 10 epochs: [0.2238 0.0512 0.2589 0.4214], Epochs since improvement 4
 28%|██▊       | 139/500 [2:39:29<6:39:18, 66.37s/it]Epoch: 140 done with learning rate 9.43E-03, Train loss: 9.20E+05, Train scatter: [0.197  0.0567 0.2622 0.4268]
L1 regularization loss: 4.54E+00, L2 regularization loss: 3.27E+00
Test scatter: [0.2013 0.0565 0.2636 0.425 ], Lowest was [0.1903 0.0493 0.2562 0.4112]
Median for last 10 epochs: [0.2238 0.0547 0.2617 0.4232], Epochs since improvement 6
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 28%|██▊       | 140/500 [2:40:58<7:17:45, 72.96s/it] 28%|██▊       | 141/500 [2:41:52<6:42:12, 67.22s/it] 28%|██▊       | 142/500 [2:43:14<7:08:08, 71.75s/it]Epoch: 142 done with learning rate 9.40E-03, Train loss: 8.32E+05, Train scatter: [0.1962 0.0508 0.2608 0.4141]
L1 regularization loss: 4.55E+00, L2 regularization loss: 3.30E+00
Test scatter: [0.1985 0.0508 0.2603 0.4084], Lowest was [0.1903 0.0493 0.2562 0.4084]
Median for last 10 epochs: [0.2013 0.0508 0.2603 0.4134], Epochs since improvement 0
 29%|██▊       | 143/500 [2:44:08<6:35:42, 66.51s/it] 29%|██▉       | 144/500 [2:45:30<7:02:37, 71.23s/it]Epoch: 144 done with learning rate 9.36E-03, Train loss: 7.81E+05, Train scatter: [0.1886 0.0486 0.2523 0.4101]
L1 regularization loss: 4.57E+00, L2 regularization loss: 3.33E+00
Test scatter: [0.1862 0.0485 0.2536 0.4096], Lowest was [0.1862 0.0485 0.2536 0.4084]
Median for last 10 epochs: [0.2013 0.0508 0.2603 0.4134], Epochs since improvement 0
 29%|██▉       | 145/500 [2:46:25<6:32:00, 66.25s/it] 29%|██▉       | 146/500 [2:47:48<7:01:07, 71.38s/it]Epoch: 146 done with learning rate 9.33E-03, Train loss: 7.38E+05, Train scatter: [0.237  0.0479 0.2543 0.4179]
L1 regularization loss: 4.57E+00, L2 regularization loss: 3.36E+00
Test scatter: [0.2345 0.048  0.2552 0.4114], Lowest was [0.1862 0.048  0.2536 0.4084]
Median for last 10 epochs: [0.2013 0.0502 0.2568 0.4114], Epochs since improvement 0
 29%|██▉       | 147/500 [2:48:43<6:30:45, 66.42s/it] 30%|██▉       | 148/500 [2:50:05<6:57:00, 71.08s/it]Epoch: 148 done with learning rate 9.29E-03, Train loss: 7.23E+05, Train scatter: [0.184  0.0503 0.2534 0.4115]
L1 regularization loss: 4.58E+00, L2 regularization loss: 3.38E+00
Test scatter: [0.1895 0.0509 0.2576 0.4126], Lowest was [0.1862 0.048  0.2536 0.4084]
Median for last 10 epochs: [0.1985 0.0508 0.2576 0.4114], Epochs since improvement 2
 30%|██▉       | 149/500 [2:51:01<6:28:10, 66.35s/it]Epoch: 150 done with learning rate 9.25E-03, Train loss: 6.15E+05, Train scatter: [0.1715 0.049  0.2447 0.4115]
L1 regularization loss: 4.58E+00, L2 regularization loss: 3.40E+00
Test scatter: [0.1716 0.0488 0.2441 0.4034], Lowest was [0.1716 0.048  0.2441 0.4034]
Median for last 10 epochs: [0.1895 0.0488 0.2552 0.4096], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 30%|███       | 150/500 [2:52:31<7:09:23, 73.61s/it] 30%|███       | 151/500 [2:53:26<6:36:21, 68.14s/it] 30%|███       | 152/500 [2:54:50<7:02:01, 72.76s/it]Epoch: 152 done with learning rate 9.21E-03, Train loss: 5.92E+05, Train scatter: [0.1769 0.0472 0.2424 0.4087]
L1 regularization loss: 4.61E+00, L2 regularization loss: 3.43E+00
Test scatter: [0.1783 0.0473 0.2442 0.4065], Lowest was [0.1716 0.0473 0.2441 0.4034]
Median for last 10 epochs: [0.1862 0.0485 0.2536 0.4096], Epochs since improvement 0
 31%|███       | 153/500 [2:55:44<6:27:45, 67.05s/it] 31%|███       | 154/500 [2:57:04<6:49:07, 70.95s/it]Epoch: 154 done with learning rate 9.17E-03, Train loss: 5.35E+05, Train scatter: [0.2071 0.0515 0.245  0.4054]
L1 regularization loss: 4.61E+00, L2 regularization loss: 3.45E+00
Test scatter: [0.2081 0.0514 0.2471 0.4017], Lowest was [0.1716 0.0473 0.2441 0.4017]
Median for last 10 epochs: [0.1895 0.0488 0.2471 0.4065], Epochs since improvement 0
 31%|███       | 155/500 [2:57:58<6:19:01, 65.92s/it] 31%|███       | 156/500 [2:59:19<6:43:54, 70.45s/it]Epoch: 156 done with learning rate 9.13E-03, Train loss: 5.27E+05, Train scatter: [0.1667 0.0467 0.2428 0.3969]
L1 regularization loss: 4.64E+00, L2 regularization loss: 3.49E+00
Test scatter: [0.1667 0.0462 0.2454 0.393 ], Lowest was [0.1667 0.0462 0.2441 0.393 ]
Median for last 10 epochs: [0.1783 0.0488 0.2454 0.4034], Epochs since improvement 0
 31%|███▏      | 157/500 [3:00:12<6:12:42, 65.20s/it] 32%|███▏      | 158/500 [3:01:33<6:39:15, 70.05s/it]Epoch: 158 done with learning rate 9.09E-03, Train loss: 4.38E+05, Train scatter: [0.1956 0.0472 0.2596 0.4128]
L1 regularization loss: 4.65E+00, L2 regularization loss: 3.51E+00
Test scatter: [0.1948 0.0472 0.2604 0.4113], Lowest was [0.1667 0.0462 0.2441 0.393 ]
Median for last 10 epochs: [0.1783 0.0473 0.2454 0.4034], Epochs since improvement 2
 32%|███▏      | 159/500 [3:02:27<6:10:51, 65.25s/it]Epoch: 160 done with learning rate 9.04E-03, Train loss: 4.03E+05, Train scatter: [0.194  0.053  0.2589 0.4197]
L1 regularization loss: 4.67E+00, L2 regularization loss: 3.54E+00
Test scatter: [0.196  0.0528 0.2603 0.4181], Lowest was [0.1667 0.0462 0.2441 0.393 ]
Median for last 10 epochs: [0.1948 0.0473 0.2471 0.4065], Epochs since improvement 4
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 32%|███▏      | 160/500 [3:03:55<6:48:16, 72.05s/it] 32%|███▏      | 161/500 [3:04:49<6:15:30, 66.46s/it] 32%|███▏      | 162/500 [3:06:09<6:38:41, 70.77s/it]Epoch: 162 done with learning rate 9.00E-03, Train loss: 2.99E+05, Train scatter: [0.2111 0.0496 0.2475 0.3978]
L1 regularization loss: 4.67E+00, L2 regularization loss: 3.56E+00
Test scatter: [0.2081 0.0494 0.2488 0.3947], Lowest was [0.1667 0.0462 0.2441 0.393 ]
Median for last 10 epochs: [0.196  0.0494 0.2488 0.4017], Epochs since improvement 6
 33%|███▎      | 163/500 [3:07:04<6:10:47, 66.02s/it] 33%|███▎      | 164/500 [3:08:26<6:35:14, 70.58s/it]Epoch: 164 done with learning rate 8.96E-03, Train loss: 2.67E+05, Train scatter: [0.1689 0.0472 0.2425 0.4028]
L1 regularization loss: 4.69E+00, L2 regularization loss: 3.59E+00
Test scatter: [0.1685 0.0469 0.2448 0.3983], Lowest was [0.1667 0.0462 0.2441 0.393 ]
Median for last 10 epochs: [0.1948 0.0472 0.2488 0.3983], Epochs since improvement 8
 33%|███▎      | 165/500 [3:09:19<6:04:50, 65.34s/it] 33%|███▎      | 166/500 [3:10:40<6:31:03, 70.25s/it]Epoch: 166 done with learning rate 8.91E-03, Train loss: 1.73E+05, Train scatter: [0.1566 0.0445 0.2334 0.3922]
L1 regularization loss: 4.70E+00, L2 regularization loss: 3.61E+00
Test scatter: [0.1547 0.0441 0.2345 0.3875], Lowest was [0.1547 0.0441 0.2345 0.3875]
Median for last 10 epochs: [0.1948 0.0472 0.2488 0.3983], Epochs since improvement 0
 33%|███▎      | 167/500 [3:11:34<6:02:09, 65.26s/it] 34%|███▎      | 168/500 [3:12:55<6:27:04, 69.95s/it]Epoch: 168 done with learning rate 8.86E-03, Train loss: 8.03E+04, Train scatter: [0.2098 0.0476 0.2834 0.3979]
L1 regularization loss: 4.70E+00, L2 regularization loss: 3.63E+00
Test scatter: [0.208  0.0476 0.2846 0.3957], Lowest was [0.1547 0.0441 0.2345 0.3875]
Median for last 10 epochs: [0.196  0.0476 0.2488 0.3957], Epochs since improvement 2
 34%|███▍      | 169/500 [3:13:50<6:00:48, 65.40s/it]Epoch: 170 done with learning rate 8.82E-03, Train loss: 3.76E+04, Train scatter: [0.1564 0.0456 0.23   0.3928]
L1 regularization loss: 4.72E+00, L2 regularization loss: 3.66E+00
Test scatter: [0.1627 0.0455 0.2313 0.3914], Lowest was [0.1547 0.0441 0.2313 0.3875]
Median for last 10 epochs: [0.1685 0.0469 0.2448 0.3947], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 34%|███▍      | 170/500 [3:15:18<6:37:54, 72.35s/it] 34%|███▍      | 171/500 [3:16:13<6:07:27, 67.01s/it] 34%|███▍      | 172/500 [3:17:34<6:29:36, 71.27s/it]Epoch: 172 done with learning rate 8.77E-03, Train loss: -6.50E+04, Train scatter: [0.1522 0.0461 0.2297 0.3969]
L1 regularization loss: 4.74E+00, L2 regularization loss: 3.69E+00
Test scatter: [0.1518 0.0456 0.232  0.3898], Lowest was [0.1518 0.0441 0.2313 0.3875]
Median for last 10 epochs: [0.1627 0.0456 0.2345 0.3914], Epochs since improvement 0
 35%|███▍      | 173/500 [3:18:29<6:01:27, 66.32s/it] 35%|███▍      | 174/500 [3:19:50<6:24:07, 70.70s/it]Epoch: 174 done with learning rate 8.72E-03, Train loss: -1.20E+05, Train scatter: [0.2175 0.0443 0.2258 0.3868]
L1 regularization loss: 4.74E+00, L2 regularization loss: 3.70E+00
Test scatter: [0.2143 0.0437 0.2287 0.3856], Lowest was [0.1518 0.0437 0.2287 0.3856]
Median for last 10 epochs: [0.1627 0.0455 0.232  0.3898], Epochs since improvement 0
 35%|███▌      | 175/500 [3:20:44<5:55:35, 65.65s/it] 35%|███▌      | 176/500 [3:22:06<6:21:41, 70.68s/it]Epoch: 176 done with learning rate 8.67E-03, Train loss: -1.77E+05, Train scatter: [0.1635 0.0433 0.243  0.3864]
L1 regularization loss: 4.74E+00, L2 regularization loss: 3.72E+00
Test scatter: [0.1605 0.0429 0.2465 0.384 ], Lowest was [0.1518 0.0429 0.2287 0.384 ]
Median for last 10 epochs: [0.1627 0.0455 0.232  0.3898], Epochs since improvement 0
 35%|███▌      | 177/500 [3:23:00<5:52:55, 65.56s/it] 36%|███▌      | 178/500 [3:24:21<6:16:32, 70.16s/it]Epoch: 178 done with learning rate 8.62E-03, Train loss: -2.11E+05, Train scatter: [0.2027 0.0452 0.2323 0.3939]
L1 regularization loss: 4.76E+00, L2 regularization loss: 3.75E+00
Test scatter: [0.2017 0.0452 0.2352 0.3934], Lowest was [0.1518 0.0429 0.2287 0.384 ]
Median for last 10 epochs: [0.1627 0.0452 0.232  0.3898], Epochs since improvement 2
 36%|███▌      | 179/500 [3:25:15<5:50:44, 65.56s/it]Epoch: 180 done with learning rate 8.57E-03, Train loss: -2.30E+05, Train scatter: [0.3357 0.0459 0.3897 0.3933]
L1 regularization loss: 4.81E+00, L2 regularization loss: 3.78E+00
Test scatter: [0.326  0.045  0.3819 0.3913], Lowest was [0.1518 0.0429 0.2287 0.384 ]
Median for last 10 epochs: [0.2017 0.045  0.2352 0.3898], Epochs since improvement 4
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 36%|███▌      | 180/500 [3:26:43<6:24:37, 72.12s/it] 36%|███▌      | 181/500 [3:27:36<5:53:23, 66.47s/it] 36%|███▋      | 182/500 [3:28:57<6:15:27, 70.84s/it]Epoch: 182 done with learning rate 8.52E-03, Train loss: -2.61E+05, Train scatter: [0.147  0.0476 0.2337 0.4007]
L1 regularization loss: 4.80E+00, L2 regularization loss: 3.79E+00
Test scatter: [0.1456 0.0471 0.2359 0.3947], Lowest was [0.1456 0.0429 0.2287 0.384 ]
Median for last 10 epochs: [0.2017 0.045  0.2359 0.3913], Epochs since improvement 0
 37%|███▋      | 183/500 [3:29:52<5:48:58, 66.05s/it] 37%|███▋      | 184/500 [3:31:13<6:12:12, 70.67s/it]Epoch: 184 done with learning rate 8.46E-03, Train loss: -3.03E+05, Train scatter: [0.1396 0.0534 0.2334 0.3889]
L1 regularization loss: 4.80E+00, L2 regularization loss: 3.82E+00
Test scatter: [0.1426 0.0529 0.2358 0.386 ], Lowest was [0.1426 0.0429 0.2287 0.384 ]
Median for last 10 epochs: [0.1605 0.0452 0.2359 0.3913], Epochs since improvement 0
 37%|███▋      | 185/500 [3:32:08<5:45:46, 65.86s/it] 37%|███▋      | 186/500 [3:33:29<6:09:03, 70.52s/it]Epoch: 186 done with learning rate 8.41E-03, Train loss: -3.19E+05, Train scatter: [0.1863 0.0423 0.221  0.3905]
L1 regularization loss: 4.81E+00, L2 regularization loss: 3.84E+00
Test scatter: [0.1765 0.0417 0.2237 0.3846], Lowest was [0.1426 0.0417 0.2237 0.384 ]
Median for last 10 epochs: [0.1765 0.0452 0.2358 0.3913], Epochs since improvement 0
 37%|███▋      | 187/500 [3:34:24<5:43:17, 65.81s/it] 38%|███▊      | 188/500 [3:35:45<6:05:47, 70.34s/it]Epoch: 188 done with learning rate 8.35E-03, Train loss: -3.42E+05, Train scatter: [0.1999 0.0431 0.2257 0.3886]
L1 regularization loss: 4.82E+00, L2 regularization loss: 3.87E+00
Test scatter: [0.2033 0.0428 0.2283 0.3881], Lowest was [0.1426 0.0417 0.2237 0.384 ]
Median for last 10 epochs: [0.1765 0.045  0.2358 0.3881], Epochs since improvement 2
 38%|███▊      | 189/500 [3:36:39<5:38:48, 65.36s/it]Epoch: 190 done with learning rate 8.30E-03, Train loss: -3.60E+05, Train scatter: [0.2147 0.053  0.2468 0.421 ]
L1 regularization loss: 4.83E+00, L2 regularization loss: 3.89E+00
Test scatter: [0.2187 0.0533 0.2508 0.4234], Lowest was [0.1426 0.0417 0.2237 0.384 ]
Median for last 10 epochs: [0.1765 0.0471 0.2358 0.3881], Epochs since improvement 4
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 38%|███▊      | 190/500 [3:38:07<6:13:06, 72.21s/it] 38%|███▊      | 191/500 [3:39:01<5:44:03, 66.81s/it] 38%|███▊      | 192/500 [3:40:22<6:03:44, 70.86s/it]Epoch: 192 done with learning rate 8.24E-03, Train loss: -3.63E+05, Train scatter: [0.1316 0.0425 0.2171 0.3905]
L1 regularization loss: 4.85E+00, L2 regularization loss: 3.93E+00
Test scatter: [0.1326 0.0418 0.2203 0.3882], Lowest was [0.1326 0.0417 0.2203 0.384 ]
Median for last 10 epochs: [0.1765 0.0428 0.2283 0.3881], Epochs since improvement 0
 39%|███▊      | 193/500 [3:41:17<5:38:09, 66.09s/it] 39%|███▉      | 194/500 [3:42:38<5:59:58, 70.58s/it]Epoch: 194 done with learning rate 8.19E-03, Train loss: -3.82E+05, Train scatter: [0.1599 0.0456 0.2419 0.4019]
L1 regularization loss: 4.85E+00, L2 regularization loss: 3.95E+00
Test scatter: [0.1686 0.0456 0.2458 0.4029], Lowest was [0.1326 0.0417 0.2203 0.384 ]
Median for last 10 epochs: [0.1765 0.0428 0.2283 0.3882], Epochs since improvement 2
 39%|███▉      | 195/500 [3:43:31<5:32:40, 65.45s/it] 39%|███▉      | 196/500 [3:44:53<5:56:22, 70.34s/it]Epoch: 196 done with learning rate 8.13E-03, Train loss: -3.81E+05, Train scatter: [0.1482 0.0424 0.2354 0.3931]
L1 regularization loss: 4.86E+00, L2 regularization loss: 3.98E+00
Test scatter: [0.1528 0.0421 0.2398 0.3942], Lowest was [0.1326 0.0417 0.2203 0.384 ]
Median for last 10 epochs: [0.1686 0.0428 0.2398 0.3942], Epochs since improvement 4
 39%|███▉      | 197/500 [3:45:47<5:31:02, 65.55s/it] 40%|███▉      | 198/500 [3:47:09<5:54:34, 70.45s/it]Epoch: 198 done with learning rate 8.07E-03, Train loss: -3.94E+05, Train scatter: [0.1781 0.0433 0.221  0.3922]
L1 regularization loss: 4.89E+00, L2 regularization loss: 4.02E+00
Test scatter: [0.1734 0.0432 0.2243 0.3901], Lowest was [0.1326 0.0417 0.2203 0.384 ]
Median for last 10 epochs: [0.1686 0.0432 0.2398 0.3942], Epochs since improvement 6
 40%|███▉      | 199/500 [3:48:02<5:27:38, 65.31s/it]Epoch: 200 done with learning rate 8.01E-03, Train loss: -3.91E+05, Train scatter: [0.1721 0.0551 0.247  0.4142]
L1 regularization loss: 4.90E+00, L2 regularization loss: 4.06E+00
Test scatter: [0.1731 0.0535 0.2486 0.4083], Lowest was [0.1326 0.0417 0.2203 0.384 ]
Median for last 10 epochs: [0.1686 0.0432 0.2398 0.3942], Epochs since improvement 8
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 40%|████      | 200/500 [3:49:32<6:02:45, 72.55s/it] 40%|████      | 201/500 [3:50:26<5:34:19, 67.09s/it] 40%|████      | 202/500 [3:51:47<5:53:51, 71.25s/it]Epoch: 202 done with learning rate 7.95E-03, Train loss: -4.01E+05, Train scatter: [0.1388 0.0516 0.2643 0.4031]
L1 regularization loss: 4.92E+00, L2 regularization loss: 4.10E+00
Test scatter: [0.1393 0.0505 0.2658 0.4019], Lowest was [0.1326 0.0417 0.2203 0.384 ]
Median for last 10 epochs: [0.1686 0.0456 0.2458 0.4019], Epochs since improvement 10
 41%|████      | 203/500 [3:52:41<5:26:02, 65.87s/it] 41%|████      | 204/500 [3:54:03<5:48:52, 70.72s/it]Epoch: 204 done with learning rate 7.89E-03, Train loss: -3.92E+05, Train scatter: [0.2566 0.0503 0.2406 0.4322]
L1 regularization loss: 4.94E+00, L2 regularization loss: 4.13E+00
Test scatter: [0.2536 0.0497 0.2425 0.4252], Lowest was [0.1326 0.0417 0.2203 0.384 ]
Median for last 10 epochs: [0.1731 0.0497 0.2425 0.4019], Epochs since improvement 12
 41%|████      | 205/500 [3:54:58<5:24:27, 65.99s/it] 41%|████      | 206/500 [3:56:20<5:47:10, 70.85s/it]Epoch: 206 done with learning rate 7.83E-03, Train loss: -3.94E+05, Train scatter: [0.2914 0.0491 0.2383 0.4256]
L1 regularization loss: 4.95E+00, L2 regularization loss: 4.16E+00
Test scatter: [0.2829 0.0486 0.2403 0.4242], Lowest was [0.1326 0.0417 0.2203 0.384 ]
Median for last 10 epochs: [0.1734 0.0497 0.2425 0.4083], Epochs since improvement 14
 41%|████▏     | 207/500 [3:57:14<5:21:05, 65.75s/it] 42%|████▏     | 208/500 [3:58:35<5:43:25, 70.57s/it]Epoch: 208 done with learning rate 7.77E-03, Train loss: -4.02E+05, Train scatter: [0.1486 0.0417 0.2256 0.3966]
L1 regularization loss: 4.96E+00, L2 regularization loss: 4.20E+00
Test scatter: [0.151  0.0414 0.2302 0.3914], Lowest was [0.1326 0.0414 0.2203 0.384 ]
Median for last 10 epochs: [0.1731 0.0497 0.2425 0.4083], Epochs since improvement 0
 42%|████▏     | 209/500 [3:59:30<5:18:35, 65.69s/it]Epoch: 210 done with learning rate 7.71E-03, Train loss: -4.02E+05, Train scatter: [0.1492 0.0509 0.2746 0.412 ]
L1 regularization loss: 4.96E+00, L2 regularization loss: 4.22E+00
Test scatter: [0.1513 0.0511 0.2769 0.4123], Lowest was [0.1326 0.0414 0.2203 0.384 ]
Median for last 10 epochs: [0.1513 0.0497 0.2425 0.4123], Epochs since improvement 2
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 42%|████▏     | 210/500 [4:00:57<5:49:24, 72.29s/it] 42%|████▏     | 211/500 [4:01:52<5:22:34, 66.97s/it] 42%|████▏     | 212/500 [4:03:13<5:41:42, 71.19s/it]Epoch: 212 done with learning rate 7.65E-03, Train loss: -3.94E+05, Train scatter: [0.1278 0.0431 0.2415 0.4018]
L1 regularization loss: 4.99E+00, L2 regularization loss: 4.25E+00
Test scatter: [0.1303 0.0424 0.244  0.3976], Lowest was [0.1303 0.0414 0.2203 0.384 ]
Median for last 10 epochs: [0.1513 0.0486 0.2425 0.4123], Epochs since improvement 0
 43%|████▎     | 213/500 [4:04:07<5:15:19, 65.92s/it] 43%|████▎     | 214/500 [4:05:28<5:36:23, 70.57s/it]Epoch: 214 done with learning rate 7.58E-03, Train loss: -2.73E+05, Train scatter: [0.2106 0.048  0.245  0.4351]
L1 regularization loss: 5.43E+00, L2 regularization loss: 4.58E+00
Test scatter: [0.2124 0.0476 0.2465 0.4286], Lowest was [0.1303 0.0414 0.2203 0.384 ]
Median for last 10 epochs: [0.1513 0.0476 0.244  0.4123], Epochs since improvement 2
 43%|████▎     | 215/500 [4:06:21<5:10:23, 65.35s/it] 43%|████▎     | 216/500 [4:07:43<5:32:06, 70.16s/it]Epoch: 216 done with learning rate 7.52E-03, Train loss: -3.66E+05, Train scatter: [0.1504 0.0501 0.2399 0.4449]
L1 regularization loss: 5.44E+00, L2 regularization loss: 4.67E+00
Test scatter: [0.1472 0.049  0.2418 0.4332], Lowest was [0.1303 0.0414 0.2203 0.384 ]
Median for last 10 epochs: [0.151  0.0476 0.244  0.4123], Epochs since improvement 4
 43%|████▎     | 217/500 [4:08:36<5:07:30, 65.19s/it] 44%|████▎     | 218/500 [4:09:57<5:27:52, 69.76s/it]Epoch: 218 done with learning rate 7.46E-03, Train loss: -2.76E+05, Train scatter: [0.1712 0.0508 0.2687 0.423 ]
L1 regularization loss: 5.56E+00, L2 regularization loss: 4.80E+00
Test scatter: [0.1711 0.0503 0.2698 0.4145], Lowest was [0.1303 0.0414 0.2203 0.384 ]
Median for last 10 epochs: [0.1513 0.049  0.2465 0.4145], Epochs since improvement 6
 44%|████▍     | 219/500 [4:10:51<5:05:12, 65.17s/it]Epoch: 220 done with learning rate 7.39E-03, Train loss: -3.96E+05, Train scatter: [0.2273 0.0579 0.2543 0.4461]
L1 regularization loss: 5.54E+00, L2 regularization loss: 4.83E+00
Test scatter: [0.2259 0.0574 0.2575 0.4434], Lowest was [0.1303 0.0414 0.2203 0.384 ]
Median for last 10 epochs: [0.1711 0.049  0.2465 0.4286], Epochs since improvement 8
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 44%|████▍     | 220/500 [4:12:18<5:34:41, 71.72s/it] 44%|████▍     | 221/500 [4:13:13<5:10:12, 66.71s/it] 44%|████▍     | 222/500 [4:14:35<5:30:51, 71.41s/it]Epoch: 222 done with learning rate 7.33E-03, Train loss: -4.09E+05, Train scatter: [0.1421 0.0453 0.2241 0.4101]
L1 regularization loss: 5.52E+00, L2 regularization loss: 4.85E+00
Test scatter: [0.144  0.0447 0.2267 0.4023], Lowest was [0.1303 0.0414 0.2203 0.384 ]
Median for last 10 epochs: [0.1711 0.049  0.2465 0.4286], Epochs since improvement 10
 45%|████▍     | 223/500 [4:15:30<5:06:24, 66.37s/it] 45%|████▍     | 224/500 [4:16:54<5:29:46, 71.69s/it]Epoch: 224 done with learning rate 7.26E-03, Train loss: -4.18E+05, Train scatter: [0.1524 0.0416 0.2201 0.4146]
L1 regularization loss: 5.50E+00, L2 regularization loss: 4.86E+00
Test scatter: [0.1528 0.0413 0.224  0.409 ], Lowest was [0.1303 0.0413 0.2203 0.384 ]
Median for last 10 epochs: [0.1528 0.049  0.2418 0.4145], Epochs since improvement 0
 45%|████▌     | 225/500 [4:17:48<5:03:58, 66.32s/it] 45%|████▌     | 226/500 [4:19:13<5:28:31, 71.94s/it]Epoch: 226 done with learning rate 7.20E-03, Train loss: -4.16E+05, Train scatter: [0.1632 0.0463 0.2204 0.4142]
L1 regularization loss: 5.48E+00, L2 regularization loss: 4.87E+00
Test scatter: [0.1607 0.0457 0.2228 0.4041], Lowest was [0.1303 0.0413 0.2203 0.384 ]
Median for last 10 epochs: [0.1607 0.0457 0.2267 0.409 ], Epochs since improvement 2
 45%|████▌     | 227/500 [4:20:10<5:07:17, 67.54s/it] 46%|████▌     | 228/500 [4:21:32<5:25:41, 71.85s/it]Epoch: 228 done with learning rate 7.13E-03, Train loss: -3.06E+05, Train scatter: [0.1597 0.0527 0.5268 0.442 ]
L1 regularization loss: 5.74E+00, L2 regularization loss: 5.06E+00
Test scatter: [0.1589 0.0517 0.5182 0.4348], Lowest was [0.1303 0.0413 0.2203 0.384 ]
Median for last 10 epochs: [0.1589 0.0457 0.2267 0.409 ], Epochs since improvement 4
 46%|████▌     | 229/500 [4:22:26<5:00:15, 66.48s/it]Epoch: 230 done with learning rate 7.06E-03, Train loss: -3.58E+05, Train scatter: [0.2638 0.049  0.2615 0.4219]
L1 regularization loss: 5.71E+00, L2 regularization loss: 5.09E+00
Test scatter: [0.257  0.0484 0.2619 0.4131], Lowest was [0.1303 0.0413 0.2203 0.384 ]
Median for last 10 epochs: [0.1589 0.0457 0.2267 0.409 ], Epochs since improvement 6
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 46%|████▌     | 230/500 [4:23:54<5:27:57, 72.88s/it] 46%|████▌     | 231/500 [4:24:49<5:02:44, 67.53s/it] 46%|████▋     | 232/500 [4:26:10<5:19:25, 71.51s/it]Epoch: 232 done with learning rate 6.99E-03, Train loss: -3.78E+05, Train scatter: [0.1746 0.0464 0.2454 0.4153]
L1 regularization loss: 5.73E+00, L2 regularization loss: 5.12E+00
Test scatter: [0.172  0.0456 0.2487 0.4071], Lowest was [0.1303 0.0413 0.2203 0.384 ]
Median for last 10 epochs: [0.1607 0.0457 0.2487 0.409 ], Epochs since improvement 8
 47%|████▋     | 233/500 [4:27:03<4:53:35, 65.98s/it] 47%|████▋     | 234/500 [4:28:24<5:12:31, 70.50s/it]Epoch: 234 done with learning rate 6.93E-03, Train loss: -4.06E+05, Train scatter: [0.1387 0.0427 0.2205 0.4119]
L1 regularization loss: 5.69E+00, L2 regularization loss: 5.11E+00
Test scatter: [0.1364 0.0421 0.2241 0.4023], Lowest was [0.1303 0.0413 0.2203 0.384 ]
Median for last 10 epochs: [0.1607 0.0457 0.2487 0.4071], Epochs since improvement 10
 47%|████▋     | 235/500 [4:29:17<4:48:14, 65.26s/it] 47%|████▋     | 236/500 [4:30:39<5:09:43, 70.39s/it]Epoch: 236 done with learning rate 6.86E-03, Train loss: -4.15E+05, Train scatter: [0.1306 0.0415 0.2152 0.4033]
L1 regularization loss: 5.68E+00, L2 regularization loss: 5.11E+00
Test scatter: [0.1301 0.041  0.2175 0.3968], Lowest was [0.1301 0.041  0.2175 0.384 ]
Median for last 10 epochs: [0.1589 0.0456 0.2487 0.4071], Epochs since improvement 0
 47%|████▋     | 237/500 [4:31:33<4:47:04, 65.49s/it] 48%|████▊     | 238/500 [4:32:55<5:07:06, 70.33s/it]Epoch: 238 done with learning rate 6.79E-03, Train loss: -3.29E+05, Train scatter: [0.1592 0.0471 0.2498 0.4311]
L1 regularization loss: 5.78E+00, L2 regularization loss: 5.20E+00
Test scatter: [0.1579 0.0464 0.2516 0.4246], Lowest was [0.1301 0.041  0.2175 0.384 ]
Median for last 10 epochs: [0.1579 0.0456 0.2487 0.4071], Epochs since improvement 2
 48%|████▊     | 239/500 [4:33:49<4:44:55, 65.50s/it]Epoch: 240 done with learning rate 6.72E-03, Train loss: -4.17E+05, Train scatter: [0.2543 0.0462 0.2876 0.4365]
L1 regularization loss: 5.73E+00, L2 regularization loss: 5.19E+00
Test scatter: [0.2487 0.0453 0.2885 0.4225], Lowest was [0.1301 0.041  0.2175 0.384 ]
Median for last 10 epochs: [0.1579 0.0453 0.2487 0.4071], Epochs since improvement 4
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 48%|████▊     | 240/500 [4:35:16<5:11:46, 71.95s/it] 48%|████▊     | 241/500 [4:36:11<4:47:47, 66.67s/it] 48%|████▊     | 242/500 [4:37:31<5:04:44, 70.87s/it]Epoch: 242 done with learning rate 6.65E-03, Train loss: -4.20E+05, Train scatter: [0.1467 0.0428 0.2256 0.4118]
L1 regularization loss: 5.72E+00, L2 regularization loss: 5.19E+00
Test scatter: [0.1492 0.0421 0.2281 0.4008], Lowest was [0.1301 0.041  0.2175 0.384 ]
Median for last 10 epochs: [0.1492 0.0421 0.2281 0.4023], Epochs since improvement 6
 49%|████▊     | 243/500 [4:38:26<4:42:15, 65.90s/it] 49%|████▉     | 244/500 [4:39:46<5:00:04, 70.33s/it]Epoch: 244 done with learning rate 6.58E-03, Train loss: -4.10E+05, Train scatter: [0.2558 0.0653 0.2582 0.4322]
L1 regularization loss: 5.82E+00, L2 regularization loss: 5.24E+00
Test scatter: [0.2549 0.0637 0.2592 0.4208], Lowest was [0.1301 0.041  0.2175 0.384 ]
Median for last 10 epochs: [0.1579 0.0453 0.2516 0.4208], Epochs since improvement 8
 49%|████▉     | 245/500 [4:40:40<4:37:58, 65.41s/it] 49%|████▉     | 246/500 [4:42:01<4:56:35, 70.06s/it]Epoch: 246 done with learning rate 6.51E-03, Train loss: -3.95E+05, Train scatter: [0.197  0.0477 0.2595 0.4285]
L1 regularization loss: 5.81E+00, L2 regularization loss: 5.25E+00
Test scatter: [0.196  0.0471 0.2606 0.4224], Lowest was [0.1301 0.041  0.2175 0.384 ]
Median for last 10 epochs: [0.196  0.0464 0.2592 0.4224], Epochs since improvement 10
 49%|████▉     | 247/500 [4:42:55<4:35:34, 65.36s/it] 50%|████▉     | 248/500 [4:44:16<4:53:57, 69.99s/it]Epoch: 248 done with learning rate 6.44E-03, Train loss: -4.20E+05, Train scatter: [0.122  0.0408 0.2175 0.4037]
L1 regularization loss: 5.82E+00, L2 regularization loss: 5.27E+00
Test scatter: [0.1278 0.0405 0.2199 0.3973], Lowest was [0.1278 0.0405 0.2175 0.384 ]
Median for last 10 epochs: [0.196  0.0453 0.2592 0.4208], Epochs since improvement 0
 50%|████▉     | 249/500 [4:45:10<4:32:15, 65.08s/it]Epoch: 250 done with learning rate 6.37E-03, Train loss: -4.42E+05, Train scatter: [0.1381 0.0437 0.2163 0.4054]
L1 regularization loss: 5.79E+00, L2 regularization loss: 5.26E+00
Test scatter: [0.1414 0.0436 0.2194 0.4014], Lowest was [0.1278 0.0405 0.2175 0.384 ]
Median for last 10 epochs: [0.1492 0.0436 0.2281 0.4014], Epochs since improvement 2
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 50%|█████     | 250/500 [4:46:38<4:59:28, 71.87s/it] 50%|█████     | 251/500 [4:47:32<4:36:24, 66.60s/it] 50%|█████     | 252/500 [4:48:53<4:53:36, 71.03s/it]Epoch: 252 done with learning rate 6.30E-03, Train loss: -4.46E+05, Train scatter: [0.1225 0.0416 0.2143 0.4011]
L1 regularization loss: 5.78E+00, L2 regularization loss: 5.26E+00
Test scatter: [0.1273 0.0412 0.2169 0.3948], Lowest was [0.1273 0.0405 0.2169 0.384 ]
Median for last 10 epochs: [0.1414 0.0436 0.2199 0.4014], Epochs since improvement 0
 51%|█████     | 253/500 [4:49:48<4:32:23, 66.17s/it] 51%|█████     | 254/500 [4:51:09<4:48:56, 70.47s/it]Epoch: 254 done with learning rate 6.23E-03, Train loss: -4.46E+05, Train scatter: [0.1305 0.042  0.221  0.4064]
L1 regularization loss: 5.78E+00, L2 regularization loss: 5.27E+00
Test scatter: [0.1324 0.0415 0.2237 0.402 ], Lowest was [0.1273 0.0405 0.2169 0.384 ]
Median for last 10 epochs: [0.1324 0.0415 0.2199 0.4014], Epochs since improvement 2
 51%|█████     | 255/500 [4:52:02<4:27:00, 65.39s/it] 51%|█████     | 256/500 [4:53:23<4:44:50, 70.04s/it]Epoch: 256 done with learning rate 6.15E-03, Train loss: -3.41E+05, Train scatter: [0.1444 0.0426 0.2589 0.4022]
L1 regularization loss: 5.94E+00, L2 regularization loss: 5.35E+00
Test scatter: [0.1471 0.0422 0.2594 0.3947], Lowest was [0.1273 0.0405 0.2169 0.384 ]
Median for last 10 epochs: [0.1324 0.0415 0.2199 0.3973], Epochs since improvement 4
 51%|█████▏    | 257/500 [4:54:17<4:23:49, 65.14s/it] 52%|█████▏    | 258/500 [4:55:38<4:42:34, 70.06s/it]Epoch: 258 done with learning rate 6.08E-03, Train loss: -4.41E+05, Train scatter: [0.1883 0.0443 0.2252 0.4143]
L1 regularization loss: 5.91E+00, L2 regularization loss: 5.38E+00
Test scatter: [0.1864 0.0434 0.2275 0.4075], Lowest was [0.1273 0.0405 0.2169 0.384 ]
Median for last 10 epochs: [0.1414 0.0422 0.2237 0.4014], Epochs since improvement 6
 52%|█████▏    | 259/500 [4:56:31<4:21:06, 65.01s/it]Epoch: 260 done with learning rate 6.01E-03, Train loss: -4.38E+05, Train scatter: [0.1581 0.0405 0.2257 0.4009]
L1 regularization loss: 5.90E+00, L2 regularization loss: 5.39E+00
Test scatter: [0.1601 0.0403 0.2299 0.3954], Lowest was [0.1273 0.0403 0.2169 0.384 ]
Median for last 10 epochs: [0.1471 0.0415 0.2275 0.3954], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 52%|█████▏    | 260/500 [4:58:00<4:47:53, 71.97s/it] 52%|█████▏    | 261/500 [4:58:55<4:26:28, 66.90s/it] 52%|█████▏    | 262/500 [5:00:17<4:43:51, 71.56s/it]Epoch: 262 done with learning rate 5.94E-03, Train loss: -4.52E+05, Train scatter: [0.1602 0.0422 0.2182 0.3979]
L1 regularization loss: 5.95E+00, L2 regularization loss: 5.41E+00
Test scatter: [0.1594 0.0417 0.2214 0.3907], Lowest was [0.1273 0.0403 0.2169 0.384 ]
Median for last 10 epochs: [0.1594 0.0417 0.2275 0.3954], Epochs since improvement 2
 53%|█████▎    | 263/500 [5:01:11<4:21:13, 66.13s/it] 53%|█████▎    | 264/500 [5:02:33<4:39:05, 70.96s/it]Epoch: 264 done with learning rate 5.86E-03, Train loss: -4.58E+05, Train scatter: [0.1504 0.0407 0.2306 0.3962]
L1 regularization loss: 5.93E+00, L2 regularization loss: 5.42E+00
Test scatter: [0.1475 0.0403 0.2315 0.389 ], Lowest was [0.1273 0.0403 0.2169 0.384 ]
Median for last 10 epochs: [0.1594 0.0417 0.2299 0.3947], Epochs since improvement 0
 53%|█████▎    | 265/500 [5:03:27<4:18:32, 66.01s/it] 53%|█████▎    | 266/500 [5:04:50<4:36:46, 70.97s/it]Epoch: 266 done with learning rate 5.79E-03, Train loss: -4.66E+05, Train scatter: [0.1208 0.0386 0.2084 0.3954]
L1 regularization loss: 5.98E+00, L2 regularization loss: 5.45E+00
Test scatter: [0.1278 0.0384 0.2117 0.3905], Lowest was [0.1273 0.0384 0.2117 0.384 ]
Median for last 10 epochs: [0.1594 0.0403 0.2275 0.3907], Epochs since improvement 0
 53%|█████▎    | 267/500 [5:05:44<4:15:23, 65.77s/it] 54%|█████▎    | 268/500 [5:07:05<4:32:46, 70.55s/it]Epoch: 268 done with learning rate 5.72E-03, Train loss: -4.25E+05, Train scatter: [0.1543 0.0412 0.2096 0.3946]
L1 regularization loss: 6.00E+00, L2 regularization loss: 5.47E+00
Test scatter: [0.1541 0.0407 0.2133 0.3891], Lowest was [0.1273 0.0384 0.2117 0.384 ]
Median for last 10 epochs: [0.1541 0.0403 0.2214 0.3905], Epochs since improvement 2
 54%|█████▍    | 269/500 [5:08:00<4:13:13, 65.77s/it]Epoch: 270 done with learning rate 5.64E-03, Train loss: -4.54E+05, Train scatter: [0.1648 0.0434 0.2305 0.4082]
L1 regularization loss: 6.01E+00, L2 regularization loss: 5.47E+00
Test scatter: [0.1617 0.0429 0.2328 0.4028], Lowest was [0.1273 0.0384 0.2117 0.384 ]
Median for last 10 epochs: [0.1541 0.0407 0.2214 0.3905], Epochs since improvement 4
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 54%|█████▍    | 270/500 [5:09:30<4:39:46, 72.98s/it] 54%|█████▍    | 271/500 [5:10:24<4:16:46, 67.28s/it] 54%|█████▍    | 272/500 [5:11:45<4:31:12, 71.37s/it]Epoch: 272 done with learning rate 5.57E-03, Train loss: -4.61E+05, Train scatter: [0.2482 0.047  0.2187 0.4147]
L1 regularization loss: 5.99E+00, L2 regularization loss: 5.49E+00
Test scatter: [0.245  0.0466 0.2212 0.4097], Lowest was [0.1273 0.0384 0.2117 0.384 ]
Median for last 10 epochs: [0.1541 0.0407 0.2212 0.3905], Epochs since improvement 6
 55%|█████▍    | 273/500 [5:12:38<4:09:36, 65.98s/it] 55%|█████▍    | 274/500 [5:14:00<4:27:14, 70.95s/it]Epoch: 274 done with learning rate 5.50E-03, Train loss: -4.83E+05, Train scatter: [0.1187 0.0441 0.2235 0.396 ]
L1 regularization loss: 6.01E+00, L2 regularization loss: 5.51E+00
Test scatter: [0.1173 0.0435 0.2251 0.3906], Lowest was [0.1173 0.0384 0.2117 0.384 ]
Median for last 10 epochs: [0.1541 0.0429 0.2212 0.3906], Epochs since improvement 0
 55%|█████▌    | 275/500 [5:14:54<4:06:54, 65.84s/it] 55%|█████▌    | 276/500 [5:16:16<4:23:54, 70.69s/it]Epoch: 276 done with learning rate 5.42E-03, Train loss: -4.73E+05, Train scatter: [0.1408 0.0392 0.2095 0.3902]
L1 regularization loss: 6.09E+00, L2 regularization loss: 5.54E+00
Test scatter: [0.1398 0.0386 0.2133 0.3834], Lowest was [0.1173 0.0384 0.2117 0.3834]
Median for last 10 epochs: [0.1541 0.0429 0.2212 0.3906], Epochs since improvement 0
 55%|█████▌    | 277/500 [5:17:10<4:03:56, 65.63s/it] 56%|█████▌    | 278/500 [5:18:32<4:20:21, 70.37s/it]Epoch: 278 done with learning rate 5.35E-03, Train loss: -4.66E+05, Train scatter: [0.1384 0.0391 0.2146 0.4015]
L1 regularization loss: 6.10E+00, L2 regularization loss: 5.56E+00
Test scatter: [0.1377 0.039  0.2171 0.3968], Lowest was [0.1173 0.0384 0.2117 0.3834]
Median for last 10 epochs: [0.1398 0.0429 0.2212 0.3968], Epochs since improvement 2
 56%|█████▌    | 279/500 [5:19:26<4:01:31, 65.57s/it]Epoch: 280 done with learning rate 5.28E-03, Train loss: -4.87E+05, Train scatter: [0.1179 0.0385 0.2076 0.3857]
L1 regularization loss: 6.11E+00, L2 regularization loss: 5.58E+00
Test scatter: [0.1199 0.0384 0.2108 0.3806], Lowest was [0.1173 0.0384 0.2108 0.3806]
Median for last 10 epochs: [0.1377 0.039  0.2171 0.3906], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 56%|█████▌    | 280/500 [5:20:54<4:25:17, 72.35s/it] 56%|█████▌    | 281/500 [5:21:48<4:04:13, 66.91s/it] 56%|█████▋    | 282/500 [5:23:11<4:19:49, 71.51s/it]Epoch: 282 done with learning rate 5.20E-03, Train loss: -4.86E+05, Train scatter: [0.1431 0.0389 0.2048 0.3919]
L1 regularization loss: 6.12E+00, L2 regularization loss: 5.59E+00
Test scatter: [0.139  0.0385 0.2079 0.3833], Lowest was [0.1173 0.0384 0.2079 0.3806]
Median for last 10 epochs: [0.1377 0.0386 0.2133 0.3834], Epochs since improvement 0
 57%|█████▋    | 283/500 [5:24:04<3:59:23, 66.19s/it] 57%|█████▋    | 284/500 [5:25:26<4:14:45, 70.76s/it]Epoch: 284 done with learning rate 5.13E-03, Train loss: -4.81E+05, Train scatter: [0.1472 0.0415 0.2102 0.4101]
L1 regularization loss: 6.15E+00, L2 regularization loss: 5.61E+00
Test scatter: [0.1439 0.041  0.2124 0.4025], Lowest was [0.1173 0.0384 0.2079 0.3806]
Median for last 10 epochs: [0.139  0.0386 0.2124 0.3834], Epochs since improvement 2
 57%|█████▋    | 285/500 [5:26:20<3:55:36, 65.75s/it] 57%|█████▋    | 286/500 [5:27:42<4:12:23, 70.76s/it]Epoch: 286 done with learning rate 5.06E-03, Train loss: -4.84E+05, Train scatter: [0.1181 0.0378 0.2134 0.3883]
L1 regularization loss: 6.18E+00, L2 regularization loss: 5.63E+00
Test scatter: [0.12   0.0373 0.2152 0.3824], Lowest was [0.1173 0.0373 0.2079 0.3806]
Median for last 10 epochs: [0.1377 0.0385 0.2124 0.3833], Epochs since improvement 0
 57%|█████▋    | 287/500 [5:28:37<3:54:07, 65.95s/it] 58%|█████▊    | 288/500 [5:29:58<4:09:06, 70.50s/it]Epoch: 288 done with learning rate 4.98E-03, Train loss: -4.97E+05, Train scatter: [0.1197 0.0371 0.2032 0.3858]
L1 regularization loss: 6.31E+00, L2 regularization loss: 5.70E+00
Test scatter: [0.1183 0.0369 0.2065 0.3791], Lowest was [0.1173 0.0369 0.2065 0.3791]
Median for last 10 epochs: [0.12   0.0384 0.2108 0.3824], Epochs since improvement 0
 58%|█████▊    | 289/500 [5:30:52<3:50:37, 65.58s/it]Epoch: 290 done with learning rate 4.91E-03, Train loss: -4.85E+05, Train scatter: [0.1146 0.0399 0.2067 0.3901]
L1 regularization loss: 6.32E+00, L2 regularization loss: 5.73E+00
Test scatter: [0.1171 0.0396 0.2098 0.3818], Lowest was [0.1171 0.0369 0.2065 0.3791]
Median for last 10 epochs: [0.12   0.0385 0.2098 0.3824], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 58%|█████▊    | 290/500 [5:32:19<4:11:55, 71.98s/it] 58%|█████▊    | 291/500 [5:33:14<3:52:58, 66.88s/it] 58%|█████▊    | 292/500 [5:34:35<4:06:21, 71.06s/it]Epoch: 292 done with learning rate 4.83E-03, Train loss: -5.02E+05, Train scatter: [0.1148 0.0387 0.2077 0.3828]
L1 regularization loss: 6.33E+00, L2 regularization loss: 5.76E+00
Test scatter: [0.1176 0.0381 0.2094 0.3743], Lowest was [0.1171 0.0369 0.2065 0.3743]
Median for last 10 epochs: [0.1183 0.0381 0.2098 0.3818], Epochs since improvement 0
 59%|█████▊    | 293/500 [5:35:28<3:46:44, 65.72s/it] 59%|█████▉    | 294/500 [5:36:50<4:01:37, 70.37s/it]Epoch: 294 done with learning rate 4.76E-03, Train loss: -5.05E+05, Train scatter: [0.1178 0.0372 0.203  0.3813]
L1 regularization loss: 6.36E+00, L2 regularization loss: 5.78E+00
Test scatter: [0.118  0.0372 0.2067 0.3756], Lowest was [0.1171 0.0369 0.2065 0.3743]
Median for last 10 epochs: [0.118  0.0373 0.2094 0.3791], Epochs since improvement 2
 59%|█████▉    | 295/500 [5:37:43<3:42:56, 65.25s/it] 59%|█████▉    | 296/500 [5:39:03<3:57:26, 69.84s/it]Epoch: 296 done with learning rate 4.69E-03, Train loss: -5.12E+05, Train scatter: [0.1407 0.0372 0.2008 0.3794]
L1 regularization loss: 6.41E+00, L2 regularization loss: 5.82E+00
Test scatter: [0.1444 0.037  0.2043 0.3757], Lowest was [0.1171 0.0369 0.2043 0.3743]
Median for last 10 epochs: [0.118  0.0372 0.2067 0.3757], Epochs since improvement 0
 59%|█████▉    | 297/500 [5:39:58<3:40:25, 65.15s/it] 60%|█████▉    | 298/500 [5:41:19<3:55:43, 70.02s/it]Epoch: 298 done with learning rate 4.61E-03, Train loss: -5.06E+05, Train scatter: [0.1165 0.0379 0.207  0.3916]
L1 regularization loss: 6.48E+00, L2 regularization loss: 5.85E+00
Test scatter: [0.1177 0.0376 0.2105 0.3855], Lowest was [0.1171 0.0369 0.2043 0.3743]
Median for last 10 epochs: [0.1177 0.0376 0.2094 0.3757], Epochs since improvement 2
 60%|█████▉    | 299/500 [5:42:12<3:37:58, 65.07s/it]Epoch: 300 done with learning rate 4.54E-03, Train loss: -5.17E+05, Train scatter: [0.1097 0.0379 0.2076 0.3865]
L1 regularization loss: 6.51E+00, L2 regularization loss: 5.89E+00
Test scatter: [0.1119 0.0376 0.2103 0.3806], Lowest was [0.1119 0.0369 0.2043 0.3743]
Median for last 10 epochs: [0.1177 0.0376 0.2094 0.3757], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 60%|██████    | 300/500 [5:43:42<4:01:04, 72.32s/it] 60%|██████    | 301/500 [5:44:36<3:42:08, 66.98s/it] 60%|██████    | 302/500 [5:46:02<3:59:51, 72.68s/it]Epoch: 302 done with learning rate 4.47E-03, Train loss: -5.17E+05, Train scatter: [0.108  0.0373 0.1998 0.3776]
L1 regularization loss: 6.58E+00, L2 regularization loss: 5.93E+00
Test scatter: [0.1084 0.0371 0.2043 0.3719], Lowest was [0.1084 0.0369 0.2043 0.3719]
Median for last 10 epochs: [0.1177 0.0372 0.2067 0.3757], Epochs since improvement 0
 61%|██████    | 303/500 [5:46:57<3:41:28, 67.45s/it] 61%|██████    | 304/500 [5:48:20<3:55:09, 71.99s/it]Epoch: 304 done with learning rate 4.39E-03, Train loss: -5.13E+05, Train scatter: [0.1179 0.0367 0.1998 0.3808]
L1 regularization loss: 6.64E+00, L2 regularization loss: 5.97E+00
Test scatter: [0.1181 0.0365 0.2046 0.3754], Lowest was [0.1084 0.0365 0.2043 0.3719]
Median for last 10 epochs: [0.1177 0.0371 0.2046 0.3757], Epochs since improvement 0
 61%|██████    | 305/500 [5:49:14<3:36:09, 66.51s/it] 61%|██████    | 306/500 [5:50:35<3:49:02, 70.84s/it]Epoch: 306 done with learning rate 4.32E-03, Train loss: -5.21E+05, Train scatter: [0.1151 0.0363 0.2022 0.3797]
L1 regularization loss: 6.71E+00, L2 regularization loss: 6.02E+00
Test scatter: [0.1188 0.0362 0.2049 0.3729], Lowest was [0.1084 0.0362 0.2043 0.3719]
Median for last 10 epochs: [0.1177 0.0371 0.2049 0.3754], Epochs since improvement 0
 61%|██████▏   | 307/500 [5:51:29<3:31:38, 65.79s/it] 62%|██████▏   | 308/500 [5:52:52<3:47:30, 71.10s/it]Epoch: 308 done with learning rate 4.25E-03, Train loss: -5.23E+05, Train scatter: [0.1138 0.0366 0.2018 0.378 ]
L1 regularization loss: 6.80E+00, L2 regularization loss: 6.07E+00
Test scatter: [0.1137 0.0363 0.2056 0.3714], Lowest was [0.1084 0.0362 0.2043 0.3714]
Median for last 10 epochs: [0.1137 0.0365 0.2049 0.3729], Epochs since improvement 0
 62%|██████▏   | 309/500 [5:53:46<3:30:00, 65.97s/it]Epoch: 310 done with learning rate 4.17E-03, Train loss: -5.22E+05, Train scatter: [0.1059 0.0369 0.2025 0.3749]
L1 regularization loss: 6.86E+00, L2 regularization loss: 6.11E+00
Test scatter: [0.1075 0.0368 0.2071 0.3704], Lowest was [0.1075 0.0362 0.2043 0.3704]
Median for last 10 epochs: [0.1137 0.0365 0.2049 0.3719], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 62%|██████▏   | 310/500 [5:55:15<3:50:59, 72.95s/it] 62%|██████▏   | 311/500 [5:56:09<3:31:40, 67.20s/it] 62%|██████▏   | 312/500 [5:57:31<3:44:29, 71.65s/it]Epoch: 312 done with learning rate 4.10E-03, Train loss: -5.31E+05, Train scatter: [0.1035 0.036  0.1987 0.376 ]
L1 regularization loss: 6.93E+00, L2 regularization loss: 6.16E+00
Test scatter: [0.1059 0.0358 0.2034 0.3703], Lowest was [0.1059 0.0358 0.2034 0.3703]
Median for last 10 epochs: [0.1137 0.0363 0.2049 0.3714], Epochs since improvement 0
 63%|██████▎   | 313/500 [5:58:25<3:26:41, 66.32s/it] 63%|██████▎   | 314/500 [5:59:46<3:38:41, 70.55s/it]Epoch: 314 done with learning rate 4.03E-03, Train loss: -5.29E+05, Train scatter: [0.105  0.0369 0.1982 0.3773]
L1 regularization loss: 7.01E+00, L2 regularization loss: 6.21E+00
Test scatter: [0.1074 0.0366 0.2032 0.3721], Lowest was [0.1059 0.0358 0.2032 0.3703]
Median for last 10 epochs: [0.1075 0.0363 0.2049 0.3714], Epochs since improvement 0
 63%|██████▎   | 315/500 [6:00:39<3:21:28, 65.34s/it] 63%|██████▎   | 316/500 [6:02:02<3:36:43, 70.67s/it]Epoch: 316 done with learning rate 3.95E-03, Train loss: -5.26E+05, Train scatter: [0.1178 0.037  0.2004 0.3754]
L1 regularization loss: 7.08E+00, L2 regularization loss: 6.25E+00
Test scatter: [0.121  0.0366 0.2045 0.3691], Lowest was [0.1059 0.0358 0.2032 0.3691]
Median for last 10 epochs: [0.1075 0.0366 0.2045 0.3704], Epochs since improvement 0
 63%|██████▎   | 317/500 [6:02:55<3:19:42, 65.48s/it] 64%|██████▎   | 318/500 [6:04:17<3:33:44, 70.46s/it]Epoch: 318 done with learning rate 3.88E-03, Train loss: -5.31E+05, Train scatter: [0.1337 0.0395 0.2065 0.3853]
L1 regularization loss: 7.14E+00, L2 regularization loss: 6.30E+00
Test scatter: [0.1331 0.0389 0.2092 0.3788], Lowest was [0.1059 0.0358 0.2032 0.3691]
Median for last 10 epochs: [0.1075 0.0366 0.2045 0.3704], Epochs since improvement 2
 64%|██████▍   | 319/500 [6:05:12<3:18:01, 65.65s/it]Epoch: 320 done with learning rate 3.81E-03, Train loss: -5.38E+05, Train scatter: [0.1025 0.0357 0.2029 0.372 ]
L1 regularization loss: 7.21E+00, L2 regularization loss: 6.35E+00
Test scatter: [0.106  0.0357 0.2069 0.3686], Lowest was [0.1059 0.0357 0.2032 0.3686]
Median for last 10 epochs: [0.1074 0.0366 0.2045 0.3703], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 64%|██████▍   | 320/500 [6:06:40<3:37:06, 72.37s/it] 64%|██████▍   | 321/500 [6:07:34<3:19:55, 67.02s/it] 64%|██████▍   | 322/500 [6:08:56<3:32:04, 71.49s/it]Epoch: 322 done with learning rate 3.74E-03, Train loss: -5.41E+05, Train scatter: [0.1061 0.0359 0.1951 0.3687]
L1 regularization loss: 7.27E+00, L2 regularization loss: 6.39E+00
Test scatter: [0.1092 0.0359 0.2002 0.3649], Lowest was [0.1059 0.0357 0.2002 0.3649]
Median for last 10 epochs: [0.1092 0.0366 0.2045 0.3691], Epochs since improvement 0
 65%|██████▍   | 323/500 [6:09:50<3:14:53, 66.07s/it] 65%|██████▍   | 324/500 [6:11:12<3:27:57, 70.90s/it]Epoch: 324 done with learning rate 3.67E-03, Train loss: -5.29E+05, Train scatter: [0.102  0.0357 0.2004 0.3703]
L1 regularization loss: 7.35E+00, L2 regularization loss: 6.42E+00
Test scatter: [0.1032 0.0357 0.204  0.3654], Lowest was [0.1032 0.0357 0.2002 0.3649]
Median for last 10 epochs: [0.1092 0.0359 0.2045 0.3686], Epochs since improvement 0
 65%|██████▌   | 325/500 [6:12:05<3:11:35, 65.69s/it] 65%|██████▌   | 326/500 [6:13:25<3:22:52, 69.96s/it]Epoch: 326 done with learning rate 3.60E-03, Train loss: -5.33E+05, Train scatter: [0.116  0.0357 0.1959 0.3694]
L1 regularization loss: 7.39E+00, L2 regularization loss: 6.46E+00
Test scatter: [0.1193 0.0358 0.201  0.3666], Lowest was [0.1032 0.0357 0.2002 0.3649]
Median for last 10 epochs: [0.1092 0.0358 0.204  0.3666], Epochs since improvement 2
 65%|██████▌   | 327/500 [6:14:19<3:07:42, 65.10s/it] 66%|██████▌   | 328/500 [6:15:40<3:20:08, 69.82s/it]Epoch: 328 done with learning rate 3.53E-03, Train loss: -5.54E+05, Train scatter: [0.1071 0.0352 0.1955 0.3701]
L1 regularization loss: 7.46E+00, L2 regularization loss: 6.51E+00
Test scatter: [0.1071 0.0354 0.2005 0.3662], Lowest was [0.1032 0.0354 0.2002 0.3649]
Median for last 10 epochs: [0.1071 0.0357 0.201  0.3662], Epochs since improvement 0
 66%|██████▌   | 329/500 [6:16:34<3:05:59, 65.26s/it]Epoch: 330 done with learning rate 3.45E-03, Train loss: -5.49E+05, Train scatter: [0.11   0.0354 0.1961 0.371 ]
L1 regularization loss: 7.53E+00, L2 regularization loss: 6.56E+00
Test scatter: [0.1107 0.0357 0.2014 0.3675], Lowest was [0.1032 0.0354 0.2002 0.3649]
Median for last 10 epochs: [0.1092 0.0357 0.201  0.3662], Epochs since improvement 2
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 66%|██████▌   | 330/500 [6:18:03<3:24:43, 72.26s/it] 66%|██████▌   | 331/500 [6:18:57<3:08:24, 66.89s/it] 66%|██████▋   | 332/500 [6:20:18<3:18:47, 71.00s/it]Epoch: 332 done with learning rate 3.38E-03, Train loss: -5.52E+05, Train scatter: [0.1061 0.036  0.1987 0.3707]
L1 regularization loss: 7.61E+00, L2 regularization loss: 6.61E+00
Test scatter: [0.1078 0.0362 0.2037 0.3683], Lowest was [0.1032 0.0354 0.2002 0.3649]
Median for last 10 epochs: [0.1078 0.0357 0.2014 0.3666], Epochs since improvement 4
 67%|██████▋   | 333/500 [6:21:12<3:03:14, 65.84s/it] 67%|██████▋   | 334/500 [6:22:34<3:15:36, 70.70s/it]Epoch: 334 done with learning rate 3.32E-03, Train loss: -5.56E+05, Train scatter: [0.1033 0.0357 0.1949 0.3685]
L1 regularization loss: 7.69E+00, L2 regularization loss: 6.65E+00
Test scatter: [0.1059 0.0359 0.1999 0.3648], Lowest was [0.1032 0.0354 0.1999 0.3648]
Median for last 10 epochs: [0.1078 0.0358 0.201  0.3666], Epochs since improvement 0
 67%|██████▋   | 335/500 [6:23:28<3:00:31, 65.64s/it] 67%|██████▋   | 336/500 [6:24:49<3:12:21, 70.38s/it]Epoch: 336 done with learning rate 3.25E-03, Train loss: -5.52E+05, Train scatter: [0.1128 0.0352 0.1937 0.3707]
L1 regularization loss: 7.76E+00, L2 regularization loss: 6.70E+00
Test scatter: [0.112  0.0354 0.1985 0.3682], Lowest was [0.1032 0.0354 0.1985 0.3648]
Median for last 10 epochs: [0.1078 0.0357 0.2005 0.3675], Epochs since improvement 0
 67%|██████▋   | 337/500 [6:25:43<2:57:37, 65.38s/it] 68%|██████▊   | 338/500 [6:27:05<3:09:43, 70.27s/it]Epoch: 338 done with learning rate 3.18E-03, Train loss: -5.55E+05, Train scatter: [0.1001 0.0351 0.1936 0.3669]
L1 regularization loss: 7.84E+00, L2 regularization loss: 6.75E+00
Test scatter: [0.102  0.0352 0.1981 0.3635], Lowest was [0.102  0.0352 0.1981 0.3635]
Median for last 10 epochs: [0.1078 0.0357 0.1999 0.3675], Epochs since improvement 0
 68%|██████▊   | 339/500 [6:27:59<2:55:53, 65.55s/it]Epoch: 340 done with learning rate 3.11E-03, Train loss: -5.48E+05, Train scatter: [0.1039 0.0349 0.1998 0.3686]
L1 regularization loss: 7.94E+00, L2 regularization loss: 6.81E+00
Test scatter: [0.1061 0.0352 0.2047 0.3648], Lowest was [0.102  0.0352 0.1981 0.3635]
Median for last 10 epochs: [0.1061 0.0354 0.1999 0.3648], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 68%|██████▊   | 340/500 [6:29:28<3:13:33, 72.58s/it] 68%|██████▊   | 341/500 [6:30:21<2:57:01, 66.80s/it] 68%|██████▊   | 342/500 [6:31:42<3:06:52, 70.97s/it]Epoch: 342 done with learning rate 3.04E-03, Train loss: -5.62E+05, Train scatter: [0.0975 0.0348 0.1917 0.367 ]
L1 regularization loss: 8.03E+00, L2 regularization loss: 6.87E+00
Test scatter: [0.102  0.0352 0.198  0.365 ], Lowest was [0.102  0.0352 0.198  0.3635]
Median for last 10 epochs: [0.1059 0.0352 0.1985 0.3648], Epochs since improvement 0
 69%|██████▊   | 343/500 [6:32:36<2:52:30, 65.92s/it] 69%|██████▉   | 344/500 [6:33:58<3:03:49, 70.70s/it]Epoch: 344 done with learning rate 2.97E-03, Train loss: -5.59E+05, Train scatter: [0.107  0.0354 0.2087 0.3739]
L1 regularization loss: 8.09E+00, L2 regularization loss: 6.91E+00
Test scatter: [0.1089 0.0359 0.2128 0.3716], Lowest was [0.102  0.0352 0.198  0.3635]
Median for last 10 epochs: [0.1061 0.0352 0.1985 0.365 ], Epochs since improvement 2
 69%|██████▉   | 345/500 [6:34:53<2:50:08, 65.86s/it] 69%|██████▉   | 346/500 [6:36:15<3:01:47, 70.83s/it]Epoch: 346 done with learning rate 2.90E-03, Train loss: -5.61E+05, Train scatter: [0.0959 0.0345 0.194  0.3639]
L1 regularization loss: 8.17E+00, L2 regularization loss: 6.96E+00
Test scatter: [0.0995 0.0349 0.1995 0.3616], Lowest was [0.0995 0.0349 0.198  0.3616]
Median for last 10 epochs: [0.102  0.0352 0.1995 0.3648], Epochs since improvement 0
 69%|██████▉   | 347/500 [6:37:09<2:47:51, 65.83s/it] 70%|██████▉   | 348/500 [6:38:31<2:58:50, 70.59s/it]Epoch: 348 done with learning rate 2.84E-03, Train loss: -5.66E+05, Train scatter: [0.0968 0.0337 0.1914 0.3642]
L1 regularization loss: 8.23E+00, L2 regularization loss: 7.00E+00
Test scatter: [0.1025 0.0344 0.1975 0.3627], Lowest was [0.0995 0.0344 0.1975 0.3616]
Median for last 10 epochs: [0.1025 0.0352 0.1995 0.3648], Epochs since improvement 0
 70%|██████▉   | 349/500 [6:39:25<2:45:19, 65.69s/it]Epoch: 350 done with learning rate 2.77E-03, Train loss: -5.75E+05, Train scatter: [0.1032 0.0338 0.19   0.3622]
L1 regularization loss: 8.31E+00, L2 regularization loss: 7.04E+00
Test scatter: [0.1062 0.0342 0.1961 0.3592], Lowest was [0.0995 0.0342 0.1961 0.3592]
Median for last 10 epochs: [0.1025 0.0349 0.198  0.3627], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 70%|███████   | 350/500 [6:40:54<3:01:45, 72.71s/it] 70%|███████   | 351/500 [6:41:48<2:46:27, 67.03s/it] 70%|███████   | 352/500 [6:43:10<2:56:12, 71.43s/it]Epoch: 352 done with learning rate 2.71E-03, Train loss: -5.75E+05, Train scatter: [0.0977 0.0338 0.1976 0.3611]
L1 regularization loss: 8.38E+00, L2 regularization loss: 7.09E+00
Test scatter: [0.1029 0.0348 0.2044 0.3617], Lowest was [0.0995 0.0342 0.1961 0.3592]
Median for last 10 epochs: [0.1029 0.0348 0.1995 0.3617], Epochs since improvement 2
 71%|███████   | 353/500 [6:44:03<2:41:48, 66.04s/it] 71%|███████   | 354/500 [6:45:25<2:52:06, 70.73s/it]Epoch: 354 done with learning rate 2.64E-03, Train loss: -5.81E+05, Train scatter: [0.0963 0.0338 0.1899 0.3605]
L1 regularization loss: 8.46E+00, L2 regularization loss: 7.13E+00
Test scatter: [0.1015 0.0345 0.1974 0.362 ], Lowest was [0.0995 0.0342 0.1961 0.3592]
Median for last 10 epochs: [0.1025 0.0345 0.1975 0.3617], Epochs since improvement 4
 71%|███████   | 355/500 [6:46:20<2:39:20, 65.94s/it] 71%|███████   | 356/500 [6:47:41<2:49:34, 70.66s/it]Epoch: 356 done with learning rate 2.58E-03, Train loss: -5.43E+05, Train scatter: [0.1027 0.0345 0.1936 0.3679]
L1 regularization loss: 8.61E+00, L2 regularization loss: 7.20E+00
Test scatter: [0.1054 0.0347 0.1993 0.365 ], Lowest was [0.0995 0.0342 0.1961 0.3592]
Median for last 10 epochs: [0.1029 0.0345 0.1975 0.362 ], Epochs since improvement 6
 71%|███████▏  | 357/500 [6:48:35<2:36:19, 65.59s/it] 72%|███████▏  | 358/500 [6:49:57<2:46:40, 70.43s/it]Epoch: 358 done with learning rate 2.51E-03, Train loss: -5.72E+05, Train scatter: [0.101  0.0343 0.1917 0.3617]
L1 regularization loss: 8.65E+00, L2 regularization loss: 7.23E+00
Test scatter: [0.1044 0.0348 0.1985 0.3589], Lowest was [0.0995 0.0342 0.1961 0.3589]
Median for last 10 epochs: [0.1044 0.0347 0.1985 0.3617], Epochs since improvement 0
 72%|███████▏  | 359/500 [6:50:51<2:33:51, 65.47s/it]Epoch: 360 done with learning rate 2.45E-03, Train loss: -5.77E+05, Train scatter: [0.0995 0.0344 0.1966 0.3733]
L1 regularization loss: 8.72E+00, L2 regularization loss: 7.27E+00
Test scatter: [0.1038 0.0348 0.2022 0.3746], Lowest was [0.0995 0.0342 0.1961 0.3589]
Median for last 10 epochs: [0.1038 0.0348 0.1993 0.362 ], Epochs since improvement 2
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 72%|███████▏  | 360/500 [6:52:18<2:48:13, 72.10s/it] 72%|███████▏  | 361/500 [6:53:13<2:35:09, 66.98s/it] 72%|███████▏  | 362/500 [6:54:36<2:44:36, 71.57s/it]Epoch: 362 done with learning rate 2.38E-03, Train loss: -5.70E+05, Train scatter: [0.0993 0.034  0.1939 0.3635]
L1 regularization loss: 8.78E+00, L2 regularization loss: 7.31E+00
Test scatter: [0.1035 0.0348 0.1999 0.3599], Lowest was [0.0995 0.0342 0.1961 0.3589]
Median for last 10 epochs: [0.1038 0.0348 0.1993 0.362 ], Epochs since improvement 4
 73%|███████▎  | 363/500 [6:55:29<2:30:54, 66.09s/it] 73%|███████▎  | 364/500 [6:56:50<2:39:56, 70.56s/it]Epoch: 364 done with learning rate 2.32E-03, Train loss: -5.68E+05, Train scatter: [0.0977 0.034  0.1928 0.3652]
L1 regularization loss: 8.86E+00, L2 regularization loss: 7.35E+00
Test scatter: [0.1031 0.0347 0.1996 0.3648], Lowest was [0.0995 0.0342 0.1961 0.3589]
Median for last 10 epochs: [0.1038 0.0348 0.1996 0.3648], Epochs since improvement 6
 73%|███████▎  | 365/500 [6:57:44<2:27:43, 65.66s/it] 73%|███████▎  | 366/500 [6:59:06<2:37:18, 70.44s/it]Epoch: 366 done with learning rate 2.26E-03, Train loss: -5.60E+05, Train scatter: [0.0956 0.034  0.1881 0.3583]
L1 regularization loss: 8.91E+00, L2 regularization loss: 7.39E+00
Test scatter: [0.0998 0.0349 0.1951 0.3581], Lowest was [0.0995 0.0342 0.1951 0.3581]
Median for last 10 epochs: [0.1035 0.0348 0.1996 0.3599], Epochs since improvement 0
 73%|███████▎  | 367/500 [7:00:00<2:25:08, 65.47s/it] 74%|███████▎  | 368/500 [7:01:22<2:34:59, 70.45s/it]Epoch: 368 done with learning rate 2.20E-03, Train loss: -5.92E+05, Train scatter: [0.0937 0.0346 0.1935 0.3617]
L1 regularization loss: 8.97E+00, L2 regularization loss: 7.42E+00
Test scatter: [0.0978 0.0355 0.2009 0.3619], Lowest was [0.0978 0.0342 0.1951 0.3581]
Median for last 10 epochs: [0.1031 0.0348 0.1999 0.3619], Epochs since improvement 0
 74%|███████▍  | 369/500 [7:02:16<2:23:02, 65.51s/it]Epoch: 370 done with learning rate 2.14E-03, Train loss: -5.81E+05, Train scatter: [0.101  0.0379 0.2052 0.3794]
L1 regularization loss: 9.04E+00, L2 regularization loss: 7.46E+00
Test scatter: [0.1064 0.0383 0.2106 0.3751], Lowest was [0.0978 0.0342 0.1951 0.3581]
Median for last 10 epochs: [0.1031 0.0349 0.1999 0.3619], Epochs since improvement 2
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 74%|███████▍  | 370/500 [7:03:45<2:37:29, 72.69s/it] 74%|███████▍  | 371/500 [7:04:38<2:23:47, 66.88s/it] 74%|███████▍  | 372/500 [7:06:01<2:32:47, 71.62s/it]Epoch: 372 done with learning rate 2.08E-03, Train loss: -5.94E+05, Train scatter: [0.093  0.0332 0.188  0.3564]
L1 regularization loss: 9.11E+00, L2 regularization loss: 7.50E+00
Test scatter: [0.0978 0.0343 0.1951 0.3584], Lowest was [0.0978 0.0342 0.1951 0.3581]
Median for last 10 epochs: [0.0998 0.0349 0.1996 0.3619], Epochs since improvement 0
 75%|███████▍  | 373/500 [7:06:55<2:20:35, 66.42s/it] 75%|███████▍  | 374/500 [7:08:18<2:29:30, 71.19s/it]Epoch: 374 done with learning rate 2.02E-03, Train loss: -5.98E+05, Train scatter: [0.0935 0.0333 0.1888 0.3568]
L1 regularization loss: 9.16E+00, L2 regularization loss: 7.53E+00
Test scatter: [0.0979 0.0342 0.1955 0.3574], Lowest was [0.0978 0.0342 0.1951 0.3574]
Median for last 10 epochs: [0.0979 0.0349 0.1955 0.3584], Epochs since improvement 0
 75%|███████▌  | 375/500 [7:09:12<2:17:41, 66.09s/it] 75%|███████▌  | 376/500 [7:10:33<2:25:56, 70.62s/it]Epoch: 376 done with learning rate 1.96E-03, Train loss: -5.90E+05, Train scatter: [0.0932 0.0332 0.1899 0.3583]
L1 regularization loss: 9.21E+00, L2 regularization loss: 7.56E+00
Test scatter: [0.0987 0.0343 0.1972 0.3621], Lowest was [0.0978 0.0342 0.1951 0.3574]
Median for last 10 epochs: [0.0979 0.0343 0.1972 0.3619], Epochs since improvement 2
 75%|███████▌  | 377/500 [7:11:26<2:14:01, 65.38s/it] 76%|███████▌  | 378/500 [7:12:49<2:23:40, 70.66s/it]Epoch: 378 done with learning rate 1.90E-03, Train loss: -6.01E+05, Train scatter: [0.0948 0.0343 0.191  0.3609]
L1 regularization loss: 9.27E+00, L2 regularization loss: 7.60E+00
Test scatter: [0.0993 0.0349 0.1975 0.3605], Lowest was [0.0978 0.0342 0.1951 0.3574]
Median for last 10 epochs: [0.0987 0.0343 0.1972 0.3605], Epochs since improvement 4
 76%|███████▌  | 379/500 [7:13:44<2:12:53, 65.90s/it]Epoch: 380 done with learning rate 1.84E-03, Train loss: -6.03E+05, Train scatter: [0.0932 0.0332 0.1874 0.3549]
L1 regularization loss: 9.33E+00, L2 regularization loss: 7.63E+00
Test scatter: [0.0993 0.0344 0.1956 0.3604], Lowest was [0.0978 0.0342 0.1951 0.3574]
Median for last 10 epochs: [0.0987 0.0343 0.1956 0.3604], Epochs since improvement 6
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 76%|███████▌  | 380/500 [7:15:15<2:27:03, 73.53s/it] 76%|███████▌  | 381/500 [7:16:09<2:14:16, 67.70s/it] 76%|███████▋  | 382/500 [7:17:30<2:20:32, 71.46s/it]Epoch: 382 done with learning rate 1.78E-03, Train loss: -5.57E+05, Train scatter: [0.1043 0.0383 0.2136 0.3773]
L1 regularization loss: 9.41E+00, L2 regularization loss: 7.67E+00
Test scatter: [0.1058 0.0387 0.2185 0.3739], Lowest was [0.0978 0.0342 0.1951 0.3574]
Median for last 10 epochs: [0.0993 0.0344 0.1972 0.3605], Epochs since improvement 8
 77%|███████▋  | 383/500 [7:18:24<2:09:04, 66.19s/it] 77%|███████▋  | 384/500 [7:19:45<2:17:04, 70.90s/it]Epoch: 384 done with learning rate 1.73E-03, Train loss: -5.88E+05, Train scatter: [0.0953 0.0335 0.1868 0.3549]
L1 regularization loss: 9.47E+00, L2 regularization loss: 7.72E+00
Test scatter: [0.1002 0.0344 0.1948 0.3553], Lowest was [0.0978 0.0342 0.1948 0.3553]
Median for last 10 epochs: [0.0993 0.0344 0.1972 0.3605], Epochs since improvement 0
 77%|███████▋  | 385/500 [7:20:39<2:06:12, 65.85s/it] 77%|███████▋  | 386/500 [7:22:00<2:13:31, 70.27s/it]Epoch: 386 done with learning rate 1.67E-03, Train loss: -6.08E+05, Train scatter: [0.0899 0.0327 0.1853 0.3511]
L1 regularization loss: 9.52E+00, L2 regularization loss: 7.74E+00
Test scatter: [0.0963 0.0342 0.1942 0.3554], Lowest was [0.0963 0.0342 0.1942 0.3553]
Median for last 10 epochs: [0.0993 0.0344 0.1956 0.3604], Epochs since improvement 0
 77%|███████▋  | 387/500 [7:22:53<2:02:41, 65.15s/it] 78%|███████▊  | 388/500 [7:24:15<2:11:00, 70.18s/it]Epoch: 388 done with learning rate 1.62E-03, Train loss: -6.08E+05, Train scatter: [0.0936 0.034  0.1922 0.3579]
L1 regularization loss: 9.56E+00, L2 regularization loss: 7.77E+00
Test scatter: [0.0992 0.0351 0.1993 0.3593], Lowest was [0.0963 0.0342 0.1942 0.3553]
Median for last 10 epochs: [0.0993 0.0344 0.1956 0.3593], Epochs since improvement 2
 78%|███████▊  | 389/500 [7:25:09<2:00:31, 65.15s/it]Epoch: 390 done with learning rate 1.56E-03, Train loss: -6.15E+05, Train scatter: [0.0974 0.0343 0.195  0.3606]
L1 regularization loss: 9.61E+00, L2 regularization loss: 7.80E+00
Test scatter: [0.0994 0.0357 0.2018 0.362 ], Lowest was [0.0963 0.0342 0.1942 0.3553]
Median for last 10 epochs: [0.0994 0.0351 0.1993 0.3593], Epochs since improvement 4
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 78%|███████▊  | 390/500 [7:26:38<2:12:46, 72.43s/it] 78%|███████▊  | 391/500 [7:27:32<2:01:29, 66.88s/it] 78%|███████▊  | 392/500 [7:28:54<2:08:39, 71.48s/it]Epoch: 392 done with learning rate 1.51E-03, Train loss: -6.08E+05, Train scatter: [0.0937 0.0329 0.1862 0.3507]
L1 regularization loss: 9.67E+00, L2 regularization loss: 7.83E+00
Test scatter: [0.0976 0.0343 0.1942 0.3536], Lowest was [0.0963 0.0342 0.1942 0.3536]
Median for last 10 epochs: [0.0992 0.0344 0.1948 0.3554], Epochs since improvement 0
 79%|███████▊  | 393/500 [7:29:48<1:57:58, 66.16s/it] 79%|███████▉  | 394/500 [7:31:10<2:05:09, 70.84s/it]Epoch: 394 done with learning rate 1.46E-03, Train loss: -6.23E+05, Train scatter: [0.0893 0.0323 0.184  0.3488]
L1 regularization loss: 9.71E+00, L2 regularization loss: 7.86E+00
Test scatter: [0.0956 0.0341 0.1933 0.3528], Lowest was [0.0956 0.0341 0.1933 0.3528]
Median for last 10 epochs: [0.0976 0.0343 0.1942 0.3554], Epochs since improvement 0
 79%|███████▉  | 395/500 [7:32:05<1:55:34, 66.04s/it] 79%|███████▉  | 396/500 [7:33:26<2:02:24, 70.62s/it]Epoch: 396 done with learning rate 1.41E-03, Train loss: -6.14E+05, Train scatter: [0.1059 0.0362 0.2178 0.3878]
L1 regularization loss: 9.76E+00, L2 regularization loss: 7.89E+00
Test scatter: [0.1087 0.0369 0.2214 0.3867], Lowest was [0.0956 0.0341 0.1933 0.3528]
Median for last 10 epochs: [0.0992 0.0351 0.1993 0.3593], Epochs since improvement 2
 79%|███████▉  | 397/500 [7:34:21<1:53:09, 65.92s/it] 80%|███████▉  | 398/500 [7:35:43<2:00:08, 70.67s/it]Epoch: 398 done with learning rate 1.36E-03, Train loss: -5.33E+05, Train scatter: [0.09   0.0332 0.1883 0.3531]
L1 regularization loss: 9.85E+00, L2 regularization loss: 7.91E+00
Test scatter: [0.0972 0.0346 0.1962 0.3563], Lowest was [0.0956 0.0341 0.1933 0.3528]
Median for last 10 epochs: [0.0976 0.0346 0.1962 0.3563], Epochs since improvement 4
 80%|███████▉  | 399/500 [7:36:36<1:50:11, 65.46s/it]Epoch: 400 done with learning rate 1.31E-03, Train loss: -6.16E+05, Train scatter: [0.0885 0.0325 0.1852 0.3498]
L1 regularization loss: 9.87E+00, L2 regularization loss: 7.93E+00
Test scatter: [0.0957 0.0341 0.1936 0.3538], Lowest was [0.0956 0.0341 0.1933 0.3528]
Median for last 10 epochs: [0.0972 0.0343 0.1942 0.3538], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 80%|████████  | 400/500 [7:38:05<2:00:53, 72.54s/it] 80%|████████  | 401/500 [7:39:00<1:50:56, 67.23s/it] 80%|████████  | 402/500 [7:40:21<1:56:36, 71.39s/it]Epoch: 402 done with learning rate 1.26E-03, Train loss: -6.09E+05, Train scatter: [0.0889 0.0325 0.1838 0.3492]
L1 regularization loss: 9.90E+00, L2 regularization loss: 7.95E+00
Test scatter: [0.0968 0.034  0.1936 0.3542], Lowest was [0.0956 0.034  0.1933 0.3528]
Median for last 10 epochs: [0.0968 0.0341 0.1936 0.3542], Epochs since improvement 0
 81%|████████  | 403/500 [7:41:15<1:46:55, 66.13s/it] 81%|████████  | 404/500 [7:42:36<1:53:08, 70.71s/it]Epoch: 404 done with learning rate 1.21E-03, Train loss: -5.11E+05, Train scatter: [0.1039 0.0389 0.2126 0.3935]
L1 regularization loss: 1.00E+01, L2 regularization loss: 8.00E+00
Test scatter: [0.1052 0.039  0.2179 0.3861], Lowest was [0.0956 0.034  0.1933 0.3528]
Median for last 10 epochs: [0.0972 0.0346 0.1962 0.3563], Epochs since improvement 2
 81%|████████  | 405/500 [7:43:29<1:43:42, 65.50s/it] 81%|████████  | 406/500 [7:44:51<1:50:14, 70.36s/it]Epoch: 406 done with learning rate 1.16E-03, Train loss: -5.64E+05, Train scatter: [0.092  0.0342 0.1902 0.3618]
L1 regularization loss: 1.01E+01, L2 regularization loss: 8.02E+00
Test scatter: [0.0969 0.035  0.1967 0.3605], Lowest was [0.0956 0.034  0.1933 0.3528]
Median for last 10 epochs: [0.0969 0.0346 0.1962 0.3563], Epochs since improvement 4
 81%|████████▏ | 407/500 [7:45:45<1:41:21, 65.40s/it] 82%|████████▏ | 408/500 [7:47:06<1:47:33, 70.14s/it]Epoch: 408 done with learning rate 1.11E-03, Train loss: -6.01E+05, Train scatter: [0.0904 0.0331 0.187  0.3548]
L1 regularization loss: 1.01E+01, L2 regularization loss: 8.03E+00
Test scatter: [0.098  0.0342 0.1947 0.3558], Lowest was [0.0956 0.034  0.1933 0.3528]
Median for last 10 epochs: [0.0969 0.0342 0.1947 0.3558], Epochs since improvement 6
 82%|████████▏ | 409/500 [7:48:00<1:38:57, 65.24s/it]Epoch: 410 done with learning rate 1.07E-03, Train loss: -6.17E+05, Train scatter: [0.0931 0.0326 0.185  0.3502]
L1 regularization loss: 1.01E+01, L2 regularization loss: 8.04E+00
Test scatter: [0.1005 0.0342 0.1939 0.3543], Lowest was [0.0956 0.034  0.1933 0.3528]
Median for last 10 epochs: [0.098  0.0342 0.1947 0.3558], Epochs since improvement 8
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 82%|████████▏ | 410/500 [7:49:28<1:48:13, 72.15s/it] 82%|████████▏ | 411/500 [7:50:22<1:39:03, 66.78s/it] 82%|████████▏ | 412/500 [7:51:45<1:44:57, 71.57s/it]Epoch: 412 done with learning rate 1.02E-03, Train loss: -6.27E+05, Train scatter: [0.0877 0.033  0.1852 0.3516]
L1 regularization loss: 1.01E+01, L2 regularization loss: 8.05E+00
Test scatter: [0.0952 0.0347 0.1942 0.3551], Lowest was [0.0952 0.034  0.1933 0.3528]
Median for last 10 epochs: [0.098  0.0347 0.1947 0.3558], Epochs since improvement 0
 83%|████████▎ | 413/500 [7:52:38<1:35:47, 66.07s/it] 83%|████████▎ | 414/500 [7:54:01<1:41:53, 71.09s/it]Epoch: 414 done with learning rate 9.77E-04, Train loss: -6.35E+05, Train scatter: [0.0883 0.0322 0.1839 0.3478]
L1 regularization loss: 1.01E+01, L2 regularization loss: 8.07E+00
Test scatter: [0.096  0.0341 0.1932 0.3528], Lowest was [0.0952 0.034  0.1932 0.3528]
Median for last 10 epochs: [0.0969 0.0342 0.1942 0.3551], Epochs since improvement 0
 83%|████████▎ | 415/500 [7:54:55<1:33:31, 66.02s/it] 83%|████████▎ | 416/500 [7:56:17<1:38:53, 70.64s/it]Epoch: 416 done with learning rate 9.34E-04, Train loss: -6.39E+05, Train scatter: [0.0874 0.0322 0.1836 0.3457]
L1 regularization loss: 1.01E+01, L2 regularization loss: 8.08E+00
Test scatter: [0.096  0.0342 0.1936 0.3539], Lowest was [0.0952 0.034  0.1932 0.3528]
Median for last 10 epochs: [0.096  0.0342 0.1939 0.3543], Epochs since improvement 2
 83%|████████▎ | 417/500 [7:57:11<1:30:53, 65.70s/it] 84%|████████▎ | 418/500 [7:58:34<1:36:47, 70.83s/it]Epoch: 418 done with learning rate 8.91E-04, Train loss: -6.41E+05, Train scatter: [0.0876 0.0321 0.1834 0.3473]
L1 regularization loss: 1.02E+01, L2 regularization loss: 8.09E+00
Test scatter: [0.0967 0.0343 0.194  0.3572], Lowest was [0.0952 0.034  0.1932 0.3528]
Median for last 10 epochs: [0.096  0.0342 0.1939 0.3543], Epochs since improvement 4
 84%|████████▍ | 419/500 [7:59:27<1:28:27, 65.53s/it]Epoch: 420 done with learning rate 8.49E-04, Train loss: -6.47E+05, Train scatter: [0.0873 0.032  0.1828 0.3458]
L1 regularization loss: 1.02E+01, L2 regularization loss: 8.11E+00
Test scatter: [0.0961 0.0342 0.1937 0.3555], Lowest was [0.0952 0.034  0.1932 0.3528]
Median for last 10 epochs: [0.096  0.0342 0.1937 0.3551], Epochs since improvement 6
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 84%|████████▍ | 420/500 [8:00:55<1:36:25, 72.32s/it] 84%|████████▍ | 421/500 [8:01:50<1:28:12, 66.99s/it] 84%|████████▍ | 422/500 [8:03:12<1:32:51, 71.43s/it]Epoch: 422 done with learning rate 8.09E-04, Train loss: -6.38E+05, Train scatter: [0.0872 0.0322 0.1825 0.3439]
L1 regularization loss: 1.02E+01, L2 regularization loss: 8.12E+00
Test scatter: [0.0959 0.0344 0.1937 0.3534], Lowest was [0.0952 0.034  0.1932 0.3528]
Median for last 10 epochs: [0.096  0.0342 0.1937 0.3539], Epochs since improvement 8
 85%|████████▍ | 423/500 [8:04:06<1:25:04, 66.30s/it] 85%|████████▍ | 424/500 [8:05:28<1:29:57, 71.02s/it]Epoch: 424 done with learning rate 7.69E-04, Train loss: -6.51E+05, Train scatter: [0.0878 0.0323 0.1831 0.3451]
L1 regularization loss: 1.02E+01, L2 regularization loss: 8.13E+00
Test scatter: [0.0955 0.0345 0.1935 0.3535], Lowest was [0.0952 0.034  0.1932 0.3528]
Median for last 10 epochs: [0.096  0.0343 0.1937 0.3539], Epochs since improvement 10
 85%|████████▌ | 425/500 [8:06:23<1:22:39, 66.13s/it] 85%|████████▌ | 426/500 [8:07:46<1:27:52, 71.24s/it]Epoch: 426 done with learning rate 7.30E-04, Train loss: -6.52E+05, Train scatter: [0.0882 0.0321 0.1825 0.343 ]
L1 regularization loss: 1.03E+01, L2 regularization loss: 8.15E+00
Test scatter: [0.0967 0.0344 0.1931 0.3538], Lowest was [0.0952 0.034  0.1931 0.3528]
Median for last 10 epochs: [0.0961 0.0344 0.1937 0.3538], Epochs since improvement 0
 85%|████████▌ | 427/500 [8:08:39<1:20:05, 65.84s/it] 86%|████████▌ | 428/500 [8:10:03<1:25:25, 71.19s/it]Epoch: 428 done with learning rate 6.92E-04, Train loss: -6.50E+05, Train scatter: [0.0871 0.0318 0.1825 0.3426]
L1 regularization loss: 1.03E+01, L2 regularization loss: 8.16E+00
Test scatter: [0.0961 0.0342 0.1938 0.3527], Lowest was [0.0952 0.034  0.1931 0.3527]
Median for last 10 epochs: [0.0961 0.0344 0.1937 0.3535], Epochs since improvement 0
 86%|████████▌ | 429/500 [8:10:57<1:18:25, 66.28s/it]Epoch: 430 done with learning rate 6.55E-04, Train loss: -6.27E+05, Train scatter: [0.0889 0.032  0.1835 0.3449]
L1 regularization loss: 1.03E+01, L2 regularization loss: 8.17E+00
Test scatter: [0.0975 0.0343 0.1945 0.3549], Lowest was [0.0952 0.034  0.1931 0.3527]
Median for last 10 epochs: [0.0961 0.0344 0.1937 0.3535], Epochs since improvement 2
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 86%|████████▌ | 430/500 [8:12:27<1:25:19, 73.13s/it] 86%|████████▌ | 431/500 [8:13:20<1:17:25, 67.32s/it] 86%|████████▋ | 432/500 [8:14:43<1:21:23, 71.82s/it]Epoch: 432 done with learning rate 6.19E-04, Train loss: -6.20E+05, Train scatter: [0.0946 0.0327 0.1863 0.3501]
L1 regularization loss: 1.03E+01, L2 regularization loss: 8.18E+00
Test scatter: [0.0996 0.0345 0.1954 0.3555], Lowest was [0.0952 0.034  0.1931 0.3527]
Median for last 10 epochs: [0.0967 0.0344 0.1938 0.3538], Epochs since improvement 4
 87%|████████▋ | 433/500 [8:15:37<1:14:10, 66.42s/it] 87%|████████▋ | 434/500 [8:16:59<1:18:29, 71.35s/it]Epoch: 434 done with learning rate 5.84E-04, Train loss: -6.54E+05, Train scatter: [0.0891 0.0318 0.1821 0.3422]
L1 regularization loss: 1.04E+01, L2 regularization loss: 8.19E+00
Test scatter: [0.0961 0.0342 0.1925 0.3521], Lowest was [0.0952 0.034  0.1925 0.3521]
Median for last 10 epochs: [0.0967 0.0343 0.1938 0.3538], Epochs since improvement 0
 87%|████████▋ | 435/500 [8:17:53<1:11:37, 66.12s/it] 87%|████████▋ | 436/500 [8:19:14<1:15:18, 70.60s/it]Epoch: 436 done with learning rate 5.49E-04, Train loss: -6.60E+05, Train scatter: [0.0877 0.0316 0.1817 0.3414]
L1 regularization loss: 1.04E+01, L2 regularization loss: 8.20E+00
Test scatter: [0.0959 0.0342 0.1924 0.3518], Lowest was [0.0952 0.034  0.1924 0.3518]
Median for last 10 epochs: [0.0961 0.0342 0.1938 0.3527], Epochs since improvement 0
 87%|████████▋ | 437/500 [8:20:08<1:08:55, 65.64s/it] 88%|████████▊ | 438/500 [8:21:30<1:12:54, 70.55s/it]Epoch: 438 done with learning rate 5.16E-04, Train loss: -6.66E+05, Train scatter: [0.0871 0.0317 0.1814 0.3413]
L1 regularization loss: 1.04E+01, L2 regularization loss: 8.21E+00
Test scatter: [0.0951 0.0345 0.1925 0.3517], Lowest was [0.0951 0.034  0.1924 0.3517]
Median for last 10 epochs: [0.0961 0.0343 0.1925 0.3521], Epochs since improvement 0
 88%|████████▊ | 439/500 [8:22:24<1:06:41, 65.59s/it]Epoch: 440 done with learning rate 4.84E-04, Train loss: -6.67E+05, Train scatter: [0.0876 0.0316 0.1811 0.341 ]
L1 regularization loss: 1.04E+01, L2 regularization loss: 8.21E+00
Test scatter: [0.0961 0.0343 0.1923 0.3515], Lowest was [0.0951 0.034  0.1923 0.3515]
Median for last 10 epochs: [0.0961 0.0343 0.1925 0.3518], Epochs since improvement 0
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 88%|████████▊ | 440/500 [8:23:52<1:12:17, 72.30s/it] 88%|████████▊ | 441/500 [8:24:46<1:05:32, 66.65s/it] 88%|████████▊ | 442/500 [8:26:09<1:09:06, 71.50s/it]Epoch: 442 done with learning rate 4.53E-04, Train loss: -6.60E+05, Train scatter: [0.0864 0.0317 0.1818 0.3412]
L1 regularization loss: 1.04E+01, L2 regularization loss: 8.22E+00
Test scatter: [0.096  0.0343 0.193  0.3525], Lowest was [0.0951 0.034  0.1923 0.3515]
Median for last 10 epochs: [0.096  0.0343 0.1925 0.3518], Epochs since improvement 2
 89%|████████▊ | 443/500 [8:27:03<1:03:02, 66.36s/it] 89%|████████▉ | 444/500 [8:28:25<1:06:21, 71.10s/it]Epoch: 444 done with learning rate 4.23E-04, Train loss: -6.56E+05, Train scatter: [0.0864 0.0316 0.1815 0.341 ]
L1 regularization loss: 1.04E+01, L2 regularization loss: 8.23E+00
Test scatter: [0.0956 0.0344 0.1929 0.3522], Lowest was [0.0951 0.034  0.1923 0.3515]
Median for last 10 epochs: [0.0959 0.0343 0.1925 0.3518], Epochs since improvement 4
 89%|████████▉ | 445/500 [8:29:19<1:00:29, 65.98s/it] 89%|████████▉ | 446/500 [8:30:40<1:03:29, 70.55s/it]Epoch: 446 done with learning rate 3.93E-04, Train loss: -6.59E+05, Train scatter: [0.0866 0.0316 0.1818 0.3414]
L1 regularization loss: 1.04E+01, L2 regularization loss: 8.24E+00
Test scatter: [0.0966 0.0344 0.1933 0.3531], Lowest was [0.0951 0.034  0.1923 0.3515]
Median for last 10 epochs: [0.096  0.0344 0.1929 0.3522], Epochs since improvement 6
 89%|████████▉ | 447/500 [8:31:34<57:42, 65.34s/it]   90%|████████▉ | 448/500 [8:32:55<1:00:50, 70.20s/it]Epoch: 448 done with learning rate 3.65E-04, Train loss: -6.70E+05, Train scatter: [0.086  0.0315 0.1811 0.3397]
L1 regularization loss: 1.04E+01, L2 regularization loss: 8.24E+00
Test scatter: [0.0953 0.0344 0.1928 0.3516], Lowest was [0.0951 0.034  0.1923 0.3515]
Median for last 10 epochs: [0.096  0.0344 0.1929 0.3522], Epochs since improvement 8
 90%|████████▉ | 449/500 [8:33:50<55:46, 65.61s/it]  Epoch: 450 done with learning rate 3.38E-04, Train loss: -6.71E+05, Train scatter: [0.0863 0.0314 0.181  0.3402]
L1 regularization loss: 1.05E+01, L2 regularization loss: 8.25E+00
Test scatter: [0.0955 0.0344 0.1929 0.3518], Lowest was [0.0951 0.034  0.1923 0.3515]
Median for last 10 epochs: [0.0956 0.0344 0.1929 0.3522], Epochs since improvement 10
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 90%|█████████ | 450/500 [8:35:19<1:00:36, 72.73s/it] 90%|█████████ | 451/500 [8:36:14<54:51, 67.18s/it]   90%|█████████ | 452/500 [8:37:36<57:18, 71.64s/it]Epoch: 452 done with learning rate 3.12E-04, Train loss: -6.75E+05, Train scatter: [0.0857 0.0315 0.1806 0.3391]
L1 regularization loss: 1.05E+01, L2 regularization loss: 8.25E+00
Test scatter: [0.0957 0.0345 0.1925 0.3518], Lowest was [0.0951 0.034  0.1923 0.3515]
Median for last 10 epochs: [0.0956 0.0344 0.1929 0.3518], Epochs since improvement 12
 91%|█████████ | 453/500 [8:38:30<51:58, 66.36s/it] 91%|█████████ | 454/500 [8:39:52<54:27, 71.03s/it]Epoch: 454 done with learning rate 2.87E-04, Train loss: -6.76E+05, Train scatter: [0.0855 0.0315 0.1805 0.3386]
L1 regularization loss: 1.05E+01, L2 regularization loss: 8.26E+00
Test scatter: [0.0954 0.0346 0.1928 0.3522], Lowest was [0.0951 0.034  0.1923 0.3515]
Median for last 10 epochs: [0.0955 0.0344 0.1928 0.3518], Epochs since improvement 14
 91%|█████████ | 455/500 [8:40:46<49:33, 66.08s/it] 91%|█████████ | 456/500 [8:42:08<51:54, 70.77s/it]Epoch: 456 done with learning rate 2.62E-04, Train loss: -6.78E+05, Train scatter: [0.0866 0.0317 0.1813 0.3398]
L1 regularization loss: 1.05E+01, L2 regularization loss: 8.26E+00
Test scatter: [0.0958 0.0346 0.1931 0.3531], Lowest was [0.0951 0.034  0.1923 0.3515]
Median for last 10 epochs: [0.0955 0.0345 0.1928 0.3518], Epochs since improvement 16
 91%|█████████▏| 457/500 [8:43:02<47:06, 65.73s/it] 92%|█████████▏| 458/500 [8:44:24<49:30, 70.73s/it]Epoch: 458 done with learning rate 2.39E-04, Train loss: -6.78E+05, Train scatter: [0.0854 0.0315 0.1805 0.3387]
L1 regularization loss: 1.05E+01, L2 regularization loss: 8.27E+00
Test scatter: [0.0953 0.0346 0.1931 0.3517], Lowest was [0.0951 0.034  0.1923 0.3515]
Median for last 10 epochs: [0.0955 0.0346 0.1929 0.3518], Epochs since improvement 18
 92%|█████████▏| 459/500 [8:45:19<45:04, 65.98s/it]Epoch: 460 done with learning rate 2.17E-04, Train loss: -6.80E+05, Train scatter: [0.0853 0.0314 0.1804 0.338 ]
L1 regularization loss: 1.05E+01, L2 regularization loss: 8.27E+00
Test scatter: [0.0953 0.0345 0.1929 0.3522], Lowest was [0.0951 0.034  0.1923 0.3515]
Median for last 10 epochs: [0.0954 0.0346 0.1929 0.3522], Epochs since improvement 20
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
 92%|█████████▏| 460/500 [8:46:49<48:40, 73.02s/it] 92%|█████████▏| 461/500 [8:47:42<43:41, 67.22s/it] 92%|█████████▏| 461/500 [8:49:03<44:45, 68.86s/it]
Epoch: 462 done with learning rate 1.96E-04, Train loss: -6.81E+05, Train scatter: [0.0852 0.0314 0.1802 0.3383]
L1 regularization loss: 1.05E+01, L2 regularization loss: 8.28E+00
Test scatter: [0.0951 0.0346 0.1927 0.352 ], Lowest was [0.0951 0.034  0.1923 0.3515]
Median for last 10 epochs: [0.0953 0.0346 0.1929 0.3522], Epochs since improvement 22
Exited after 462 epochs due to early stopping
31743.80 seconds spent training, 63.488 seconds per epoch. Processed 1097 trees per second
/home/cj1223/work/GraphMerge/dev/eval_plot.py:26: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, ax =plt.subplots(1,2, figsize=(12,6))
[0.09514333 0.03462221 0.19271457 0.35197756]
{'epoch_exit': 461, 'scatter_m_star': 0.09514333, 'lowest_m_star': 0.095089234, 'last20_m_star': 0.09542278, 'last10_m_star': 0.09531242, 'scatter_v_disk': 0.034622207, 'lowest_v_disk': 0.034028377, 'last20_v_disk': 0.034531757, 'last10_v_disk': 0.034600236, 'scatter_m_cold': 0.19271457, 'lowest_m_cold': 0.19228835, 'last20_m_cold': 0.19288743, 'last10_m_cold': 0.19289146, 'scatter_sfr_100': 0.35197756, 'lowest_sfr_100': 0.3515227, 'last20_sfr_100': 0.35209584, 'last10_sfr_100': 0.35220343}
Finished 1/1
Exited training after None epochs
Experiment 6 done: 6 / 6
