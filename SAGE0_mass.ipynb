{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b34bc22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, pickle, time, os, random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch_geometric as tg\n",
    "from torch_geometric.loader import DataLoader\n",
    "# accelerate huggingface to GPU\n",
    "if torch.cuda.is_available():\n",
    "    from accelerate import Accelerator\n",
    "    accelerator = Accelerator()\n",
    "    device = accelerator.device\n",
    "\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc5ddb58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['medium_beforeafter',\n",
       " 'medium_redshift_50_all',\n",
       " 'test_onlyhmass_smass',\n",
       " 'medium_redshift_80_all',\n",
       " 'vlarge_all_final',\n",
       " 'test_cut',\n",
       " 'medium_all_q_diffbeforeafter',\n",
       " 'vlarge_redshift_85_all',\n",
       " 'medium_all_smass',\n",
       " 'medium_onlyhmass_smass',\n",
       " 'medium_scaleonly_smass',\n",
       " 'vlarge_redshift_50_all',\n",
       " 'test_beforeafter',\n",
       " 'vlarge_all_smass_standard',\n",
       " 'test_all_smass',\n",
       " 'vlarge_all_smass_power',\n",
       " 'medium_redshift_15_all',\n",
       " 'transformers',\n",
       " 'medium_noinfonoedge_smass',\n",
       " 'medium_onlyedge_smass',\n",
       " 'medium_all_variance',\n",
       " 'medium_redshift_1_all',\n",
       " 'medium_all_q',\n",
       " 'medium_redshift_10_all',\n",
       " 'medium_all_final',\n",
       " 'vlarge_redshift_95_all',\n",
       " 'test_variance',\n",
       " 'small_all_q_variancehalomass',\n",
       " 'medium_all_residual',\n",
       " 'medium_all',\n",
       " 'test',\n",
       " 'vlarge_all_robust_smass',\n",
       " 'test_final',\n",
       " 'vlarge_redshift_99_all',\n",
       " 'vlarge_redshift_99.99_all',\n",
       " 'vlarge_redshift_75_all',\n",
       " 'med_all_residual',\n",
       " 'medium_redshift_25_all',\n",
       " 'big_all_final',\n",
       " 'medium_redshift_0.1_all',\n",
       " 'vlarge_all_smass',\n",
       " 'one_all_q',\n",
       " 'test_redshift_scan',\n",
       " 'medium_all_finalhalo',\n",
       " 'test_dev',\n",
       " '105lim_all_smass',\n",
       " 'medium_redshift_5_all',\n",
       " 'small_all_q',\n",
       " 'vlarge_redshift_0_all',\n",
       " 'testpy',\n",
       " 'medium_noinfo_smass',\n",
       " 'medium_hmassonly_smass']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../../../../scratch/gpfs/cj1223/GraphStorage/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f70e3c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29132/1568306029.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'../../../../scratch/gpfs/cj1223/GraphStorage/{case}/data.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/juptorch/lib/python3.9/site-packages/torch/storage.py\u001b[0m in \u001b[0;36m_load_from_bytes\u001b[0;34m(b)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_load_from_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# case='medium_hmassonly_smass'\n",
    "# case='medium_scaleonly_smass'\n",
    "### this didn't work as well as I initially thought, must have done something wrong, but still better than base\n",
    "# case='medium_onlyedge_smass' \n",
    "# case='medium_noinfo_smass' \n",
    "# case='vlarge_all_smass'\n",
    "case='vlarge_all_smass_standard'\n",
    "case='vlarge_all_smass_power'\n",
    "\n",
    "\n",
    "\n",
    "data=pickle.load(open(f'../../../../scratch/gpfs/cj1223/GraphStorage/{case}/data.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed1a4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4d0e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls=[]\n",
    "for d in data:\n",
    "    ls.append(len(d.x.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014550b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log10(ls));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e0a576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, BatchNorm1d, LayerNorm\n",
    "from torch_geometric.nn import SAGEConv, global_mean_pool, norm, global_max_pool, global_add_pool\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, nlin=3):\n",
    "        super(GCN, self).__init__()\n",
    "        \n",
    "        self.conv1 = SAGEConv(data[0].num_node_features, hidden_channels) ##use meta-layer\n",
    "        ## https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.meta.MetaLayer\n",
    "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.conv4 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.conv5 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        \n",
    "        \n",
    "        # Our final linear layer will define our output\n",
    "        self.norm = LayerNorm(normalized_shape=hidden_channels) # layer_norm instead\n",
    "        self.lin1 = Linear(hidden_channels, 1)\n",
    "        \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv4(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv5(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = global_add_pool(x, batch)  ## Miles says use sumpool\n",
    "\n",
    "        x = self.lin1(self.norm(x))\n",
    "#         x=self.lin1(x)\n",
    "        return x\n",
    "    \n",
    "model = GCN(hidden_channels=64)\n",
    "next(model.parameters()).is_cuda ##check number one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86c2310",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "n_epochs=50\n",
    "n_trials=1\n",
    "batch_size=512\n",
    "split=0.8\n",
    "test_data=data[int(len(data)*split):]\n",
    "train_data=data[:int(len(data)*split)]\n",
    "l1_lambda = 0\n",
    "l2_lambda = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dde5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "trains, tests, scatter = [], [], []\n",
    "yss, preds=[],[]\n",
    "model = GCN(hidden_channels=64)\n",
    "train_loader=DataLoader(train_data, batch_size=batch_size, shuffle=1, num_workers=4)\n",
    "test_loader=DataLoader(test_data, batch_size=batch_size, shuffle=0,num_workers=4)    \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "_, _, test_loader = accelerator.prepare(model, optimizer, test_loader)\n",
    "model, optimizer, train_loader = accelerator.prepare(model, optimizer, train_loader)\n",
    "print('GPU ', next(model.parameters()).is_cuda)\n",
    "# Initialize our train function\n",
    "def train():\n",
    "    model.train()\n",
    "    \n",
    "    for data in train_loader:  \n",
    "        out = model(data.x, data.edge_index, data.batch)  \n",
    "        loss = criterion(out, data.y.view(-1,1)) \n",
    "        l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "        l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "\n",
    "\n",
    "        loss = loss + l1_lambda * l1_norm + l2_lambda * l2_norm\n",
    "#             loss.backward()\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step() \n",
    "        optimizer.zero_grad() \n",
    "    print(loss, l1_norm*l1_lambda, l2_norm*l2_lambda)\n",
    " # test function\n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    with torch.no_grad(): ##this solves it!!!\n",
    "        for dat in loader: \n",
    "            out = model(dat.x, dat.edge_index, dat.batch) \n",
    "    #         print(out)\n",
    "            correct += torch.std(out - dat.y.view(-1,1))\n",
    "    return correct/len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7519e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loader, model):\n",
    "    '''returns targets and predictions'''\n",
    "    ys, pred,xs, Mh=[],[],[], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for dat in loader: \n",
    "            out = model(dat.x, dat.edge_index, dat.batch) \n",
    "            pred.append(out.view(1,-1).cpu().detach().numpy())\n",
    "            ys.append(np.array(dat.y.cpu().numpy())) \n",
    "            u, counts = np.unique(dat.batch.cpu().numpy(), return_counts=1)\n",
    "            xs.append(np.array(torch.tensor_split(dat.x.cpu(), torch.cumsum(torch.tensor(counts[:-1]),0)), dtype=object))\n",
    "            ## compile lists\n",
    "    ys=np.hstack(ys)\n",
    "    pred=np.hstack(pred)[0]\n",
    "    xs=np.hstack(xs)\n",
    "    xn=[]\n",
    "    for x in xs:\n",
    "        x0=x.cpu().detach().numpy()\n",
    "        xn.append(x0)\n",
    "        Mh.append(x0[0][3])\n",
    "    return ys,pred,xn, Mh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154b04b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this uses about 1 GB of memory on the GPU\n",
    "\n",
    "start=time.time()\n",
    "# for epoch in range(n_epochs):\n",
    "for epoch in range(2):\n",
    "    train()\n",
    "\n",
    "#     if (epoch+1)%2==0:\n",
    "#         train_acc = test(train_loader)\n",
    "#         test_acc = test(test_loader)\n",
    "#         tr_acc.append(train_acc)\n",
    "#         te_acc.append(test_acc)\n",
    "#         print(f'Epoch: {epoch+1:03d}, Train scatter: {np.sqrt(train_acc):.4f}, Test scatter: {np.sqrt(test_acc):.4f}')\n",
    "stop=time.time()\n",
    "spent=stop-start\n",
    "print(f\"{spent:.2f} seconds spent training, {spent/n_epochs:.3f} seconds per epoch. Processed {len(data)*split*n_epochs/spent:.0f} trees per second\")\n",
    "# ys, pred=[],[]\n",
    "# def test(loader):\n",
    "#     model.eval()\n",
    "\n",
    "#     correct = 0\n",
    "#     for dat in loader: \n",
    "#         out = model(dat.x, dat.edge_index, dat.batch) \n",
    "#         pred.append(out.view(1,-1).cpu().detach().numpy())\n",
    "#         ys.append(np.array(dat.y.cpu())) \n",
    "# ys=np.hstack(ys)\n",
    "# pred=np.hstack(pred)[0]\n",
    "# scatter.append(np.std(ys-pred))\n",
    "# yss.append(ys)\n",
    "# preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880eb42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = test(train_loader, model)\n",
    "test_acc = test(test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5612a8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytr, ptr,xtr, Mhtr = train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba3237a",
   "metadata": {},
   "outputs": [],
   "source": [
    "yte, pte, xte, Mhte = test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04e53ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "hist=plt.hist2d(Mhtr,ptr-ytr, bins=100, norm=mpl.colors.LogNorm(), cmap=plt.cm.cividis);\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d45caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(yte, pte, 'ro')\n",
    "plt.title(np.std(yte-pte))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35f5c44e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], []]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_acc, te_acc=[],[]\n",
    "tests.append(te_acc)\n",
    "trains.append(tr_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "559b60b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0034, device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(test_loader) #this uses 7.5 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "787d0660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0036, device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(train_loader) #this crashes everything, estimate is that it uses \\approx 30 GB and I have no idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dbcf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(scatter), np.std(scatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a209af",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75ab5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tr_acc, label='train loss')\n",
    "plt.plot(te_acc, label='test loss')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b840398b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys, pred=[],[]  \n",
    "def test(loader):\n",
    "    model.eval()\n",
    " \n",
    "    correct = 0\n",
    "    for dat in loader: \n",
    "        out = model(dat.x, dat.edge_index, dat.batch) \n",
    "        pred.append(out.view(1,-1).cpu().detach().numpy())\n",
    "        ys.append(np.array(dat.y.cpu())) \n",
    "test(test_loader)\n",
    "# test(train_loader)\n",
    "ys=np.hstack(ys)\n",
    "pred=np.hstack(pred)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bee0e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax= plt.subplots(2, figsize=(9,12))\n",
    "bins=50\n",
    "ax[0].hist(ys, bins=bins, range=(np.percentile(np.hstack([ys,pred]), [1,99])), label='true',  histtype='step')\n",
    "ax[0].hist(pred, bins=bins, range=(np.percentile(np.hstack([ys,pred]), [1,99])), label='pred', histtype='step')\n",
    "ax[1].hist((ys-pred), bins=bins,range=(np.percentile(ys-pred, [1,99])),  histtype='step', label='residuals');\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "ax[0].set(title='Predicted/true stellar mass', xlabel='log10(M_stellar)', ylabel='N')\n",
    "ax[1].set(title=f'Residuals, scatter is {np.std(ys-pred):.3f}',xlabel='log10(M_stellar)_true-log10(M_stellar)_predicted', ylabel='N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2fb326",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax =plt.subplots(figsize=(12,9))\n",
    "ax.plot(ys,pred, 'ro')\n",
    "ax.plot([min(ys),max(ys)],[min(ys),max(ys)], 'k--', label='Perfect correspondance')\n",
    "ax.set(xlabel='true',ylabel='predicted', title='True/predicted corr')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47be80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax =plt.subplots(figsize=(12,9))\n",
    "h = ax.hist2d(ys,pred, bins=50, range=[np.percentile(ys, [1,99]),np.percentile(pred, [1,99])])\n",
    "fig.colorbar(h[3], ax=ax)\n",
    "ax.plot([min(ys),max(ys)],[min(ys),max(ys)], 'k--', label='Perfect correspondance')\n",
    "ax.set(xlabel='true',ylabel='predicted', title='True/predicted corr')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf45a39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mh=[]\n",
    "for x in xes:\n",
    "    Mh.append(x[0,3])\n",
    "plt.plot(Mh, pred, 'o', markersize=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ae3964",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8b7de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=[], []\n",
    "for d in data:\n",
    "    x.append(d.x[0,3].numpy())\n",
    "    y.append(d.y.numpy())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e412224",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'o', markersize=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fab1e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "## remember to validate that test/val/train are actually comparable\n",
    "## create toy problem (predict variance of all halo masses eg, verifying that in takes into account edges)\n",
    "## same, predict average of the difference of post-merge halo and pre merger halos\n",
    "\n",
    "## try different pools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a1492e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xte, yte, lte = [], [], []\n",
    "for d in test_data:\n",
    "    xte.append(d.x.numpy()[0,3])\n",
    "    yte.append(d.y.numpy())    \n",
    "    lte.append(np.log10(len(d.x.numpy()))   ) \n",
    "yte=np.vstack(yte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb021061",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr, ytr, ltr = [], [], []\n",
    "for d in train_data:\n",
    "    xtr.append(d.x.numpy()[0,3])\n",
    "    ytr.append(d.y.numpy())    \n",
    "    ltr.append(np.log10(len(d.x.numpy()))   ) \n",
    "ytr=np.vstack(ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca9df90",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins=20\n",
    "fig, ax = plt.subplots(3, figsize=(12,12))\n",
    "ax[0].hist(xtr, bins=bins, histtype='step', density=1, label='train')\n",
    "ax[0].hist(xte, bins=bins, histtype='step', density=1, label='test')\n",
    "ax[0].set(xlabel='Final halo mass')\n",
    "\n",
    "ax[1].hist(ytr, bins=bins, histtype='step', density=1, label='train')\n",
    "ax[1].hist(yte, bins=bins, histtype='step', density=1, label='test')\n",
    "ax[1].set(xlabel='Stellar mass')\n",
    "\n",
    "ax[2].hist(ltr, bins=bins, histtype='step', density=1, label='train')\n",
    "ax[2].hist(lte, bins=bins, histtype='step', density=1, label='test');\n",
    "ax[2].set(xlabel='log10(Tree length)')\n",
    "\n",
    "for a in ax:\n",
    "    a.set(yscale='log')\n",
    "    a.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ef4451bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c431f906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.cpu_count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
