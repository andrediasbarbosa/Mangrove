{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b34bc22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, pickle, time, os, random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch_geometric as tg\n",
    "from torch_geometric.loader import DataLoader\n",
    "# accelerate huggingface to GPU\n",
    "if torch.cuda.is_available():\n",
    "    from accelerate import Accelerator\n",
    "    accelerator = Accelerator()\n",
    "    device = accelerator.device\n",
    "\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc5ddb58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['medium_beforeafter',\n",
       " 'medium_redshift_50_all',\n",
       " 'test_onlyhmass_smass',\n",
       " 'medium_redshift_80_all',\n",
       " 'vlarge_all_final',\n",
       " 'test_cut',\n",
       " 'medium_all_q_diffbeforeafter',\n",
       " 'vlarge_redshift_85_all',\n",
       " 'medium_all_smass',\n",
       " 'medium_onlyhmass_smass',\n",
       " 'medium_scaleonly_smass',\n",
       " 'vlarge_redshift_50_all',\n",
       " 'test_beforeafter',\n",
       " 'test_all_smass',\n",
       " 'medium_redshift_15_all',\n",
       " 'transformers',\n",
       " 'medium_noinfonoedge_smass',\n",
       " 'medium_onlyedge_smass',\n",
       " 'medium_all_variance',\n",
       " 'medium_redshift_1_all',\n",
       " 'medium_all_q',\n",
       " 'medium_redshift_10_all',\n",
       " 'medium_all_final',\n",
       " 'vlarge_redshift_95_all',\n",
       " 'test_variance',\n",
       " 'small_all_q_variancehalomass',\n",
       " 'medium_all_residual',\n",
       " 'medium_all',\n",
       " 'test',\n",
       " 'vlarge_all_robust_smass',\n",
       " 'test_final',\n",
       " 'vlarge_redshift_99_all',\n",
       " 'vlarge_redshift_99.99_all',\n",
       " 'vlarge_redshift_75_all',\n",
       " 'med_all_residual',\n",
       " 'medium_redshift_25_all',\n",
       " 'medium_redshift_0.1_all',\n",
       " 'vlarge_all_smass',\n",
       " 'one_all_q',\n",
       " 'test_redshift_scan',\n",
       " 'medium_all_finalhalo',\n",
       " 'test_dev',\n",
       " '105lim_all_smass',\n",
       " 'medium_redshift_5_all',\n",
       " 'small_all_q',\n",
       " 'vlarge_redshift_0_all',\n",
       " 'testpy',\n",
       " 'medium_noinfo_smass',\n",
       " 'medium_hmassonly_smass']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir('../../../../scratch/gpfs/cj1223/GraphStorage/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f70e3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# case='medium_hmassonly_smass'\n",
    "# case='medium_scaleonly_smass'\n",
    "### this didn't work as well as I initially thought, must have done something wrong, but still better than base\n",
    "# case='medium_onlyedge_smass' \n",
    "# case='medium_noinfo_smass' \n",
    "case='vlarge_all_final'\n",
    "\n",
    "data=pickle.load(open(f'../../../../scratch/gpfs/cj1223/GraphStorage/{case}/data.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea015b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 108694)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.array([data[0].y])), len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78e0a576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, BatchNorm1d, LayerNorm\n",
    "from torch_geometric.nn import SAGEConv, global_mean_pool, norm, global_max_pool, global_add_pool\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, nlin=3):\n",
    "        super(GCN, self).__init__()\n",
    "        \n",
    "        self.conv1 = SAGEConv(data[0].num_node_features, hidden_channels) ##use meta-layer\n",
    "        ## https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.meta.MetaLayer\n",
    "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.conv4 = SAGEConv(hidden_channels, hidden_channels)\n",
    "#         self.conv5 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        \n",
    "        \n",
    "        # Our final linear layer will define our output\n",
    "        self.norm = LayerNorm(normalized_shape=hidden_channels) # layer_norm instead\n",
    "        self.lin1 = Linear(hidden_channels, 1)\n",
    "        \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv4(x, edge_index)\n",
    "        x = x.relu()\n",
    "#         x = self.conv5(x, edge_index)\n",
    "#         x = x.relu()\n",
    "        x = global_add_pool(x, batch)  ## Miles says use sumpool\n",
    "\n",
    "        x = self.lin1(self.norm(x))\n",
    "#         x=self.lin1(x)\n",
    "        return x\n",
    "    \n",
    "model = GCN(hidden_channels=32)\n",
    "next(model.parameters()).is_cuda ##check number one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d86c2310",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss()\n",
    "n_epochs=50\n",
    "n_trials=1\n",
    "batch_size=128\n",
    "split=0.8\n",
    "test_data=data[int(len(data)*split):]\n",
    "train_data=data[:int(len(data)*split)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "154b04b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU  True\n",
      "Epoch: 002, Train scatter: 0.1801, Test scatter: 0.1794\n",
      "Epoch: 004, Train scatter: 0.2049, Test scatter: 0.1930\n",
      "Epoch: 006, Train scatter: 0.1560, Test scatter: 0.1492\n",
      "Epoch: 008, Train scatter: 0.1598, Test scatter: 0.1544\n",
      "Epoch: 010, Train scatter: 0.1610, Test scatter: 0.1565\n",
      "Epoch: 012, Train scatter: 0.1583, Test scatter: 0.1509\n",
      "Epoch: 014, Train scatter: 0.1528, Test scatter: 0.1472\n",
      "Epoch: 016, Train scatter: 0.1521, Test scatter: 0.1471\n",
      "Epoch: 018, Train scatter: 0.1552, Test scatter: 0.1502\n",
      "Epoch: 020, Train scatter: 0.1531, Test scatter: 0.1498\n",
      "Epoch: 022, Train scatter: 0.1515, Test scatter: 0.1464\n",
      "Epoch: 024, Train scatter: 0.1510, Test scatter: 0.1464\n",
      "Epoch: 026, Train scatter: 0.1546, Test scatter: 0.1496\n",
      "Epoch: 028, Train scatter: 0.1652, Test scatter: 0.1628\n",
      "Epoch: 030, Train scatter: 0.1578, Test scatter: 0.1521\n",
      "Epoch: 032, Train scatter: 0.1520, Test scatter: 0.1470\n",
      "Epoch: 034, Train scatter: 0.1497, Test scatter: 0.1454\n",
      "Epoch: 036, Train scatter: 0.1505, Test scatter: 0.1466\n",
      "Epoch: 038, Train scatter: 0.1496, Test scatter: 0.1453\n",
      "Epoch: 040, Train scatter: 0.1478, Test scatter: 0.1447\n",
      "Epoch: 042, Train scatter: 0.1485, Test scatter: 0.1456\n",
      "Epoch: 044, Train scatter: 0.1489, Test scatter: 0.1455\n",
      "Epoch: 046, Train scatter: 0.1473, Test scatter: 0.1435\n",
      "Epoch: 048, Train scatter: 0.1487, Test scatter: 0.1460\n",
      "Epoch: 050, Train scatter: 0.1473, Test scatter: 0.1451\n",
      "363.28 seconds spent training, 7.266 seconds per epoch. Processed 11968 trees per second\n"
     ]
    }
   ],
   "source": [
    "trains, tests, scatter = [], [], []\n",
    "yss, preds=[],[]\n",
    "for _ in range(n_trials):\n",
    "    model = GCN(hidden_channels=32)\n",
    "    train_loader=DataLoader(train_data, batch_size=batch_size, shuffle=1)\n",
    "    test_loader=DataLoader(test_data, batch_size=batch_size, shuffle=0)    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "    _, _, test_loader = accelerator.prepare(model, optimizer, test_loader)\n",
    "    model, optimizer, train_loader = accelerator.prepare(model, optimizer, train_loader)\n",
    "    print('GPU ', next(model.parameters()).is_cuda)\n",
    "    # Initialize our train function\n",
    "    def train():\n",
    "        model.train()\n",
    "\n",
    "        for data in train_loader:  \n",
    "            out = model(data.x, data.edge_index, data.batch)  \n",
    "            loss = criterion(out, data.y.view(-1,1)) \n",
    "#             loss.backward()\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step() \n",
    "            optimizer.zero_grad() \n",
    "     # test function\n",
    "    def test(loader):\n",
    "        model.eval()\n",
    "\n",
    "        correct = 0\n",
    "        for dat in loader: \n",
    "            out = model(dat.x, dat.edge_index, dat.batch) \n",
    "    #         print(out)\n",
    "            correct += (torch.square(out - dat.y.view(-1,1))).sum() \n",
    "        return correct / len(loader.dataset) \n",
    "    tr_acc, te_acc=[],[]\n",
    "    start=time.time()\n",
    "    for epoch in range(n_epochs):\n",
    "        train()\n",
    "\n",
    "        if (epoch+1)%2==0:\n",
    "            train_acc = test(train_loader).cpu().detach().numpy()\n",
    "            test_acc = test(test_loader).cpu().detach().numpy()\n",
    "            tr_acc.append(np.sqrt(train_acc))\n",
    "            te_acc.append(np.sqrt(test_acc))\n",
    "            print(f'Epoch: {epoch+1:03d}, Train scatter: {np.sqrt(train_acc):.4f}, Test scatter: {np.sqrt(test_acc):.4f}')\n",
    "    stop=time.time()\n",
    "    spent=stop-start\n",
    "    tests.append(te_acc)\n",
    "    trains.append(tr_acc)\n",
    "    print(f\"{spent:.2f} seconds spent training, {spent/n_epochs:.3f} seconds per epoch. Processed {len(data)*split*n_epochs/spent:.0f} trees per second\")\n",
    "    ys, pred=[],[]\n",
    "    def test(loader):\n",
    "        model.eval()\n",
    "\n",
    "        correct = 0\n",
    "        for dat in loader: \n",
    "            out = model(dat.x, dat.edge_index, dat.batch) \n",
    "            pred.append(out.view(1,-1).cpu().detach().numpy())\n",
    "            ys.append(np.array(dat.y.cpu())) \n",
    "    test(test_loader)\n",
    "    ys=np.hstack(ys)\n",
    "    pred=np.hstack(pred)[0]\n",
    "    scatter.append(np.std(ys-pred))\n",
    "    yss.append(ys)\n",
    "    preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dbcf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(scatter), np.std(scatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a209af",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75ab5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tr_acc, label='train loss')\n",
    "plt.plot(te_acc, label='test loss')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b840398b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys, pred=[],[]  \n",
    "def test(loader):\n",
    "    model.eval()\n",
    " \n",
    "    correct = 0\n",
    "    for dat in loader: \n",
    "        out = model(dat.x, dat.edge_index, dat.batch) \n",
    "        pred.append(out.view(1,-1).cpu().detach().numpy())\n",
    "        ys.append(np.array(dat.y.cpu())) \n",
    "test(test_loader)\n",
    "# test(train_loader)\n",
    "ys=np.hstack(ys)\n",
    "pred=np.hstack(pred)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bee0e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax= plt.subplots(2, figsize=(9,12))\n",
    "bins=50\n",
    "ax[0].hist(ys, bins=bins, range=(np.percentile(np.hstack([ys,pred]), [1,99])), label='true',  histtype='step')\n",
    "ax[0].hist(pred, bins=bins, range=(np.percentile(np.hstack([ys,pred]), [1,99])), label='pred', histtype='step')\n",
    "ax[1].hist((ys-pred), bins=bins,range=(np.percentile(ys-pred, [1,99])),  histtype='step', label='residuals');\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "ax[0].set(title='Predicted/true stellar mass', xlabel='log10(M_stellar)', ylabel='N')\n",
    "ax[1].set(title=f'Residuals, scatter is {np.std(ys-pred):.3f}',xlabel='log10(M_stellar)_true-log10(M_stellar)_predicted', ylabel='N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2fb326",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax =plt.subplots(figsize=(12,9))\n",
    "ax.plot(ys,pred, 'ro')\n",
    "ax.plot([min(ys),max(ys)],[min(ys),max(ys)], 'k--', label='Perfect correspondance')\n",
    "ax.set(xlabel='true',ylabel='predicted', title='True/predicted corr')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47be80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax =plt.subplots(figsize=(12,9))\n",
    "h = ax.hist2d(ys,pred, bins=50, range=[np.percentile(ys, [1,99]),np.percentile(pred, [1,99])])\n",
    "fig.colorbar(h[3], ax=ax)\n",
    "ax.plot([min(ys),max(ys)],[min(ys),max(ys)], 'k--', label='Perfect correspondance')\n",
    "ax.set(xlabel='true',ylabel='predicted', title='True/predicted corr')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf45a39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mh=[]\n",
    "for x in xes:\n",
    "    Mh.append(x[0,3])\n",
    "plt.plot(Mh, pred, 'o', markersize=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ae3964",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8b7de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y=[], []\n",
    "for d in data:\n",
    "    x.append(d.x[0,3].numpy())\n",
    "    y.append(d.y.numpy())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e412224",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'o', markersize=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fab1e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "## remember to validate that test/val/train are actually comparable\n",
    "## create toy problem (predict variance of all halo masses eg, verifying that in takes into account edges)\n",
    "## same, predict average of the difference of post-merge halo and pre merger halos\n",
    "\n",
    "## try different pools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a1492e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xte, yte, lte = [], [], []\n",
    "for d in test_data:\n",
    "    xte.append(d.x.numpy()[0,3])\n",
    "    yte.append(d.y.numpy())    \n",
    "    lte.append(np.log10(len(d.x.numpy()))   ) \n",
    "yte=np.vstack(yte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb021061",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr, ytr, ltr = [], [], []\n",
    "for d in train_data:\n",
    "    xtr.append(d.x.numpy()[0,3])\n",
    "    ytr.append(d.y.numpy())    \n",
    "    ltr.append(np.log10(len(d.x.numpy()))   ) \n",
    "ytr=np.vstack(ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca9df90",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins=20\n",
    "fig, ax = plt.subplots(3, figsize=(12,12))\n",
    "ax[0].hist(xtr, bins=bins, histtype='step', density=1, label='train')\n",
    "ax[0].hist(xte, bins=bins, histtype='step', density=1, label='test')\n",
    "ax[0].set(xlabel='Final halo mass')\n",
    "\n",
    "ax[1].hist(ytr, bins=bins, histtype='step', density=1, label='train')\n",
    "ax[1].hist(yte, bins=bins, histtype='step', density=1, label='test')\n",
    "ax[1].set(xlabel='Stellar mass')\n",
    "\n",
    "ax[2].hist(ltr, bins=bins, histtype='step', density=1, label='train')\n",
    "ax[2].hist(lte, bins=bins, histtype='step', density=1, label='test');\n",
    "ax[2].set(xlabel='log10(Tree length)')\n",
    "\n",
    "for a in ax:\n",
    "    a.set(yscale='log')\n",
    "    a.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
