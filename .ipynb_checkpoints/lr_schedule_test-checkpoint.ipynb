{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a51a1cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "769\n"
     ]
    }
   ],
   "source": [
    "import torch, os, pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, BatchNorm1d, LayerNorm\n",
    "from torch_geometric.nn import SAGEConv, global_mean_pool, norm, global_max_pool, global_add_pool\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, nlin=3):\n",
    "        super(GCN, self).__init__()\n",
    "        \n",
    "        self.conv1 = SAGEConv(43, hidden_channels) ##use meta-layer\n",
    "        ## https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.meta.MetaLayer\n",
    "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.conv4 = SAGEConv(hidden_channels, hidden_channels)\n",
    "#         self.conv5 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        \n",
    "        \n",
    "        # Our final linear layer will define our output\n",
    "        self.norm = LayerNorm(normalized_shape=hidden_channels) # layer_norm instead\n",
    "        self.lin1 = Linear(hidden_channels, 1)\n",
    "        \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv4(x, edge_index)\n",
    "        x = x.relu()\n",
    "#         x = self.conv5(x, edge_index)\n",
    "#         x = x.relu()\n",
    "        x = global_add_pool(x, batch)  ## Miles says use sumpool\n",
    "\n",
    "        x = self.lin1(self.norm(x))\n",
    "#         x=self.lin1(x)\n",
    "        return x\n",
    "    \n",
    "model = GCN(hidden_channels=32)\n",
    "next(model.parameters()).is_cuda ##check number one\n",
    "\n",
    "case='test_all_smass'\n",
    "data=pickle.load(open(f'../../../../scratch/gpfs/cj1223/GraphStorage/{case}/data.pkl', 'rb'))\n",
    "print(len(data))\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "n_epochs=100\n",
    "from torch_geometric.loader import DataLoader\n",
    "train_loader=DataLoader(data, batch_size=64, shuffle=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c942b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4d1addb",
   "metadata": {},
   "outputs": [],
   "source": [
    "warm_up=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8f21e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs=[]\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "s1 = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=2)\n",
    "s2 = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "# scheduler = torch.optim.lr_scheduler.SequentialLR(optimizer, [s1,s2], milestones=[10])\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 10, eta_min=1e-5)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(n_epochs):\n",
    "    for data in train_loader:  \n",
    "        out = model(data.x, data.edge_index, data.batch)  \n",
    "        loss = criterion(out, data.y.view(-1,1)) \n",
    "        loss.backward()\n",
    "    #         accelerator.backward(loss)\n",
    "        optimizer.step() \n",
    "        optimizer.zero_grad()\n",
    "    if epoch<warm_up:    \n",
    "        s1.step()\n",
    "#         lrs.append(s1.get_last_lr())\n",
    "    else:\n",
    "        scheduler.step()\n",
    "        s2.step()\n",
    "        \n",
    "    lrs.append(optimizer.state_dict()['param_groups'][0]['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "467c47cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class warmup_exp():\n",
    "    def __init__(self,optimizer, g_up=2, g_down=0.95, warmup=5, period=5, eta_min=1e-5):\n",
    "        self.warmup=warmup\n",
    "        self.s1 = ExponentialLR(optimizer, gamma=g_up)\n",
    "        self.s2 = ExponentialLR(optimizer, gamma=g_down)\n",
    "    def step(self, epoch):\n",
    "        if epoch<self.warmup:    \n",
    "            self.s1.step()\n",
    "        else:\n",
    "            self.s2.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b60c7f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class warmup_expcos():\n",
    "    def __init__(self,optimizer, g_up=2, g_down=0.95, warmup=5, period=5, eta_min=1e-5):\n",
    "        self.warmup=warmup\n",
    "        self.s1 = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=g_up)\n",
    "        self.s2 = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=g_down)\n",
    "        #eta_min cannot be bigger than or equal to initial learning rate\n",
    "        self.cos = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, period, eta_min=eta_min) \n",
    "    def step(self, epoch):\n",
    "        if epoch<self.warmup:    \n",
    "            self.s1.step()\n",
    "        else:\n",
    "            self.cos.step()\n",
    "            self.s2.step()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de1ec9ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 1 lr_lambdas, but got 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_14562/2342947296.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlambda1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlambda2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.95\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLambdaLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_lambda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlambda1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/juptorch/lib/python3.9/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, optimizer, lr_lambda, last_epoch, verbose)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_lambda\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m                 raise ValueError(\"Expected {} lr_lambdas, but got {}\".format(\n\u001b[0m\u001b[1;32m    201\u001b[0m                     len(optimizer.param_groups), len(lr_lambda)))\n\u001b[1;32m    202\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_lambdas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_lambda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 1 lr_lambdas, but got 2"
     ]
    }
   ],
   "source": [
    "lambda1 = lambda epoch: epoch // 30\n",
    "lambda2 = lambda epoch: 0.95 ** epoch\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda1, lambda2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "151166be",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"param 'initial_lr' is not specified in param_groups[0] when resuming an optimizer\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10060/3871370721.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.003\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarmup_exp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# sch=warmup_expcos(optimizer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_10060/1348799808.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, optimizer, g_up, g_down, warmup)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_up\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_down\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarmup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarmup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarmup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExponentialLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExponentialLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/juptorch/lib/python3.9/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, optimizer, gamma, last_epoch, verbose)\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mExponentialLR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_lr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/juptorch/lib/python3.9/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, optimizer, last_epoch, verbose)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m'initial_lr'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                     raise KeyError(\"param 'initial_lr' is not specified \"\n\u001b[0m\u001b[1;32m     40\u001b[0m                                    \"in param_groups[{}] when resuming an optimizer\".format(i))\n\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_lrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'initial_lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"param 'initial_lr' is not specified in param_groups[0] when resuming an optimizer\""
     ]
    }
   ],
   "source": [
    "lrs=[]\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "sch=warmup_exp(optimizer)\n",
    "# sch=warmup_expcos(optimizer)\n",
    "\n",
    "# scheduler = torch.optim.lr_scheduler.SequentialLR(optimizer, [s1,s2], milestones=[10])\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 10, eta_min=1e-5)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(n_epochs):\n",
    "    for data in train_loader:  \n",
    "        out = model(data.x, data.edge_index, data.batch)  \n",
    "        loss = criterion(out, data.y.view(-1,1)) \n",
    "        optimizer.step() \n",
    "        optimizer.zero_grad()\n",
    "        scheduler.step(epoch)\n",
    "    lrs.append(optimizer.state_dict()['param_groups'][0]['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674ebec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ccb24bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8192.0, 8192.0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrs[1]/lrs[0], lrs[3]/lrs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa0d581",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs=[]\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "model.train()\n",
    "for epoch in range(n_epochs):\n",
    "    for data in train_loader:  \n",
    "        out = model(data.x, data.edge_index, data.batch)  \n",
    "        loss = criterion(out, data.y.view(-1,1)) \n",
    "        loss.backward()\n",
    "    #         accelerator.backward(loss)\n",
    "        optimizer.step() \n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "c5cf99e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0011460500150447146"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.state_dict()['param_groups'][0]['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "88d9feae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(torch.optim.lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1cb4cfc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2af0fa2f8910>]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8fc10847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.5e-06"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0eab3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dev.lr_schedule import warmup_exp, warmup_expcos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85693e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_schedule(s):\n",
    "    schedule  = s\n",
    "\n",
    "    import dev.lr_schedule as lr_module\n",
    "\n",
    "    schedule_class = getattr(lr_module, schedule)\n",
    "\n",
    "    return schedule_class\n",
    "\n",
    "scheduler=get_lr_schedule('warmup_exp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84229bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "lrs=[]\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "# sch=warmup_exp(optimizer, g_up=2)\n",
    "# sch=warmup_expcos(optimizer)\n",
    "scheduler=scheduler(optimizer)\n",
    "# scheduler = torch.optim.lr_scheduler.SequentialLR(optimizer, [s1,s2], milestones=[10])\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 10, eta_min=1e-5)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(n_epochs):\n",
    "    for data in train_loader:  \n",
    "        out = model(data.x, data.edge_index, data.batch)  \n",
    "        loss = criterion(out, data.y.view(-1,1)) \n",
    "        optimizer.step() \n",
    "        optimizer.zero_grad()\n",
    "#     sch.step(epoch)\n",
    "    scheduler.step(epoch)\n",
    "\n",
    "    lrs.append(optimizer.state_dict()['param_groups'][0]['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126746e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "43cac274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79.37147732541435"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrs[1]/lrs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fef2aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ExponentialLR, CosineAnnealingLR, MultiplicativeLR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2af066d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
